Directory structure:
└── zalando-postgres-operator/
    ├── README.md
    ├── CODEOWNERS
    ├── CONTRIBUTING.md
    ├── LICENSE
    ├── MAINTAINERS
    ├── Makefile
    ├── SECURITY.md
    ├── build-ci.sh
    ├── delivery.yaml
    ├── go.mod
    ├── go.sum
    ├── mkdocs.yml
    ├── run_operator_locally.sh
    ├── .flake8
    ├── .golangci.yml
    ├── .zappr.yaml
    ├── charts/
    │   ├── postgres-operator/
    │   │   ├── Chart.yaml
    │   │   ├── index.yaml
    │   │   ├── postgres-operator-1.10.1.tgz
    │   │   ├── postgres-operator-1.11.0.tgz
    │   │   ├── postgres-operator-1.12.2.tgz
    │   │   ├── postgres-operator-1.13.0.tgz
    │   │   ├── postgres-operator-1.14.0.tgz
    │   │   ├── postgres-operator-1.9.0.tgz
    │   │   ├── values.yaml
    │   │   ├── .helmignore
    │   │   ├── crds/
    │   │   │   ├── operatorconfigurations.yaml
    │   │   │   ├── postgresqls.yaml
    │   │   │   └── postgresteams.yaml
    │   │   └── templates/
    │   │       ├── NOTES.txt
    │   │       ├── _helpers.tpl
    │   │       ├── clusterrole-postgres-pod.yaml
    │   │       ├── clusterrole.yaml
    │   │       ├── clusterrolebinding.yaml
    │   │       ├── configmap.yaml
    │   │       ├── deployment.yaml
    │   │       ├── operatorconfiguration.yaml
    │   │       ├── postgres-pod-priority-class.yaml
    │   │       ├── service.yaml
    │   │       ├── serviceaccount.yaml
    │   │       └── user-facing-clusterroles.yaml
    │   └── postgres-operator-ui/
    │       ├── Chart.yaml
    │       ├── index.yaml
    │       ├── postgres-operator-ui-1.10.1.tgz
    │       ├── postgres-operator-ui-1.11.0.tgz
    │       ├── postgres-operator-ui-1.12.2.tgz
    │       ├── postgres-operator-ui-1.13.0.tgz
    │       ├── postgres-operator-ui-1.14.0.tgz
    │       ├── postgres-operator-ui-1.9.0.tgz
    │       ├── values.yaml
    │       ├── .helmignore
    │       └── templates/
    │           ├── NOTES.txt
    │           ├── _helpers.tpl
    │           ├── clusterrole.yaml
    │           ├── clusterrolebinding.yaml
    │           ├── deployment.yaml
    │           ├── ingress.yaml
    │           ├── service.yaml
    │           └── serviceaccount.yaml
    ├── cmd/
    │   └── main.go
    ├── docker/
    │   ├── DebugDockerfile
    │   ├── Dockerfile
    │   └── build_operator.sh
    ├── docs/
    │   ├── administrator.md
    │   ├── developer.md
    │   ├── index.md
    │   ├── operator-ui.md
    │   ├── quickstart.md
    │   ├── user.md
    │   ├── diagrams/
    │   │   ├── Makefile
    │   │   ├── neutral_operator.excalidraw
    │   │   ├── operator.tex
    │   │   └── pod.tex
    │   └── reference/
    │       ├── cluster_manifest.md
    │       ├── command_line_and_environment.md
    │       └── operator_parameters.md
    ├── e2e/
    │   ├── README.md
    │   ├── Dockerfile
    │   ├── Makefile
    │   ├── exec.sh
    │   ├── exec_into_env.sh
    │   ├── kind-cluster-postgres-operator-e2e-tests.yaml
    │   ├── requirements.txt
    │   ├── run.sh
    │   ├── scripts/
    │   │   ├── cleanup.sh
    │   │   ├── get_logs.sh
    │   │   └── watch_objects.sh
    │   └── tests/
    │       ├── __init__.py
    │       ├── k8s_api.py
    │       └── test_e2e.py
    ├── hack/
    │   ├── custom-boilerplate.go.txt
    │   ├── tools.go
    │   ├── update-codegen.sh
    │   └── verify-codegen.sh
    ├── kubectl-pg/
    │   ├── README.md
    │   ├── build.sh
    │   ├── go.mod
    │   ├── go.sum
    │   ├── main.go
    │   └── cmd/
    │       ├── addDb.go
    │       ├── addUser.go
    │       ├── check.go
    │       ├── connect.go
    │       ├── create.go
    │       ├── delete.go
    │       ├── extVolume.go
    │       ├── list.go
    │       ├── logs.go
    │       ├── root.go
    │       ├── scale.go
    │       ├── update.go
    │       ├── util.go
    │       └── version.go
    ├── logical-backup/
    │   ├── Dockerfile
    │   └── dump.sh
    ├── manifests/
    │   ├── api-service.yaml
    │   ├── complete-postgres-manifest.yaml
    │   ├── configmap.yaml
    │   ├── custom-team-membership.yaml
    │   ├── e2e-storage-class.yaml
    │   ├── fake-teams-api.yaml
    │   ├── fes.crd.yaml
    │   ├── infrastructure-roles-configmap.yaml
    │   ├── infrastructure-roles-new.yaml
    │   ├── infrastructure-roles.yaml
    │   ├── kustomization.yaml
    │   ├── minimal-fake-pooler-deployment.yaml
    │   ├── minimal-master-replica-svcmonitor.yaml
    │   ├── minimal-postgres-lowest-version-manifest.yaml
    │   ├── minimal-postgres-manifest.yaml
    │   ├── operator-service-account-rbac-openshift.yaml
    │   ├── operator-service-account-rbac.yaml
    │   ├── operatorconfiguration.crd.yaml
    │   ├── platform-credentials.yaml
    │   ├── postgres-operator.yaml
    │   ├── postgres-pod-priority-class.yaml
    │   ├── postgresql-operator-default-configuration.yaml
    │   ├── postgresql.crd.yaml
    │   ├── postgresteam.crd.yaml
    │   ├── standby-manifest.yaml
    │   └── user-facing-clusterroles.yaml
    ├── mocks/
    │   └── mocks.go
    ├── pkg/
    │   ├── apis/
    │   │   ├── acid.zalan.do/
    │   │   │   ├── register.go
    │   │   │   └── v1/
    │   │   │       ├── const.go
    │   │   │       ├── crds.go
    │   │   │       ├── doc.go
    │   │   │       ├── marshal.go
    │   │   │       ├── operator_configuration_type.go
    │   │   │       ├── postgres_team_type.go
    │   │   │       ├── postgresql_type.go
    │   │   │       ├── register.go
    │   │   │       ├── util.go
    │   │   │       ├── util_test.go
    │   │   │       └── zz_generated.deepcopy.go
    │   │   └── zalando.org/
    │   │       ├── register.go
    │   │       └── v1/
    │   │           ├── fabriceventstream.go
    │   │           ├── register.go
    │   │           └── zz_generated.deepcopy.go
    │   ├── apiserver/
    │   │   ├── apiserver.go
    │   │   └── apiserver_test.go
    │   ├── cluster/
    │   │   ├── cluster.go
    │   │   ├── cluster_test.go
    │   │   ├── connection_pooler.go
    │   │   ├── connection_pooler_new_test.go
    │   │   ├── connection_pooler_test.go
    │   │   ├── database.go
    │   │   ├── exec.go
    │   │   ├── filesystems.go
    │   │   ├── k8sres.go
    │   │   ├── k8sres_test.go
    │   │   ├── majorversionupgrade.go
    │   │   ├── pod.go
    │   │   ├── pod_test.go
    │   │   ├── resources.go
    │   │   ├── streams.go
    │   │   ├── streams_test.go
    │   │   ├── sync.go
    │   │   ├── sync_test.go
    │   │   ├── types.go
    │   │   ├── util.go
    │   │   ├── util_test.go
    │   │   ├── volumes.go
    │   │   └── volumes_test.go
    │   ├── controller/
    │   │   ├── controller.go
    │   │   ├── logs_and_api.go
    │   │   ├── node.go
    │   │   ├── node_test.go
    │   │   ├── operator_config.go
    │   │   ├── pod.go
    │   │   ├── postgresql.go
    │   │   ├── postgresql_test.go
    │   │   ├── types.go
    │   │   ├── util.go
    │   │   └── util_test.go
    │   ├── generated/
    │   │   ├── clientset/
    │   │   │   └── versioned/
    │   │   │       ├── clientset.go
    │   │   │       ├── doc.go
    │   │   │       ├── fake/
    │   │   │       │   ├── clientset_generated.go
    │   │   │       │   ├── doc.go
    │   │   │       │   └── register.go
    │   │   │       ├── scheme/
    │   │   │       │   ├── doc.go
    │   │   │       │   └── register.go
    │   │   │       └── typed/
    │   │   │           ├── acid.zalan.do/
    │   │   │           │   └── v1/
    │   │   │           │       ├── acid.zalan.do_client.go
    │   │   │           │       ├── doc.go
    │   │   │           │       ├── generated_expansion.go
    │   │   │           │       ├── operatorconfiguration.go
    │   │   │           │       ├── postgresql.go
    │   │   │           │       ├── postgresteam.go
    │   │   │           │       └── fake/
    │   │   │           │           ├── doc.go
    │   │   │           │           ├── fake_acid.zalan.do_client.go
    │   │   │           │           ├── fake_operatorconfiguration.go
    │   │   │           │           ├── fake_postgresql.go
    │   │   │           │           └── fake_postgresteam.go
    │   │   │           └── zalando.org/
    │   │   │               └── v1/
    │   │   │                   ├── doc.go
    │   │   │                   ├── fabriceventstream.go
    │   │   │                   ├── generated_expansion.go
    │   │   │                   ├── zalando.org_client.go
    │   │   │                   └── fake/
    │   │   │                       ├── doc.go
    │   │   │                       ├── fake_fabriceventstream.go
    │   │   │                       └── fake_zalando.org_client.go
    │   │   ├── informers/
    │   │   │   └── externalversions/
    │   │   │       ├── factory.go
    │   │   │       ├── generic.go
    │   │   │       ├── acid.zalan.do/
    │   │   │       │   ├── interface.go
    │   │   │       │   └── v1/
    │   │   │       │       ├── interface.go
    │   │   │       │       ├── postgresql.go
    │   │   │       │       └── postgresteam.go
    │   │   │       ├── internalinterfaces/
    │   │   │       │   └── factory_interfaces.go
    │   │   │       └── zalando.org/
    │   │   │           ├── interface.go
    │   │   │           └── v1/
    │   │   │               ├── fabriceventstream.go
    │   │   │               └── interface.go
    │   │   └── listers/
    │   │       ├── acid.zalan.do/
    │   │       │   └── v1/
    │   │       │       ├── expansion_generated.go
    │   │       │       ├── postgresql.go
    │   │       │       └── postgresteam.go
    │   │       └── zalando.org/
    │   │           └── v1/
    │   │               ├── expansion_generated.go
    │   │               └── fabriceventstream.go
    │   ├── spec/
    │   │   ├── types.go
    │   │   └── types_test.go
    │   ├── teams/
    │   │   ├── postgres_team.go
    │   │   └── postgres_team_test.go
    │   └── util/
    │       ├── util.go
    │       ├── util_test.go
    │       ├── config/
    │       │   ├── config.go
    │       │   ├── config_test.go
    │       │   └── util.go
    │       ├── constants/
    │       │   ├── annotations.go
    │       │   ├── aws.go
    │       │   ├── kubernetes.go
    │       │   ├── pooler.go
    │       │   ├── postgresql.go
    │       │   ├── roles.go
    │       │   ├── streams.go
    │       │   └── units.go
    │       ├── filesystems/
    │       │   ├── ext234.go
    │       │   └── filesystems.go
    │       ├── httpclient/
    │       │   └── httpclient.go
    │       ├── k8sutil/
    │       │   └── k8sutil.go
    │       ├── nicediff/
    │       │   └── diff.go
    │       ├── patroni/
    │       │   ├── patroni.go
    │       │   └── patroni_test.go
    │       ├── retryutil/
    │       │   ├── retry_util.go
    │       │   └── retry_util_test.go
    │       ├── ringlog/
    │       │   └── ringlog.go
    │       ├── teams/
    │       │   ├── teams.go
    │       │   └── teams_test.go
    │       ├── users/
    │       │   └── users.go
    │       └── volumes/
    │           ├── ebs.go
    │           ├── ebs_test.go
    │           ├── volumes.go
    │           └── volumes_test.go
    ├── ui/
    │   ├── Dockerfile
    │   ├── MANIFEST.in
    │   ├── Makefile
    │   ├── requirements.txt
    │   ├── run_local.sh
    │   ├── setup.py
    │   ├── start_server.sh
    │   ├── tox.ini
    │   ├── .dockerignore
    │   ├── app/
    │   │   ├── README.rst
    │   │   ├── package.json
    │   │   ├── webpack.config.js
    │   │   ├── .eslintignore
    │   │   ├── .eslintrc.yml
    │   │   └── src/
    │   │       ├── app.js
    │   │       ├── app.tag.pug
    │   │       ├── edit.tag.pug
    │   │       ├── help-edit.tag.pug
    │   │       ├── help-general.tag.pug
    │   │       ├── logs.tag.pug
    │   │       ├── new.tag.pug
    │   │       ├── postgresql.tag.pug
    │   │       ├── postgresqls.tag.pug
    │   │       ├── prism.js
    │   │       ├── restore.tag.pug
    │   │       └── status.tag.pug
    │   ├── manifests/
    │   │   ├── deployment.yaml
    │   │   ├── ingress.yaml
    │   │   ├── kustomization.yaml
    │   │   ├── service.yaml
    │   │   └── ui-service-account-rbac.yaml
    │   └── operator_ui/
    │       ├── __init__.py
    │       ├── __main__.py
    │       ├── backoff.py
    │       ├── cluster_discovery.py
    │       ├── main.py
    │       ├── mock.py
    │       ├── spiloutils.py
    │       ├── update.py
    │       ├── utils.py
    │       ├── adapters/
    │       │   ├── __init__.py
    │       │   └── logger.py
    │       ├── static/
    │       │   ├── prism.css
    │       │   ├── prism.js
    │       │   └── styles.css
    │       └── templates/
    │           └── index.html
    └── .github/
        ├── ISSUE_TEMPLATE/
        │   └── postgres-operator-issue-template.md
        ├── PULL_REQUEST_TEMPLATE/
        │   └── postgres-operator-pull-request-template.md
        └── workflows/
            ├── publish_ghcr_image.yaml
            ├── run_e2e.yaml
            └── run_tests.yaml

================================================
File: README.md
================================================
# Postgres Operator

![Tests](https://github.com/zalando/postgres-operator/workflows/operator-tests/badge.svg)
![E2E Tests](https://github.com/zalando/postgres-operator/workflows/operator-e2e-tests/badge.svg)
[![Coverage Status](https://coveralls.io/repos/github/zalando/postgres-operator/badge.svg?branch=master)](https://coveralls.io/github/zalando/postgres-operator?branch=master)

<img src="docs/diagrams/logo.png" width="200">

The Postgres Operator delivers an easy to run highly-available [PostgreSQL](https://www.postgresql.org/)
clusters on Kubernetes (K8s) powered by [Patroni](https://github.com/zalando/patroni).
It is configured only through Postgres manifests (CRDs) to ease integration into automated CI/CD
pipelines with no access to Kubernetes API directly, promoting infrastructure as code vs manual operations.

### Operator features

* Rolling updates on Postgres cluster changes, incl. quick minor version updates
* Live volume resize without pod restarts (AWS EBS, PVC)
* Database connection pooling with PGBouncer
* Support fast in place major version upgrade. Supports global upgrade of all clusters.
* Restore and cloning Postgres clusters on AWS, GCS and Azure
* Additionally logical backups to S3 or GCS bucket can be configured
* Standby cluster from S3 or GCS WAL archive
* Configurable for non-cloud environments
* Basic credential and user management on K8s, eases application deployments
* Support for custom TLS certificates
* UI to create and edit Postgres cluster manifests
* Compatible with OpenShift

### PostgreSQL features

* Supports PostgreSQL 17, starting from 13+
* Streaming replication cluster via Patroni
* Point-In-Time-Recovery with
[pg_basebackup](https://www.postgresql.org/docs/17/app-pgbasebackup.html) /
[WAL-E](https://github.com/wal-e/wal-e) via [Spilo](https://github.com/zalando/spilo)
* Preload libraries: [bg_mon](https://github.com/CyberDem0n/bg_mon),
[pg_stat_statements](https://www.postgresql.org/docs/17/pgstatstatements.html),
[pgextwlist](https://github.com/dimitri/pgextwlist),
[pg_auth_mon](https://github.com/RafiaSabih/pg_auth_mon)
* Incl. popular Postgres extensions such as
[decoderbufs](https://github.com/debezium/postgres-decoderbufs),
[hypopg](https://github.com/HypoPG/hypopg),
[pg_cron](https://github.com/citusdata/pg_cron),
[pg_partman](https://github.com/pgpartman/pg_partman),
[pg_stat_kcache](https://github.com/powa-team/pg_stat_kcache),
[pgq](https://github.com/pgq/pgq),
[pgvector](https://github.com/pgvector/pgvector),
[plpgsql_check](https://github.com/okbob/plpgsql_check),
[postgis](https://postgis.net/),
[set_user](https://github.com/pgaudit/set_user) and
[timescaledb](https://github.com/timescale/timescaledb)

The Postgres Operator has been developed at Zalando and is being used in
production for over five years.

## Supported Postgres & K8s versions

| Release   | Postgres versions | K8s versions      | Golang  |
| :-------- | :---------------: | :---------------: | :-----: |
| v1.14.0   | 13 &rarr; 17      | 1.27+             | 1.23.4  |
| v1.13.0   | 12 &rarr; 16      | 1.27+             | 1.22.5  |
| v1.12.0   | 11 &rarr; 16      | 1.27+             | 1.22.3  |
| v1.11.0   | 11 &rarr; 16      | 1.27+             | 1.21.7  |
| v1.10.1   | 10 &rarr; 15      | 1.21+             | 1.19.8  |
| v1.9.0    | 10 &rarr; 15      | 1.21+             | 1.18.9  |

## Getting started

For a quick first impression follow the instructions of this
[tutorial](docs/quickstart.md).

## Supported setups of Postgres and Applications

![Features](docs/diagrams/neutral_operator_dark.png#gh-dark-mode-only)
![Features](docs/diagrams/neutral_operator_light.png#gh-light-mode-only)

## Documentation

There is a browser-friendly version of this documentation at
[postgres-operator.readthedocs.io](https://postgres-operator.readthedocs.io)

* [How it works](docs/index.md)
* [Installation](docs/quickstart.md#deployment-options)
* [The Postgres experience on K8s](docs/user.md)
* [The Postgres Operator UI](docs/operator-ui.md)
* [DBA options - from RBAC to backup](docs/administrator.md)
* [Build, debug and extend the operator](docs/developer.md)
* [Configuration options](docs/reference/operator_parameters.md)
* [Postgres manifest reference](docs/reference/cluster_manifest.md)
* [Command-line options and environment variables](docs/reference/command_line_and_environment.md)


================================================
File: CODEOWNERS
================================================
# global owners
*        @sdudoladov @Jan-M @FxKu @jopadi @idanovinda @hughcapet @macedigital


================================================
File: CONTRIBUTING.md
================================================
# Contributing guidelines

Wanna contribute to the Postgres Operator? Yay - here is how!

## Reporting issues

Before filing an issue, if you have a question about Postgres Operator or have
a problem using it, please read the [concepts](docs/index.md) page or use the
different guides that we provide for [users](docs/user.md),
[developers](docs/developer.md) or [admins](docs/administrator). Also double
check with the current issues on our [Issues Tracker](https://github.com/zalando/postgres-operator/issues).

## Contributing a pull request

1. Submit a comment to the relevant issue or create a new issue describing your
   proposed change.
2. Do a fork, develop and test your code changes.
3. Include documentation
4. Submit a pull request.

You'll get feedback about your pull request as soon as possible.

Happy Operator hacking ;-)


================================================
File: LICENSE
================================================
The MIT License (MIT)

Copyright (c) 2024 Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================
File: MAINTAINERS
================================================
Sergey Dudoladov <sergey.dudoladov@zalando.de>
Felix Kunde <felix.kunde@zalando.de>
Jan Mussler <jan.mussler@zalando.de>
Jociele Padilha <jociele.padilha@zalando.de>
Ida Novindasari <ida.novindasari@zalando.de>
Polina Bungina <polina.bungina@zalando.de>
Matthias Adler <matthias.adler@zalando.de>


================================================
File: Makefile
================================================
.PHONY: clean local test linux macos mocks docker push e2e

BINARY ?= postgres-operator
BUILD_FLAGS ?= -v
CGO_ENABLED ?= 0
ifeq ($(RACE),1)
	BUILD_FLAGS += -race -a
    CGO_ENABLED=1
endif

LOCAL_BUILD_FLAGS ?= $(BUILD_FLAGS)
LDFLAGS ?= -X=main.version=$(VERSION)
DOCKERDIR = docker

IMAGE ?= registry.opensource.zalan.do/acid/$(BINARY)
TAG ?= $(VERSION)
GITHEAD = $(shell git rev-parse --short HEAD)
GITURL = $(shell git config --get remote.origin.url)
GITSTATUS = $(shell git status --porcelain || echo "no changes")
SOURCES = cmd/main.go
VERSION ?= $(shell git describe --tags --always --dirty)
DIRS := cmd pkg
PKG := `go list ./... | grep -v /vendor/`

ifeq ($(DEBUG),1)
	DOCKERFILE = DebugDockerfile
	DEBUG_POSTFIX := -debug-$(shell date hhmmss)
	BUILD_FLAGS += -gcflags "-N -l"
else
	DOCKERFILE = Dockerfile
endif

ifeq ($(FRESH),1)
  DEBUG_FRESH=$(shell date +"%H-%M-%S")
endif

ifdef CDP_PULL_REQUEST_NUMBER
	CDP_TAG := -${CDP_BUILD_VERSION}
endif

ifndef GOPATH
	GOPATH := $(HOME)/go
endif

PATH := $(GOPATH)/bin:$(PATH)
SHELL := env PATH=$(PATH) $(SHELL)

default: local

clean:
	rm -rf build

local: ${SOURCES}
	hack/verify-codegen.sh
	CGO_ENABLED=${CGO_ENABLED} go build -o build/${BINARY} $(LOCAL_BUILD_FLAGS) -ldflags "$(LDFLAGS)" $^

linux: ${SOURCES}
	GOOS=linux GOARCH=amd64 CGO_ENABLED=${CGO_ENABLED} go build -o build/linux/${BINARY} ${BUILD_FLAGS} -ldflags "$(LDFLAGS)" $^

macos: ${SOURCES}
	GOOS=darwin GOARCH=amd64 CGO_ENABLED=${CGO_ENABLED} go build -o build/macos/${BINARY} ${BUILD_FLAGS} -ldflags "$(LDFLAGS)" $^

docker: ${DOCKERDIR}/${DOCKERFILE}
	echo `(env)`
	echo "Tag ${TAG}"
	echo "Version ${VERSION}"
	echo "CDP tag ${CDP_TAG}"
	echo "git describe $(shell git describe --tags --always --dirty)"
	docker build --rm -t "$(IMAGE):$(TAG)$(CDP_TAG)$(DEBUG_FRESH)$(DEBUG_POSTFIX)" -f "${DOCKERDIR}/${DOCKERFILE}" --build-arg VERSION="${VERSION}" .

indocker-race:
	docker run --rm -v "${GOPATH}":"${GOPATH}" -e GOPATH="${GOPATH}" -e RACE=1 -w ${PWD} golang:1.23.4 bash -c "make linux"

push:
	docker push "$(IMAGE):$(TAG)$(CDP_TAG)"

mocks:
	GO111MODULE=on go generate ./...

tools:
	GO111MODULE=on go get k8s.io/client-go@kubernetes-1.30.4
	GO111MODULE=on go install github.com/golang/mock/mockgen@v1.6.0
	GO111MODULE=on go mod tidy

fmt:
	@gofmt -l -w -s $(DIRS)

vet:
	@go vet $(PKG)
	@staticcheck $(PKG)

deps: tools
	GO111MODULE=on go mod vendor

test:
	hack/verify-codegen.sh
	GO111MODULE=on go test ./...

codegen:
	hack/update-codegen.sh

e2e: docker # build operator image to be tested
	cd e2e; make e2etest


================================================
File: SECURITY.md
================================================
# Security

If you have discovered a security vulnerability, please email tech-security@zalando.de.


================================================
File: build-ci.sh
================================================
#!/bin/sh
set -e -x

team_repo="$GOPATH/src/github.com/zalando/"
project_dir="$team_repo/postgres-operator"

mkdir -p "$team_repo"

ln -s "$PWD" "$project_dir"
cd "$project_dir"

make deps clean docker push


================================================
File: delivery.yaml
================================================
version: "2017-09-20"
pipeline:
    - id: build-postgres-operator
      type: script
      vm_config:
        type: linux
        size: large
        image: cdp-runtime/go
      cache:
        paths:
          - /go/pkg/mod       # pkg cache for Go modules
          - ~/.cache/go-build # Go build cache
      commands:
        - desc: Run unit tests
          cmd: |
            make deps mocks test

        - desc: Build Docker image
          cmd: |
            IS_PR_BUILD=${CDP_PULL_REQUEST_NUMBER+"true"}
            if [[ ${CDP_TARGET_BRANCH} == "master" && ${IS_PR_BUILD} != "true" ]]
            then
              IMAGE=registry-write.opensource.zalan.do/acid/postgres-operator
            else
              IMAGE=registry-write.opensource.zalan.do/acid/postgres-operator-test
            fi
            export IMAGE
            make docker push

    - id: build-operator-ui
      type: script
      vm_config:
        type: linux

      commands:
        - desc: 'Prepare environment'
          cmd: |
            apt-get update
            apt-get install -y build-essential

        - desc: 'Compile JavaScript app'
          cmd: |
            cd ui
            make appjs

        - desc: 'Build and push Docker image'
          cmd: |
            cd ui
            IS_PR_BUILD=${CDP_PULL_REQUEST_NUMBER+"true"}
            if [[ ${CDP_TARGET_BRANCH} == "master" && ${IS_PR_BUILD} != "true" ]]
            then
              IMAGE=registry-write.opensource.zalan.do/acid/postgres-operator-ui
            else
              IMAGE=registry-write.opensource.zalan.do/acid/postgres-operator-ui-test
            fi
            export IMAGE
            make docker
            make push

    - id: build-logical-backup
      type: script
      vm_config:
        type: linux

      commands:
        - desc: Build image
          cmd: |
            cd logical-backup
            export TAG=$(git describe --tags --always --dirty)
            IMAGE="registry-write.opensource.zalan.do/acid/logical-backup"
            docker build --rm -t "$IMAGE:$TAG$CDP_TAG" .
            docker push "$IMAGE:$TAG$CDP_TAG"


================================================
File: go.mod
================================================
module github.com/zalando/postgres-operator

go 1.23.4

require (
	github.com/aws/aws-sdk-go v1.53.8
	github.com/golang/mock v1.6.0
	github.com/lib/pq v1.10.9
	github.com/motomux/pretty v0.0.0-20161209205251-b2aad2c9a95d
	github.com/pkg/errors v0.9.1
	github.com/r3labs/diff v1.1.0
	github.com/sirupsen/logrus v1.9.3
	github.com/stretchr/testify v1.9.0
	golang.org/x/crypto v0.31.0
	golang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3
	gopkg.in/yaml.v2 v2.4.0
	k8s.io/api v0.30.4
	k8s.io/apiextensions-apiserver v0.25.9
	k8s.io/apimachinery v0.30.4
	k8s.io/client-go v0.30.4
	k8s.io/code-generator v0.25.9
)

require (
	github.com/Masterminds/semver v1.5.0
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/emicklei/go-restful/v3 v3.11.0 // indirect
	github.com/evanphx/json-patch v4.12.0+incompatible // indirect
	github.com/go-logr/logr v1.4.1 // indirect
	github.com/go-openapi/jsonpointer v0.19.6 // indirect
	github.com/go-openapi/jsonreference v0.20.2 // indirect
	github.com/go-openapi/swag v0.22.3 // indirect
	github.com/gogo/protobuf v1.3.2 // indirect
	github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da // indirect
	github.com/golang/protobuf v1.5.4 // indirect
	github.com/google/gnostic-models v0.6.8 // indirect
	github.com/google/go-cmp v0.6.0 // indirect
	github.com/google/gofuzz v1.2.0 // indirect
	github.com/google/uuid v1.3.0 // indirect
	github.com/gorilla/websocket v1.5.0 // indirect
	github.com/imdario/mergo v0.3.6 // indirect
	github.com/jmespath/go-jmespath v0.4.0 // indirect
	github.com/josharian/intern v1.0.0 // indirect
	github.com/json-iterator/go v1.1.12 // indirect
	github.com/kr/text v0.2.0 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/moby/spdystream v0.2.0 // indirect
	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
	github.com/modern-go/reflect2 v1.0.2 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/spf13/pflag v1.0.5 // indirect
	golang.org/x/mod v0.17.0 // indirect
	golang.org/x/net v0.25.0 // indirect
	golang.org/x/oauth2 v0.10.0 // indirect
	golang.org/x/sync v0.10.0 // indirect
	golang.org/x/sys v0.28.0 // indirect
	golang.org/x/term v0.27.0 // indirect
	golang.org/x/text v0.21.0 // indirect
	golang.org/x/time v0.3.0 // indirect
	golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d // indirect
	google.golang.org/appengine v1.6.7 // indirect
	google.golang.org/protobuf v1.33.0 // indirect
	gopkg.in/inf.v0 v0.9.1 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
	k8s.io/gengo v0.0.0-20220902162205-c0856e24416d // indirect
	k8s.io/gengo/v2 v2.0.0-20240228010128-51d4e06bde70 // indirect
	k8s.io/klog/v2 v2.120.1 // indirect
	k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340 // indirect
	k8s.io/utils v0.0.0-20230726121419-3b25d923346b // indirect
	sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect
	sigs.k8s.io/structured-merge-diff/v4 v4.4.1 // indirect
	sigs.k8s.io/yaml v1.3.0 // indirect
)


================================================
File: go.sum
================================================
github.com/Masterminds/semver v1.5.0 h1:H65muMkzWKEuNDnfl9d70GUjFniHKHRbFPGBuZ3QEww=
github.com/Masterminds/semver v1.5.0/go.mod h1:MB6lktGJrhw8PrUyiEoblNEGEQ+RzHPF078ddwwvV3Y=
github.com/armon/go-socks5 v0.0.0-20160902184237-e75332964ef5 h1:0CwZNZbxp69SHPdPJAN/hZIm0C4OItdklCFmMRWYpio=
github.com/armon/go-socks5 v0.0.0-20160902184237-e75332964ef5/go.mod h1:wHh0iHkYZB8zMSxRWpUBQtwG5a7fFgvEO+odwuTv2gs=
github.com/aws/aws-sdk-go v1.53.8 h1:eoqGb1WOHIrCFKo1d51cMcnt1ralfLFaEqRkC5Zzv8k=
github.com/aws/aws-sdk-go v1.53.8/go.mod h1:LF8svs817+Nz+DmiMQKTO3ubZ/6IaTpq3TjupRn3Eqk=
github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/emicklei/go-restful/v3 v3.11.0 h1:rAQeMHw1c7zTmncogyy8VvRZwtkmkZ4FxERmMY4rD+g=
github.com/emicklei/go-restful/v3 v3.11.0/go.mod h1:6n3XBCmQQb25CM2LCACGz8ukIrRry+4bhvbpWn3mrbc=
github.com/evanphx/json-patch v4.12.0+incompatible h1:4onqiflcdA9EOZ4RxV643DvftH5pOlLGNtQ5lPWQu84=
github.com/evanphx/json-patch v4.12.0+incompatible/go.mod h1:50XU6AFN0ol/bzJsmQLiYLvXMP4fmwYFNcr97nuDLSk=
github.com/go-logr/logr v0.2.0/go.mod h1:z6/tIYblkpsD+a4lm/fGIIU9mZ+XfAiaFtq7xTgseGU=
github.com/go-logr/logr v1.4.1 h1:pKouT5E8xu9zeFC39JXRDukb6JFQPXM5p5I91188VAQ=
github.com/go-logr/logr v1.4.1/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=
github.com/go-openapi/jsonpointer v0.19.6 h1:eCs3fxoIi3Wh6vtgmLTOjdhSpiqphQ+DaPn38N2ZdrE=
github.com/go-openapi/jsonpointer v0.19.6/go.mod h1:osyAmYz/mB/C3I+WsTTSgw1ONzaLJoLCyoi6/zppojs=
github.com/go-openapi/jsonreference v0.20.2 h1:3sVjiK66+uXK/6oQ8xgcRKcFgQ5KXa2KvnJRumpMGbE=
github.com/go-openapi/jsonreference v0.20.2/go.mod h1:Bl1zwGIM8/wsvqjsOQLJ/SH+En5Ap4rVB5KVcIDZG2k=
github.com/go-openapi/swag v0.22.3 h1:yMBqmnQ0gyZvEb/+KzuWZOXgllrXT4SADYbvDaXHv/g=
github.com/go-openapi/swag v0.22.3/go.mod h1:UzaqsxGiab7freDnrUUra0MwWfN/q7tE4j+VcZ0yl14=
github.com/go-task/slim-sprig v0.0.0-20230315185526-52ccab3ef572 h1:tfuBGBXKqDEevZMzYi5KSi8KkcZtzBcTgAUUtapy0OI=
github.com/go-task/slim-sprig v0.0.0-20230315185526-52ccab3ef572/go.mod h1:9Pwr4B2jHnOSGXyyzV8ROjYa2ojvAY6HCGYYfMoC3Ls=
github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=
github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=
github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da h1:oI5xCqsCo564l8iNU+DwB5epxmsaqB+rhGL0m5jtYqE=
github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=
github.com/golang/mock v1.6.0 h1:ErTB+efbowRARo13NNdxyJji2egdxLGQhRaY+DUumQc=
github.com/golang/mock v1.6.0/go.mod h1:p6yTPP+5HYm5mzsMV8JkE6ZKdX+/wYM6Hr+LicevLPs=
github.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
github.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=
github.com/google/gnostic-models v0.6.8 h1:yo/ABAfM5IMRsS1VnXjTBvUb61tFIHozhlYvRgGre9I=
github.com/google/gnostic-models v0.6.8/go.mod h1:5n7qKqH0f5wFt+aWF8CW6pZLLNOfYuF5OpfBSENuI8U=
github.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
github.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
github.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=
github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
github.com/google/gofuzz v1.1.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
github.com/google/gofuzz v1.2.0 h1:xRy4A+RhZaiKjJ1bPfwQ8sedCA+YS2YcCHW6ec7JMi0=
github.com/google/gofuzz v1.2.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
github.com/google/pprof v0.0.0-20210720184732-4bb14d4b1be1 h1:K6RDEckDVWvDI9JAJYCmNdQXq6neHJOYx3V6jnqNEec=
github.com/google/pprof v0.0.0-20210720184732-4bb14d4b1be1/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=
github.com/google/uuid v1.3.0 h1:t6JiXgmwXMjEs8VusXIJk2BXHsn+wx8BZdTaoZ5fu7I=
github.com/google/uuid v1.3.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
github.com/gorilla/websocket v1.4.2/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=
github.com/gorilla/websocket v1.5.0 h1:PPwGk2jz7EePpoHN/+ClbZu8SPxiqlu12wZP/3sWmnc=
github.com/gorilla/websocket v1.5.0/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=
github.com/imdario/mergo v0.3.6 h1:xTNEAn+kxVO7dTZGu0CegyqKZmoWFI0rF8UxjlB2d28=
github.com/imdario/mergo v0.3.6/go.mod h1:2EnlNZ0deacrJVfApfmtdGgDfMuh/nq6Ok1EcJh5FfA=
github.com/jmespath/go-jmespath v0.4.0 h1:BEgLn5cpjn8UN1mAw4NjwDrS35OdebyEtFe+9YPoQUg=
github.com/jmespath/go-jmespath v0.4.0/go.mod h1:T8mJZnbsbmF+m6zOOFylbeCJqk5+pHWvzYPziyZiYoo=
github.com/jmespath/go-jmespath/internal/testify v1.5.1 h1:shLQSRRSCCPj3f2gpwzGwWFoC7ycTf1rcQZHOlsJ6N8=
github.com/jmespath/go-jmespath/internal/testify v1.5.1/go.mod h1:L3OGu8Wl2/fWfCI6z80xFu9LTZmf1ZRjMHUOPmWr69U=
github.com/josharian/intern v1.0.0 h1:vlS4z54oSdjm0bgjRigI+G1HpF+tI+9rE5LLzOg8HmY=
github.com/josharian/intern v1.0.0/go.mod h1:5DoeVV0s6jJacbCEi61lwdGj/aVlrQvzHFFd8Hwg//Y=
github.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=
github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=
github.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=
github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=
github.com/kr/pretty v0.2.0/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=
github.com/kr/pretty v0.2.1/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=
github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=
github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/lib/pq v1.10.9 h1:YXG7RB+JIjhP29X+OtkiDnYaXQwpS4JEWq7dtCCRUEw=
github.com/lib/pq v1.10.9/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=
github.com/mailru/easyjson v0.7.7 h1:UGYAvKxe3sBsEDzO8ZeWOSlIQfWFlxbzLZe7hwFURr0=
github.com/mailru/easyjson v0.7.7/go.mod h1:xzfreul335JAWq5oZzymOObrkdz5UnU4kGfJJLY9Nlc=
github.com/moby/spdystream v0.2.0 h1:cjW1zVyyoiM0T7b6UoySUFqzXMoqRckQtXwGPiBhOM8=
github.com/moby/spdystream v0.2.0/go.mod h1:f7i0iNDQJ059oMTcWxx8MA/zKFIuD/lY+0GqbN2Wy8c=
github.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=
github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
github.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9Gz0M=
github.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=
github.com/motomux/pretty v0.0.0-20161209205251-b2aad2c9a95d h1:LznySqW8MqVeFh+pW6rOkFdld9QQ7jRydBKKM6jyPVI=
github.com/motomux/pretty v0.0.0-20161209205251-b2aad2c9a95d/go.mod h1:u3hJ0kqCQu/cPpsu3RbCOPZ0d7V3IjPjv1adNRleM9I=
github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 h1:C3w9PqII01/Oq1c1nUAm88MOHcQC9l5mIlSMApZMrHA=
github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=
github.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f h1:y5//uYreIhSUg3J1GEMiLbxo1LJaP8RfCpH6pymGZus=
github.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f/go.mod h1:ZdcZmHo+o7JKHSa8/e818NopupXU1YMK5fe1lsApnBw=
github.com/onsi/ginkgo/v2 v2.15.0 h1:79HwNRBAZHOEwrczrgSOPy+eFTTlIGELKy5as+ClttY=
github.com/onsi/ginkgo/v2 v2.15.0/go.mod h1:HlxMHtYF57y6Dpf+mc5529KKmSq9h2FpCF+/ZkwUxKM=
github.com/onsi/gomega v1.31.0 h1:54UJxxj6cPInHS3a35wm6BK/F9nHYueZ1NVujHDrnXE=
github.com/onsi/gomega v1.31.0/go.mod h1:DW9aCi7U6Yi40wNVAvT6kzFnEVEI5n3DloYBiKiT6zk=
github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=
github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/r3labs/diff v1.1.0 h1:V53xhrbTHrWFWq3gI4b94AjgEJOerO1+1l0xyHOBi8M=
github.com/r3labs/diff v1.1.0/go.mod h1:7WjXasNzi0vJetRcB/RqNl5dlIsmXcTTLmF5IoH6Xig=
github.com/rogpeppe/go-internal v1.10.0 h1:TMyTOH3F/DB16zRVcYyreMH6GnZZrwQVAoYjRBZyWFQ=
github.com/rogpeppe/go-internal v1.10.0/go.mod h1:UQnix2H7Ngw/k4C5ijL5+65zddjncjaFoBhdsK/akog=
github.com/sirupsen/logrus v1.9.3 h1:dueUQJ1C2q9oE3F7wvmSGAaVtTmUizReu6fjN8uqzbQ=
github.com/sirupsen/logrus v1.9.3/go.mod h1:naHLuLoDiP4jHNo9R0sCBMtWGeIprob74mVsIT4qYEQ=
github.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=
github.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=
github.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=
github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=
github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=
github.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=
github.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=
github.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
github.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
github.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
github.com/yuin/goldmark v1.3.5/go.mod h1:mwnBkeHKe2W/ZEtQ+71ViKU8L12m81fl3OWwC1Zlc8k=
golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
golang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=
golang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=
golang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3 h1:hNQpMuAJe5CtcUqCXaWga3FHu+kQvCqcsoVaQgSV60o=
golang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3/go.mod h1:idGWGoKP1toJGkd5/ig9ZLuPcZBC3ewk7SzmH0uou08=
golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
golang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
golang.org/x/mod v0.4.2/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
golang.org/x/mod v0.17.0 h1:zY54UmvipHiNd+pm+m0x9KhZ9hl1/7QNMyxXbc6ICqA=
golang.org/x/mod v0.17.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=
golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
golang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=
golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
golang.org/x/net v0.0.0-20210405180319-a5a99cb37ef4/go.mod h1:p54w0d4576C0XHj96bSt6lcn1PtDYWL6XObtHCRCNQM=
golang.org/x/net v0.25.0 h1:d/OCCoBEUq33pjydKrGQhw7IlUPI2Oylr+8qLx49kac=
golang.org/x/net v0.25.0/go.mod h1:JkAGAh7GEvH74S6FOH42FLoXpXbE/aqXSrIQjXgsiwM=
golang.org/x/oauth2 v0.10.0 h1:zHCpF2Khkwy4mMB4bv0U37YtJdTGW8jI0glAApi0Kh8=
golang.org/x/oauth2 v0.10.0/go.mod h1:kTpgurOux7LqtuxjuyZa4Gj2gdezIt/jQtGnNFfypQI=
golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.10.0 h1:3NQrjDixjgGwUOCaF8w2+VYHv0Ve/vGYSbdkTa98gmQ=
golang.org/x/sync v0.10.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=
golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210330210617-4fbd30eecc44/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210510120138-977fb7262007/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.28.0 h1:Fksou7UEQUWlKvIdsqzJmUmCX3cZuD2+P3XyyzwMhlA=
golang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=
golang.org/x/term v0.27.0 h1:WP60Sv1nlK1T6SupCHbXzSaN0b9wUmsPoRS9b61A23Q=
golang.org/x/term v0.27.0/go.mod h1:iMsnZpn0cago0GOrHO2+Y7u7JPn5AylBrcoWkElMTSM=
golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=
golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
golang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=
golang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=
golang.org/x/time v0.3.0 h1:rg5rLMjNzMS1RkNLzCG38eapWhnYLFYXDXj2gOlr8j4=
golang.org/x/time v0.3.0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
golang.org/x/tools v0.0.0-20200505023115-26f46d2f7ef8/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=
golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=
golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=
golang.org/x/tools v0.1.1/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=
golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d h1:vU5i/LfpvrRCpgM/VPfJLg5KjxD3E+hfT1SH+d9zLwg=
golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d/go.mod h1:aiJjzUbINMkxbQROHiO6hDPo2LHcIPhhQsa9DLh0yGk=
golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
google.golang.org/appengine v1.6.7 h1:FZR1q0exgwxzPzp/aF+VccGrSfxfPpkBqjIIEq3ru6c=
google.golang.org/appengine v1.6.7/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=
google.golang.org/protobuf v1.33.0 h1:uNO2rsAINq/JlFpSdYEKIZ0uKD/R9cpdv0T+yoGwGmI=
google.golang.org/protobuf v1.33.0/go.mod h1:c6P6GXX6sHbq/GpV6MGZEdwhWPcYBgnhAHhKbcUYpos=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/inf.v0 v0.9.1 h1:73M5CoZyi3ZLMOyDlQh031Cx6N9NDJ2Vvfl76EDAgDc=
gopkg.in/inf.v0 v0.9.1/go.mod h1:cWUDdTG/fYaXco+Dcufb5Vnc6Gp2YChqWtbxRZE0mXw=
gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
gopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=
gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
k8s.io/api v0.30.4 h1:XASIELmW8w8q0i1Y4124LqPoWMycLjyQti/fdYHYjCs=
k8s.io/api v0.30.4/go.mod h1:ZqniWRKu7WIeLijbbzetF4U9qZ03cg5IRwl8YVs8mX0=
k8s.io/apiextensions-apiserver v0.25.9 h1:Pycd6lm2auABp9wKQHCFSEPG+NPdFSTJXPST6NJFzB8=
k8s.io/apiextensions-apiserver v0.25.9/go.mod h1:ijGxmSG1GLOEaWhTuaEr0M7KUeia3mWCZa6FFQqpt1M=
k8s.io/apimachinery v0.30.4 h1:5QHQI2tInzr8LsT4kU/2+fSeibH1eIHswNx480cqIoY=
k8s.io/apimachinery v0.30.4/go.mod h1:iexa2somDaxdnj7bha06bhb43Zpa6eWH8N8dbqVjTUc=
k8s.io/client-go v0.30.4 h1:eculUe+HPQoPbixfwmaSZGsKcOf7D288tH6hDAdd+wY=
k8s.io/client-go v0.30.4/go.mod h1:IBS0R/Mt0LHkNHF4E6n+SUDPG7+m2po6RZU7YHeOpzc=
k8s.io/code-generator v0.25.9 h1:lgyAV9AIRYNxZxgLRXqsCAtqJLHvakot41CjEqD5W0w=
k8s.io/code-generator v0.25.9/go.mod h1:DHfpdhSUrwqF0f4oLqCtF8gYbqlndNetjBEz45nWzJI=
k8s.io/gengo v0.0.0-20220902162205-c0856e24416d h1:U9tB195lKdzwqicbJvyJeOXV7Klv+wNAWENRnXEGi08=
k8s.io/gengo v0.0.0-20220902162205-c0856e24416d/go.mod h1:FiNAH4ZV3gBg2Kwh89tzAEV2be7d5xI0vBa/VySYy3E=
k8s.io/gengo/v2 v2.0.0-20240228010128-51d4e06bde70 h1:NGrVE502P0s0/1hudf8zjgwki1X/TByhmAoILTarmzo=
k8s.io/gengo/v2 v2.0.0-20240228010128-51d4e06bde70/go.mod h1:VH3AT8AaQOqiGjMF9p0/IM1Dj+82ZwjfxUP1IxaHE+8=
k8s.io/klog/v2 v2.2.0/go.mod h1:Od+F08eJP+W3HUb4pSrPpgp9DGU4GzlpG/TmITuYh/Y=
k8s.io/klog/v2 v2.120.1 h1:QXU6cPEOIslTGvZaXvFWiP9VKyeet3sawzTOvdXb4Vw=
k8s.io/klog/v2 v2.120.1/go.mod h1:3Jpz1GvMt720eyJH1ckRHK1EDfpxISzJ7I9OYgaDtPE=
k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340 h1:BZqlfIlq5YbRMFko6/PM7FjZpUb45WallggurYhKGag=
k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340/go.mod h1:yD4MZYeKMBwQKVht279WycxKyM84kkAx2DPrTXaeb98=
k8s.io/utils v0.0.0-20230726121419-3b25d923346b h1:sgn3ZU783SCgtaSJjpcVVlRqd6GSnlTLKgpAAttJvpI=
k8s.io/utils v0.0.0-20230726121419-3b25d923346b/go.mod h1:OLgZIPagt7ERELqWJFomSt595RzquPNLL48iOWgYOg0=
sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd h1:EDPBXCAspyGV4jQlpZSudPeMmr1bNJefnuqLsRAsHZo=
sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd/go.mod h1:B8JuhiUyNFVKdsE8h686QcCxMaH6HrOAZj4vswFpcB0=
sigs.k8s.io/structured-merge-diff/v4 v4.4.1 h1:150L+0vs/8DA78h1u02ooW1/fFq/Lwr+sGiqlzvrtq4=
sigs.k8s.io/structured-merge-diff/v4 v4.4.1/go.mod h1:N8hJocpFajUSSeSJ9bOZ77VzejKZaXsTtZo4/u7Io08=
sigs.k8s.io/yaml v1.2.0/go.mod h1:yfXDCHCao9+ENCvLSE62v9VSji2MKu5jeNfTrofGhJc=
sigs.k8s.io/yaml v1.3.0 h1:a2VclLzOGrwOHDiV8EfBGhvjHvP46CtW5j6POvhYGGo=
sigs.k8s.io/yaml v1.3.0/go.mod h1:GeOyir5tyXNByN85N/dRIT9es5UQNerPYEKK56eTBm8=


================================================
File: mkdocs.yml
================================================
site_name: Postgres Operator
repo_url: https://github.com/zalando/postgres-operator
theme: readthedocs

nav:
  - Concepts: 'index.md'
  - Quickstart: 'quickstart.md'
  - Postgres Operator UI: 'operator-ui.md'
  - Admin guide: 'administrator.md'
  - User guide: 'user.md'
  - Developer guide: 'developer.md'
  - Reference:
    - Config parameters: 'reference/operator_parameters.md'
    - Manifest parameters: 'reference/cluster_manifest.md'
    - CLI options and environment: 'reference/command_line_and_environment.md'


================================================
File: run_operator_locally.sh
================================================
#!/usr/bin/env bash
#
# Deploy a Postgres Operator to a minikube aka local Kubernetes cluster
# Optionally re-build the operator binary beforehand to test local changes

# Known limitations:
# 1) minikube provides a single node K8s cluster. That is, you will not be able test functions like pod
#    migration between multiple nodes locally
# 2) this script configures the operator via configmap, not the operator CRD


# enable unofficial bash strict mode
set -o errexit
set -o nounset
set -o pipefail
IFS=$'\n\t'


readonly PATH_TO_LOCAL_OPERATOR_MANIFEST="/tmp/local-postgres-operator-manifest.yaml"
readonly PATH_TO_PORT_FORWARED_KUBECTL_PID="/tmp/kubectl-port-forward.pid"
readonly PATH_TO_THE_PG_CLUSTER_MANIFEST="/tmp/minimal-postgres-manifest.yaml"
readonly LOCAL_PORT="8080"
readonly OPERATOR_PORT="8080"


# minikube needs time to create resources,
# so the script retries actions until all the resources become available
function retry(){

    local -r retry_cmd="$1"
    local -r retry_msg="$2"

    # Time out after three minutes.
    for i in {1..60}; do
        if  eval "$retry_cmd"; then
            return 0
        fi
        echo "$retry_msg"
        sleep 3
    done

    >2& echo "The command $retry_cmd timed out"
    return 1
}

function display_help(){
    echo "Usage: $0 [ -r | --rebuild-operator ] [ -h | --help ] [ -n | --deploy-new-operator-image ] [ -t | --deploy-pg-to-namespace-test ]"
}

function clean_up(){

    echo "==== CLEAN UP PREVIOUS RUN ==== "

    local status
    status=$(minikube status --format "{{.Host}}" || true)

    if [[ "$status" = "Running" ]] || [[ "$status" = "Stopped" ]]; then
        echo "Delete the existing local cluster so that we can cleanly apply resources from scratch..."
        minikube delete
    fi

    if [[ -e "$PATH_TO_LOCAL_OPERATOR_MANIFEST" ]]; then
        rm -v "$PATH_TO_LOCAL_OPERATOR_MANIFEST"
    fi

    # the kubectl process does the port-forwarding between operator and local ports
    # we restart the process to bind to the same port again (see end of script)
    if [[ -e "$PATH_TO_PORT_FORWARED_KUBECTL_PID" ]]; then

        local pid
        pid=$( < "$PATH_TO_PORT_FORWARED_KUBECTL_PID")

        # the process dies if a minikube stops between two invocations of the script
        if kill "$pid" > /dev/null  2>&1; then
            echo "Kill the kubectl process responsible for port forwarding for minikube so that we can re-use the same ports for forwarding later..."
        fi
        rm -v  "$PATH_TO_PORT_FORWARED_KUBECTL_PID"

    fi
}


function start_minikube(){

    echo "==== START MINIKUBE ===="
    echo "May take a few minutes ..."

    minikube start
    kubectl config set-context minikube

    echo "==== MINIKUBE STATUS ===="
    minikube status
    echo ""
}


function build_operator_binary(){

    # redirecting stderr greatly reduces non-informative output during normal builds
    echo "Build operator binary (stderr redirected to /dev/null)..."
    make clean deps local test > /dev/null 2>&1

}


function deploy_self_built_image() {

    echo "==== DEPLOY CUSTOM OPERATOR IMAGE ==== "

    build_operator_binary

    # the fastest way to run a docker image locally is to reuse the docker from minikube
    # set docker env vars so that docker can talk to the Docker daemon inside the minikube
    eval $(minikube docker-env)

    # image tag consists of a git tag or a unique commit prefix
    # and the "-dev" suffix if there are uncommited changes in the working dir
    local -x TAG
    TAG=$(git describe --tags --always --dirty="-dev")
    readonly TAG

    # build the image
    make docker > /dev/null 2>&1

    # update the tag in the postgres operator conf
    # since the image with this tag already exists on the machine,
    # docker should not attempt to fetch it from the registry due to imagePullPolicy
    sed -e "s/\(image\:.*\:\).*$/\1$TAG/; s/smoke-tested-//" manifests/postgres-operator.yaml > "$PATH_TO_LOCAL_OPERATOR_MANIFEST"

    retry "kubectl apply -f \"$PATH_TO_LOCAL_OPERATOR_MANIFEST\"" "attempt to create $PATH_TO_LOCAL_OPERATOR_MANIFEST resource"
}


function start_operator(){

    echo "==== START OPERATOR ===="
    echo "Certain operations may be retried multiple times..."

    # the order of resource initialization is significant
    local file
    for file  in "configmap.yaml" "operator-service-account-rbac.yaml"
    do
        retry "kubectl  create -f manifests/\"$file\"" "attempt to create $file resource"
    done

    cp  manifests/postgres-operator.yaml $PATH_TO_LOCAL_OPERATOR_MANIFEST

    if [[ "$should_build_custom_operator" = true ]]; then # set in main()
        deploy_self_built_image
    else
        retry "kubectl create -f ${PATH_TO_LOCAL_OPERATOR_MANIFEST}" "attempt to create ${PATH_TO_LOCAL_OPERATOR_MANIFEST} resource"
    fi

    local -r msg="Wait for the postgresql custom resource definition to register..."
    local -r cmd="kubectl get crd | grep --quiet 'postgresqls.acid.zalan.do'"
    retry "$cmd" "$msg "

}


function forward_ports(){

    echo "==== FORWARD OPERATOR PORT $OPERATOR_PORT TO LOCAL PORT $LOCAL_PORT  ===="

    local operator_pod
    operator_pod=$(kubectl get pod -l name=postgres-operator -o jsonpath={.items..metadata.name})

    # Spawn `kubectl port-forward` in the background to keep current terminal
    # responsive. Hide stdout because otherwise there is a note about each TCP
    # connection. Do not hide stderr so port-forward setup errors can be
    # debugged. Sometimes the port-forward setup fails because expected k8s
    # state isn't achieved yet. Try to detect that case and then run the
    # command again (in a finite loop).
    for _attempt in {1..20}; do
        # Delay between retry attempts. First attempt should already be
        # delayed.
        echo "soon: invoke kubectl port-forward command (attempt $_attempt)"
        sleep 5

        # With the --pod-running-timeout=4s argument the process is expected
        # to terminate within about that time if the pod isn't ready yet.
        kubectl port-forward --pod-running-timeout=4s "$operator_pod" "$LOCAL_PORT":"$OPERATOR_PORT" 1> /dev/null &
        _kubectl_pid=$!
        _pf_success=true

        # A successful `kubectl port-forward` setup can pragmatically be
        # detected with a time-based criterion: it is a long-running process if
        # successfully set up. If it does not terminate within deadline then
        # consider the setup successful. Overall, observe the process for
        # roughly 7 seconds. If it terminates before that it's certainly an
        # error. If it did not terminate within that time frame then consider
        # setup successful.
        for ib in {1..7}; do
            sleep 1
            # Portable and non-blocking test: is process still running?
            if kill -s 0 -- "${_kubectl_pid}" >/dev/null 2>&1; then
                echo "port-forward process is still running"
            else
                # port-forward process seems to have terminated, reap zombie
                set +e
                # `wait` is now expected to be non-blocking, and exits with the
                # exit code of pid (first arg).
                wait $_kubectl_pid
                _kubectl_rc=$?
                set -e
                echo "port-forward process terminated with exit code ${_kubectl_rc}"
                _pf_success=false
                break
            fi
        done

        if [ ${_pf_success} = true ]; then
            echo "port-forward setup seems successful. leave retry loop."
            break
        fi

    done

    if [ "${_pf_success}" = false ]; then
        echo "port-forward setup failed after retrying. exit."
        exit 1
    fi

    echo "${_kubectl_pid}" > "$PATH_TO_PORT_FORWARED_KUBECTL_PID"
}


function check_health(){

    echo "==== RUN HEALTH CHECK ==== "

    local -r check_cmd="curl --location --silent --output /dev/null http://127.0.0.1:$LOCAL_PORT/clusters"
    local -r check_msg="Wait for port forwarding to take effect"
    echo "Command for checking: $check_cmd"

    if  retry "$check_cmd" "$check_msg"; then
        echo "==== SUCCESS: OPERATOR IS RUNNING ==== "
        echo "To stop it cleanly, run 'minikube delete'"
    else
        >2& echo "==== FAILURE: OPERATOR DID NOT START OR PORT FORWARDING DID NOT WORK"
        exit 1
    fi
}


function submit_postgresql_manifest(){

    echo "==== SUBMIT MINIMAL POSTGRES MANIFEST ==== "

    local namespace="default"
    cp manifests/minimal-postgres-manifest.yaml $PATH_TO_THE_PG_CLUSTER_MANIFEST

    if $should_deploy_pg_to_namespace_test; then
          kubectl create namespace test
          namespace="test"
          sed --in-place 's/namespace: default/namespace: test/'  $PATH_TO_THE_PG_CLUSTER_MANIFEST
    fi

    kubectl create -f $PATH_TO_THE_PG_CLUSTER_MANIFEST
    echo "The operator will create the PG cluster with minimal manifest $PATH_TO_THE_PG_CLUSTER_MANIFEST in the ${namespace} namespace"

}


function main(){

    if ! [[ $(basename "$PWD") == "postgres-operator" ]]; then
        echo "Please execute the script only from the root directory of the Postgres Operator repo."
        exit 1
    fi

    trap "echo 'If you observe issues with minikube VM not starting/not proceeding, consider deleting the .minikube dir and/or rebooting before re-running the script'" EXIT

    local should_build_custom_operator=false
    local should_deploy_pg_to_namespace_test=false
    local should_replace_operator_image=false

    while true
    do
        # if the 1st param is unset, use the empty string as a default value
        case "${1:-}" in
            -h | --help)
                display_help
                exit 0
                ;;
            -r | --rebuild-operator) # with minikube restart
                should_build_custom_operator=true
                break
                ;;
            -n | --deploy-new-operator-image) # without minikube restart that takes minutes
                should_replace_operator_image=true
                break
                ;;
            -t | --deploy-pg-to-namespace-test) # to test multi-namespace support locally
                should_deploy_pg_to_namespace_test=true
                break
                ;;
            *)  break
                ;;
        esac
    done

    if ${should_replace_operator_image}; then
       deploy_self_built_image
       exit 0
    fi

    clean_up
    start_minikube
    start_operator
    submit_postgresql_manifest
    forward_ports
    check_health

    exit 0
}


main "$@"


================================================
File: .flake8
================================================
[flake8]
exclude=.git,__pycache__
max-line-length=120


================================================
File: .golangci.yml
================================================
# https://github.com/golangci/golangci/wiki/Configuration

service:
  prepare:
    - make deps


================================================
File: .zappr.yaml
================================================
# for github.com
X-Zalando-Team: "acid"
# type should be one of [code, doc, config, tools, secrets]
# code will be the default value, if X-Zalando-Type is not found in .zappr.yml
X-Zalando-Type: code

approvals:
  groups:
    zalando:
      minimum: 2
      from:
        orgs:
          - zalando

================================================
File: charts/postgres-operator/Chart.yaml
================================================
apiVersion: v2
name: postgres-operator
version: 1.14.0
appVersion: 1.14.0
home: https://github.com/zalando/postgres-operator
description: Postgres Operator creates and manages PostgreSQL clusters running in Kubernetes
keywords:
- postgres
- operator
- cloud-native
- patroni
- spilo
maintainers:
- name: Zalando
  email: opensource@zalando.de
sources:
- https://github.com/zalando/postgres-operator
engine: gotpl


================================================
File: charts/postgres-operator/index.yaml
================================================
apiVersion: v1
entries:
  postgres-operator:
  - apiVersion: v2
    appVersion: 1.14.0
    created: "2024-12-23T11:25:32.596716566+01:00"
    description: Postgres Operator creates and manages PostgreSQL clusters running
      in Kubernetes
    digest: 36e1571f3f455b213f16cdda7b1158648e8e84deb804ba47ed6b9b6d19263ba8
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-1.14.0.tgz
    version: 1.14.0
  - apiVersion: v2
    appVersion: 1.13.0
    created: "2024-12-23T11:25:32.591136261+01:00"
    description: Postgres Operator creates and manages PostgreSQL clusters running
      in Kubernetes
    digest: a839601689aea0a7e6bc0712a5244d435683cf3314c95794097ff08540e1dfef
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-1.13.0.tgz
    version: 1.13.0
  - apiVersion: v2
    appVersion: 1.12.2
    created: "2024-12-23T11:25:32.585419709+01:00"
    description: Postgres Operator creates and manages PostgreSQL clusters running
      in Kubernetes
    digest: 65858d14a40d7fd90c32bd9fc60021acc9555c161079f43a365c70171eaf21d8
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-1.12.2.tgz
    version: 1.12.2
  - apiVersion: v2
    appVersion: 1.11.0
    created: "2024-12-23T11:25:32.580077286+01:00"
    description: Postgres Operator creates and manages PostgreSQL clusters running
      in Kubernetes
    digest: 3914b5e117bda0834f05c9207f007e2ac372864cf6e86dcc2e1362bbe46c14d9
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-1.11.0.tgz
    version: 1.11.0
  - apiVersion: v2
    appVersion: 1.10.1
    created: "2024-12-23T11:25:32.574641578+01:00"
    description: Postgres Operator creates and manages PostgreSQL clusters running
      in Kubernetes
    digest: cc3baa41753da92466223d0b334df27e79c882296577b404a8e9071411fcf19c
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-1.10.1.tgz
    version: 1.10.1
  - apiVersion: v2
    appVersion: 1.9.0
    created: "2024-12-23T11:25:32.604748814+01:00"
    description: Postgres Operator creates and manages PostgreSQL clusters running
      in Kubernetes
    digest: 64df90c898ca591eb3a330328173ffaadfbf9ddd474d8c42ed143edc9e3f4276
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-1.9.0.tgz
    version: 1.9.0
generated: "2024-12-23T11:25:32.568598763+01:00"


================================================
File: charts/postgres-operator/values.yaml
================================================
image:
  registry: ghcr.io
  repository: zalando/postgres-operator
  tag: v1.14.0
  pullPolicy: "IfNotPresent"

# Optionally specify an array of imagePullSecrets.
# Secrets must be manually created in the namespace.
# ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
# imagePullSecrets:
# - name: myRegistryKeySecretName

podAnnotations: {}
podLabels: {}

configTarget: "OperatorConfigurationCRD"

# JSON logging format
enableJsonLogging: false

# general configuration parameters
configGeneral:
  # the deployment should create/update the CRDs
  enable_crd_registration: true
  # specify categories under which crds should be listed
  crd_categories:
  - "all"
  # update only the statefulsets without immediately doing the rolling update
  enable_lazy_spilo_upgrade: false
  # set the PGVERSION env var instead of providing the version via postgresql.bin_dir in SPILO_CONFIGURATION
  enable_pgversion_env_var: true
  # start any new database pod without limitations on shm memory
  enable_shm_volume: true
  # enables backwards compatible path between Spilo 12 and Spilo 13+ images
  enable_spilo_wal_path_compat: false
  # operator will sync only clusters where name starts with teamId prefix
  enable_team_id_clustername_prefix: false
  # etcd connection string for Patroni. Empty uses K8s-native DCS.
  etcd_host: ""
  # Spilo docker image
  docker_image: ghcr.io/zalando/spilo-17:4.0-p2

  # key name for annotation to ignore globally configured instance limits
  # ignore_instance_limits_annotation_key: ""

  # Select if setup uses endpoints (default), or configmaps to manage leader (DCS=k8s)
  # kubernetes_use_configmaps: false

  # min number of instances in Postgres cluster. -1 = no limit
  min_instances: -1
  # max number of instances in Postgres cluster. -1 = no limit
  max_instances: -1
  # period between consecutive repair requests
  repair_period: 5m
  # period between consecutive sync requests
  resync_period: 30m
  # can prevent certain cases of memory overcommitment
  # set_memory_request_to_limit: false

  # map of sidecar names to docker images
  # sidecar_docker_images:
  #  example: "exampleimage:exampletag"

  # number of routines the operator spawns to process requests concurrently
  workers: 8

# parameters describing Postgres users
configUsers:
  # roles to be granted to database owners
  # additional_owner_roles:
  # - cron_admin

  # enable password rotation for app users that are not database owners
  enable_password_rotation: false
  # rotation interval for updating credentials in K8s secrets of app users
  password_rotation_interval: 90
  # retention interval to keep rotation users
  password_rotation_user_retention: 180
  # postgres username used for replication between instances
  replication_username: standby
  # postgres superuser name to be created by initdb
  super_username: postgres

configMajorVersionUpgrade:
  # "off": no upgrade, "manual": manifest triggers action, "full": minimal version violation triggers too
  major_version_upgrade_mode: "manual"
  # upgrades will only be carried out for clusters of listed teams when mode is "off"
  # major_version_upgrade_team_allow_list:
  # - acid

  # minimal Postgres major version that will not automatically be upgraded
  minimal_major_version: "13"
  # target Postgres major version when upgrading clusters automatically
  target_major_version: "17"

configKubernetes:
  # list of additional capabilities for postgres container
  # additional_pod_capabilities:
  # - "SYS_NICE"

  # default DNS domain of K8s cluster where operator is running
  cluster_domain: cluster.local
  # additional labels assigned to the cluster objects
  cluster_labels:
    application: spilo
  # label assigned to Kubernetes objects created by the operator
  cluster_name_label: cluster-name
  # additional annotations to add to every database pod
  # custom_pod_annotations:
  #   keya: valuea
  #   keyb: valueb

  # key name for annotation that compares manifest value with current date
  # delete_annotation_date_key: "delete-date"

  # key name for annotation that compares manifest value with cluster name
  # delete_annotation_name_key: "delete-clustername"

  # list of annotations propagated from cluster manifest to statefulset and deployment
  # downscaler_annotations:
  # - deployment-time
  # - downscaler/*

  # allow user secrets in other namespaces than the Postgres cluster
  enable_cross_namespace_secret: false
  # use finalizers to ensure all managed resources are deleted prior to the postgresql CR
  # this avoids stale resources in case the operator misses a delete event or is not running
  # during deletion
  enable_finalizers: false
  # enables initContainers to run actions before Spilo is started
  enable_init_containers: true
  # toggles if child resources should have an owner reference to the postgresql CR
  enable_owner_references: false
  # toggles if operator should delete PVCs on cluster deletion
  enable_persistent_volume_claim_deletion: true
  # toggles pod anti affinity on the Postgres pods
  enable_pod_antiaffinity: false
  # toggles PDB to set to MinAvailabe 0 or 1
  enable_pod_disruption_budget: true
  # toogles readiness probe for database pods
  enable_readiness_probe: false
  # toggles if operator should delete secrets on cluster deletion
  enable_secrets_deletion: true
  # enables sidecar containers to run alongside Spilo in the same pod
  enable_sidecars: true

  # annotations to be ignored when comparing statefulsets, services etc.
  # ignored_annotations:
  # - k8s.v1.cni.cncf.io/network-status

  # namespaced name of the secret containing infrastructure roles names and passwords
  # infrastructure_roles_secret_name: postgresql-infrastructure-roles

  # list of annotation keys that can be inherited from the cluster manifest
  # inherited_annotations:
  # - owned-by

  # list of label keys that can be inherited from the cluster manifest
  # inherited_labels:
  # - application
  # - environment

  # timeout for successful migration of master pods from unschedulable node
  # master_pod_move_timeout: 20m

  # set of labels that a running and active node should possess to be considered ready
  # node_readiness_label:
  #   status: ready

  # defines how nodeAffinity from manifest should be merged with node_readiness_label
  # node_readiness_label_merge: "OR"

  # namespaced name of the secret containing the OAuth2 token to pass to the teams API
  # oauth_token_secret_name: postgresql-operator

  # toggle if `spilo-role=master` selector should be added to the PDB (Pod Disruption Budget)
  pdb_master_label_selector: true
  # defines the template for PDB names
  pdb_name_format: "postgres-{cluster}-pdb"
  # specify the PVC retention policy when scaling down and/or deleting
  persistent_volume_claim_retention_policy:
    when_deleted: "retain"
    when_scaled: "retain"
  # switches pod anti affinity type to `preferredDuringSchedulingIgnoredDuringExecution`
  pod_antiaffinity_preferred_during_scheduling: false
  # override topology key for pod anti affinity
  pod_antiaffinity_topology_key: "kubernetes.io/hostname"
  # namespaced name of the ConfigMap with environment variables to populate on every pod
  # pod_environment_configmap: "default/my-custom-config"
  # name of the Secret (in cluster namespace) with environment variables to populate on every pod
  # pod_environment_secret: "my-custom-secret"

  # specify the pod management policy of stateful sets of Postgres clusters
  pod_management_policy: "ordered_ready"
  # label assigned to the Postgres pods (and services/endpoints)
  pod_role_label: spilo-role
  # service account definition as JSON/YAML string to be used by postgres cluster pods
  # pod_service_account_definition: ""

  # role binding definition as JSON/YAML string to be used by pod service account
  # pod_service_account_role_binding_definition: ""

  # Postgres pods are terminated forcefully after this timeout
  pod_terminate_grace_period: 5m
  # template for database user secrets generated by the operator,
  # here username contains the namespace in the format namespace.username
  # if the user is in different namespace than cluster and cross namespace secrets
  # are enabled via `enable_cross_namespace_secret` flag in the configuration.
  secret_name_template: "{username}.{cluster}.credentials.{tprkind}.{tprgroup}"
  # sharing unix socket of PostgreSQL (`pg_socket`) with the sidecars
  share_pgsocket_with_sidecars: false
  # set user and group for the spilo container (required to run Spilo as non-root process)
  # spilo_runasuser: 101
  # spilo_runasgroup: 103

  # group ID with write-access to volumes (required to run Spilo as non-root process)
  # spilo_fsgroup: 103

  # whether the Spilo container should run in privileged mode
  spilo_privileged: false
  # whether the Spilo container should run with additional permissions other than parent.
  # required by cron which needs setuid
  spilo_allow_privilege_escalation: true
  # storage resize strategy, available options are: ebs, pvc, off or mixed
  storage_resize_mode: pvc
  # pod toleration assigned to instances of every Postgres cluster
  # toleration:
  #   key: db-only
  #   operator: Exists
  #   effect: NoSchedule

  # operator watches for postgres objects in the given namespace
  watched_namespace: "*"  # listen to all namespaces

# configure resource requests for the Postgres pods
configPostgresPodResources:
  # CPU limits for the postgres containers
  default_cpu_limit: "1"
  # CPU request value for the postgres containers
  default_cpu_request: 100m
  # memory limits for the postgres containers
  default_memory_limit: 500Mi
  # memory request value for the postgres containers
  default_memory_request: 100Mi
  # optional upper boundary for CPU request
  # max_cpu_request: "1"

  # optional upper boundary for memory request
  # max_memory_request: 4Gi

  # hard CPU minimum required to properly run a Postgres cluster
  min_cpu_limit: 250m
  # hard memory minimum required to properly run a Postgres cluster
  min_memory_limit: 250Mi

# timeouts related to some operator actions
configTimeouts:
  # interval between consecutive attempts of operator calling the Patroni API
  patroni_api_check_interval: 1s
  # timeout when waiting for successful response from Patroni API
  patroni_api_check_timeout: 5s
  # timeout when waiting for the Postgres pods to be deleted
  pod_deletion_wait_timeout: 10m
  # timeout when waiting for pod role and cluster labels
  pod_label_wait_timeout: 10m
  # interval between consecutive attempts waiting for postgresql CRD to be created
  ready_wait_interval: 3s
  # timeout for the complete postgres CRD creation
  ready_wait_timeout: 30s
  # interval to wait between consecutive attempts to check for some K8s resources
  resource_check_interval: 3s
  # timeout when waiting for the presence of a certain K8s resource (e.g. Sts, PDB)
  resource_check_timeout: 10m

# configure behavior of load balancers
configLoadBalancer:
  # DNS zone for cluster DNS name when load balancer is configured for cluster
  db_hosted_zone: db.example.com
  # annotations to apply to service when load balancing is enabled
  # custom_service_annotations:
  #   keyx: valuez
  #   keya: valuea

  # toggles service type load balancer pointing to the master pod of the cluster
  enable_master_load_balancer: false
  # toggles service type load balancer pointing to the master pooler pod of the cluster
  enable_master_pooler_load_balancer: false
  # toggles service type load balancer pointing to the replica pod of the cluster
  enable_replica_load_balancer: false
  # toggles service type load balancer pointing to the replica pooler pod of the cluster
  enable_replica_pooler_load_balancer: false
  # define external traffic policy for the load balancer
  external_traffic_policy: "Cluster"
  # defines the DNS name string template for the master load balancer cluster
  master_dns_name_format: "{cluster}.{namespace}.{hostedzone}"
  # deprecated DNS template for master load balancer using team name
  master_legacy_dns_name_format: "{cluster}.{team}.{hostedzone}"
  # defines the DNS name string template for the replica load balancer cluster
  replica_dns_name_format: "{cluster}-repl.{namespace}.{hostedzone}"
  # deprecated DNS template for replica load balancer using team name
  replica_legacy_dns_name_format: "{cluster}-repl.{team}.{hostedzone}"

# options to aid debugging of the operator itself
configDebug:
  # toggles verbose debug logs from the operator
  debug_logging: true
  # toggles operator functionality that require access to the postgres database
  enable_database_access: true

# parameters affecting logging and REST API listener
configLoggingRestApi:
  # REST API listener listens to this port
  api_port: 8080
  # number of entries in the cluster history ring buffer
  cluster_history_entries: 1000
  # number of lines in the ring buffer used to store cluster logs
  ring_log_lines: 100

# configure interaction with non-Kubernetes objects from AWS or GCP
configAwsOrGcp:
  # Additional Secret (aws or gcp credentials) to mount in the pod
  # additional_secret_mount: "some-secret-name"

  # Path to mount the above Secret in the filesystem of the container(s)
  # additional_secret_mount_path: "/some/dir"

  # AWS region used to store EBS volumes
  aws_region: eu-central-1

  # enable automatic migration on AWS from gp2 to gp3 volumes
  enable_ebs_gp3_migration: false
  # defines maximum volume size in GB until which auto migration happens
  # enable_ebs_gp3_migration_max_size: 1000

  # GCP credentials that will be used by the operator / pods
  # gcp_credentials: ""

  # AWS IAM role to supply in the iam.amazonaws.com/role annotation of Postgres pods
  # kube_iam_role: ""

  # S3 bucket to use for shipping postgres daily logs
  # log_s3_bucket: ""

  # S3 bucket to use for shipping WAL segments with WAL-E
  # wal_s3_bucket: ""

  # GCS bucket to use for shipping WAL segments with WAL-E
  # wal_gs_bucket: ""

  # Azure Storage Account to use for shipping WAL segments with WAL-G
  # wal_az_storage_account: ""

# configure K8s cron job managed by the operator
configLogicalBackup:
  # Azure Storage Account specs to store backup results
  # logical_backup_azure_storage_account_name: ""
  # logical_backup_azure_storage_container: ""
  # logical_backup_azure_storage_account_key: ""

  # resources for logical backup pod, if empty configPostgresPodResources will be used
  # logical_backup_cpu_limit: ""
  # logical_backup_cpu_request: ""
  # logical_backup_memory_limit: ""
  # logical_backup_memory_request: ""

  # image for pods of the logical backup job (example runs pg_dumpall)
  logical_backup_docker_image: "ghcr.io/zalando/postgres-operator/logical-backup:v1.14.0"
  # path of google cloud service account json file
  # logical_backup_google_application_credentials: ""

  # prefix for the backup job name
  logical_backup_job_prefix: "logical-backup-"
  # storage provider - either "s3", "gcs" or "az"
  logical_backup_provider: "s3"
  # S3 Access Key ID
  logical_backup_s3_access_key_id: ""
  # S3 bucket to store backup results
  logical_backup_s3_bucket: "my-bucket-url"
  # S3 bucket prefix to use
  logical_backup_s3_bucket_prefix: "spilo"
  # S3 region of bucket
  logical_backup_s3_region: ""
  # S3 endpoint url when not using AWS
  logical_backup_s3_endpoint: ""
  # S3 Secret Access Key
  logical_backup_s3_secret_access_key: ""
  # S3 server side encryption
  logical_backup_s3_sse: "AES256"
  # S3 retention time for stored backups for example "2 week" or "7 days"
  logical_backup_s3_retention_time: ""
  # backup schedule in the cron format
  logical_backup_schedule: "30 00 * * *"
  # secret to be used as reference for env variables in cronjob
  logical_backup_cronjob_environment_secret: ""

# automate creation of human users with teams API service
configTeamsApi:
  # team_admin_role will have the rights to grant roles coming from PG manifests
  enable_admin_role_for_users: true
  # operator watches for PostgresTeam CRs to assign additional teams and members to clusters
  enable_postgres_team_crd: false
  # toogle to create additional superuser teams from PostgresTeam CRs
  enable_postgres_team_crd_superusers: false
  # toggle to automatically rename roles of former team members and deny LOGIN
  enable_team_member_deprecation: false
  # toggle to grant superuser to team members created from the Teams API
  enable_team_superuser: false
  # toggles usage of the Teams API by the operator
  enable_teams_api: false
  # should contain a URL to use for authentication (username and token)
  # pam_configuration: https://info.example.com/oauth2/tokeninfo?access_token= uid realm=/employees

  # operator will add all team member roles to this group and add a pg_hba line
  pam_role_name: zalandos
  # List of teams which members need the superuser role in each Postgres cluster
  postgres_superuser_teams:
    - postgres_superusers
  # List of roles that cannot be overwritten by an application, team or infrastructure role
  protected_role_names:
    - admin
    - cron_admin
  # Suffix to add if members are removed from TeamsAPI or PostgresTeam CRD
  role_deletion_suffix: "_deleted"
  # role name to grant to team members created from the Teams API
  team_admin_role: admin
  # postgres config parameters to apply to each team member role
  team_api_role_configuration:
    log_statement: all
  # URL of the Teams API service
  # teams_api_url: http://fake-teams-api.default.svc.cluster.local

# configure connection pooler deployment created by the operator
configConnectionPooler:
  # db schema to install lookup function into
  connection_pooler_schema: "pooler"
  # db user for pooler to use
  connection_pooler_user: "pooler"
  # docker image
  connection_pooler_image: "registry.opensource.zalan.do/acid/pgbouncer:master-32"
  # max db connections the pooler should hold
  connection_pooler_max_db_connections: 60
  # default pooling mode
  connection_pooler_mode: "transaction"
  # number of pooler instances
  connection_pooler_number_of_instances: 2
  # default resources
  connection_pooler_default_cpu_request: 500m
  connection_pooler_default_memory_request: 100Mi
  connection_pooler_default_cpu_limit: "1"
  connection_pooler_default_memory_limit: 100Mi

configPatroni:
  # enable Patroni DCS failsafe_mode feature
  enable_patroni_failsafe_mode: false

# Zalando's internal CDC stream feature
enableStreams: false

rbac:
  # Specifies whether RBAC resources should be created
  create: true
  # Specifies whether ClusterRoles that are aggregated into the K8s default roles should be created. (https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings)
  createAggregateClusterRoles: false

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

podServiceAccount:
  # The name of the ServiceAccount to be used by postgres cluster pods
  # If not set a name is generated using the fullname template and "-pod" suffix
  name: "postgres-pod"

# priority class for operator pod
priorityClassName: ""

# priority class for database pods
podPriorityClassName:
  # If create is false with no name set, no podPriorityClassName is specified.
  # Hence, the pod priorityClass is the one with globalDefault set.
  # If there is no PriorityClass with globalDefault set, the priority of Pods with no priorityClassName is zero.
  create: true
  # If not set a name is generated using the fullname template and "-pod" suffix
  name: ""
  priority: 1000000

resources:
  limits:
    cpu: 500m
    memory: 500Mi
  requests:
    cpu: 100m
    memory: 250Mi

securityContext:
  runAsUser: 1000
  runAsNonRoot: true
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false

# Allow to setup operator Deployment readiness probe
readinessProbe:
  initialDelaySeconds: 5
  periodSeconds: 10

# configure extra environment variables
# Extra environment variables are writen in kubernetes format and added "as is" to the pod's env variables
# https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/
# https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#environment-variables
extraEnvs:
  []
  # Exemple of settings maximum amount of memory / cpu that can be used by go process (to match resources.limits)
  # - name: MY_VAR
  #   value: my-value
  # - name: GOMAXPROCS
  #   valueFrom:
  #     resourceFieldRef:
  #       resource: limits.cpu
  # - name: GOMEMLIMIT
  #   valueFrom:
  #     resourceFieldRef:
  #       resource: limits.memory

# Affinity for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

# Node labels for pod assignment
# Ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

# Tolerations for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []

controllerID:
  # Specifies whether a controller ID should be defined for the operator
  # Note, all postgres manifest must then contain the following annotation to be found by this operator
  # "acid.zalan.do/controller": <controller-ID-of-the-operator>
  create: false
  # The name of the controller ID to use.
  # If not set and create is true, a name is generated using the fullname template
  name:


================================================
File: charts/postgres-operator/.helmignore
================================================
# Patterns to ignore when building packages.
# This supports shell glob matching, relative path matching, and
# negation (prefixed with !). Only one pattern per line.
.DS_Store
# Common VCS dirs
.git/
.gitignore
.bzr/
.bzrignore
.hg/
.hgignore
.svn/
# Common backup files
*.swp
*.bak
*.tmp
*~
# Various IDEs
.project
.idea/
*.tmproj


================================================
File: charts/postgres-operator/crds/operatorconfigurations.yaml
================================================
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: operatorconfigurations.acid.zalan.do
  labels:
    app.kubernetes.io/name: postgres-operator
spec:
  group: acid.zalan.do
  names:
    kind: OperatorConfiguration
    listKind: OperatorConfigurationList
    plural: operatorconfigurations
    singular: operatorconfiguration
    shortNames:
    - opconfig
    categories:
    - all
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    subresources:
      status: {}
    additionalPrinterColumns:
    - name: Image
      type: string
      description: Spilo image to be used for Pods
      jsonPath: .configuration.docker_image
    - name: Cluster-Label
      type: string
      description: Label for K8s resources created by operator
      jsonPath: .configuration.kubernetes.cluster_name_label
    - name: Service-Account
      type: string
      description: Name of service account to be used
      jsonPath: .configuration.kubernetes.pod_service_account_name
    - name: Min-Instances
      type: integer
      description: Minimum number of instances per Postgres cluster
      jsonPath: .configuration.min_instances
    - name: Age
      type: date
      jsonPath: .metadata.creationTimestamp
    schema:
      openAPIV3Schema:
        type: object
        required:
          - kind
          - apiVersion
          - configuration
        properties:
          kind:
            type: string
            enum:
            - OperatorConfiguration
          apiVersion:
            type: string
            enum:
            - acid.zalan.do/v1
          configuration:
            type: object
            properties:
              crd_categories:
                type: array
                nullable: true
                items:
                  type: string
              docker_image:
                type: string
                default: "ghcr.io/zalando/spilo-17:4.0-p2"
              enable_crd_registration:
                type: boolean
                default: true
              enable_crd_validation:
                type: boolean
                description: deprecated
                default: true
              enable_lazy_spilo_upgrade:
                type: boolean
                default: false
              enable_pgversion_env_var:
                type: boolean
                default: true
              enable_shm_volume:
                type: boolean
                default: true
              enable_spilo_wal_path_compat:
                type: boolean
                default: false
              enable_team_id_clustername_prefix:
                type: boolean
                default: false
              etcd_host:
                type: string
                default: ""
              ignore_instance_limits_annotation_key:
                type: string
              kubernetes_use_configmaps:
                type: boolean
                default: false
              max_instances:
                type: integer
                description: "-1 = disabled"
                minimum: -1
                default: -1
              min_instances:
                type: integer
                description: "-1 = disabled"
                minimum: -1
                default: -1
              resync_period:
                type: string
                default: "30m"
              repair_period:
                type: string
                default: "5m"
              set_memory_request_to_limit:
                type: boolean
                default: false
              sidecar_docker_images:
                type: object
                additionalProperties:
                  type: string
              sidecars:
                type: array
                nullable: true
                items:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              workers:
                type: integer
                minimum: 1
                default: 8
              users:
                type: object
                properties:
                  additional_owner_roles:
                    type: array
                    nullable: true
                    items:
                      type: string
                  enable_password_rotation:
                    type: boolean
                    default: false
                  password_rotation_interval:
                    type: integer
                    default: 90
                  password_rotation_user_retention:
                    type: integer
                    default: 180
                  replication_username:
                     type: string
                     default: standby
                  super_username:
                     type: string
                     default: postgres
              major_version_upgrade:
                type: object
                properties:
                  major_version_upgrade_mode:
                    type: string
                    default: "manual"
                  major_version_upgrade_team_allow_list:
                    type: array
                    items:
                      type: string
                  minimal_major_version:
                    type: string
                    default: "13"
                  target_major_version:
                    type: string
                    default: "17"
              kubernetes:
                type: object
                properties:
                  additional_pod_capabilities:
                    type: array
                    items:
                      type: string
                  cluster_domain:
                    type: string
                    default: "cluster.local"
                  cluster_labels:
                    type: object
                    additionalProperties:
                      type: string
                    default:
                      application: spilo
                  cluster_name_label:
                    type: string
                    default: "cluster-name"
                  custom_pod_annotations:
                    type: object
                    additionalProperties:
                      type: string
                  delete_annotation_date_key:
                    type: string
                  delete_annotation_name_key:
                    type: string
                  downscaler_annotations:
                    type: array
                    items:
                      type: string
                  enable_cross_namespace_secret:
                    type: boolean
                    default: false
                  enable_finalizers:
                    type: boolean
                    default: false
                  enable_init_containers:
                    type: boolean
                    default: true
                  enable_owner_references:
                    type: boolean
                    default: false
                  enable_persistent_volume_claim_deletion:
                    type: boolean
                    default: true
                  enable_pod_antiaffinity:
                    type: boolean
                    default: false
                  enable_pod_disruption_budget:
                    type: boolean
                    default: true
                  enable_readiness_probe:
                    type: boolean
                    default: false
                  enable_secrets_deletion:
                    type: boolean
                    default: true
                  enable_sidecars:
                    type: boolean
                    default: true
                  ignored_annotations:
                    type: array
                    items:
                      type: string
                  infrastructure_roles_secret_name:
                    type: string
                  infrastructure_roles_secrets:
                    type: array
                    nullable: true
                    items:
                      type: object
                      required:
                        - secretname
                        - userkey
                        - passwordkey
                      properties:
                        secretname:
                          type: string
                        userkey:
                          type: string
                        passwordkey:
                          type: string
                        rolekey:
                          type: string
                        defaultuservalue:
                          type: string
                        defaultrolevalue:
                          type: string
                        details:
                          type: string
                        template:
                          type: boolean
                  inherited_annotations:
                    type: array
                    items:
                      type: string
                  inherited_labels:
                    type: array
                    items:
                      type: string
                  master_pod_move_timeout:
                    type: string
                    default: "20m"
                  node_readiness_label:
                    type: object
                    additionalProperties:
                      type: string
                  node_readiness_label_merge:
                    type: string
                    enum:
                      - "AND"
                      - "OR"
                  oauth_token_secret_name:
                    type: string
                    default: "postgresql-operator"
                  pdb_master_label_selector:
                    type: boolean
                    default: true
                  pdb_name_format:
                    type: string
                    default: "postgres-{cluster}-pdb"
                  persistent_volume_claim_retention_policy:
                    type: object
                    properties:
                      when_deleted:
                        type: string
                        enum:
                          - "delete"
                          - "retain"
                      when_scaled:
                        type: string
                        enum:
                          - "delete"
                          - "retain"
                  pod_antiaffinity_preferred_during_scheduling:
                    type: boolean
                    default: false
                  pod_antiaffinity_topology_key:
                    type: string
                    default: "kubernetes.io/hostname"
                  pod_environment_configmap:
                    type: string
                  pod_environment_secret:
                    type: string
                  pod_management_policy:
                    type: string
                    enum:
                      - "ordered_ready"
                      - "parallel"
                    default: "ordered_ready"
                  pod_priority_class_name:
                    type: string
                  pod_role_label:
                    type: string
                    default: "spilo-role"
                  pod_service_account_definition:
                    type: string
                    default: ""
                  pod_service_account_name:
                    type: string
                    default: "postgres-pod"
                  pod_service_account_role_binding_definition:
                    type: string
                    default: ""
                  pod_terminate_grace_period:
                    type: string
                    default: "5m"
                  secret_name_template:
                    type: string
                    default: "{username}.{cluster}.credentials.{tprkind}.{tprgroup}"
                  share_pgsocket_with_sidecars:
                    type: boolean
                    default: false
                  spilo_allow_privilege_escalation:
                    type: boolean
                    default: true
                  spilo_runasuser:
                    type: integer
                  spilo_runasgroup:
                    type: integer
                  spilo_fsgroup:
                    type: integer
                  spilo_privileged:
                    type: boolean
                    default: false
                  storage_resize_mode:
                    type: string
                    enum:
                      - "ebs"
                      - "mixed"
                      - "pvc"
                      - "off"
                    default: "pvc"
                  toleration:
                    type: object
                    additionalProperties:
                      type: string
                  watched_namespace:
                    type: string
              postgres_pod_resources:
                type: object
                properties:
                  default_cpu_limit:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$|^$'
                  default_cpu_request:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$|^$'
                  default_memory_limit:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$|^$'
                  default_memory_request:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$|^$'
                  max_cpu_request:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$|^$'
                  max_memory_request:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$|^$'
                  min_cpu_limit:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$|^$'
                  min_memory_limit:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$|^$'
              timeouts:
                type: object
                properties:
                  patroni_api_check_interval:
                    type: string
                    default: "1s"
                  patroni_api_check_timeout:
                    type: string
                    default: "5s"
                  pod_label_wait_timeout:
                    type: string
                    default: "10m"
                  pod_deletion_wait_timeout:
                    type: string
                    default: "10m"
                  ready_wait_interval:
                    type: string
                    default: "4s"
                  ready_wait_timeout:
                    type: string
                    default: "30s"
                  resource_check_interval:
                    type: string
                    default: "3s"
                  resource_check_timeout:
                    type: string
                    default: "10m"
              load_balancer:
                type: object
                properties:
                  custom_service_annotations:
                    type: object
                    additionalProperties:
                      type: string
                  db_hosted_zone:
                    type: string
                    default: "db.example.com"
                  enable_master_load_balancer:
                    type: boolean
                    default: true
                  enable_master_pooler_load_balancer:
                    type: boolean
                    default: false
                  enable_replica_load_balancer:
                    type: boolean
                    default: false
                  enable_replica_pooler_load_balancer:
                    type: boolean
                    default: false
                  external_traffic_policy:
                    type: string
                    enum:
                      - "Cluster"
                      - "Local"
                    default: "Cluster"
                  master_dns_name_format:
                    type: string
                    default: "{cluster}.{namespace}.{hostedzone}"
                  master_legacy_dns_name_format:
                    type: string
                    default: "{cluster}.{team}.{hostedzone}"
                  replica_dns_name_format:
                    type: string
                    default: "{cluster}-repl.{namespace}.{hostedzone}"
                  replica_legacy_dns_name_format:
                    type: string
                    default: "{cluster}-repl.{team}.{hostedzone}"
              aws_or_gcp:
                type: object
                properties:
                  additional_secret_mount:
                    type: string
                  additional_secret_mount_path:
                    type: string
                  aws_region:
                    type: string
                    default: "eu-central-1"
                  enable_ebs_gp3_migration:
                    type: boolean
                    default: false
                  enable_ebs_gp3_migration_max_size:
                    type: integer
                    default: 1000
                  gcp_credentials:
                    type: string
                  kube_iam_role:
                    type: string
                  log_s3_bucket:
                    type: string
                  wal_az_storage_account:
                    type: string
                  wal_gs_bucket:
                    type: string
                  wal_s3_bucket:
                    type: string
              logical_backup:
                type: object
                properties:
                  logical_backup_azure_storage_account_name:
                    type: string
                  logical_backup_azure_storage_container:
                    type: string
                  logical_backup_azure_storage_account_key:
                    type: string
                  logical_backup_cpu_limit:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                  logical_backup_cpu_request:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                  logical_backup_docker_image:
                    type: string
                    default: "ghcr.io/zalando/postgres-operator/logical-backup:v1.13.0"
                  logical_backup_google_application_credentials:
                    type: string
                  logical_backup_job_prefix:
                    type: string
                    default: "logical-backup-"
                  logical_backup_memory_limit:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                  logical_backup_memory_request:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                  logical_backup_provider:
                    type: string
                    enum:
                      - "az"
                      - "gcs"
                      - "s3"
                    default: "s3"
                  logical_backup_s3_access_key_id:
                    type: string
                  logical_backup_s3_bucket:
                    type: string
                  logical_backup_s3_bucket_prefix:
                    type: string
                  logical_backup_s3_endpoint:
                    type: string
                  logical_backup_s3_region:
                    type: string
                  logical_backup_s3_secret_access_key:
                    type: string
                  logical_backup_s3_sse:
                    type: string
                  logical_backup_s3_retention_time:
                    type: string
                  logical_backup_schedule:
                    type: string
                    pattern: '^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$'
                    default: "30 00 * * *"
                  logical_backup_cronjob_environment_secret:
                    type: string
              debug:
                type: object
                properties:
                  debug_logging:
                    type: boolean
                    default: true
                  enable_database_access:
                    type: boolean
                    default: true
              teams_api:
                type: object
                properties:
                  enable_admin_role_for_users:
                    type: boolean
                    default: true
                  enable_postgres_team_crd:
                    type: boolean
                    default: true
                  enable_postgres_team_crd_superusers:
                    type: boolean
                    default: false
                  enable_team_member_deprecation:
                    type: boolean
                    default: false
                  enable_team_superuser:
                    type: boolean
                    default: false
                  enable_teams_api:
                    type: boolean
                    default: true
                  pam_configuration:
                    type: string
                    default: "https://info.example.com/oauth2/tokeninfo?access_token= uid realm=/employees"
                  pam_role_name:
                    type: string
                    default: "zalandos"
                  postgres_superuser_teams:
                    type: array
                    items:
                      type: string
                  protected_role_names:
                    type: array
                    items:
                      type: string
                    default:
                    - admin
                    - cron_admin
                  role_deletion_suffix:
                    type: string
                    default: "_deleted"
                  team_admin_role:
                    type: string
                    default: "admin"
                  team_api_role_configuration:
                    type: object
                    additionalProperties:
                      type: string
                    default:
                      log_statement: all
                  teams_api_url:
                    type: string
                    default: "https://teams.example.com/api/"
              logging_rest_api:
                type: object
                properties:
                  api_port:
                    type: integer
                    default: 8080
                  cluster_history_entries:
                    type: integer
                    default: 1000
                  ring_log_lines:
                    type: integer
                    default: 100
              scalyr:  # deprecated
                type: object
                properties:
                  scalyr_api_key:
                    type: string
                  scalyr_cpu_limit:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                    default: "1"
                  scalyr_cpu_request:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                    default: "100m"
                  scalyr_image:
                    type: string
                  scalyr_memory_limit:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                    default: "500Mi"
                  scalyr_memory_request:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                    default: "50Mi"
                  scalyr_server_url:
                    type: string
                    default: "https://upload.eu.scalyr.com"
              connection_pooler:
                type: object
                properties:
                  connection_pooler_schema:
                    type: string
                    default: "pooler"
                  connection_pooler_user:
                    type: string
                    default: "pooler"
                  connection_pooler_image:
                    type: string
                    default: "registry.opensource.zalan.do/acid/pgbouncer:master-32"
                  connection_pooler_max_db_connections:
                    type: integer
                    default: 60
                  connection_pooler_mode:
                    type: string
                    enum:
                      - "session"
                      - "transaction"
                    default: "transaction"
                  connection_pooler_number_of_instances:
                    type: integer
                    minimum: 1
                    default: 2
                  connection_pooler_default_cpu_limit:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                  connection_pooler_default_cpu_request:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                  connection_pooler_default_memory_limit:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                  connection_pooler_default_memory_request:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
              patroni:
                type: object
                properties:
                  enable_patroni_failsafe_mode:
                    type: boolean
                    default: false
          status:
            type: object
            additionalProperties:
              type: string


================================================
File: charts/postgres-operator/crds/postgresqls.yaml
================================================
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: postgresqls.acid.zalan.do
  labels:
    app.kubernetes.io/name: postgres-operator
spec:
  group: acid.zalan.do
  names:
    kind: postgresql
    listKind: postgresqlList
    plural: postgresqls
    singular: postgresql
    shortNames:
    - pg
    categories:
    - all
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    subresources:
      status: {}
    additionalPrinterColumns:
    - name: Team
      type: string
      description: Team responsible for Postgres cluster
      jsonPath: .spec.teamId
    - name: Version
      type: string
      description: PostgreSQL version
      jsonPath: .spec.postgresql.version
    - name: Pods
      type: integer
      description: Number of Pods per Postgres cluster
      jsonPath: .spec.numberOfInstances
    - name: Volume
      type: string
      description: Size of the bound volume
      jsonPath: .spec.volume.size
    - name: CPU-Request
      type: string
      description: Requested CPU for Postgres containers
      jsonPath: .spec.resources.requests.cpu
    - name: Memory-Request
      type: string
      description: Requested memory for Postgres containers
      jsonPath: .spec.resources.requests.memory
    - name: Age
      type: date
      jsonPath: .metadata.creationTimestamp
    - name: Status
      type: string
      description: Current sync status of postgresql resource
      jsonPath: .status.PostgresClusterStatus
    schema:
      openAPIV3Schema:
        type: object
        required:
          - kind
          - apiVersion
          - spec
        properties:
          kind:
            type: string
            enum:
              - postgresql
          apiVersion:
            type: string
            enum:
              - acid.zalan.do/v1
          spec:
            type: object
            required:
              - numberOfInstances
              - teamId
              - postgresql
              - volume
            properties:
              additionalVolumes:
                type: array
                items:
                  type: object
                  required:
                    - name
                    - mountPath
                    - volumeSource
                  properties:
                    isSubPathExpr:
                      type: boolean
                    name:
                      type: string
                    mountPath:
                      type: string
                    subPath:
                      type: string
                    targetContainers:
                      type: array
                      nullable: true
                      items:
                        type: string
                    volumeSource:
                      type: object
                      x-kubernetes-preserve-unknown-fields: true
              allowedSourceRanges:
                type: array
                nullable: true
                items:
                  type: string
                  pattern: '^(\d|[1-9]\d|1\d\d|2[0-4]\d|25[0-5])\.(\d|[1-9]\d|1\d\d|2[0-4]\d|25[0-5])\.(\d|[1-9]\d|1\d\d|2[0-4]\d|25[0-5])\.(\d|[1-9]\d|1\d\d|2[0-4]\d|25[0-5])\/(\d|[1-2]\d|3[0-2])$'
              clone:
                type: object
                required:
                  - cluster
                properties:
                  cluster:
                    type: string
                  s3_endpoint:
                    type: string
                  s3_access_key_id:
                    type: string
                  s3_secret_access_key:
                    type: string
                  s3_force_path_style:
                    type: boolean
                  s3_wal_path:
                    type: string
                  timestamp:
                    type: string
                    pattern: '^([0-9]+)-(0[1-9]|1[012])-(0[1-9]|[12][0-9]|3[01])[Tt]([01][0-9]|2[0-3]):([0-5][0-9]):([0-5][0-9]|60)(\.[0-9]+)?(([+-]([01][0-9]|2[0-3]):[0-5][0-9]))$'
                    # The regexp matches the date-time format (RFC 3339 Section 5.6) that specifies a timezone as an offset relative to UTC
                    # Example: 1996-12-19T16:39:57-08:00
                    # Note: this field requires a timezone
                  uid:
                    format: uuid
                    type: string
              connectionPooler:
                type: object
                properties:
                  dockerImage:
                    type: string
                  maxDBConnections:
                    type: integer
                  mode:
                    type: string
                    enum:
                      - "session"
                      - "transaction"
                  numberOfInstances:
                    type: integer
                    minimum: 1
                  resources:
                    type: object
                    properties:
                      limits:
                        type: object
                        properties:
                          cpu:
                            type: string
                            pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                          memory:
                            type: string
                            pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                      requests:
                        type: object
                        properties:
                          cpu:
                            type: string
                            pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                          memory:
                            type: string
                            pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                  schema:
                    type: string
                  user:
                    type: string
              databases:
                type: object
                additionalProperties:
                  type: string
                # Note: usernames specified here as database owners must be declared in the users key of the spec key.
              dockerImage:
                type: string
              enableConnectionPooler:
                type: boolean
              enableReplicaConnectionPooler:
                type: boolean
              enableLogicalBackup:
                type: boolean
              enableMasterLoadBalancer:
                type: boolean
              enableMasterPoolerLoadBalancer:
                type: boolean
              enableReplicaLoadBalancer:
                type: boolean
              enableReplicaPoolerLoadBalancer:
                type: boolean
              enableShmVolume:
                type: boolean
              env:
                type: array
                nullable: true
                items:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              init_containers:
                type: array
                description: deprecated
                nullable: true
                items:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              initContainers:
                type: array
                nullable: true
                items:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              logicalBackupRetention:
                type: string
              logicalBackupSchedule:
                type: string
                pattern: '^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$'
              maintenanceWindows:
                type: array
                items:
                  type: string
                  pattern: '^\ *((Mon|Tue|Wed|Thu|Fri|Sat|Sun):(2[0-3]|[01]?\d):([0-5]?\d)|(2[0-3]|[01]?\d):([0-5]?\d))-((2[0-3]|[01]?\d):([0-5]?\d)|(2[0-3]|[01]?\d):([0-5]?\d))\ *$'
              masterServiceAnnotations:
                type: object
                additionalProperties:
                  type: string
              nodeAffinity:
                type: object
                properties:
                  preferredDuringSchedulingIgnoredDuringExecution:
                    type: array
                    items:
                      type: object
                      required:
                      - preference
                      - weight
                      properties:
                        preference:
                          type: object
                          properties:
                            matchExpressions:
                              type: array
                              items:
                                type: object
                                required:
                                - key
                                - operator
                                properties:
                                  key:
                                    type: string
                                  operator:
                                    type: string
                                  values:
                                    type: array
                                    items:
                                      type: string
                            matchFields:
                              type: array
                              items:
                                type: object
                                required:
                                - key
                                - operator
                                properties:
                                  key:
                                    type: string
                                  operator:
                                    type: string
                                  values:
                                    type: array
                                    items:
                                      type: string
                        weight:
                          format: int32
                          type: integer
                  requiredDuringSchedulingIgnoredDuringExecution:
                    type: object
                    required:
                    - nodeSelectorTerms
                    properties:
                      nodeSelectorTerms:
                        type: array
                        items:
                          type: object
                          properties:
                            matchExpressions:
                              type: array
                              items:
                                type: object
                                required:
                                - key
                                - operator
                                properties:
                                  key:
                                    type: string
                                  operator:
                                    type: string
                                  values:
                                    type: array
                                    items:
                                      type: string
                            matchFields:
                              type: array
                              items:
                                type: object
                                required:
                                - key
                                - operator
                                properties:
                                  key:
                                    type: string
                                  operator:
                                    type: string
                                  values:
                                    type: array
                                    items:
                                      type: string
              numberOfInstances:
                type: integer
                minimum: 0
              patroni:
                type: object
                properties:
                  failsafe_mode:
                    type: boolean
                  initdb:
                    type: object
                    additionalProperties:
                      type: string
                  loop_wait:
                    type: integer
                  maximum_lag_on_failover:
                    type: integer
                  pg_hba:
                    type: array
                    items:
                      type: string
                  retry_timeout:
                    type: integer
                  slots:
                    type: object
                    additionalProperties:
                      type: object
                      additionalProperties:
                        type: string
                  synchronous_mode:
                    type: boolean
                  synchronous_mode_strict:
                    type: boolean
                  synchronous_node_count:
                    type: integer
                  ttl:
                    type: integer
              podAnnotations:
                type: object
                additionalProperties:
                  type: string
              pod_priority_class_name:
                type: string
                description: deprecated
              podPriorityClassName:
                type: string
              postgresql:
                type: object
                required:
                  - version
                properties:
                  version:
                    type: string
                    enum:
                      - "13"
                      - "14"
                      - "15"
                      - "16"
                      - "17"
                  parameters:
                    type: object
                    additionalProperties:
                      type: string
              preparedDatabases:
                type: object
                additionalProperties:
                  type: object
                  properties:
                    defaultUsers:
                      type: boolean
                    extensions:
                      type: object
                      additionalProperties:
                        type: string
                    schemas:
                      type: object
                      additionalProperties:
                        type: object
                        properties:
                          defaultUsers:
                            type: boolean
                          defaultRoles:
                            type: boolean
                    secretNamespace:
                      type: string
              replicaLoadBalancer:
                type: boolean
                description: deprecated
              replicaServiceAnnotations:
                type: object
                additionalProperties:
                  type: string
              resources:
                type: object
                properties:
                  limits:
                    type: object
                    properties:
                      cpu:
                        type: string
                        # Decimal natural followed by m, or decimal natural followed by
                        # dot followed by up to three decimal digits.
                        #
                        # This is because the Kubernetes CPU resource has millis as the
                        # maximum precision.  The actual values are checked in code
                        # because the regular expression would be huge and horrible and
                        # not very helpful in validation error messages; this one checks
                        # only the format of the given number.
                        #
                        # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu
                        pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                        # Note: the value specified here must not be zero or be lower
                        # than the corresponding request.
                      memory:
                        type: string
                        # You can express memory as a plain integer or as a fixed-point
                        # integer using one of these suffixes: E, P, T, G, M, k. You can
                        # also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki
                        #
                        # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                        # Note: the value specified here must not be zero or be higher
                        # than the corresponding limit.
                      hugepages-2Mi:
                        type: string
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                      hugepages-1Gi:
                        type: string
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                  requests:
                    type: object
                    properties:
                      cpu:
                        type: string
                        pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                      memory:
                        type: string
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                      hugepages-2Mi:
                        type: string
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                      hugepages-1Gi:
                        type: string
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
              schedulerName:
                type: string
              serviceAnnotations:
                type: object
                additionalProperties:
                  type: string
              sidecars:
                type: array
                nullable: true
                items:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              spiloRunAsUser:
                type: integer
              spiloRunAsGroup:
                type: integer
              spiloFSGroup:
                type: integer
              standby:
                type: object
                properties:
                  s3_wal_path:
                    type: string
                  gs_wal_path:
                    type: string
                  standby_host:
                    type: string
                  standby_port:
                    type: string
                oneOf:
                - required:
                  - s3_wal_path
                - required:
                  - gs_wal_path
                - required:
                  - standby_host
              streams:
                type: array
                items:
                  type: object
                  required:
                    - applicationId
                    - database
                    - tables
                  properties:
                    applicationId:
                      type: string
                    batchSize:
                      type: integer
                    cpu:
                      type: string
                      pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                    database:
                      type: string
                    enableRecovery:
                      type: boolean
                    filter:
                      type: object
                      additionalProperties:
                        type: string
                    memory:
                      type: string
                      pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                    tables:
                      type: object
                      additionalProperties:
                        type: object
                        required:
                          - eventType
                        properties:
                          eventType:
                            type: string
                          idColumn:
                            type: string
                          ignoreRecovery:
                            type: boolean
                          payloadColumn:
                            type: string
                          recoveryEventType:
                            type: string
              teamId:
                type: string
              tls:
                type: object
                required:
                  - secretName
                properties:
                  secretName:
                    type: string
                  certificateFile:
                    type: string
                  privateKeyFile:
                    type: string
                  caFile:
                    type: string
                  caSecretName:
                    type: string
              tolerations:
                type: array
                items:
                  type: object
                  properties:
                    key:
                      type: string
                    operator:
                      type: string
                      enum:
                        - Equal
                        - Exists
                    value:
                      type: string
                    effect:
                      type: string
                      enum:
                        - NoExecute
                        - NoSchedule
                        - PreferNoSchedule
                    tolerationSeconds:
                      type: integer
              useLoadBalancer:
                type: boolean
                description: deprecated
              users:
                type: object
                additionalProperties:
                  type: array
                  nullable: true
                  items:
                    type: string
                    enum:
                    - bypassrls
                    - BYPASSRLS
                    - nobypassrls
                    - NOBYPASSRLS
                    - createdb
                    - CREATEDB
                    - nocreatedb
                    - NOCREATEDB
                    - createrole
                    - CREATEROLE
                    - nocreaterole
                    - NOCREATEROLE
                    - inherit
                    - INHERIT
                    - noinherit
                    - NOINHERIT
                    - login
                    - LOGIN
                    - nologin
                    - NOLOGIN
                    - replication
                    - REPLICATION
                    - noreplication
                    - NOREPLICATION
                    - superuser
                    - SUPERUSER
                    - nosuperuser
                    - NOSUPERUSER
              usersIgnoringSecretRotation:
                type: array
                nullable: true
                items:
                  type: string
              usersWithInPlaceSecretRotation:
                type: array
                nullable: true
                items:
                  type: string
              usersWithSecretRotation:
                type: array
                nullable: true
                items:
                  type: string
              volume:
                type: object
                required:
                  - size
                properties:
                  isSubPathExpr:
                    type: boolean
                  iops:
                    type: integer
                  selector:
                    type: object
                    properties:
                      matchExpressions:
                        type: array
                        items:
                          type: object
                          required:
                            - key
                            - operator
                          properties:
                            key:
                              type: string
                            operator:
                              type: string
                              enum:
                                - DoesNotExist
                                - Exists
                                - In
                                - NotIn
                            values:
                              type: array
                              items:
                                type: string
                      matchLabels:
                        type: object
                        x-kubernetes-preserve-unknown-fields: true
                  size:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                    # Note: the value specified here must not be zero.
                  storageClass:
                    type: string
                  subPath:
                    type: string
                  throughput:
                    type: integer
          status:
            type: object
            additionalProperties:
              type: string


================================================
File: charts/postgres-operator/crds/postgresteams.yaml
================================================
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: postgresteams.acid.zalan.do
  labels:
    app.kubernetes.io/name: postgres-operator
spec:
  group: acid.zalan.do
  names:
    kind: PostgresTeam
    listKind: PostgresTeamList
    plural: postgresteams
    singular: postgresteam
    shortNames:
    - pgteam
    categories:
    - all
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    subresources:
      status: {}
    schema:
      openAPIV3Schema:
        type: object
        required:
          - kind
          - apiVersion
          - spec
        properties:
          kind:
            type: string
            enum:
              - PostgresTeam
          apiVersion:
            type: string
            enum:
              - acid.zalan.do/v1
          spec:
            type: object
            properties:
              additionalSuperuserTeams:
                type: object
                description: "Map for teamId and associated additional superuser teams"
                additionalProperties:
                  type: array
                  nullable: true
                  description: "List of teams to become Postgres superusers"
                  items:
                    type: string
              additionalTeams:
                type: object
                description: "Map for teamId and associated additional teams"
                additionalProperties:
                  type: array
                  nullable: true
                  description: "List of teams whose members will also be added to the Postgres cluster"
                  items:
                    type: string
              additionalMembers:
                type: object
                description: "Map for teamId and associated additional users"
                additionalProperties:
                  type: array
                  nullable: true
                  description: "List of users who will also be added to the Postgres cluster"
                  items:
                    type: string


================================================
File: charts/postgres-operator/templates/NOTES.txt
================================================
To verify that postgres-operator has started, run:

  kubectl --namespace={{ .Release.Namespace }} get pods -l "app.kubernetes.io/name={{ template "postgres-operator.name" . }}"


================================================
File: charts/postgres-operator/templates/_helpers.tpl
================================================
{{/* vim: set filetype=mustache: */}}
{{/*
Expand the name of the chart.
*/}}
{{- define "postgres-operator.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" -}}
{{- end -}}

{{/*
Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
If release name contains chart name it will be used as a full name.
*/}}
{{- define "postgres-operator.fullname" -}}
{{- if .Values.fullnameOverride -}}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" -}}
{{- else -}}
{{- $name := default .Chart.Name .Values.nameOverride -}}
{{- if contains $name .Release.Name -}}
{{- .Release.Name | trunc 63 | trimSuffix "-" -}}
{{- else -}}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" -}}
{{- end -}}
{{- end -}}
{{- end -}}

{{/*
Create a service account name.
*/}}
{{- define "postgres-operator.serviceAccountName" -}}
{{ default (include "postgres-operator.fullname" .) .Values.serviceAccount.name }}
{{- end -}}

{{/*
Create a pod service account name.
*/}}
{{- define "postgres-pod.serviceAccountName" -}}
{{ default (printf "%s-%v" (include "postgres-operator.fullname" .) "pod") .Values.podServiceAccount.name }}
{{- end -}}

{{/*
Create a pod priority class name.
*/}}
{{- define "postgres-pod.priorityClassName" -}}
{{ default (printf "%s-%v" (include "postgres-operator.fullname" .) "pod") .Values.podPriorityClassName.name }}
{{- end -}}

{{/*
Create a controller ID.
*/}}
{{- define "postgres-operator.controllerID" -}}
{{ default (include "postgres-operator.fullname" .) .Values.controllerID.name }}
{{- end -}}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "postgres-operator.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" -}}
{{- end -}}

{{/*
Flatten nested config options when ConfigMap is used as ConfigTarget
*/}}
{{- define "flattenValuesForConfigMap" }}
{{- range $key, $value := . }}
    {{- if kindIs "slice" $value }}
{{ $key }}: {{ join "," $value | quote }}
    {{- else if kindIs "map" $value }}
        {{- $list := list }}
        {{- range $subKey, $subValue := $value }}
            {{- $list = append $list (printf "%s:%s" $subKey $subValue) }}
        {{- end }}
{{ $key }}: {{ join "," $list | quote }}
    {{- else }}
{{ $key }}: {{ $value | quote }}
    {{- end }}
{{- end }}
{{- end }}


================================================
File: charts/postgres-operator/templates/clusterrole-postgres-pod.yaml
================================================
{{ if .Values.rbac.create }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ include "postgres-pod.serviceAccountName" . }}
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
rules:
# Patroni needs to watch and manage config maps or endpoints
{{- if toString .Values.configGeneral.kubernetes_use_configmaps | eq "true" }}
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
{{- else }}
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
{{- end }}
# Patroni needs to watch pods
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - patch
  - update
  - watch
# to let Patroni create a headless service
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - create
{{- if toString .Values.configKubernetes.spilo_privileged | eq "true" }}
# to run privileged pods
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - privileged
  verbs:
  - use
{{- end }}
{{ end }}


================================================
File: charts/postgres-operator/templates/clusterrole.yaml
================================================
{{ if .Values.rbac.create }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ include "postgres-operator.serviceAccountName" . }}
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
rules:
# all verbs allowed for custom operator resources
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  - postgresqls/status
  - operatorconfigurations
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
# operator only reads PostgresTeams
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresteams
  verbs:
  - get
  - list
  - watch
# all verbs allowed for event streams
{{- if .Values.enableStreams }}
- apiGroups:
  - zalando.org
  resources:
  - fabriceventstreams
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
{{- end }}
# to create or get/update CRDs when starting up
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - get
{{- if toString .Values.configGeneral.enable_crd_registration | eq "true" }}
  - create
  - patch
  - update
{{- end }}
# to send events to the CRs
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
# to manage endpoints/configmaps which are also used by Patroni
{{- if toString .Values.configGeneral.kubernetes_use_configmaps | eq "true" }}
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
{{- else }}
# to read configuration from ConfigMaps
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
{{- end }}
# to CRUD secrets for database access
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - create
  - delete
  - get
  - patch
  - update
# to check nodes for node readiness label
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
# to read or delete existing PVCs. Creation via StatefulSet
- apiGroups:
  - ""
  resources:
  - persistentvolumeclaims
  verbs:
  - delete
  - get
  - list
  - patch
{{- if or (toString .Values.configKubernetes.storage_resize_mode | eq "pvc") (toString .Values.configKubernetes.storage_resize_mode | eq "mixed") }}
  - update
{{- end }}
 # to read existing PVs. Creation should be done via dynamic provisioning
- apiGroups:
  - ""
  resources:
  - persistentvolumes
  verbs:
  - get
  - list
{{- if toString .Values.configKubernetes.storage_resize_mode | eq "ebs" }}
  - update  # only for resizing AWS volumes
{{- end }}
# to watch Spilo pods and do rolling updates. Creation via StatefulSet
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - delete
  - get
  - list
  - patch
  - update
  - watch
# to resize the filesystem in Spilo pods when increasing volume size
- apiGroups:
  - ""
  resources:
  - pods/exec
  verbs:
  - create
# to CRUD services to point to Postgres cluster instances
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - create
  - delete
  - get
  - patch
  - update
# to CRUD the StatefulSet which controls the Postgres cluster instances
- apiGroups:
  - apps
  resources:
  - statefulsets
  - deployments
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
# to CRUD cron jobs for logical backups
- apiGroups:
  - batch
  resources:
  - cronjobs
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
# to get namespaces operator resources can run in
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
# to define PDBs. Update happens via delete/create
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - create
  - delete
  - get
# to create ServiceAccounts in each namespace the operator watches
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - get
  - create
# to create role bindings to the postgres-pod service account
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  verbs:
  - get
  - create
{{- if toString .Values.configKubernetes.spilo_privileged | eq "true" }}
# to run privileged pods
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - privileged
  verbs:
  - use
{{- end }}
{{ end }}


================================================
File: charts/postgres-operator/templates/clusterrolebinding.yaml
================================================
{{ if .Values.rbac.create }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: {{ include "postgres-operator.serviceAccountName" . }}
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: {{ include "postgres-operator.serviceAccountName" . }}
subjects:
- kind: ServiceAccount
  name: {{ include "postgres-operator.serviceAccountName" . }}
  namespace: {{ .Release.Namespace }}
{{ end }}


================================================
File: charts/postgres-operator/templates/configmap.yaml
================================================
{{- if eq .Values.configTarget "ConfigMap" }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "postgres-operator.fullname" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
data:
{{- if or .Values.podPriorityClassName.create .Values.podPriorityClassName.name }}
  pod_priority_class_name: {{ include "postgres-pod.priorityClassName" . }}
{{- end }}
  pod_service_account_name: {{ include "postgres-pod.serviceAccountName" . }}
{{- include "flattenValuesForConfigMap" .Values.configGeneral | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configUsers | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configMajorVersionUpgrade | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configKubernetes | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configTimeouts | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configLoadBalancer | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configAwsOrGcp | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configLogicalBackup | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configDebug | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configLoggingRestApi | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configTeamsApi | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configConnectionPooler | indent 2 }}
{{- include "flattenValuesForConfigMap" .Values.configPatroni | indent 2 }}
{{- end }}


================================================
File: charts/postgres-operator/templates/deployment.yaml
================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  name: {{ template "postgres-operator.fullname" . }}
  namespace: {{ .Release.Namespace }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      annotations:
      {{- if eq .Values.configTarget "ConfigMap" }}
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
      {{- else }}
        checksum/config: {{ include (print $.Template.BasePath "/operatorconfiguration.yaml") . | sha256sum }}
      {{- end }}
    {{- if .Values.podAnnotations }}
{{ toYaml .Values.podAnnotations | indent 8 }}
    {{- end }}
      labels:
        app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
      {{- if .Values.podLabels }}
{{ toYaml .Values.podLabels | indent 8 }}
      {{- end }}
    spec:
      serviceAccountName: {{ include "postgres-operator.serviceAccountName" . }}
      containers:
      - name: {{ .Chart.Name }}
        image: "{{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        env:
      {{- if .Values.enableJsonLogging }}
        - name: ENABLE_JSON_LOGGING
          value: "true"
      {{- end }}
      {{- if eq .Values.configTarget "ConfigMap" }}
        - name: CONFIG_MAP_NAME
          value: {{ template "postgres-operator.fullname" . }}
      {{- else }}
        - name: POSTGRES_OPERATOR_CONFIGURATION_OBJECT
          value: {{ template "postgres-operator.fullname" . }}
      {{- end }}
      {{- if .Values.controllerID.create }}
        - name: CONTROLLER_ID
          value: {{ template "postgres-operator.controllerID" . }}
      {{- end }}
      {{- if .Values.extraEnvs }}
{{ toYaml .Values.extraEnvs | indent 8 }}
      {{- end }}
        resources:
{{ toYaml .Values.resources | indent 10 }}
        securityContext:
{{ toYaml .Values.securityContext | indent 10 }}
        {{- if .Values.readinessProbe }}
        readinessProbe:
          httpGet:
            path: /readyz
            port: {{ .Values.configLoggingRestApi.api_port }}
          initialDelaySeconds: {{ .Values.readinessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.readinessProbe.periodSeconds }}
        {{- end }}
      {{- if .Values.imagePullSecrets }}
      imagePullSecrets:
{{ toYaml .Values.imagePullSecrets | indent 8 }}
      {{- end }}
      affinity:
{{ toYaml .Values.affinity | indent 8 }}
      nodeSelector:
{{ toYaml .Values.nodeSelector | indent 8 }}
      tolerations:
{{ toYaml .Values.tolerations | indent 8 }}
    {{- if .Values.priorityClassName }}
      priorityClassName: {{ .Values.priorityClassName }}
    {{- end }}


================================================
File: charts/postgres-operator/templates/operatorconfiguration.yaml
================================================
{{- if eq .Values.configTarget "OperatorConfigurationCRD" }}
apiVersion: "acid.zalan.do/v1"
kind: OperatorConfiguration
metadata:
  name: {{ template "postgres-operator.fullname" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
configuration:
{{ tpl (toYaml .Values.configGeneral) . | indent 2 }}
  users:
{{ tpl (toYaml .Values.configUsers) . | indent 4 }}
  major_version_upgrade:
{{ tpl (toYaml .Values.configMajorVersionUpgrade) . | indent 4 }}
  kubernetes:
    {{- if .Values.podPriorityClassName.name }}
    pod_priority_class_name: {{ .Values.podPriorityClassName.name }}
    {{- end }}
    pod_service_account_name: {{ include "postgres-pod.serviceAccountName" . }}
    oauth_token_secret_name: {{ template "postgres-operator.fullname" . }}
{{ tpl (toYaml .Values.configKubernetes) . | indent 4 }}
  postgres_pod_resources:
{{ tpl (toYaml .Values.configPostgresPodResources) . | indent 4 }}
  timeouts:
{{ tpl (toYaml .Values.configTimeouts) . | indent 4 }}
  load_balancer:
{{ tpl (toYaml .Values.configLoadBalancer) . | indent 4 }}
  aws_or_gcp:
{{ tpl (toYaml .Values.configAwsOrGcp) . | indent 4 }}
  logical_backup:
{{ tpl (toYaml .Values.configLogicalBackup) . | indent 4 }}
  debug:
{{ tpl (toYaml .Values.configDebug) . | indent 4 }}
  teams_api:
{{ tpl (toYaml .Values.configTeamsApi) . | indent 4 }}
  logging_rest_api:
{{ tpl (toYaml .Values.configLoggingRestApi) . | indent 4 }}
  connection_pooler:
{{ tpl (toYaml .Values.configConnectionPooler) . | indent 4 }}
  patroni:
{{ tpl (toYaml .Values.configPatroni) . | indent 4 }}
{{- end }}


================================================
File: charts/postgres-operator/templates/postgres-pod-priority-class.yaml
================================================
{{- if .Values.podPriorityClassName.create }}
apiVersion: scheduling.k8s.io/v1
description: 'Use only for databases controlled by Postgres operator'
kind: PriorityClass
metadata:
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  name: {{ include "postgres-pod.priorityClassName" . }}
  namespace: {{ .Release.Namespace }}
preemptionPolicy: PreemptLowerPriority
globalDefault: false
value: {{ .Values.podPriorityClassName.priority }}
{{- end }}


================================================
File: charts/postgres-operator/templates/service.yaml
================================================
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  name: {{ template "postgres-operator.fullname" . }}
  namespace: {{ .Release.Namespace }}
spec:
  type: ClusterIP
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}


================================================
File: charts/postgres-operator/templates/serviceaccount.yaml
================================================
{{ if .Values.serviceAccount.create }}
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ include "postgres-operator.serviceAccountName" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
{{ end }}


================================================
File: charts/postgres-operator/templates/user-facing-clusterroles.yaml
================================================
{{ if .Values.rbac.createAggregateClusterRoles }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  name: {{ template "postgres-operator.fullname" . }}:users:admin
rules:
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  - postgresqls/status
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  name: {{ template "postgres-operator.fullname" . }}:users:edit
rules:
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  verbs:
  - create
  - update
  - patch
  - delete

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-view: "true"
    app.kubernetes.io/name: {{ template "postgres-operator.name" . }}
    helm.sh/chart: {{ template "postgres-operator.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  name: {{ template "postgres-operator.fullname" . }}:users:view
rules:
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  - postgresqls/status
  verbs:
  - get
  - list
  - watch
{{ end }}


================================================
File: charts/postgres-operator-ui/Chart.yaml
================================================
apiVersion: v2
name: postgres-operator-ui
version: 1.14.0
appVersion: 1.14.0
home: https://github.com/zalando/postgres-operator
description: Postgres Operator UI provides a graphical interface for a convenient database-as-a-service user experience
keywords:
- postgres
- operator
- ui
- cloud-native
- patroni
- spilo
maintainers:
- name: Zalando
  email: opensource@zalando.de
sources:
- https://github.com/zalando/postgres-operator
engine: gotpl


================================================
File: charts/postgres-operator-ui/index.yaml
================================================
apiVersion: v1
entries:
  postgres-operator-ui:
  - apiVersion: v2
    appVersion: 1.14.0
    created: "2024-12-23T11:26:07.721761867+01:00"
    description: Postgres Operator UI provides a graphical interface for a convenient
      database-as-a-service user experience
    digest: e87ed898079a852957a67a4caf3fbd27b9098e413f5d961b7a771a6ae8b3e17c
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - ui
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator-ui
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-ui-1.14.0.tgz
    version: 1.14.0
  - apiVersion: v2
    appVersion: 1.13.0
    created: "2024-12-23T11:26:07.719409282+01:00"
    description: Postgres Operator UI provides a graphical interface for a convenient
      database-as-a-service user experience
    digest: e0444e516b50f82002d1a733527813c51759a627cefdd1005cea73659f824ea8
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - ui
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator-ui
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-ui-1.13.0.tgz
    version: 1.13.0
  - apiVersion: v2
    appVersion: 1.12.2
    created: "2024-12-23T11:26:07.717202918+01:00"
    description: Postgres Operator UI provides a graphical interface for a convenient
      database-as-a-service user experience
    digest: cbcef400c23ccece27d97369ad629278265c013e0a45c0b7f33e7568a082fedd
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - ui
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator-ui
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-ui-1.12.2.tgz
    version: 1.12.2
  - apiVersion: v2
    appVersion: 1.11.0
    created: "2024-12-23T11:26:07.714792146+01:00"
    description: Postgres Operator UI provides a graphical interface for a convenient
      database-as-a-service user experience
    digest: a45f2284045c2a9a79750a36997386444f39b01ac722b17c84b431457577a3a2
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - ui
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator-ui
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-ui-1.11.0.tgz
    version: 1.11.0
  - apiVersion: v2
    appVersion: 1.10.1
    created: "2024-12-23T11:26:07.712194397+01:00"
    description: Postgres Operator UI provides a graphical interface for a convenient
      database-as-a-service user experience
    digest: 2e5e7a82aebee519ec57c6243eb8735124aa4585a3a19c66ffd69638fbeb11ce
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - ui
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator-ui
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-ui-1.10.1.tgz
    version: 1.10.1
  - apiVersion: v2
    appVersion: 1.9.0
    created: "2024-12-23T11:26:07.723891496+01:00"
    description: Postgres Operator UI provides a graphical interface for a convenient
      database-as-a-service user experience
    digest: df434af6c8b697fe0631017ecc25e3c79e125361ae6622347cea41a545153bdc
    home: https://github.com/zalando/postgres-operator
    keywords:
    - postgres
    - operator
    - ui
    - cloud-native
    - patroni
    - spilo
    maintainers:
    - email: opensource@zalando.de
      name: Zalando
    name: postgres-operator-ui
    sources:
    - https://github.com/zalando/postgres-operator
    urls:
    - postgres-operator-ui-1.9.0.tgz
    version: 1.9.0
generated: "2024-12-23T11:26:07.709192608+01:00"


================================================
File: charts/postgres-operator-ui/values.yaml
================================================
# Default values for postgres-operator-ui.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

# configure ui image
image:
  registry: ghcr.io
  repository: zalando/postgres-operator-ui
  tag: v1.14.0
  pullPolicy: "IfNotPresent"

# Optionally specify an array of imagePullSecrets.
# Secrets must be manually created in the namespace.
# ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
# imagePullSecrets:
#   - name:

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

# configure UI pod resources
resources:
  limits:
    cpu: 200m
    memory: 200Mi
  requests:
    cpu: 100m
    memory: 100Mi

# configure UI ENVs
envs:
  # IMPORTANT: While operator chart and UI chart are independent, this is the interface between
  # UI and operator API. Insert the service name of the operator API here!
  appUrl: "http://localhost:8081"
  operatorApiUrl: "http://postgres-operator:8080"
  operatorClusterNameLabel: "cluster-name"
  resourcesVisible: "False"
  # Set to "*" to allow viewing/creation of clusters in all namespaces
  targetNamespace: "default"
  teams:
    - "acid"

# Extra pod annotations
podAnnotations:
  {}

# configure extra UI ENVs
# Extra ENVs are writen in kubenertes format and added "as is" to the pod's env variables
# https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/
# https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#environment-variables
# UI specific env variables can be found here: https://github.com/zalando/postgres-operator/blob/master/ui/operator_ui/main.py
extraEnvs:
  []
  # Exemple of settings to make snapshot view working in the ui when using AWS
  # - name: WALE_S3_ENDPOINT
  #   value: https+path://s3.us-east-1.amazonaws.com:443
  # - name: SPILO_S3_BACKUP_PREFIX
  #   value: spilo/
  # - name: AWS_ACCESS_KEY_ID
  #   valueFrom:
  #     secretKeyRef:
  #       name: <postgres operator secret with AWS token>
  #       key: AWS_ACCESS_KEY_ID
  # - name: AWS_SECRET_ACCESS_KEY
  #   valueFrom:
  #     secretKeyRef:
  #       name: <postgres operator secret with AWS token>
  #       key: AWS_SECRET_ACCESS_KEY
  # - name: AWS_DEFAULT_REGION
  #   valueFrom:
  #     secretKeyRef:
  #       name: <postgres operator secret with AWS token>
  #       key: AWS_DEFAULT_REGION
  # - name: SPILO_S3_BACKUP_BUCKET
  #   value: <s3 bucket used by the operator>
  # - name: "USE_AWS_INSTANCE_PROFILE"
  #   value: "true"

# configure UI service
service:
  type: "ClusterIP"
  port: "80"
  # If the type of the service is NodePort a port can be specified using the nodePort field
  # If the nodePort field is not specified, or if it has no value, then a random port is used
  # nodePort: 32521
  annotations:
    {}

# configure UI ingress. If needed: "enabled: true"
ingress:
  enabled: false
  annotations:
    {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  ingressClassName: ""
  hosts:
    - host: ui.example.org
      paths: ["/"]
  tls: []
  #  - secretName: ui-tls
  #    hosts:
  #      - ui.exmaple.org

# priority class for operator-ui pod
priorityClassName: ""

# Affinity for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

# Node labels for pod assignment
# Ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

# Tolerations for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []


================================================
File: charts/postgres-operator-ui/.helmignore
================================================
# Patterns to ignore when building packages.
# This supports shell glob matching, relative path matching, and
# negation (prefixed with !). Only one pattern per line.
.DS_Store
# Common VCS dirs
.git/
.gitignore
.bzr/
.bzrignore
.hg/
.hgignore
.svn/
# Common backup files
*.swp
*.bak
*.tmp
*~
# Various IDEs
.project
.idea/
*.tmproj
.vscode/


================================================
File: charts/postgres-operator-ui/templates/NOTES.txt
================================================
To verify that postgres-operator has started, run:

  kubectl --namespace={{ .Release.Namespace }} get pods -l "app.kubernetes.io/name={{ template "postgres-operator-ui.name" . }}"

================================================
File: charts/postgres-operator-ui/templates/_helpers.tpl
================================================
{{/* vim: set filetype=mustache: */}}
{{/*
Expand the name of the chart.
*/}}
{{- define "postgres-operator-ui.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" -}}
{{- end -}}

{{/*
Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
If release name contains chart name it will be used as a full name.
*/}}
{{- define "postgres-operator-ui.fullname" -}}
{{- if .Values.fullnameOverride -}}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" -}}
{{- else -}}
{{- $name := default .Chart.Name .Values.nameOverride -}}
{{- if contains $name .Release.Name -}}
{{- .Release.Name | trunc 63 | trimSuffix "-" -}}
{{- else -}}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" -}}
{{- end -}}
{{- end -}}
{{- end -}}

{{/*
Create a service account name.
*/}}
{{- define "postgres-operator-ui.serviceAccountName" -}}
{{ default (include "postgres-operator-ui.fullname" .) .Values.serviceAccount.name }}
{{- end -}}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "postgres-operator-ui.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" -}}
{{- end -}}


================================================
File: charts/postgres-operator-ui/templates/clusterrole.yaml
================================================
{{ if .Values.rbac.create }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ include "postgres-operator-ui.serviceAccountName" . }}
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator-ui.name" . }}
    helm.sh/chart: {{ template "postgres-operator-ui.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
rules:
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
- apiGroups:
  - apps
  resources:
  - deployments
  - statefulsets
  verbs:
  - get
  - list
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
{{ end }}


================================================
File: charts/postgres-operator-ui/templates/clusterrolebinding.yaml
================================================
{{ if .Values.rbac.create }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: {{ include "postgres-operator-ui.serviceAccountName" . }}
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator-ui.name" . }}
    helm.sh/chart: {{ template "postgres-operator-ui.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: {{ include "postgres-operator-ui.serviceAccountName" . }}
subjects:
- kind: ServiceAccount
  name: {{ include "postgres-operator-ui.serviceAccountName" . }}
  namespace: {{ .Release.Namespace }}
{{ end }}


================================================
File: charts/postgres-operator-ui/templates/deployment.yaml
================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator-ui.name" . }}
    helm.sh/chart: {{ template "postgres-operator-ui.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  name: {{ template "postgres-operator-ui.fullname" . }}
  namespace: {{ .Release.Namespace }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ template "postgres-operator-ui.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ template "postgres-operator-ui.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
    spec:
      serviceAccountName: {{ include "postgres-operator-ui.serviceAccountName" . }}
      {{- if .Values.imagePullSecrets }}
      imagePullSecrets:
      {{ toYaml .Values.imagePullSecrets | indent 8 }}
      {{- end }}
      containers:
        - name: "service"
          image: "{{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: 8081
              protocol: "TCP"
          readinessProbe:
            httpGet:
              path: "/health"
              port: 8081
            initialDelaySeconds: 5
            timeoutSeconds: 1
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          env:
            - name: "APP_URL"
              value: {{ .Values.envs.appUrl }}
            - name: "OPERATOR_API_URL"
              value: {{ .Values.envs.operatorApiUrl | quote }}
            - name: "OPERATOR_CLUSTER_NAME_LABEL"
              value: {{ .Values.envs.operatorClusterNameLabel | quote }}
            - name: "RESOURCES_VISIBLE"
              value: {{ .Values.envs.resourcesVisible | quote }}
            - name: "TARGET_NAMESPACE"
              value: {{ .Values.envs.targetNamespace | quote }}
            - name: "TEAMS"
              value: |-
                [
                  {{- range(initial .Values.envs.teams) }}
                  {{ . | quote }},
                  {{- end }}
                  {{ last .Values.envs.teams | quote }}
                ]
            - name: "OPERATOR_UI_CONFIG"
              value: |-
                {
                  "docs_link":"https://postgres-operator.readthedocs.io/en/latest/",
                  "dns_format_string": "{0}.{1}",
                  "databases_visible": true,
                  "master_load_balancer_visible": true,
                  "nat_gateways_visible": false,
                  "replica_load_balancer_visible": true,
                  "resources_visible": true,
                  "users_visible": true,
                  "cost_ebs": 0.0952,
                  "cost_iops": 0.006,
                  "cost_throughput": 0.0476,
                  "cost_core": 0.0575,
                  "cost_memory": 0.014375,
                  "free_iops": 3000,
                  "free_throughput": 125,
                  "limit_iops": 16000,
                  "limit_throughput": 1000,
                  "postgresql_versions": [
                    "17",
                    "16",
                    "15",
                    "14",
                    "13"
                  ]
                }
            {{- if .Values.extraEnvs }}
            {{- .Values.extraEnvs | toYaml | nindent 12 }}
            {{- end }}
      affinity:
{{ toYaml .Values.affinity | indent 8 }}
      nodeSelector:
{{ toYaml .Values.nodeSelector | indent 8 }}
      tolerations:
{{ toYaml .Values.tolerations | indent 8 }}
    {{- if .Values.priorityClassName }}
      priorityClassName: {{ .Values.priorityClassName }}
    {{- end }}


================================================
File: charts/postgres-operator-ui/templates/ingress.yaml
================================================
{{- if .Values.ingress.enabled -}}
{{- $fullName := include "postgres-operator-ui.fullname" . -}}
{{- $svcPort := .Values.service.port -}}

{{- if semverCompare ">=1.19-0" .Capabilities.KubeVersion.GitVersion -}}
apiVersion: networking.k8s.io/v1
{{- else if semverCompare ">=1.14-0" .Capabilities.KubeVersion.GitVersion -}}
apiVersion: networking.k8s.io/v1beta1
{{- else -}}
apiVersion: extensions/v1beta1
{{- end }}
kind: Ingress
metadata:
  name: {{ $fullName }}
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator-ui.name" . }}
    helm.sh/chart: {{ template "postgres-operator-ui.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  {{- with .Values.ingress.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
{{- if .Values.ingress.ingressClassName }}
  ingressClassName: {{ .Values.ingress.ingressClassName }}
{{- end }}
{{- if .Values.ingress.tls }}
  tls:
  {{- range .Values.ingress.tls }}
    - hosts:
      {{- range .hosts }}
        - {{ . | quote }}
      {{- end }}
      secretName: {{ .secretName }}
  {{- end }}
{{- end }}
  rules:
  {{- range .Values.ingress.hosts }}
    - host: {{ .host | quote }}
      http:
        paths:
        {{- range .paths }}
          - path: {{ . }}
            {{ if semverCompare ">=1.19-0" $.Capabilities.KubeVersion.GitVersion -}}
            pathType: Prefix
            backend:
              service:
                name: {{ $fullName }}
                port:
                  number: {{ $svcPort }}
            {{- else -}}
            backend:
              serviceName: {{ $fullName }}
              servicePort: {{ $svcPort }}
            {{- end -}}
        {{- end }}
  {{- end }}
{{- end }}


================================================
File: charts/postgres-operator-ui/templates/service.yaml
================================================
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator-ui.name" . }}
    helm.sh/chart: {{ template "postgres-operator-ui.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  {{- with .Values.service.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
  name: {{ template "postgres-operator-ui.fullname" . }}
  namespace: {{ .Release.Namespace }}
spec:
  ports:
    - port: {{ .Values.service.port }}
      targetPort: 8081
      {{- if and (eq .Values.service.type "NodePort") .Values.service.nodePort }}
      nodePort: {{ .Values.service.nodePort }}
      {{- end }}
      protocol: TCP
  selector:
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/name: {{ template "postgres-operator-ui.name" . }}
  type: {{ .Values.service.type }}




================================================
File: charts/postgres-operator-ui/templates/serviceaccount.yaml
================================================
{{ if .Values.serviceAccount.create }}
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ include "postgres-operator-ui.serviceAccountName" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/name: {{ template "postgres-operator-ui.name" . }}
    helm.sh/chart: {{ template "postgres-operator-ui.chart" . }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
{{ end }}


================================================
File: cmd/main.go
================================================
package main

import (
	"flag"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	log "github.com/sirupsen/logrus"

	"github.com/zalando/postgres-operator/pkg/controller"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
)

var (
	kubeConfigFile string
	outOfCluster   bool
	version        string
	config         spec.ControllerConfig
)

func mustParseDuration(d string) time.Duration {
	duration, err := time.ParseDuration(d)
	if err != nil {
		panic(err)
	}
	return duration
}

func init() {
	flag.StringVar(&kubeConfigFile, "kubeconfig", "", "Path to kubeconfig file with authorization and master location information.")
	flag.BoolVar(&outOfCluster, "outofcluster", false, "Whether the operator runs in- our outside of the Kubernetes cluster.")
	flag.BoolVar(&config.NoDatabaseAccess, "nodatabaseaccess", false, "Disable all access to the database from the operator side.")
	flag.BoolVar(&config.NoTeamsAPI, "noteamsapi", false, "Disable all access to the teams API")
	flag.IntVar(&config.KubeQPS, "kubeqps", 10, "Kubernetes api requests per second.")
	flag.IntVar(&config.KubeBurst, "kubeburst", 20, "Kubernetes api requests burst limit.")
	flag.Parse()

	config.EnableJsonLogging = os.Getenv("ENABLE_JSON_LOGGING") == "true"

	configMapRawName := os.Getenv("CONFIG_MAP_NAME")
	if configMapRawName != "" {

		err := config.ConfigMapName.Decode(configMapRawName)
		if err != nil {
			log.Fatalf("incorrect config map name: %v", configMapRawName)
		}

		log.Printf("Fully qualified configmap name: %v", config.ConfigMapName)

	}
	if crdInterval := os.Getenv("CRD_READY_WAIT_INTERVAL"); crdInterval != "" {
		config.CRDReadyWaitInterval = mustParseDuration(crdInterval)
	} else {
		config.CRDReadyWaitInterval = 4 * time.Second
	}

	if crdTimeout := os.Getenv("CRD_READY_WAIT_TIMEOUT"); crdTimeout != "" {
		config.CRDReadyWaitTimeout = mustParseDuration(crdTimeout)
	} else {
		config.CRDReadyWaitTimeout = 30 * time.Second
	}
}

func main() {
	var err error

	if config.EnableJsonLogging {
		log.SetFormatter(&log.JSONFormatter{})
	}
	log.SetOutput(os.Stdout)
	log.Printf("Spilo operator %s\n", version)

	sigs := make(chan os.Signal, 1)
	stop := make(chan struct{})
	signal.Notify(sigs, os.Interrupt, syscall.SIGTERM) // Push signals into channel

	wg := &sync.WaitGroup{} // Goroutines can add themselves to this to be waited on

	config.RestConfig, err = k8sutil.RestConfig(kubeConfigFile, outOfCluster)
	if err != nil {
		log.Fatalf("couldn't get REST config: %v", err)
	}

	config.RestConfig.QPS = float32(config.KubeQPS)
	config.RestConfig.Burst = config.KubeBurst

	c := controller.NewController(&config, "")

	c.Run(stop, wg)

	sig := <-sigs
	log.Printf("Shutting down... %+v", sig)

	close(stop) // Tell goroutines to stop themselves
	wg.Wait()   // Wait for all to be stopped
}


================================================
File: docker/DebugDockerfile
================================================
FROM golang:1.23-alpine
LABEL maintainer="Team ACID @ Zalando <team-acid@zalando.de>"

# We need root certificates to deal with teams api over https
RUN apk -U add --no-cache ca-certificates delve

COPY build/* /

RUN addgroup -g 1000 pgo
RUN adduser -D -u 1000 -G pgo -g 'Postgres Operator' pgo

USER pgo:pgo
RUN ls -l /

CMD ["/dlv", "--listen=:7777", "--headless=true", "--api-version=2", "exec", "/postgres-operator"]


================================================
File: docker/Dockerfile
================================================
ARG BASE_IMAGE=registry.opensource.zalan.do/library/alpine-3:latest
FROM golang:1.23-alpine AS builder
ARG VERSION=latest

COPY  . /go/src/github.com/zalando/postgres-operator
WORKDIR /go/src/github.com/zalando/postgres-operator

RUN GO111MODULE=on go mod vendor \
    && CGO_ENABLED=0 go build -o build/postgres-operator -v -ldflags "-X=main.version=${VERSION}" cmd/main.go

FROM ${BASE_IMAGE}
LABEL maintainer="Team ACID @ Zalando <team-acid@zalando.de>"
LABEL org.opencontainers.image.source="https://github.com/zalando/postgres-operator"

# We need root certificates to deal with teams api over https
RUN apk -U upgrade --no-cache \
    && apk add --no-cache curl ca-certificates

COPY --from=builder /go/src/github.com/zalando/postgres-operator/build/* /

RUN addgroup -g 1000 pgo
RUN adduser -D -u 1000 -G pgo -g 'Postgres Operator' pgo

USER 1000:1000

ENTRYPOINT ["/postgres-operator"]


================================================
File: docker/build_operator.sh
================================================
#!/bin/bash

export DEBIAN_FRONTEND=noninteractive

arch=$(dpkg --print-architecture)

set -ex

# Install dependencies

apt-get update
apt-get install -y wget

(
    cd /tmp
    wget -q "https://storage.googleapis.com/golang/go1.23.4.linux-${arch}.tar.gz" -O go.tar.gz
    tar -xf go.tar.gz
    mv go /usr/local
    ln -s /usr/local/go/bin/go /usr/bin/go
    go version
)

# Build

export PATH="$PATH:$HOME/go/bin"
export GOPATH="$HOME/go"
mkdir -p build

GO111MODULE=on go mod vendor
CGO_ENABLED=0 go build -o build/postgres-operator -v -ldflags "$OPERATOR_LDFLAGS" cmd/main.go


================================================
File: docs/developer.md
================================================
<h1>Developer Guide</h1>

Read this guide if you want to debug the operator, fix bugs or contribute new
features and tests.

## Setting up Go

Postgres Operator is written in Go. Use the [installation instructions](https://golang.org/doc/install#install)
if you don't have Go on your system. You won't be able to compile the operator
with Go older than 1.17. We recommend installing [the latest one](https://golang.org/dl/).

Go projects expect their source code and all the dependencies to be located
under the [GOPATH](https://github.com/golang/go/wiki/GOPATH). Normally, one
would create a directory for the GOPATH (i.e. ~/go) and place the source code
under the ~/go/src sub directories.

Given the schema above, the Postgres Operator source code located at
`github.com/zalando/postgres-operator` should be put at
-`~/go/src/github.com/zalando/postgres-operator`.

```bash
export GOPATH=~/go
mkdir -p ${GOPATH}/src/github.com/zalando/
cd ${GOPATH}/src/github.com/zalando/
git clone https://github.com/zalando/postgres-operator.git
```

## Building the operator

We use [Go Modules](https://github.com/golang/go/wiki/Modules) for handling
dependencies. When using Go below v1.13 you need to explicitly enable Go modules
by setting the `GO111MODULE` environment variable to `on`. The make targets do
this for you, so simply run

```bash
make deps
```

This would take a while to complete. You have to redo `make deps` every time
your dependencies list changes, i.e. after adding a new library dependency.

Build the operator with the `make docker` command. You may define the TAG
variable to assign an explicit tag to your Docker image and the IMAGE to set
the image name. By default, the tag is computed with
`git describe --tags --always --dirty` and the image is
`registry.opensource.zalan.do/acid/postgres-operator`

```bash
export TAG=$(git describe --tags --always --dirty)
make docker
```

Building the operator binary (for testing the out-of-cluster option):

```bash
make
```

The binary will be placed into the build directory.

## Deploying self build image

The fastest way to run and test your Docker image locally is to reuse the Docker
environment from [minikube](https://github.com/kubernetes/minikube/releases)
or use the `load docker-image` from [kind](https://kind.sigs.k8s.io/). The
following steps will get you the Docker image built and deployed.

```bash
# minikube
eval $(minikube docker-env)
make docker

# kind
make docker
kind load docker-image registry.opensource.zalan.do/acid/postgres-operator:${TAG} --name <kind-cluster-name>
```

Then create a new Postgres Operator deployment.

### Deploying manually with manifests and kubectl

You can reuse the provided manifest but replace the version and tag. Don't forget to also apply
configuration and RBAC manifests first, e.g.:

```bash
kubectl create -f manifests/configmap.yaml
kubectl create -f manifests/operator-service-account-rbac.yaml
sed -e "s/\(image\:.*\:\).*$/\1$TAG/" -e "s/\(imagePullPolicy\:\).*$/\1 Never/" manifests/postgres-operator.yaml | kubectl create  -f -

# check if the operator is coming up
kubectl get pod -l name=postgres-operator
```

### Deploying with Helm chart

Yoy can reuse the provided Helm chart to deploy local operator build with the following command:
```bash
helm install postgres-operator ./charts/postgres-operator --namespace zalando-operator --set image.tag=${TAG} --set image.pullPolicy=Never
```

## Code generation

The operator employs K8s-provided code generation to obtain deep copy methods
and K8s-like APIs for its custom resource definitions, namely the
Postgres CRD and the operator CRD. The usage of the code generation follows
conventions from the K8s community. Relevant scripts live in the `hack`
directory:
* `update-codegen.sh` triggers code generation for the APIs defined in `pkg/apis/acid.zalan.do/`,
* `verify-codegen.sh` checks if the generated code is up-to-date (to be used within CI).

The `/pkg/generated/` contains the resultant code. To make these scripts work,
you may need to `export GOPATH=$(go env GOPATH)`

References for code generation are:
* [Relevant pull request](https://github.com/zalando/postgres-operator/pull/369)
See comments there for minor issues that can sometimes broke the generation process.
* [Code generator source code](https://github.com/kubernetes/code-generator)
* [Code Generation for CustomResources](https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/) - intro post on the topic
* Code generation in [Prometheus](https://github.com/coreos/prometheus-operator) and [etcd](https://github.com/coreos/etcd-operator) operators

To debug the generated API locally, use the
[kubectl proxy](https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/)
and `kubectl --v=8` log level to display contents of HTTP requests (run the
operator itself with `--v=8` to log all REST API requests). To attach a debugger
to the operator, use the `-outofcluster` option to run the operator locally on
the developer's laptop (and not in a docker container).

## Debugging the operator

There is a web interface in the operator to observe its internal state. The
operator listens on port 8080. It is possible to expose it to the
`localhost:8080` by doing:

```bash
kubectl --context minikube port-forward $(kubectl --context minikube get pod -l name=postgres-operator -o jsonpath={.items..metadata.name}) 8080:8080
```

The inner query gets the name of the Postgres Operator pod, and the outer one
enables port forwarding. Afterwards, you can access the operator API with:

```bash
curl --location http://127.0.0.1:8080/$endpoint | jq .
```

The available endpoints are listed below. Note that the worker ID is an integer
from 0 up to 'workers' - 1 (value configured in the operator configuration and
defaults to 4)

* /databases - all databases per cluster
* /workers/all/queue - state of the workers queue (cluster events to process)
* /workers/$id/queue - state of the queue for the worker $id
* /workers/$id/logs - log of the operations performed by a given worker
* /clusters/ - list of teams and clusters known to the operator
* /clusters/$team - list of clusters for the given team
* /clusters/$team/$namespace/$clustername - detailed status of the cluster,
  including the specifications for CRD, master and replica services, endpoints
  and statefulsets, as well as any errors and the worker that cluster is
  assigned to.
* /clusters/$team/$namespace/$clustername/logs/ - logs of all operations
  performed to the cluster so far.
* /clusters/$team/$namespace/$clustername/history/ - history of cluster changes
  triggered by the changes of the manifest (shows the somewhat obscure diff and
  what exactly has triggered the change)

The operator also supports pprof endpoints listed at the
[pprof package](https://golang.org/pkg/net/http/pprof/), such as:

* /debug/pprof/
* /debug/pprof/cmdline
* /debug/pprof/profile
* /debug/pprof/symbol
* /debug/pprof/trace

It's possible to attach a debugger to troubleshoot postgres-operator inside a
Docker container. It's possible with [gdb](https://www.gnu.org/software/gdb/)
and [delve](https://github.com/derekparker/delve). Since the latter one is a
specialized debugger for Go, we will use it as an example. To use it you need:

* Install delve locally

```bash
go get -u github.com/derekparker/delve/cmd/dlv
```

* Add following dependencies to the `Dockerfile`

```
RUN apk --no-cache add go git musl-dev
RUN go get github.com/derekparker/delve/cmd/dlv
```

* Update the `Makefile` to build the project with debugging symbols. For that
  you need to add `gcflags` to a build target for corresponding OS (e.g.
  GNU/Linux)

```
-gcflags "-N -l"
```

* Run `postgres-operator` under the delve. For that you need to replace
  `ENTRYPOINT` with the following `CMD`:

```
CMD ["/root/go/bin/dlv", "--listen=:DLV_PORT", "--headless=true", "--api-version=2", "exec", "/postgres-operator"]
```

* Forward the listening port

```bash
kubectl port-forward POD_NAME DLV_PORT:DLV_PORT
```

* Attach to it

```bash
dlv connect 127.0.0.1:DLV_PORT
```

## Unit tests

Prerequisites:

```bash
make deps
make mocks
```

To run all unit tests, you can simply do:

```bash
go test ./pkg/...
```

In case if you need to debug your unit test, it's possible to use delve:

```bash
dlv test ./pkg/util/retryutil/
Type 'help' for list of commands.
(dlv) c
PASS
```

To test the multi-namespace setup, you can use

```bash
./run_operator_locally.sh --rebuild-operator
```
It will automatically create an `acid-minimal-cluster` in the namespace `test`.
Then you can for example check the Patroni logs:

```bash
kubectl logs acid-minimal-cluster-0
```

## Unit tests with Mocks and K8s Fake API

Whenever possible you should rely on leveraging proper mocks and K8s fake client that allows full fledged testing of K8s objects in your unit tests.

To enable mocks, a code annotation is needed:
[Mock code gen annotation](https://github.com/zalando/postgres-operator/blob/master/pkg/util/volumes/volumes.go#L3)

To generate mocks run:
```bash
make mocks
```

Examples for mocks can be found in:
[Example mock usage](https://github.com/zalando/postgres-operator/blob/master/pkg/cluster/volumes_test.go#L248)

Examples for fake K8s objects can be found in:
[Example fake K8s client usage](https://github.com/zalando/postgres-operator/blob/master/pkg/cluster/volumes_test.go#L166)

## End-to-end tests

The operator provides reference end-to-end (e2e) tests to
ensure various infrastructure parts work smoothly together. The test code is available at `e2e/tests`.
The special `registry.opensource.zalan.do/acid/postgres-operator-e2e-tests-runner` image is used to run the tests. The container mounts the local `e2e/tests` directory at runtime, so whatever you modify in your local copy of the tests will be executed by a test runner. By maintaining a separate test runner image we avoid the need to re-build the e2e test image on every build. 

Each e2e execution tests a Postgres Operator image built from the current git branch. The test
runner creates a new local K8s cluster using [kind](https://kind.sigs.k8s.io/),
utilizes provided manifest examples, and runs e2e tests contained in the `tests`
folder. The K8s API client in the container connects to the `kind` cluster via
the standard Docker `bridge` network. The kind cluster is deleted if tests
finish successfully or on each new run in case it still exists.

End-to-end tests are executed automatically during builds (for more details,
see the [README](https://github.com/zalando/postgres-operator/blob/master/e2e/README.md) in the `e2e` folder):

```bash
make e2e
```

End-to-end tests are written in Python and use `flake8` for code quality.
Please run flake8 [before submitting a PR](http://flake8.pycqa.org/en/latest/user/using-hooks.html).

## Introduce additional configuration parameters

In the case you want to add functionality to the operator that shall be
controlled via the operator configuration there are a few places that need to
be updated. As explained [here](reference/operator_parameters.md), it's possible
to configure the operator either with a ConfigMap or CRD, but currently we aim
to synchronize parameters everywhere.

When choosing a parameter name for a new option in a Postgres cluster manifest,
keep in mind the naming conventions there. We use `camelCase` for manifest
parameters (with exceptions for certain Patroni/Postgres options) and
`snake_case` variables in the configuration. Only introduce new manifest
variables if you feel a per-cluster configuration is necessary.

Note: If one option is defined in the operator configuration and in the cluster
[manifest](https://github.com/zalando/postgres-operator/blob/master/manifests/complete-postgres-manifest.yaml), the latter takes
precedence.

### Go code

Update the following Go files that obtain the configuration parameter from the
manifest files:
* [operator_configuration_type.go](https://github.com/zalando/postgres-operator/blob/master/pkg/apis/acid.zalan.do/v1/operator_configuration_type.go)
* [operator_config.go](https://github.com/zalando/postgres-operator/blob/master/pkg/controller/operator_config.go)
* [config.go](https://github.com/zalando/postgres-operator/blob/master/pkg/util/config/config.go)

Postgres manifest parameters are defined in the [api package](https://github.com/zalando/postgres-operator/blob/master/pkg/apis/acid.zalan.do/v1/postgresql_type.go).
The operator behavior has to be implemented at least in [k8sres.go](https://github.com/zalando/postgres-operator/blob/master/pkg/cluster/k8sres.go).
Validation of CRD parameters is controlled in [crds.go](https://github.com/zalando/postgres-operator/blob/master/pkg/apis/acid.zalan.do/v1/crds.go).
Please, reflect your changes in tests, for example in:
* [config_test.go](https://github.com/zalando/postgres-operator/blob/master/pkg/util/config/config_test.go)
* [k8sres_test.go](https://github.com/zalando/postgres-operator/blob/master/pkg/cluster/k8sres_test.go)
* [util_test.go](https://github.com/zalando/postgres-operator/blob/master/pkg/apis/acid.zalan.do/v1/util_test.go)

### Updating manifest files

For the CRD-based configuration, please update the following files:
* the default [OperatorConfiguration](https://github.com/zalando/postgres-operator/blob/master/manifests/postgresql-operator-default-configuration.yaml)
* the CRD's [validation](https://github.com/zalando/postgres-operator/blob/master/manifests/operatorconfiguration.crd.yaml)
* the CRD's validation in the [Helm chart](https://github.com/zalando/postgres-operator/blob/master/charts/postgres-operator/crds/operatorconfigurations.yaml)

Add new options also to the Helm chart's [values file](https://github.com/zalando/postgres-operator/blob/master/charts/postgres-operator/values.yaml) file.
It follows the OperatorConfiguration CRD layout. Nested values will be flattened for the ConfigMap.
Last but no least, update the [ConfigMap](https://github.com/zalando/postgres-operator/blob/master/manifests/configmap.yaml) manifest example as well.

### Updating documentation

Finally, add a section for each new configuration option and/or cluster manifest
parameter in the reference documents:
* [config reference](reference/operator_parameters.md)
* [manifest reference](reference/cluster_manifest.md)

It also helps users to explain new features with examples in the
[administrator docs](administrator.md).


================================================
File: docs/index.md
================================================
<h1>Concepts</h1>

The Postgres [operator](https://coreos.com/blog/introducing-operators.html)
manages PostgreSQL clusters on Kubernetes (K8s):

1. The operator watches additions, updates, and deletions of PostgreSQL cluster
   manifests and changes the running clusters accordingly.  For example, when a
   user submits a new manifest, the operator fetches that manifest and spawns a
   new Postgres cluster along with all necessary entities such as K8s
   StatefulSets and Postgres roles.  See this
   [Postgres cluster manifest](https://github.com/zalando/postgres-operator/blob/master/manifests/complete-postgres-manifest.yaml)
   for settings that a manifest may contain.

2. The operator also watches updates to [its own configuration](https://github.com/zalando/postgres-operator/blob/master/manifests/configmap.yaml)
   and alters running Postgres clusters if necessary.  For instance, if the
   Docker image in a pod is changed, the operator carries out the rolling
   update, which means it re-spawns pods of each managed StatefulSet one-by-one
   with the new Docker image.

3. Finally, the operator periodically synchronizes the actual state of each
   Postgres cluster with the desired state defined in the cluster's manifest.

4. The operator aims to be hands free as configuration works only via manifests.
   This enables easy integration in automated deploy pipelines with no access to
   K8s directly.

## Scope

The scope of the Postgres Operator is on provisioning, modifying configuration
and cleaning up Postgres clusters that use Patroni, basically to make it easy
and convenient to run Patroni based clusters on K8s. The provisioning
and modifying includes K8s resources on one side but also e.g. database
and role provisioning once the cluster is up and running. We try to leave as
much work as possible to K8s and to Patroni where it fits, especially
the cluster bootstrap and high availability. The operator is however involved
in some overarching orchestration, like rolling updates to improve the user
experience.

Monitoring or tuning Postgres is not in scope of the operator in the current
state. However, with globally configurable sidecars we provide enough
flexibility to complement it with other tools like [ZMON](https://opensource.zalando.com/zmon/),
[Prometheus](https://prometheus.io/) or more Postgres specific options.


## Overview of involved entities

Here is a diagram, that summarizes what would be created by the operator, when a
new Postgres cluster CRD is submitted:

![postgresql-operator](diagrams/operator.png "K8s resources, created by operator")

This picture is not complete without an overview of what is inside a single
cluster pod, so let's zoom in:

![pod](diagrams/pod.png "Database pod components")

These two diagrams should help you to understand the basics of what kind of
functionality the operator provides.

## Status

This project is currently in active development. It is however already
[used internally by Zalando](https://jobs.zalando.com/tech/blog/postgresql-in-a-time-of-kubernetes/)
in order to run Postgres clusters on K8s in larger numbers for staging
environments and a growing number of production clusters. In this environment
the operator is deployed to multiple K8s clusters, where users deploy
manifests via our CI/CD infrastructure or rely on a slim user interface to
create manifests.

Please, report any issues discovered to https://github.com/zalando/postgres-operator/issues.

## Talks

- "Watching after your PostGIS herd" talk by Felix Kunde, FOSS4G 2021: [video](https://www.youtube.com/watch?v=T96FvjSv98A) | [slides](https://docs.google.com/presentation/d/1IICz2RsjNAcosKVGFna7io-65T2zBbGcBHFFtca24cc/edit?usp=sharing)

- "PostgreSQL on K8S at Zalando: Two years in production" talk by Alexander Kukushkin, FOSSDEM 2020: [video](https://fosdem.org/2020/schedule/event/postgresql_postgresql_on_k8s_at_zalando_two_years_in_production/) | [slides](https://fosdem.org/2020/schedule/event/postgresql_postgresql_on_k8s_at_zalando_two_years_in_production/attachments/slides/3883/export/events/attachments/postgresql_postgresql_on_k8s_at_zalando_two_years_in_production/slides/3883/PostgreSQL_on_K8s_at_Zalando_Two_years_in_production.pdf)

- "Postgres as a Service at Zalando" talk by Jan Mußler, DevOpsDays Poznań 2019: [video](https://www.youtube.com/watch?v=FiWS5m72XI8)

- "Building your own PostgreSQL-as-a-Service on Kubernetes" talk by Alexander Kukushkin, KubeCon NA 2018: [video](https://www.youtube.com/watch?v=G8MnpkbhClc) | [slides](https://static.sched.com/hosted_files/kccna18/1d/Building%20your%20own%20PostgreSQL-as-a-Service%20on%20Kubernetes.pdf)

- "PostgreSQL and Kubernetes: DBaaS without a vendor-lock" talk by Oleksii Kliukin, PostgreSQL Sessions 2018: [video](https://www.youtube.com/watch?v=q26U2rQcqMw) | [slides](https://speakerdeck.com/alexeyklyukin/postgresql-and-kubernetes-dbaas-without-a-vendor-lock)

- "PostgreSQL High Availability on Kubernetes with Patroni" talk by Oleksii Kliukin, Atmosphere 2018: [video](https://www.youtube.com/watch?v=cFlwQOPPkeg) | [slides](https://speakerdeck.com/alexeyklyukin/postgresql-high-availability-on-kubernetes-with-patroni)

- "Blue elephant on-demand: Postgres + Kubernetes" talk by Oleksii Kliukin and Jan Mussler, FOSDEM 2018: [video](https://fosdem.org/2018/schedule/event/blue_elephant_on_demand_postgres_kubernetes/) | [slides (pdf)](https://www.postgresql.eu/events/fosdem2018/sessions/session/1735/slides/59/FOSDEM%202018_%20Blue_Elephant_On_Demand.pdf)

- "Kube-Native Postgres" talk by Josh Berkus, KubeCon 2017: [video](https://www.youtube.com/watch?v=Zn1vd7sQ_bc)

## Posts

- Series of blog posts on how to use the Zalando Operator, configure backups and use etcd as DCS by [thedatabaseme](https://thedatabaseme.de/tag/zalando-operator/), Mar. 2022-23.

- "Zalando Postgres Operator in Production: the way of Helm" by Zangir Kapishov on [medium](https://medium.com/@zkapishov/zalando-postgres-operator-in-production-the-way-of-helm-ccfd639ccb2d), Jan. 2023.

- "Chaos testing of a Postgres cluster managed by the Zalando Postgres Operator" by Nikolay Sivko on [coroot](https://coroot.com/blog/chaos-testing-zalando-postgres-operator), Aug. 2022.

- "Getting started with the Zalando Operator for PostgreSQL" by Daniel Westermann on [dbi services blog](https://www.dbi-services.com/blog/getting-started-with-the-zalando-operator-for-postgresql/), Mar. 2021.

- "Our experience with Postgres Operator for Kubernetes by Zalando" by Nikolay Bogdanov on [Palark blog](https://blog.palark.com/our-experience-with-postgres-operator-for-kubernetes-by-zalando/), Feb. 2021.

- "How to set up continuous backups and monitoring" by Pål Kristensen on [GitHub](https://github.com/zalando/postgres-operator/issues/858#issuecomment-608136253), Mar. 2020.

- "Postgres on Kubernetes with the Zalando operator" by Vito Botta on [has_many :code](https://vitobotta.com/2020/02/05/postgres-kubernetes-zalando-operator/), Feb. 2020.

- "Running PostgreSQL in Google Kubernetes Engine" by Kenneth Rørvik on [Repill Linpro blog](https://www.redpill-linpro.com/techblog/2019/09/28/postgres-in-kubernetes.html), Sep. 2019.

- "Zalando Postgres Operator: One Year Later" by Sergey Dudoladov on [Open Source Zalando](https://opensource.zalando.com/blog/2018/11/postgres-operator/), Nov. 2018


================================================
File: docs/operator-ui.md
================================================
<h1>Postgres Operator UI</h1>

The Postgres Operator UI provides a graphical interface for a convenient
database-as-a-service user experience. Once the operator is set up by database
and/or Kubernetes (K8s) admins it's very easy for other teams to create, clone,
watch, edit and delete their own Postgres clusters. Information on the setup
and technical details can be found in the [admin docs](administrator.md#setting-up-the-postgres-operator-ui).

## Create a new cluster

In the top menu select the "New cluster" option and adjust the values in the
text fields. The cluster name is composed of the team plus given name. Among the
available options are [enabling load balancers](administrator.md#load-balancers-and-allowed-ip-ranges),
[volume size](user.md#increase-volume-size),
[users and databases](user.md#manifest-roles) and
[pod resources](cluster-manifest.md#postgres-container-resources).

![pgui-new-cluster](diagrams/pgui-new-cluster.png "Create a new cluster")

On the left side you will see a preview of the Postgres cluster manifest which
is applied when clicking on the green "Create cluster" button.

## Cluster starting up

After the manifest is applied to K8s the Postgres Operator will create all
necessary resources. The progress of this process can nicely be followed in UI
status page.

![pgui-cluster-startup](diagrams/pgui-cluster-startup.png "Cluster starting up")

![pgui-waiting-for-master](diagrams/pgui-waiting-for-master.png "Waiting for master pod")

Usually, the startup should only take up to 1 minute. If you feel the process
got stuck click on the "Logs" button to inspect the operator logs. If the logs
look fine, but the UI seems to got stuck, check if you are have configured the
same [cluster name label](https://github.com/zalando/postgres-operator/blob/master/ui/manifests/deployment.yaml#L45) like for the
[operator](https://github.com/zalando/postgres-operator/blob/master/manifests/configmap.yaml#L13).

From the "Status" field in the top menu you can also retrieve the logs and queue
of each worker the operator is using. The number of concurrent workers can be
[configured](reference/operator_parameters.md#general).

![pgui-operator-logs](diagrams/pgui-operator-logs.png "Checking operator logs")

Once the startup has finished you will see the cluster address path. When load
balancers are enabled the listed path can be used as the host name when
connecting to PostgreSQL. But, make sure your IP is within the specified
`allowedSourceRanges`.

![pgui-finished-setup](diagrams/pgui-finished-setup.png "Status page of ready cluster")

## Update and delete clusters

Created clusters are listed under the menu "PostgreSQL clusters". You can get
back to cluster's status page via the "Status" button. From both menus you can
choose to edit the manifest, [clone](user.md#how-to-clone-an-existing-postgresql-cluster)
or delete a cluster.

![pgui-cluster-list](diagrams/pgui-cluster-list.png "List of PostgreSQL clusters")

Note, that not all [manifest options](reference/cluster_manifest.md) are yet
supported in the UI. If you try to add them in the editor view it won't have an
effect. Use `kubectl` commands instead. The displayed manifest on the left side
will also show parameters patched that way.

When deleting a cluster you are asked to type in its namespace and name to
confirm the action.

![pgui-delete-cluster](diagrams/pgui-delete-cluster.png "Confirm cluster deletion")


================================================
File: docs/quickstart.md
================================================
<h1>Quickstart</h1>

This guide aims to give you a quick look and feel for using the Postgres
Operator on a local Kubernetes environment.

## Prerequisites

Since the Postgres Operator is designed for the Kubernetes (K8s) framework,
hence set it up first. For local tests we recommend to use one of the following
solutions:

* [minikube](https://github.com/kubernetes/minikube/releases), which creates a
  single-node K8s cluster inside a VM (requires KVM or VirtualBox),
* [kind](https://kind.sigs.k8s.io/) and [k3d](https://k3d.io), which allows creating multi-nodes K8s
  clusters running on Docker (requires Docker)

To interact with the K8s infrastructure install its CLI runtime [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-via-curl).

This quickstart assumes that you have started minikube or created a local kind
cluster. Note that you can also use built-in K8s support in the Docker Desktop
for Mac to follow the steps of this tutorial. You would have to replace
`minikube start` and `minikube delete` with your launch actions for the Docker
built-in K8s support.

## Configuration Options

Configuring the Postgres Operator is only possible before deploying a new
Postgres cluster. This can work in two ways: via a ConfigMap or a custom
`OperatorConfiguration` object. More details on configuration can be found
[here](reference/operator_parameters.md).

## Deployment options

The Postgres Operator can be deployed in the following ways:

* Manual deployment
* Kustomization
* Helm chart

### Manual deployment setup on Kubernetes

The Postgres Operator can be installed simply by applying yaml manifests. Note,
we provide the `/manifests` directory as an example only; you should consider
adjusting the manifests to your K8s environment (e.g. namespaces).

```bash
# First, clone the repository and change to the directory
git clone https://github.com/zalando/postgres-operator.git
cd postgres-operator

# apply the manifests in the following order
kubectl create -f manifests/configmap.yaml  # configuration
kubectl create -f manifests/operator-service-account-rbac.yaml  # identity and permissions
kubectl create -f manifests/postgres-operator.yaml  # deployment
kubectl create -f manifests/api-service.yaml  # operator API to be used by UI
```

There is a [Kustomization](https://github.com/kubernetes-sigs/kustomize)
manifest that [combines the mentioned resources](https://github.com/zalando/postgres-operator/blob/master/manifests/kustomization.yaml)
(except for the CRD) - it can be used with kubectl 1.14 or newer as easy as:

```bash
kubectl apply -k github.com/zalando/postgres-operator/manifests
```

For convenience, we have automated starting the operator with minikube using the
`run_operator_locally` script. It applies the [`acid-minimal-cluster`](https://github.com/zalando/postgres-operator/blob/master/manifests/minimal-postgres-manifest.yaml).
manifest.

```bash
./run_operator_locally.sh
```

### Manual deployment setup on OpenShift

To install the Postgres Operator in OpenShift you have to change the config
parameter `kubernetes_use_configmaps` to `"true"`. Otherwise, the operator
and Patroni will store leader and config keys in `Endpoints` that are not
supported in OpenShift. This requires also a slightly different set of rules
for the `postgres-operator` and `postgres-pod` cluster roles.

```bash
oc create -f manifests/operator-service-account-rbac-openshift.yaml
```

### Helm chart

Alternatively, the operator can be installed by using the provided
[Helm](https://helm.sh/) chart which saves you the manual steps. The charts
for both the Postgres Operator and its UI are hosted via the `gh-pages` branch.
They only work only with Helm v3. Helm v2 support was dropped with v1.8.0.

```bash
# add repo for postgres-operator
helm repo add postgres-operator-charts https://opensource.zalando.com/postgres-operator/charts/postgres-operator

# install the postgres-operator
helm install postgres-operator postgres-operator-charts/postgres-operator

# add repo for postgres-operator-ui
helm repo add postgres-operator-ui-charts https://opensource.zalando.com/postgres-operator/charts/postgres-operator-ui

# install the postgres-operator-ui
helm install postgres-operator-ui postgres-operator-ui-charts/postgres-operator-ui
```

## Check if Postgres Operator is running

Starting the operator may take a few seconds. Check if the operator pod is
running before applying a Postgres cluster manifest.

```bash
# if you've created the operator using yaml manifests
kubectl get pod -l name=postgres-operator

# if you've created the operator using helm chart
kubectl get pod -l app.kubernetes.io/name=postgres-operator
```

If the operator doesn't get into `Running` state, either check the latest K8s
events of the deployment or pod with `kubectl describe` or inspect the operator
logs:

```bash
kubectl logs "$(kubectl get pod -l name=postgres-operator --output='name')"
```

## Deploy the operator UI

In the following paragraphs we describe how to access and manage PostgreSQL
clusters from the command line with kubectl. But it can also be done from the
browser-based [Postgres Operator UI](operator-ui.md). Before deploying the UI
make sure the operator is running and its REST API is reachable through a
[K8s service](https://github.com/zalando/postgres-operator/blob/master/manifests/api-service.yaml). The URL to this API must be
configured in the [deployment manifest](https://github.com/zalando/postgres-operator/blob/master/ui/manifests/deployment.yaml#L43)
of the UI.

To deploy the UI simply apply all its manifests files or use the UI helm chart:

```bash
# manual deployment
kubectl apply -f ui/manifests/

# or kustomization
kubectl apply -k github.com/zalando/postgres-operator/ui/manifests

# or helm chart
helm install postgres-operator-ui ./charts/postgres-operator-ui
```

Like with the operator, check if the UI pod gets into `Running` state:

```bash
# if you've created the operator using yaml manifests
kubectl get pod -l name=postgres-operator-ui

# if you've created the operator using helm chart
kubectl get pod -l app.kubernetes.io/name=postgres-operator-ui
```

You can now access the web interface by port forwarding the UI pod (mind the
label selector) and enter `localhost:8081` in your browser:

```bash
kubectl port-forward svc/postgres-operator-ui 8081:80
```

Available option are explained in detail in the [UI docs](operator-ui.md).

## Create a Postgres cluster

If the operator pod is running it listens to new events regarding `postgresql`
resources. Now, it's time to submit your first Postgres cluster manifest.

```bash
# create a Postgres cluster
kubectl create -f manifests/minimal-postgres-manifest.yaml
```

After the cluster manifest is submitted and passed the validation the operator
will create Service and Endpoint resources and a StatefulSet which spins up new
Pod(s) given the number of instances specified in the manifest. All resources
are named like the cluster. The database pods can be identified by their number
suffix, starting from `-0`. They run the [Spilo](https://github.com/zalando/spilo)
container image by Zalando. As for the services and endpoints, there will be one
for the master pod and another one for all the replicas (`-repl` suffix). Check
if all components are coming up. Use the label `application=spilo` to filter and
list the label `spilo-role` to see who is currently the master.

```bash
# check the deployed cluster
kubectl get postgresql

# check created database pods
kubectl get pods -l application=spilo -L spilo-role

# check created service resources
kubectl get svc -l application=spilo -L spilo-role
```

## Connect to the Postgres cluster via psql

You can create a port-forward on a database pod to connect to Postgres. See the
[user guide](user.md#connect-to-postgresql) for instructions. With minikube it's
also easy to retrieve the connections string from the K8s service that is
pointing to the master pod:

```bash
export HOST_PORT=$(minikube service acid-minimal-cluster --url | sed 's,.*/,,')
export PGHOST=$(echo $HOST_PORT | cut -d: -f 1)
export PGPORT=$(echo $HOST_PORT | cut -d: -f 2)
```

Retrieve the password from the K8s Secret that is created in your cluster.
Non-encrypted connections are rejected by default, so set the SSL mode to
require:

```bash
export PGPASSWORD=$(kubectl get secret postgres.acid-minimal-cluster.credentials.postgresql.acid.zalan.do -o 'jsonpath={.data.password}' | base64 -d)
export PGSSLMODE=require
psql -U postgres
```

## Delete a Postgres cluster

To delete a Postgres cluster simply delete the `postgresql` custom resource.

```bash
kubectl delete postgresql acid-minimal-cluster
```

This should remove the associated StatefulSet, database Pods, Services and
Endpoints. The PersistentVolumes are released and the PodDisruptionBudgets are
deleted. Secrets however are not deleted and backups will remain in place.

When deleting a cluster while it is still starting up or got stuck during that
phase it can [happen](https://github.com/zalando/postgres-operator/issues/551)
that the `postgresql` resource is deleted leaving orphaned components behind.
This can cause troubles when creating a new Postgres cluster. For a fresh setup
you can delete your local minikube or kind cluster and start again.


================================================
File: docs/user.md
================================================
<h1>User Guide</h1>

Learn how to work with the Postgres Operator in a Kubernetes (K8s) environment.

## Create a manifest for a new PostgreSQL cluster

Make sure you have [set up](quickstart.md) the operator. Then you can create a
new Postgres cluster by applying manifest like this [minimal example](https://github.com/zalando/postgres-operator/blob/master/manifests/minimal-postgres-manifest.yaml):

```yaml
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-minimal-cluster
spec:
  teamId: "acid"
  volume:
    size: 1Gi
  numberOfInstances: 2
  users:
    # database owner
    zalando:
    - superuser
    - createdb

    # role for application foo
    foo_user: # or 'foo_user: []'

  #databases: name->owner
  databases:
    foo: zalando
  postgresql:
    version: "17"
```

Once you cloned the Postgres Operator [repository](https://github.com/zalando/postgres-operator)
you can find this example also in the manifests folder:

```bash
kubectl create -f manifests/minimal-postgres-manifest.yaml
```

Make sure, the `spec` section of the manifest contains at least a `teamId`, the
`numberOfInstances` and the `postgresql` object with the `version` specified.
The minimum volume size to run the `postgresql` resource on Elastic Block
Storage (EBS) is `1Gi`.

Note, that when `enable_team_id_clustername_prefix` is set to `true` the name
of the cluster must start with the `teamId` and `-`. At Zalando we use team IDs
(nicknames) to lower chances of duplicate cluster names and colliding entities.
The team ID would also be used to query an API to get all members of a team
and create [database roles](#teams-api-roles) for them. Besides, the maximum
cluster name length is 53 characters.

## Watch pods being created

Check if the database pods are coming up. Use the label `application=spilo` to
filter and list the label `spilo-role` to see when the master is promoted and
replicas get their labels.

```bash
kubectl get pods -l application=spilo -L spilo-role -w
```

The operator also emits K8s events to the Postgresql CRD which can be inspected
in the operator logs or with:

```bash
kubectl describe postgresql acid-minimal-cluster
```

## Connect to PostgreSQL

With a `port-forward` on one of the database pods (e.g. the master) you can
connect to the PostgreSQL database from your machine. Use labels to filter for
the master pod of our test cluster.

```bash
# get name of master pod of acid-minimal-cluster
export PGMASTER=$(kubectl get pods -o jsonpath={.items..metadata.name} -l application=spilo,cluster-name=acid-minimal-cluster,spilo-role=master -n default)

# set up port forward
kubectl port-forward $PGMASTER 6432:5432 -n default
```

Open another CLI and connect to the database using e.g. the psql client.
When connecting with a manifest role like `foo_user` user, read its password
from the K8s secret which was generated when creating `acid-minimal-cluster`.
As non-encrypted connections are rejected by default set SSL mode to `require`:

```bash
export PGPASSWORD=$(kubectl get secret postgres.acid-minimal-cluster.credentials.postgresql.acid.zalan.do -o 'jsonpath={.data.password}' | base64 -d)
export PGSSLMODE=require
psql -U postgres -h localhost -p 6432
```

## Password encryption

Passwords are encrypted with `md5` hash generation by default. However, it is
possible to use the more recent `scram-sha-256` method by changing the
`password_encryption` parameter in the Postgres config. You can define it
directly from the cluster manifest:

```yaml
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-minimal-cluster
spec:
  [...]
  postgresql:
    version: "17"
    parameters:
      password_encryption: scram-sha-256
```

## Defining database roles in the operator

Postgres Operator allows defining roles to be created in the resulting database
cluster. It covers three use-cases:

* `manifest roles`: create application roles specific to the cluster described
in the manifest.
* `infrastructure roles`: create application roles that should be automatically
created on every cluster managed by the operator.
* `teams API roles`: automatically create users for every member of the team
owning the database cluster.

In the next sections, we will cover those use cases in more details. Note, that
the Postgres Operator can also create databases with pre-defined owner, reader
and writer roles which saves you the manual setup. Read more in the next
chapter.

### Manifest roles

Manifest roles are defined directly in the cluster manifest. See
[minimal postgres manifest](https://github.com/zalando/postgres-operator/blob/master/manifests/minimal-postgres-manifest.yaml)
for an example of `zalando` role, defined with `superuser` and `createdb` flags.

Manifest roles are defined as a dictionary, with a role name as a key and a
list of role options as a value. For a role without any options it is best to
supply the empty list `[]`. It is also possible to leave this field empty as in
our example manifests. In certain cases such empty field may be missing later
removed by K8s [due to the `null` value it gets](https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-apply-calculates-differences-and-merges-changes)
(`foobar_user:` is equivalent to `foobar_user: null`).

The operator accepts the following options:  `superuser`, `inherit`, `login`,
`nologin`, `createrole`, `createdb`, `replication`, `bypassrls`.

By default, manifest roles are login roles (aka users), unless `nologin` is
specified explicitly.

The operator automatically generates a password for each manifest role and
places it in the secret named
`{username}.{clustername}.credentials.postgresql.acid.zalan.do` in the
same namespace as the cluster. This way, the application running in the
K8s cluster and connecting to Postgres can obtain the password right from the
secret, without ever sharing it outside of the cluster.

At the moment it is not possible to define membership of the manifest role in
other roles.

To define the secrets for the users in a different namespace than that of the
cluster, one can set `enable_cross_namespace_secret` and declare the namespace
for the secrets in the manifest in the following manner (note, that it has to
be reflected in the `database` section, too),

```yaml
spec:
  users:
    # users with secret in different namespace
    appspace.db_user:
    - createdb
  databases:
    # namespace notation is part of user name
    app_db: appspace.db_user
```

Here, anything before the first dot is considered the namespace and the text after
the first dot is the username. Also, the postgres roles of these usernames would
be in the form of `namespace.username`.

For such usernames, the secret is created in the given namespace and its name is
of the following form,
`{namespace}.{username}.{clustername}.credentials.postgresql.acid.zalan.do`

### Infrastructure roles

An infrastructure role is a role that should be present on every PostgreSQL
cluster managed by the operator. An example of such a role is a monitoring
user. There are two ways to define them:

* With the infrastructure roles secret only
* With both the the secret and the infrastructure role ConfigMap.

#### Infrastructure roles secret

Infrastructure roles can be specified by the `infrastructure_roles_secrets`
parameter where you can reference multiple existing secrets. Prior to `v1.6.0`
the operator could only reference one secret with the
`infrastructure_roles_secret_name` option. However, this secret could contain
multiple roles using the same set of keys plus incrementing index.

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: postgresql-infrastructure-roles
data:
  user1: ZGJ1c2Vy
  password1: c2VjcmV0
  inrole1: b3BlcmF0b3I=
  user2: ...
```

The block above describes the infrastructure role 'dbuser' with password
'secret' that is a member of the 'operator' role. The resulting role will
automatically be a login role.

With the new option users can configure the names of secret keys that contain
the user name, password etc. The secret itself is referenced by the
`secretname` key. If the secret uses a template for multiple roles as described
above list them separately.

```yaml
apiVersion: "acid.zalan.do/v1"
kind: OperatorConfiguration
metadata:
  name: postgresql-operator-configuration
configuration:
  kubernetes:
    infrastructure_roles_secrets:
    - secretname: "postgresql-infrastructure-roles"
      userkey: "user1"
      passwordkey: "password1"
      rolekey: "inrole1"
    - secretname: "postgresql-infrastructure-roles"
      userkey: "user2"
      ...
```

Note, only the CRD-based configuration allows for referencing multiple secrets.
As of now, the ConfigMap is restricted to either one or the existing template
option with `infrastructure_roles_secret_name`. Please, refer to the example
manifests to understand how `infrastructure_roles_secrets` has to be configured
for the [configmap](https://github.com/zalando/postgres-operator/blob/master/manifests/configmap.yaml) or [CRD configuration](https://github.com/zalando/postgres-operator/blob/master/manifests/postgresql-operator-default-configuration.yaml).

If both `infrastructure_roles_secret_name` and `infrastructure_roles_secrets`
are defined the operator will create roles for both of them. So make sure,
they do not collide. Note also, that with definitions that solely use the
infrastructure roles secret there is no way to specify role options (like
superuser or nologin) or role memberships. This is where the additional
ConfigMap comes into play.

#### Secret plus ConfigMap

A [ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/)
allows for defining more details regarding the infrastructure roles. Therefore,
one should use the new style that specifies infrastructure roles using both the
secret and a ConfigMap. The ConfigMap must have the same name as the secret.
The secret should contain an entry with 'rolename:rolepassword' for each role.

```yaml
dbuser: c2VjcmV0
```

And the role description for that user should be specified in the ConfigMap.

```yaml
data:
  dbuser: |
    inrole: [operator, admin]  # following roles will be assigned to the new user
    user_flags:
    - createdb
    db_parameters:  # db parameters, applied for this particular user
      log_statement: all
```

One can allow membership in multiple roles via the `inrole` array parameter,
define role flags via the `user_flags` list and supply per-role options through
the `db_parameters` dictionary. All those parameters are optional.

Both definitions can be mixed in the infrastructure role secret, as long as
your new-style definition can be clearly distinguished from the old-style one
(for instance, do not name new-style roles `userN`).

Since an infrastructure role is created uniformly on all clusters managed by
the operator, it makes no sense to define it without the password. Such
definitions will be ignored with a prior warning.

See [infrastructure roles secret](https://github.com/zalando/postgres-operator/blob/master/manifests/infrastructure-roles.yaml)
and [infrastructure roles configmap](https://github.com/zalando/postgres-operator/blob/master/manifests/infrastructure-roles-configmap.yaml)
for the examples.

### Teams API roles

These roles are meant for database activity of human users. It's possible to
configure the operator to automatically create database roles for lets say all
employees of one team. They are not listed in the manifest and there are no K8s
secrets created for them. Instead they would use an OAuth2 token to connect. To
get all members of the team the operator queries a defined API endpoint that
returns usernames. A minimal Teams API should work like this:

```
/.../<teamname> -> ["name","anothername"]
```

A ["fake" Teams API](https://github.com/zalando/postgres-operator/blob/master/manifests/fake-teams-api.yaml) deployment is provided
in the manifests folder to set up a basic API around whatever services is used
for user management. The Teams API's URL is set in the operator's
[configuration](reference/operator_parameters.md#automatic-creation-of-human-users-in-the-database)
and `enable_teams_api` must be set to `true`. There are more settings available
to choose superusers, group roles, [PAM configuration](https://github.com/CyberDem0n/pam-oauth2)
etc. An OAuth2 token can be passed to the Teams API via a secret. The name for
this secret is configurable with the `oauth_token_secret_name` parameter.

### Additional teams and members per cluster

Postgres clusters are associated with one team by providing the `teamID` in
the manifest. Additional superuser teams can be configured as mentioned in
the previous paragraph. However, this is a global setting. To assign
additional teams, superuser teams and single users to clusters of a given
team, use the [PostgresTeam CRD](https://github.com/zalando/postgres-operator/blob/master/manifests/postgresteam.crd.yaml).

Note, by default the `PostgresTeam` support is disabled in the configuration.
Switch `enable_postgres_team_crd` flag to `true` and the operator will start to
watch for this CRD. Make sure, the cluster role is up to date and contains a
section for [PostgresTeam](https://github.com/zalando/postgres-operator/blob/master/manifests/operator-service-account-rbac.yaml#L30).

#### Additional teams

To assign additional teams and single users to clusters of a given team,
define a mapping with the `PostgresTeam` Kubernetes resource. The Postgres
Operator will read such team mappings each time it syncs all Postgres clusters.

```yaml
apiVersion: "acid.zalan.do/v1"
kind: PostgresTeam
metadata:
  name: custom-team-membership
spec:
  additionalTeams:
    a-team:
    - "b-team"
```

With the example above the operator will create login roles for all members
of `b-team` in every cluster owned by `a-team`. It's possible to do vice versa
for clusters of `b-team` in one manifest:

```yaml
spec:
  additionalTeams:
    a-team:
    - "b-team"
    b-team:
    - "a-team"
```

You see, the `PostgresTeam` CRD is a global team mapping and independent from
the Postgres manifests. It is possible to define multiple mappings, even with
redundant content - the Postgres operator will create one internal cache from
it. Additional teams are resolved transitively, meaning you will also add
users for their `additionalTeams`, e.g.:

```yaml
spec:
  additionalTeams:
    a-team:
    - "b-team"
    - "c-team"
    b-team:
    - "a-team"
```

This creates roles for members of the `c-team` team not only in all clusters
owned by `a-team`, but as well in cluster owned by `b-team`, as `a-team` is
an `additionalTeam` to `b-team`

Not, you can also define `additionalSuperuserTeams` in the `PostgresTeam`
manifest. By default, this option is disabled and must be configured with
`enable_postgres_team_crd_superusers` to make it work.

#### Virtual teams

There can be "virtual teams" that do not exist in the Teams API. It can make
it easier to map a group of teams to many other teams:

```yaml
spec:
  additionalTeams:
    a-team:
    - "virtual-team"
    b-team:
    - "virtual-team"
    virtual-team:
    - "c-team"
    - "d-team"
```

This example would create roles for members of `c-team` and `d-team` plus
additional `virtual-team` members in clusters owned by `a-team` or `b-team`.

#### Teams changing their names

With `PostgresTeams` it is also easy to cover team name changes. Just add
the mapping between old and new team name and the rest can stay the same.
E.g. if team `a-team`'s name would change to `f-team` in the teams API it
could be reflected in a `PostgresTeam` mapping with just two lines:

```yaml
spec:
  additionalTeams:
    a-team:
    - "f-team"
```

This is helpful, because Postgres cluster names are immutable and can not
be changed. Only via cloning it could get a different name starting with the
new `teamID`.

#### Additional members

Single members might be excluded from teams although they continue to work
with the same people. However, the teams API would not reflect this anymore.
To still add a database role for former team members list their role under
the `additionalMembers` section of the `PostgresTeam` resource:

```yaml
apiVersion: "acid.zalan.do/v1"
kind: PostgresTeam
metadata:
  name: custom-team-membership
spec:
  additionalMembers:
    a-team:
    - "tia"
```

This will create the login role `tia` in every cluster owned by `a-team`.
The user can connect to databases like the other team members.

The `additionalMembers` map can also be used to define users of virtual
teams, e.g. for `virtual-team` we used above:

```yaml
spec:
  additionalMembers:
    virtual-team:
    - "flynch"
    - "rdecker"
    - "briggs"
```

#### Removed members

The Postgres Operator does not delete database roles when users are removed
from manifests. But, using the `PostgresTeam` custom resource or Teams API it
is very easy to add roles to many clusters. Manually reverting such a change
is cumbersome. Therefore, if members are removed from a `PostgresTeam` or the
Teams API the operator can rename roles appending a configured suffix to the
name (see `role_deletion_suffix` option) and revoke the `LOGIN` privilege.
The suffix makes it easy then for a cleanup script to remove those deprecated
roles completely. Switch `enable_team_member_deprecation` to `true` to enable
this behavior.

When a role is re-added to a `PostgresTeam` manifest (or to the source behind
the Teams API) the operator will check for roles with the configured suffix
and if found, rename the role back to the original name and grant `LOGIN`
again.

## Prepared databases with roles and default privileges

The `users` section in the manifests only allows for creating database roles
with global privileges. Fine-grained data access control or role membership can
not be defined and must be set up by the user in the database. But, the Postgres
Operator offers a separate section to specify `preparedDatabases` that will be
created with pre-defined owner, reader and writer roles for each individual
database and, optionally, for each database schema, too. `preparedDatabases`
also enable users to specify PostgreSQL extensions that shall be created in a
given database schema.

### Default database and schema

A prepared database is already created by adding an empty `preparedDatabases`
section to the manifest. The database will then be called like the Postgres
cluster manifest (`-` are replaced with `_`) and will also contain a schema
called `data`.

```yaml
spec:
  preparedDatabases: {}
```

### Default NOLOGIN roles

Given an example with a specified database and schema:

```yaml
spec:
  preparedDatabases:
    foo:
      schemas:
        bar: {}
```

Postgres Operator will create the following NOLOGIN roles:

| Role name      | Member of      | Admin         |
| -------------- | -------------- | ------------- |
| foo_owner      |                | admin         |
| foo_reader     |                | foo_owner     |
| foo_writer     | foo_reader     | foo_owner     |
| foo_bar_owner  |                | foo_owner     |
| foo_bar_reader |                | foo_bar_owner |
| foo_bar_writer | foo_bar_reader | foo_bar_owner |

The `<dbname>_owner` role is the database owner and should be used when creating
new database objects. All members of the `admin` role, e.g. teams API roles, can
become the owner with the `SET ROLE` command. [Default privileges](https://www.postgresql.org/docs/17/sql-alterdefaultprivileges.html)
are configured for the owner role so that the `<dbname>_reader` role
automatically gets read-access (SELECT) to new tables and sequences and the
`<dbname>_writer` receives write-access (INSERT, UPDATE, DELETE on tables,
USAGE and UPDATE on sequences). Both get USAGE on types and EXECUTE on
functions.

The same principle applies for database schemas which are owned by the
`<dbname>_<schema>_owner` role. `<dbname>_<schema>_reader` is read-only,
`<dbname>_<schema>_writer` has write access and inherit reading from the reader
role. Note, that the `<dbname>_*` roles have access incl. default privileges on
all schemas, too. If you don't need the dedicated schema roles - i.e. you only
use one schema - you can disable the creation like this:

```yaml
spec:
  preparedDatabases:
    foo:
      schemas:
        bar:
          defaultRoles: false
```

Then, the schemas are owned by the database owner, too.

### Default LOGIN roles

The roles described in the previous paragraph can be granted to LOGIN roles from
the `users` section in the manifest. Optionally, the Postgres Operator can also
create default LOGIN roles for the database and each schema individually. These
roles will get the `_user` suffix and they inherit all rights from their NOLOGIN
counterparts. Therefore, you cannot have `defaultRoles` set to `false` and enable
`defaultUsers` at the same time.

| Role name           | Member of      | Admin         |
| ------------------- | -------------- | ------------- |
| foo_owner_user      | foo_owner      | admin         |
| foo_reader_user     | foo_reader     | foo_owner     |
| foo_writer_user     | foo_writer     | foo_owner     |
| foo_bar_owner_user  | foo_bar_owner  | foo_owner     |
| foo_bar_reader_user | foo_bar_reader | foo_bar_owner |
| foo_bar_writer_user | foo_bar_writer | foo_bar_owner |

These default users are enabled in the manifest with the `defaultUsers` flag:

```yaml
spec:
  preparedDatabases:
    foo:
      defaultUsers: true
      schemas:
        bar:
          defaultUsers: true
```

Default access privileges are also defined for LOGIN roles on database and
schema creation. This means they are currently not set when `defaultUsers`
(or `defaultRoles` for schemas) are enabled at a later point in time.

For all LOGIN roles the operator will create K8s secrets in the namespace
specified in `secretNamespace`, if `enable_cross_namespace_secret` is set to
`true` in the config. Otherwise, they are created in the same namespace like
the Postgres cluster. Unlike roles specified with `namespace.username` under
`users`, the namespace will not be part of the role name here. Keep in mind
that the underscores in a role name are replaced with dashes in the K8s
secret name.

```yaml
spec:
  preparedDatabases:
    foo:
      defaultUsers: true
      secretNamespace: appspace
```

### Schema `search_path` for default roles

The schema [`search_path`](https://www.postgresql.org/docs/17/ddl-schemas.html#DDL-SCHEMAS-PATH)
for each role will include the role name and the schemas, this role should have
access to. So `foo_bar_writer` does not have to schema-qualify tables from
schemas `foo_bar_writer, bar`, while `foo_writer` can look up `foo_writer` and
any schema listed under `schemas`. To register the default `public` schema in
the `search_path` (because some extensions are installed there) one has to add
the following (assuming no extra roles are desired only for the public schema):

```yaml
spec:
  preparedDatabases:
    foo:
      schemas:
        public:
          defaultRoles: false
```

### Database extensions

Prepared databases also allow for creating Postgres extensions. They will be
created by the database owner in the specified schema.

```yaml
spec:
  preparedDatabases:
    foo:
      extensions:
        pg_partman: public
        postgis: data
```

Some extensions require SUPERUSER rights on creation unless they are not
allowed by the [pgextwlist](https://github.com/dimitri/pgextwlist) extension,
that is shipped with the Spilo image. To see which extensions are on the list
check the `extwlist.extension` parameter in the postgresql.conf file.

```bash
SHOW extwlist.extensions;
```

Make sure that `pgextlist` is also listed under `shared_preload_libraries` in
the PostgreSQL configuration. Then the database owner should be able to create
the extension specified in the manifest.

### From `databases` to `preparedDatabases`

If you wish to create the role setup described above for databases listed under
the `databases` key, you have to make sure that the owner role follows the
`<dbname>_owner` naming convention of `preparedDatabases`. As roles are synced
first, this can be done with one edit:

```yaml
# before
spec:
  databases:
    foo: db_owner

# after
spec:
  databases:
    foo: foo_owner
  preparedDatabases:
    foo:
      schemas:
        my_existing_schema: {}
```

Adding existing database schemas to the manifest to create roles for them as
well is up the user and not done by the operator. Remember that if you don't
specify any schema a new database schema called `data` will be created. When
everything got synced (roles, schemas, extensions), you are free to remove the
database from the `databases` section. Note, that the operator does not delete
database objects or revoke privileges when removed from the manifest.

## Resource definition

The compute resources to be used for the Postgres containers in the pods can be
specified in the postgresql cluster manifest.

```yaml
spec:
  resources:
    requests:
      cpu: 10m
      memory: 100Mi
    limits:
      cpu: 300m
      memory: 300Mi
```

The minimum limits to properly run the `postgresql` resource are configured to
`250m` for `cpu` and `250Mi` for `memory`. If a lower value is set in the
manifest the operator will raise the limits to the configured minimum values.
If no resources are defined in the manifest they will be obtained from the
configured [default requests](reference/operator_parameters.md#kubernetes-resource-requests).
If neither defaults nor minimum limits are configured the operator will not
specify any resources and it's up to K8s (or your own) admission hooks to
handle it.

### HugePages support

The operator supports [HugePages](https://www.postgresql.org/docs/17/kernel-resources.html#LINUX-HUGEPAGES).
To enable HugePages, set the matching resource requests and/or limits in the manifest:

```yaml
spec:
  resources:
    requests:
      hugepages-2Mi: 250Mi
      hugepages-1Gi: 1Gi
    limits:
      hugepages-2Mi: 500Mi
      hugepages-1Gi: 2Gi
```

There are no minimums or maximums and the default is 0 for both HugePage sizes,
but Kubernetes will not spin up the pod if the requested HugePages cannot be allocated.
For more information on HugePages in Kubernetes, see also
[https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/](https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/)

## Use taints, tolerations and node affinity for dedicated PostgreSQL nodes

To ensure Postgres pods are running on nodes without any other application pods,
you can use [taints and tolerations](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
and configure the required toleration in the manifest. Tolerations can also be
defined in the [operator config](administrator.md#use-taints-and-tolerations-for-dedicated-postgresql-nodes)
to apply for all Postgres clusters.

```yaml
spec:
  tolerations:
  - key: postgres
    operator: Exists
    effect: NoSchedule
```

If you need the pods to be scheduled on specific nodes you may use [node affinity](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/)
to specify a set of label(s), of which a prospective host node must have at least one. This could be used to
place nodes with certain hardware capabilities (e.g. SSD drives) in certain environments or network segments,
e.g. for PCI compliance.

```yaml
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-minimal-cluster
spec:
  teamId: "ACID"
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: environment
          operator: In
          values:
          - pci
```

If you need to define a `nodeAffinity` for all your Postgres clusters use the
`node_readiness_label` [configuration](administrator.md#node-readiness-labels).

## In-place major version upgrade

Starting with Spilo 13, operator supports in-place major version upgrade to a
higher major version (e.g. from PG 14 to PG 16). To trigger the upgrade,
simply increase the version in the manifest. It is your responsibility to test
your applications against the new version before the upgrade; downgrading is
not supported. The easiest way to do so is to try the upgrade on the cloned
cluster first (see next chapter). More details can be found in the
[admin docs](administrator.md#minor-and-major-version-upgrade).

## How to clone an existing PostgreSQL cluster

You can spin up a new cluster as a clone of the existing one, using a `clone`
section in the spec. There are two options here:

* Clone from an S3 bucket (recommended)
* Clone directly from a source cluster

Note, that cloning can also be used for [major version upgrades](administrator.md#minor-and-major-version-upgrade)
of PostgreSQL.

### Clone from S3

Cloning from S3 has the advantage that there is no impact on your production
database. A new Postgres cluster is created by restoring the data of another
source cluster. If you create it in the same Kubernetes environment, use a
different name.

```yaml
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-minimal-cluster-clone
spec:
  clone:
    uid: "efd12e58-5786-11e8-b5a7-06148230260c"
    cluster: "acid-minimal-cluster"
    timestamp: "2017-12-19T12:40:33+01:00"
```

Here `cluster` is a name of a source cluster that is going to be cloned. A new
cluster will be cloned from S3, using the latest backup before the `timestamp`.
Note, a time zone is required for `timestamp` in the format of `+00:00` (UTC).

The operator will try to find the WAL location based on the configured
`wal_[s3|gs]_bucket` or `wal_az_storage_account` and the specified `uid`.
You can find the UID of the source cluster in its metadata:

```yaml
apiVersion: acid.zalan.do/v1
kind: postgresql
metadata:
  name: acid-minimal-cluster
  uid: efd12e58-5786-11e8-b5a7-06148230260c
```

If your source cluster uses a WAL location different from the global
configuration you can specify the full path under `s3_wal_path`. For
[Google Cloud Platform](administrator.md#google-cloud-platform-setup)
or [Azure](administrator.md#azure-setup)
it can only be set globally with [custom Pod environment variables](administrator.md#custom-pod-environment-variables)
or locally in the Postgres manifest's [`env`](administrator.md#via-postgres-cluster-manifest) section.


For non AWS S3 following settings can be set to support cloning from other S3
implementations:

```yaml
spec:
  clone:
    uid: "efd12e58-5786-11e8-b5a7-06148230260c"
    cluster: "acid-minimal-cluster"
    timestamp: "2017-12-19T12:40:33+01:00"
    s3_wal_path: "s3://custom/path/to/bucket"
    s3_endpoint: https://s3.acme.org
    s3_access_key_id: 0123456789abcdef0123456789abcdef
    s3_secret_access_key: 0123456789abcdef0123456789abcdef
    s3_force_path_style: true
```

### Clone directly

Another way to get a fresh copy of your source DB cluster is via
[pg_basebackup](https://www.postgresql.org/docs/17/app-pgbasebackup.html). To
use this feature simply leave out the timestamp field from the clone section.
The operator will connect to the service of the source cluster by name. If the
cluster is called test, then the connection string will look like host=test
port=5432), which means that you can clone only from clusters within the same
namespace.

```yaml
spec:
  clone:
    cluster: "acid-minimal-cluster"
```

Be aware that on a busy source database this can result in an elevated load!

## Restore in place

There is also a possibility to restore a database without cloning it. The
advantage to this is that there is no need to change anything on the
application side. However, as it involves deleting the database first, this
process is of course riskier than cloning (which involves adjusting the
connection parameters of the app).

First, make sure there is no writing activity on your DB, and save the UID.
Then delete the `postgresql` K8S resource:

```bash
zkubectl delete postgresql acid-test-restore
```

Then deploy a new manifest with the same name, referring to itself
(both name and UID) in the `clone` section:

```yaml
metadata:
  name: acid-minimal-cluster
  # [...]
spec:
  # [...]
  clone:
    cluster: "acid-minimal-cluster"  # the same as metadata.name above!
    uid: "<original_UID>"
    timestamp: "2022-04-01T10:11:12.000+00:00"
```

This will create a new database cluster with the same name but different UID,
whereas the database will be in the state it was at the specified time.

:warning: The backups and WAL files for the original DB are retained under the
original UID, making it possible retry restoring. However, it is probably
better to create a temporary clone for experimenting or finding out to which
point you should restore.

## Setting up a standby cluster

Standby cluster is a [Patroni feature](https://github.com/zalando/patroni/blob/master/docs/replica_bootstrap.rst#standby-cluster)
that first clones a database, and keeps replicating changes afterwards. It can
exist in a different location than its source database, but unlike cloning,
the PostgreSQL version between source and target cluster has to be the same.

To start a cluster as standby, add the following `standby` section in the YAML
file. You can stream changes from archived WAL files (AWS S3 or Google Cloud
Storage) or from a remote primary. Only one option can be specfied in the
manifest:

```yaml
spec:
  standby:
    s3_wal_path: "s3://<bucketname>/spilo/<source_db_cluster>/<UID>/wal/<PGVERSION>"
```

For GCS, you have to define STANDBY_GOOGLE_APPLICATION_CREDENTIALS as a
[custom pod environment variable](administrator.md#custom-pod-environment-variables).
It is not set from the config to allow for overridding.

```yaml
spec:
  standby:
    gs_wal_path: "gs://<bucketname>/spilo/<source_db_cluster>/<UID>/wal/<PGVERSION>"
```

For a remote primary you specify the host address and optionally the port.
If you leave out the port Patroni will use `"5432"`.

```yaml
spec:
  standby:
    standby_host: "acid-minimal-cluster.default"
    standby_port: "5433"
```

Note, that the pods and services use the same role labels like for normal clusters:
The standby leader is labeled as `master`. When using the `standby_host` option
you have to copy the credentials from the source cluster's secrets to successfully
bootstrap a standby cluster (see next chapter).

### Providing credentials of source cluster

A standby cluster is replicating the data (including users and passwords) from
the source database and is read-only. The system and application users (like
standby, postgres etc.) all have a password that does not match the credentials
stored in secrets which are created by the operator. You have two options:

a. Create secrets manually beforehand and paste the credentials of the source
   cluster
b. Let the operator create the secrets when it bootstraps the standby cluster.
   Patch the secrets with the credentials of the source cluster. Replace the
   spilo pods.

Otherwise, you will see errors in the Postgres logs saying users cannot log in
and the operator logs will complain about not being able to sync resources.
If you stream changes from a remote primary you have to align the secrets or
the standby cluster will not start up.

If you stream changes from WAL files and you only run a standby leader, you
can safely ignore the secret mismatch, as it will be sorted out once the
cluster is detached from the source. It is also harmless if you do not plan it.
But, when you create a standby replica, too, fix the credentials right away.
WAL files will pile up on the standby leader if no connection can be
established between standby replica(s).

### Promote the standby

One big advantage of standby clusters is that they can be promoted to a proper
database cluster. This means it will stop replicating changes from the source,
and start accept writes itself. This mechanism makes it possible to move
databases from one place to another with minimal downtime.

Before promoting a standby cluster, make sure that the standby is not behind
the source database. You should ideally stop writes to your source cluster and
then create a dummy database object that you check for being replicated in the
target to verify all data has been copied.

To promote, remove the `standby` section from the postgres cluster manifest.
A rolling update will be triggered removing the `STANDBY_*` environment
variables from the pods, followed by a Patroni config update that promotes the
cluster.

### Adding standby section after promotion

Turning a running cluster into a standby is not easily possible and should be
avoided. The best way is to remove the cluster and resubmit the manifest
after a short wait of a few minutes. Adding the `standby` section would turn
the database cluster in read-only mode on next operator SYNC cycle but it
does not sync automatically with the source cluster again.

## Sidecar Support

Each cluster can specify arbitrary sidecars to run. These containers could be
used for log aggregation, monitoring, backups or other tasks. A sidecar can be
specified like this:

```yaml
spec:
  sidecars:
    - name: "container-name"
      image: "company/image:tag"
      resources:
        limits:
          cpu: 500m
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 100Mi
      env:
        - name: "ENV_VAR_NAME"
          value: "any-k8s-env-things"
      command: ['sh', '-c', 'echo "logging" > /opt/logs.txt']
```

In addition to any environment variables you specify, the following environment
variables are always passed to sidecars:

  - `POD_NAME` - field reference to `metadata.name`
  - `POD_NAMESPACE` - field reference to `metadata.namespace`
  - `POSTGRES_USER` - the superuser that can be used to connect to the database
  - `POSTGRES_PASSWORD` - the password for the superuser

The PostgreSQL volume is shared with sidecars and is mounted at
`/home/postgres/pgdata`.

**Note**: The operator will not create a cluster if sidecar containers are
specified but globally disabled in the configuration. The `enable_sidecars`
option must be set to `true`.

If you want to add a sidecar to every cluster managed by the operator, you can specify it in the [operator configuration](administrator.md#sidecars-for-postgres-clusters) instead.

### Accessing the PostgreSQL socket from sidecars

If enabled by the `share_pgsocket_with_sidecars` option in the operator
configuration the PostgreSQL socket is placed in a volume of type `emptyDir`
named `postgresql-run`. To allow access to the socket from any sidecar
container simply add a VolumeMount to this volume to your sidecar spec.

```yaml
  - name: "container-name"
    image: "company/image:tag"
    volumeMounts:
    - mountPath: /var/run
      name: postgresql-run
```

If you do not want to globally enable this feature and only use it for single
Postgres clusters, specify an `EmptyDir` volume under `additionalVolumes` in
the manifest:

```yaml
spec:
  additionalVolumes:
  - name: postgresql-run
    mountPath: /var/run/postgresql
    targetContainers:
    - all
    volumeSource:
      emptyDir: {}
  sidecars: 
  - name: "container-name"
    image: "company/image:tag"
    volumeMounts:
    - mountPath: /var/run
      name: postgresql-run
```

## InitContainers Support

Each cluster can specify arbitrary init containers to run. These containers can
be used to run custom actions before any normal and sidecar containers start.
An init container can be specified like this:

```yaml
spec:
  initContainers:
    - name: "container-name"
      image: "company/image:tag"
      env:
        - name: "ENV_VAR_NAME"
          value: "any-k8s-env-things"
```

`initContainers` accepts full `v1.Container` definition.

**Note**: The operator will not create a cluster if `initContainers` are
specified but globally disabled in the configuration. The
`enable_init_containers` option must be set to `true`.

## Increase volume size

Postgres operator supports statefulset volume resize without doing a rolling
update. For that you need to change the size field of the volume description
in the cluster manifest and apply the change:

```yaml
spec:
  volume:
    size: 5Gi # new volume size
```

The operator compares the new value of the size field with the previous one and
acts on differences. The `storage_resize_mode` can be configured. By default,
the operator will adjust the PVCs and leave it to K8s and the infrastructure to
apply the change.

When using AWS with gp3 volumes you should set the mode to `mixed` because it
will also adjust the IOPS and throughput that can be defined in the manifest.
Check the [AWS docs](https://aws.amazon.com/ebs/general-purpose/) to learn
about default and maximum values. Keep in mind that AWS rate-limits updating
volume specs to no more than once every 6 hours.

```yaml
spec:
  volume:
    size: 5Gi # new volume size
    iops: 4000
    throughput: 500
```

The operator can only enlarge volumes. Shrinking is not supported and will emit
a warning. However, it can be done manually after updating the manifest. You
have to delete the PVC, which will hang until you also delete the corresponding
pod. Proceed with the next pod when the cluster is healthy again and replicas
are streaming.

## Logical backups

You can enable logical backups (SQL dumps) from the cluster manifest by adding
the following parameter in the spec section:

```yaml
spec:
  enableLogicalBackup: true
```

The operator will create and sync a K8s cron job to do periodic logical backups
of this particular Postgres cluster. Due to the [limitation of K8s cron jobs](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#cron-job-limitations)
it is highly advisable to set up additional monitoring for this feature; such
monitoring is outside the scope of operator responsibilities. See
[configuration reference](reference/cluster_manifest.md) and
[administrator documentation](administrator.md) for details on how backups are
executed.

## Connection pooler

The operator can create a database side connection pooler for those applications
where an application side pooler is not feasible, but a number of connections is
high. To create a connection pooler together with a database, modify the
manifest:

```yaml
spec:
  enableConnectionPooler: true
  enableReplicaConnectionPooler: true
```

This will tell the operator to create a connection pooler with default
configuration, through which one can access the master via a separate service
`{cluster-name}-pooler`. With the first option, connection pooler for master service
is created and with the second option, connection pooler for replica is created.
Note that both of these flags are independent of each other and user can set or
unset any of them as per their requirements without any effect on the other.

In most of the cases the
[default configuration](reference/operator_parameters.md#connection-pooler-configuration)
should be good enough. To configure a new connection pooler individually for
each Postgres cluster, specify:

```
spec:
  connectionPooler:
    # how many instances of connection pooler to create
    numberOfInstances: 2

    # in which mode to run, session or transaction
    mode: "transaction"

    # schema, which operator will create in each database
    # to install credentials lookup function for connection pooler
    schema: "pooler"

    # user, which operator will create for connection pooler
    user: "pooler"

    # resources for each instance
    resources:
      requests:
        cpu: 500m
        memory: 100Mi
      limits:
        cpu: "1"
        memory: 100Mi
```

The `enableConnectionPooler` flag is not required when the `connectionPooler`
section is present in the manifest. But, it can be used to disable/remove the
pooler while keeping its configuration.

By default, [`PgBouncer`](https://www.pgbouncer.org/) is used as connection pooler.
To find out about pool modes read the `PgBouncer` [docs](https://www.pgbouncer.org/config.html#pooler_mode)
(but it should be the general approach between different implementation).

Note, that using `PgBouncer` a meaningful resource CPU limit should be 1 core
or less (there is a way to utilize more than one, but in K8s it's easier just to
spin up more instances).

## Custom TLS certificates

By default, the Spilo image generates its own TLS certificate during startup.
However, this certificate cannot be verified and thus doesn't protect from
active MITM attacks. In this section we show how to specify a custom TLS
certificate which is mounted in the database pods via a K8s Secret.

Before applying these changes, in k8s the operator must also be configured with
the `spilo_fsgroup` set to the GID matching the postgres user group. If you
don't know the value, use `103` which is the GID from the default Spilo image
(`spilo_fsgroup=103` in the cluster request spec).

OpenShift allocates the users and groups dynamically (based on scc), and their
range is different in every namespace. Due to this dynamic behaviour, it's not
trivial to know at deploy time the uid/gid of the user in the cluster.
Therefore, instead of using a global `spilo_fsgroup` setting in operator
configuration or use the `spiloFSGroup` field per Postgres cluster manifest.

For testing purposes, you can generate a self-signed certificate with openssl:
```sh
openssl req -x509 -nodes -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/CN=acid.zalan.do"
```

Upload the cert as a kubernetes secret:
```sh
kubectl create secret tls pg-tls \
  --key tls.key \
  --cert tls.crt
```

When doing client auth, CA can come optionally from the same secret:
```sh
kubectl create secret generic pg-tls \
  --from-file=tls.crt=server.crt \
  --from-file=tls.key=server.key \
  --from-file=ca.crt=ca.crt
```

Then configure the postgres resource with the TLS secret:

```yaml
apiVersion: "acid.zalan.do/v1"
kind: postgresql

metadata:
  name: acid-test-cluster
spec:
  tls:
    secretName: "pg-tls"
    caFile: "ca.crt" # add this if the secret is configured with a CA
```

Optionally, the CA can be provided by a different secret:
```sh
kubectl create secret generic pg-tls-ca --from-file=ca.crt=ca.crt
```

Then configure the postgres resource with the TLS secret:

```yaml
apiVersion: "acid.zalan.do/v1"
kind: postgresql

metadata:
  name: acid-test-cluster
spec:
  tls:
    secretName: "pg-tls"    # this should hold tls.key and tls.crt
    caSecretName: "pg-tls-ca" # this should hold ca.crt
    caFile: "ca.crt" # add this if the secret is configured with a CA
```

Alternatively, it is also possible to use
[cert-manager](https://cert-manager.io/docs/) to generate these secrets.

Certificate rotation is handled in the Spilo image which checks every 5
minutes if the certificates have changed and reloads postgres accordingly.

### TLS certificates for connection pooler

By default, the pgBouncer image generates its own TLS certificate like Spilo.
When the `tls` section is specfied in the manifest it will be used for the
connection pooler pod(s) as well. The security context options are hard coded
to `runAsUser: 100` and `runAsGroup: 101`. The `fsGroup` will be the same
like for Spilo.

As of now, the operator does not sync the pooler deployment automatically
which means that changes in the pod template are not caught. You need to
toggle `enableConnectionPooler` to set environment variables, volumes, secret
mounts and securityContext required for TLS support in the pooler pod.


================================================
File: docs/diagrams/Makefile
================================================
OBJ=$(patsubst %.tex, %.png, $(wildcard *.tex))

.PHONY: all

all: $(OBJ)

%.pdf: %.tex
	lualatex $< -shell-escape $@

%.png: %.pdf
	convert -flatten -density 300 $< -quality 90 $@


================================================
File: docs/diagrams/operator.tex
================================================
\documentclass{article}
\usepackage{tikz}
\usepackage[graphics,tightpage,active]{preview}
\usetikzlibrary{arrows, shadows.blur, positioning, fit, calc, backgrounds}
\usepackage{lscape}

\pagenumbering{gobble}

\PreviewEnvironment{tikzpicture}
\PreviewEnvironment{equation}
\PreviewEnvironment{equation*}
\newlength{\imagewidth}
\newlength{\imagescale}
\pagestyle{empty}
\thispagestyle{empty}

\begin{document}
\begin{center}
\begin{tikzpicture}[
  scale=0.5,transform shape,
  font=\sffamily,
  every matrix/.style={ampersand replacement=\&,column sep=2cm,row sep=2cm},
  operator/.style={draw,solid,thick,circle,fill=red!20,inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  component/.style={draw,solid,thick,rounded corners,fill=yellow!20,inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  border/.style={draw,dashed,rounded corners,fill=gray!20,inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  pod/.style={draw,solid,thick,rounded corners,fill=blue!20, inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  service/.style={draw,solid,thick,rounded corners,fill=blue!20, inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  endpoint/.style={draw,solid,thick,rounded corners,fill=blue!20, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  secret/.style={draw,solid,thick,rounded corners,fill=blue!20, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  pvc/.style={draw,solid,thick,rounded corners,fill=blue!20, inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  label/.style={rectangle,inner sep=0,outer sep=0},
  to/.style={->,>=stealth',shorten >=1pt,semithick,font=\sffamily\footnotesize},
  every node/.style={align=center}]

  % Position the nodes using a matrix layout

  \matrix{
    \& \node[component] (crd) {CRD}; \\
    \& \node[operator] (operator) {Operator}; \\
        \path
          node[service] (service-master) {Master}
          node[label, right of=service-master] (service-middle) {}
          node[label, below of=service-middle] (services-label) {Services}
          node[service, right=.5cm of service-master] (service-replica) {Replica}
          node[border, behind path,
               fit=(service-master)(service-replica)(services-label)
          ] (services) {};
    \&
    \node[component] (sts) {Statefulset}; \& \node[component] (pdb) {Pod Disruption Budget}; \\
    \path
      node[service] (master-endpoint) {Master}
      node[service, right=.5cm of master-endpoint] (replica-endpoint) {Replica}
      node[label, right of=master-endpoint] (endpoint-middle) {}
      node[label, below of=endpoint-middle] (endpoint-label) {Endpoints}
      node[border, behind path,
           fit=(master-endpoint)(replica-endpoint)(endpoint-label)
      ] (endpoints) {}; \&
    \node[component] (pod-template) {Pod Template}; \&
    \node[border] (secrets) {
        \begin{tikzpicture}[]
            \node[secret] (users-secret) at (0, 0) {Users};
            \node[secret] (robots-secret) at (2, 0) {Robots};
            \node[secret] (standby-secret) at (4, 0) {Standby};
        \end{tikzpicture} \\
        Secrets
    }; \\ \&
    \path
      node[pod] (replica1-pod) {Replica}
      node[pod, left=.5cm of replica1-pod] (master-pod) {Master}
      node[pod, right=.5cm of replica1-pod] (replica2-pod) {Replica}
      node[label, below of=replica1-pod] (pod-label) {Pods}
      node[border, behind path,
           fit=(master-pod)(replica1-pod)(replica2-pod)(pod-label)
      ] (pods) {}; \\ \&
    \path
      node[pvc] (replica1-pvc) {Replica}
      node[pvc, left=.5cm of replica1-pvc] (master-pvc) {Master}
      node[pvc, right=.5cm of replica1-pvc] (replica2-pvc) {Replica}
      node[label, below of=replica1-pvc] (pvc-label) {Persistent Volume Claims}
      node[border, behind path,
           fit=(master-pvc)(replica1-pvc)(replica2-pvc)(pvc-label)
      ] (pvcs) {}; \&
    \\ \& \\
  };

  % Draw the arrows between the nodes and label them.
  \draw[to] (crd) -- node[midway,above] {} node[midway,below] {} (operator);
  \draw[to] (operator) -- node[midway,above] {} node[midway,below] {} (sts);
  \draw[to] (operator) -- node[midway,above] {} node[midway,below] {} (secrets);
  \draw[to] (operator) -| node[midway,above] {} node[midway,below] {} (pdb);
  \draw[to] (service-master) -- node[midway,above] {} node[midway,below] {} (master-endpoint);
  \draw[to] (service-replica) -- node[midway,above] {} node[midway,below] {} (replica-endpoint);
  \draw[to] (master-pod) -- node[midway,above] {} node[midway,below] {} (master-pvc);
  \draw[to] (replica1-pod) -- node[midway,above] {} node[midway,below] {} (replica1-pvc);
  \draw[to] (replica2-pod) -- node[midway,above] {} node[midway,below] {} (replica2-pvc);
  \draw[to] (operator) -| node[midway,above] {} node[midway,below] {} (services);
  \draw[to] (sts) -- node[midway,above] {} node[midway,below] {} (pod-template);
  \draw[to] (pod-template) -- node[midway,above] {} node[midway,below] {} (pods);
\end{tikzpicture}
\end{center}
\end{document}


================================================
File: docs/diagrams/pod.tex
================================================
\documentclass{article}
\usepackage{tikz}
\usepackage[graphics,tightpage,active]{preview}
\usetikzlibrary{arrows, shadows.blur, positioning, fit, calc, backgrounds}
\usepackage{lscape}

\pagenumbering{gobble}

\PreviewEnvironment{tikzpicture}
\PreviewEnvironment{equation}
\PreviewEnvironment{equation*}
\newlength{\imagewidth}
\newlength{\imagescale}
\pagestyle{empty}
\thispagestyle{empty}

\begin{document}
\begin{center}
\begin{tikzpicture}[
  scale=0.5,transform shape,
  font=\sffamily,
  every matrix/.style={ampersand replacement=\&,column sep=2cm,row sep=2cm},
  pod/.style={draw,solid,thick,circle,fill=red!20,inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  component/.style={draw,solid,thick,rounded corners,fill=yellow!20,inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  border/.style={draw,dashed,rounded corners,fill=gray!20,inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  volume/.style={draw,solid,thick,rounded corners,fill=blue!20, inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  sidecar/.style={draw,solid,thick,rounded corners,fill=blue!20, inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  k8s-label/.style={draw,solid,thick,rounded corners,fill=blue!20, minimum width=1.5cm, inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  affinity/.style={draw,solid,thick,rounded corners,fill=blue!20, minimum width=2cm, inner sep=.3cm, blur shadow={shadow blur steps=5,shadow blur extra rounding=1.3pt}},
  label/.style={rectangle,inner sep=0,outer sep=0},
  to/.style={->,>=stealth',shorten >=1pt,semithick,font=\sffamily\footnotesize},
  every node/.style={align=center}]

  % Position the nodes using a matrix layout

  \matrix{
    \path
      node[k8s-label] (app-label) {App}
      node[k8s-label, right=.25cm of app-label] (role-label) {Role}
      node[k8s-label, right=.25cm of role-label] (custom-label) {Custom}
      node[label, below of=role-label] (k8s-label-label) {K8s Labels}
      node[border, behind path,
           fit=(app-label)(role-label)(custom-label)(k8s-label-label)
      ] (k8s-labels) {};  \& \&
    \path
      node[affinity] (affinity) {Affinity}
      node[label, right=.25cm of affinity] (affinity-middle) {}
      node[affinity, right=.25cm of affinity-middle] (anti-affinity) {Anti-affinity}
      node[label, below of=affinity-middle] (affinity-label) {Assigning to nodes}
      node[border, behind path,
           fit=(affinity)(anti-affinity)(affinity-label)
      ] (affinity) {}; \\
    \& \node[pod] (pod) {Pod}; \& \\
    \path
      node[volume, minimum width={width("shm-volume")}] (data-volume) {Data}
      node[volume, right=.25cm of data-volume, minimum width={width("shm-volume")}] (tokens-volume) {Tokens}
      node[volume, right=.25cm of tokens-volume] (shm-volume) {/dev/shm}
      node[label, below of=tokens-volume] (volumes-label) {Volumes}
      node[border, behind path,
           fit=(data-volume)(shm-volume)(tokens-volume)(volumes-label)
      ] (volumes) {}; \&
    \node[component] (spilo) {Spilo}; \&
    \node[sidecar] (scalyr) {Scalyr}; \& \\ \&
    \path
      node[component] (patroni) {Patroni}
      node[component, below=.25cm of patroni] (postgres) {PostgreSQL}
      node[border, behind path,
           fit=(postgres)(patroni)
      ] (spilo-components) {}; \&
    \path
      node[sidecar] (custom-sidecar1) {User defined}
      node[label, right=.25cm of custom-sidecar1] (sidecars-middle) {}
      node[sidecar, right=.25cm of sidecars-middle] (custom-sidecar2) {User defined}
      node[label, below of=sidecars-middle] (sidecars-label) {Custom sidecars}
      node[border, behind path,
           fit=(custom-sidecar1)(custom-sidecar2)(sidecars-label)
      ] (sidecars) {};
    \\ \& \\
  };

  % Draw the arrows between the nodes and label them.
  \draw[to] (pod) to [bend left=25] (volumes);
  \draw[to] (pod) to [bend left=25] (k8s-labels);
  \draw[to] (pod) to [bend right=25] (affinity);
  \draw[to] (pod) to [bend right=25] (scalyr);
  \draw[to] (pod) to [bend right=25] (sidecars);
  \draw[to] (pod) -- node[midway,above] {} node[midway,below] {} (spilo);
  \draw[to] (spilo) -- node[midway,above] {} node[midway,below] {} (spilo-components);

\end{tikzpicture}
\end{center}
\end{document}


================================================
File: docs/reference/cluster_manifest.md
================================================
<h1>Cluster manifest reference</h1>

Individual Postgres clusters are described by the Kubernetes *cluster manifest*
that has the structure defined by the `postgresql` CRD (custom resource
definition). The following section describes the structure of the manifest and
the purpose of individual keys. You can take a look at the examples of the
[minimal](https://github.com/zalando/postgres-operator/blob/master/manifests/minimal-postgres-manifest.yaml)
and the
[complete](https://github.com/zalando/postgres-operator/blob/master/manifests/complete-postgres-manifest.yaml)
cluster manifests.

When Kubernetes resources, such as memory, CPU or volumes, are configured,
their amount is usually described as a string together with the units of
measurements. Please, refer to the [Kubernetes
documentation](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/)
for the possible values of those.

:exclamation: If both operator configmap/CRD and a Postgres cluster manifest
define the same parameter, the value from the Postgres cluster manifest is
applied.

## Manifest structure

A Postgres manifest is a `YAML` document. On the top level both individual
parameters and parameter groups can be defined. Parameter names are written
in camelCase.

## Cluster metadata

Those parameters are grouped under the `metadata` top-level key.

* **name**
  the name of the cluster. Must start with the `teamId` followed by a dash.
  Changing it after the cluster creation is not supported. Required field.

* **namespace**
  the namespace where the operator creates Kubernetes objects (i.e. pods,
  services, secrets) for the cluster. Changing it after the cluster creation
  results in deploying or updating a completely separate cluster in the target
  namespace. Optional (if present, should match the namespace where the
  manifest is applied).

* **labels**
  if labels are matching one of the `inherited_labels` [configured in the
  operator parameters](operator_parameters.md#kubernetes-resources),
  they will automatically be added to all the objects (StatefulSet, Service,
  Endpoints, etc.) that are created by the operator.
  Labels that are set here but not listed as `inherited_labels` in the operator
  parameters are ignored.

## Top-level parameters

These parameters are grouped directly under  the `spec` key in the manifest.

* **teamId**
  name of the team the cluster belongs to. Required field.

* **numberOfInstances**
  total number of  instances for a given cluster. The operator parameters
  `max_instances` and `min_instances` may also adjust this number. Required
  field.

* **dockerImage**
  custom Docker image that overrides the **docker_image** operator parameter.
  It should be a [Spilo](https://github.com/zalando/spilo) image. Optional.

* **schedulerName**
  specifies the scheduling profile for database pods. If no value is provided
  K8s' `default-scheduler` will be used. Optional.

* **spiloRunAsUser**
  sets the user ID which should be used in the container to run the process.
  This must be set to run the container without root. By default the container
  runs with root. This option only works for Spilo versions >= 1.6-p3.

* **spiloRunAsGroup**
  sets the group ID which should be used in the container to run the process.
  This must be set to run the container without root. By default the container
  runs with root. This option only works for Spilo versions >= 1.6-p3.

* **spiloFSGroup**
  the Persistent Volumes for the Spilo pods in the StatefulSet will be owned and
  writable by the group ID specified. This will override the **spilo_fsgroup**
  operator parameter. This is required to run Spilo as a non-root process, but
  requires a custom Spilo image. Note the FSGroup of a Pod cannot be changed
  without recreating a new Pod. Optional.

* **enableMasterLoadBalancer**
  boolean flag to override the operator defaults (set by the
  `enable_master_load_balancer` parameter) to define whether to enable the load
  balancer pointing to the Postgres primary. Optional.

* **enableMasterPoolerLoadBalancer**
  boolean flag to override the operator defaults (set by the
  `enable_master_pooler_load_balancer` parameter) to define whether to enable
  the load balancer for master pooler pods pointing to the Postgres primary.
  Optional.

* **enableReplicaLoadBalancer**
  boolean flag to override the operator defaults (set by the
  `enable_replica_load_balancer` parameter) to define whether to enable the
  load balancer pointing to the Postgres standby instances. Optional.

* **enableReplicaPoolerLoadBalancer**
  boolean flag to override the operator defaults (set by the
  `enable_replica_pooler_load_balancer` parameter) to define whether to enable
  the load balancer for replica pooler pods pointing to the Postgres standby
  instances. Optional.

* **allowedSourceRanges**
  when one or more load balancers are enabled for the cluster, this parameter
  defines the comma-separated range of IP networks (in CIDR-notation). The
  corresponding load balancer is accessible only to the networks defined by
  this parameter. Optional, when empty the load balancer service becomes
  inaccessible from outside of the Kubernetes cluster.

* **maintenanceWindows**
  a list which defines specific time frames when certain maintenance operations
  such as automatic major upgrades or master pod migration. Accepted formats
  are "01:00-06:00" for daily maintenance windows or "Sat:00:00-04:00" for specific
  days, with all times in UTC.

* **users**
  a map of usernames to user flags for the users that should be created in the
  cluster by the operator. User flags are a list, allowed elements are
  `SUPERUSER`, `REPLICATION`, `INHERIT`, `LOGIN`, `NOLOGIN`, `CREATEROLE`,
  `CREATEDB`, `BYPASSRLS`. A login user is created by default unless NOLOGIN is
  specified, in which case the operator creates a role. One can specify empty
  flags by providing a JSON empty array '*[]*'. If the config option
  `enable_cross_namespace_secret` is enabled you can specify the namespace in
  the user name in the form `{namespace}.{username}` and the operator will
  create the K8s secret in that namespace. The part after the first `.` is
  considered to be the user name. Optional.

* **usersWithSecretRotation**
  list of users to enable credential rotation in K8s secrets. The rotation
  interval can only be configured globally. On each rotation a new user will
  be added in the database replacing the `username` value in the secret of
  the listed user. Although, rotation users inherit all rights from the
  original role, keep in mind that ownership is not transferred. See more
  details in the [administrator docs](https://github.com/zalando/postgres-operator/blob/master/docs/administrator.md#password-rotation-in-k8s-secrets).

* **usersWithInPlaceSecretRotation**
  list of users to enable in-place password rotation in K8s secrets. The
  rotation interval can only be configured globally. On each rotation the
  password value will be replaced in the secrets which the operator reflects
  in the database, too. List only users here that rarely connect to the
  database, like a flyway user running a migration on Pod start. See more
  details in the [administrator docs](https://github.com/zalando/postgres-operator/blob/master/docs/administrator.md#password-replacement-without-extra-users).

* **usersIgnoringSecretRotation**
  if you have secret rotation enabled globally you can define a list of
  of users that should opt out from it, for example if you store credentials
  outside of K8s, too, and corresponding deployments cannot dynamically
  reference secrets. Note, you can also opt out from the rotation by removing
  users from the manifest's `users` section. The operator will not drop them
  from the database. Optional.

* **databases**
  a map of database names to database owners for the databases that should be
  created by the operator. The owner users should already exist on the cluster
  (i.e. mentioned in the `user` parameter). Optional.

* **tolerations**
  a list of tolerations that apply to the cluster pods. Each element of that
  list is a dictionary with the following fields: `key`, `operator`, `value`,
  `effect` and `tolerationSeconds`. Each field is optional. See [Kubernetes
  examples](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
  for details on tolerations and possible values of those keys. When set, this
  value overrides the `pod_toleration` setting from the operator. Optional.

* **podPriorityClassName**
  a name of the [priority
  class](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass)
  that should be assigned to the cluster pods. When not specified, the value
  is taken from the `pod_priority_class_name` operator parameter, if not set
  then the default priority class is taken. The priority class itself must be
  defined in advance. Optional.

* **podAnnotations**
  A map of key value pairs that gets attached as [annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/)
  to each pod created for the database.

* **serviceAnnotations**
  A map of key value pairs that gets attached as [annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/)
  to the services created for the database cluster. Check the
  [administrator docs](https://github.com/zalando/postgres-operator/blob/master/docs/administrator.md#load-balancers-and-allowed-ip-ranges)
  for more information regarding default values and overwrite rules.

* **masterServiceAnnotations**
  A map of key value pairs that gets attached as [annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/)
  to the master service created for the database cluster. Check the
  [administrator docs](https://github.com/zalando/postgres-operator/blob/master/docs/administrator.md#load-balancers-and-allowed-ip-ranges)
  for more information regarding default values and overwrite rules.
  This field overrides `serviceAnnotations` with the same key for the master
  service if not empty.

* **replicaServiceAnnotations**
  A map of key value pairs that gets attached as [annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/)
  to the replica service created for the database cluster. Check the
  [administrator docs](https://github.com/zalando/postgres-operator/blob/master/docs/administrator.md#load-balancers-and-allowed-ip-ranges)
  for more information regarding default values and overwrite rules.
  This field overrides `serviceAnnotations` with the same key for the replica
  service if not empty.

* **enableShmVolume**
  Start a database pod without limitations on shm memory. By default Docker
  limit `/dev/shm` to `64M` (see e.g. the [docker
  issue](https://github.com/docker-library/postgres/issues/416), which could be
  not enough if PostgreSQL uses parallel workers heavily. If this option is
  present and value is `true`, to the target database pod will be mounted a new
  tmpfs volume to remove this limitation. If it's not present, the decision
  about mounting a volume will be made based on operator configuration
  (`enable_shm_volume`, which is `true` by default). It it's present and value
  is `false`, then no volume will be mounted no matter how operator was
  configured (so you can override the operator configuration). Optional.

* **enableConnectionPooler**
  Tells the operator to create a connection pooler with a database for the master
  service. If this field is true, a connection pooler deployment will be created even if
  `connectionPooler` section is empty. Optional, not set by default.

* **enableReplicaConnectionPooler**
  Tells the operator to create a connection pooler with a database for the replica
  service. If this field is true, a connection pooler deployment for replica
  will be created even if `connectionPooler` section is empty. Optional, not set by default.

* **enableLogicalBackup**
  Determines if the logical backup of this cluster should be taken and uploaded
  to S3. Default: false. Optional.

* **logicalBackupRetention**
  You can set a retention time for the logical backup cron job to remove old backup
  files after a new backup has been uploaded. Example values are "3 days", "2 weeks", or
  "1 month". It takes precedence over the global `logical_backup_s3_retention_time`
  configuration. Currently only supported for AWS. Optional.

* **logicalBackupSchedule**
  Schedule for the logical backup K8s cron job. Please take
  [the reference schedule format](https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#schedule)
  into account. It takes precedence over the global `logical_backup_schedule`
  configuration. Optional.

* **additionalVolumes**
  List of additional volumes to mount in each container of the statefulset pod.
  Each item must contain a `name`, `mountPath`, and `volumeSource` which is a
  [kubernetes volumeSource](https://godoc.org/k8s.io/api/core/v1#VolumeSource).
  It allows you to mount existing PersistentVolumeClaims, ConfigMaps and Secrets inside the StatefulSet.
  Also an `emptyDir` volume can be shared between initContainer and statefulSet.
  Additionaly, you can provide a `SubPath` for volume mount (a file in a configMap source volume, for example).
  Set `isSubPathExpr` to true if you want to include [API environment variables](https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath-expanded-environment).
  You can also specify in which container the additional Volumes will be mounted with the `targetContainers` array option.
  If `targetContainers` is empty, additional volumes will be mounted only in the `postgres` container.
  If you set the `all` special item, it will be mounted in all containers (postgres + sidecars).
  Else you can set the list of target containers in which the additional volumes will be mounted (eg : postgres, telegraf)

## Prepared Databases

The operator can create databases with default owner, reader and writer roles
without the need to specifiy them under `users` or `databases` sections. Those
parameters are grouped under the `preparedDatabases` top-level key. For more
information, see [user docs](../user.md#prepared-databases-with-roles-and-default-privileges).

* **defaultUsers**
  The operator will always create default `NOLOGIN` roles for defined prepared
  databases, but if `defaultUsers` is set to `true` three additional `LOGIN`
  roles with `_user` suffix will get created. Default is `false`.

* **extensions**
  map of extensions with target database schema that the operator will install
  in the database. Optional.

* **schemas**
  map of schemas that the operator will create. Optional - if no schema is
  listed, the operator will create a schema called `data`. Under each schema
  key, it can be defined if `defaultRoles` (NOLOGIN) and `defaultUsers` (LOGIN)
  roles shall be created that have schema-exclusive privileges.
  By default, `defaultRoles` is `true` and `defaultUsers` is false.

* **secretNamespace**
  for each default LOGIN role the operator will create a secret. You can
  specify the namespace in which these secrets will get created, if
  `enable_cross_namespace_secret` is set to `true` in the config. Otherwise,
  the cluster namespace is used.

## Postgres parameters

Those parameters are grouped under the `postgresql` top-level key, which is
required in the manifest.

* **version**
  the Postgres major version of the cluster. Looks at the [Spilo
  project](https://github.com/zalando/spilo/releases) for the list of supported
  versions. Changing the cluster version once the cluster has been bootstrapped
  is not supported. Required field.

* **parameters**
  a dictionary of Postgres parameter names and values to apply to the resulting
  cluster. Optional (Spilo automatically sets reasonable defaults for parameters
  like `work_mem` or `max_connections`).

## Patroni parameters

Those parameters are grouped under the `patroni` top-level key. See the [Patroni
documentation](https://patroni.readthedocs.io/en/latest/SETTINGS.html) for the
explanation of `ttl` and `loop_wait` parameters.

* **initdb**
  a map of key-value pairs describing initdb parameters. For `data-checksums`,
  `debug`, `no-locale`, `noclean`, `nosync` and `sync-only` parameters use
  `true` as the value if you want to set them. Changes to this option do not
  affect the already initialized clusters. Optional.

* **pg_hba**
  list of custom `pg_hba` lines to replace default ones. Note that the default
  ones include

  ```
  hostssl all +pamrole all pam
  ```
  where pamrole is the name of the role for the pam authentication; any
  custom `pg_hba` should include the pam line to avoid breaking pam
  authentication. Optional.

* **ttl**
  Patroni `ttl` parameter value, optional. The default is set by the Spilo
  Docker image. Optional.

* **loop_wait**
  Patroni `loop_wait` parameter value, optional. The default is set by the
  Spilo Docker image. Optional.

* **retry_timeout**
  Patroni `retry_timeout` parameter value, optional. The default is set by the
  Spilo Docker image. Optional.

* **maximum_lag_on_failover**
  Patroni `maximum_lag_on_failover` parameter value, optional. The default is
  set by the Spilo Docker image. Optional.

* **slots**
  permanent replication slots that Patroni preserves after failover by
  re-creating them on the new primary immediately after doing a promote. Slots
  could be reconfigured with the help of `patronictl edit-config`. It is the
  responsibility of a user to avoid clashes in names between replication slots
  automatically created by Patroni for cluster members and permanent replication
  slots. Optional.

* **synchronous_mode**
  Patroni `synchronous_mode` parameter value. The default is set to `false`. Optional.

* **synchronous_mode_strict**
  Patroni `synchronous_mode_strict` parameter value. Can be used in addition to `synchronous_mode`. The default is set to `false`. Optional.

* **synchronous_node_count**
  Patroni `synchronous_node_count` parameter value. Note, this option is only available for Spilo images with Patroni 2.0+. The default is set to `1`. Optional.

* **failsafe_mode**
  Patroni `failsafe_mode` parameter value. If enabled, Patroni will cope
  with DCS outages by avoiding leader demotion. See the Patroni documentation
  [here](https://patroni.readthedocs.io/en/master/dcs_failsafe_mode.html) for more details.
  This feature is included since Patroni 3.0.0. Hence, check the container
  image in use if this feature is included in the used Patroni version. The
  default is set to `false`. Optional. 
  
## Postgres container resources

Those parameters define [CPU and memory requests and limits](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/)
for the Postgres container. They are grouped under the `resources` top-level
key with subgroups `requests` and `limits`. 

### Requests

CPU and memory requests for the Postgres container.

* **cpu**
  CPU requests for the Postgres container. Optional, overrides the
  `default_cpu_requests` operator configuration parameter.

* **memory**
  memory requests for the Postgres container. Optional, overrides the
  `default_memory_request` operator configuration parameter.

* **hugepages-2Mi**
  hugepages-2Mi requests for the sidecar container.
  Optional, defaults to not set.

* **hugepages-1Gi**
  1Gi hugepages requests for the sidecar container.
  Optional, defaults to not set.

### Limits

CPU and memory limits for the Postgres container.

* **cpu**
  CPU limits for the Postgres container. Optional, overrides the
  `default_cpu_limits` operator configuration parameter.

* **memory**
  memory limits for the Postgres container. Optional, overrides the
  `default_memory_limits` operator configuration parameter.

* **hugepages-2Mi**
  hugepages-2Mi requests for the sidecar container.
  Optional, defaults to not set.

* **hugepages-1Gi**
  1Gi hugepages requests for the sidecar container.
  Optional, defaults to not set.

## Parameters defining how to clone the cluster from another one

Those parameters are applied when the cluster should be a clone of another one
that is either already running or has a basebackup on S3. They are grouped
under the `clone` top-level key and do not affect the already running cluster.

* **cluster**
  name of the cluster to clone from. Translated to either the service name or
  the key inside the S3 bucket containing base backups. Required when the
  `clone` section is present.

* **uid**
  Kubernetes UID of the cluster to clone from. Since cluster name is not a
  unique identifier of the cluster (as identically named clusters may exist in
  different namespaces) , the operator uses UID in the S3 bucket name in order
  to guarantee uniqueness. Has no effect when cloning from the running
  clusters. Optional.

* **timestamp**
  the timestamp up to which the recovery should proceed. The operator always
  configures non-inclusive recovery target, stopping right before the given
  timestamp. When this parameter is set the operator will not consider cloning
  from the live cluster, even if it is running, and instead goes to S3. Optional.

* **s3_wal_path**
  the url to S3 bucket containing the WAL archive of the cluster to be cloned.
  Optional.

* **s3_endpoint**
  the url of the S3-compatible service should be set when cloning from non AWS
  S3. Optional.

* **s3_access_key_id**
  the access key id, used for authentication on S3 service. Optional.

* **s3_secret_access_key**
  the secret access key, used for authentication on S3 service. Optional.

* **s3_force_path_style**
  to enable path-style addressing(i.e., http://s3.amazonaws.com/BUCKET/KEY)
  when connecting to an S3-compatible service that lack of support for
  sub-domain style bucket URLs (i.e., http://BUCKET.s3.amazonaws.com/KEY).
  Optional.

## Standby cluster

On startup, an existing `standby` top-level key creates a standby Postgres
cluster streaming from a remote location - either from a S3 or GCS WAL
archive or a remote primary. Only one of options is allowed and required
if the `standby` key is present.

* **s3_wal_path**
  the url to S3 bucket containing the WAL archive of the remote primary.

* **gs_wal_path**
  the url to GS bucket containing the WAL archive of the remote primary.

* **standby_host**
  hostname or IP address of the primary to stream from.

* **standby_port**
  TCP port on which the primary is listening for connections. Patroni will
  use `"5432"` if not set.

## Volume properties

Those parameters are grouped under the `volume` top-level key and define the
properties of the persistent storage that stores Postgres data.

* **size**
  the size of the target volume. Usual Kubernetes size modifiers, i.e. `Gi`
  or `Mi`, apply. Required.

* **storageClass**
  the name of the Kubernetes storage class to draw the persistent volume from.
  See [Kubernetes
  documentation](https://kubernetes.io/docs/concepts/storage/storage-classes/)
  for the details on storage classes. Optional.

* **subPath**
  Subpath to use when mounting volume into Spilo container. Optional.

* **isSubPathExpr**
  Set it to true if the specified subPath is an expression. Optional.

* **iops**
  When running the operator on AWS the latest generation of EBS volumes (`gp3`)
  allows for configuring the number of IOPS. Maximum is 16000. Optional.

* **throughput**
  When running the operator on AWS the latest generation of EBS volumes (`gp3`)
  allows for configuring the throughput in MB/s. Maximum is 1000. Optional.

* **selector**
  A label query over PVs to consider for binding. See the [Kubernetes 
  documentation](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
  for details on using `matchLabels` and `matchExpressions`. Optional

## Sidecar definitions

Those parameters are defined under the `sidecars` key. They consist of a list
of dictionaries, each defining one sidecar (an extra container running
along the main Postgres container on the same pod). The following keys can be
defined in the sidecar dictionary:

* **name**
  name of the sidecar. Required.

* **image**
  Docker image of the sidecar. Required.

* **env**
  a dictionary of environment variables. Use usual Kubernetes definition
  (https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)
  for environment variables. Optional.

* **resources**
  [CPU and memory requests and limits](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container)
  for each sidecar container. Optional.

### Requests

CPU and memory requests for the sidecar container.

* **cpu**
  CPU requests for the sidecar container. Optional, overrides the
  `default_cpu_requests` operator configuration parameter. Optional.

* **memory**
  memory requests for the sidecar container. Optional, overrides the
  `default_memory_request` operator configuration parameter. Optional.

* **hugepages-2Mi**
  hugepages-2Mi requests for the sidecar container.
  Optional, defaults to not set.

* **hugepages-1Gi**
  1Gi hugepages requests for the sidecar container.
  Optional, defaults to not set.

### Limits

CPU and memory limits for the sidecar container.

* **cpu**
  CPU limits for the sidecar container. Optional, overrides the
  `default_cpu_limits` operator configuration parameter. Optional.

* **memory**
  memory limits for the sidecar container. Optional, overrides the
  `default_memory_limits` operator configuration parameter. Optional.

* **hugepages-2Mi**
  hugepages-2Mi requests for the sidecar container.
  Optional, defaults to not set.

* **hugepages-1Gi**
  1Gi hugepages requests for the sidecar container.
  Optional, defaults to not set.

## Connection pooler

Parameters are grouped under the `connectionPooler` top-level key and specify
configuration for connection pooler. If this section is not empty, a connection
pooler will be created for master service only even if `enableConnectionPooler`
is not present. But if this section is present then it defines the configuration
for both master and replica pooler services (if `enableReplicaConnectionPooler`
 is enabled).

* **numberOfInstances**
  How many instances of connection pooler to create.

* **schema**
  Database schema to create for credentials lookup function.

* **user**
  User to create for connection pooler to be able to connect to a database.
  You can also choose a role from the `users` section or a system user role.

* **dockerImage**
  Which docker image to use for connection pooler deployment.

* **maxDBConnections**
  How many connections the pooler can max hold. This value is divided among the
  pooler pods.

* **mode**
  In which mode to run connection pooler, transaction or session.

* **resources**
  Resource configuration for connection pooler deployment.

## Custom TLS certificates

Those parameters are grouped under the `tls` top-level key. Note, you have to
define `spiloFSGroup` in the Postgres cluster manifest or `spilo_fsgroup` in
the global configuration before adding the `tls` section'.

* **secretName**
  By setting the `secretName` value, the cluster will switch to load the given
  Kubernetes Secret into the container as a volume and uses that as the
  certificate instead. It is up to the user to create and manage the
  Kubernetes Secret either by hand or using a tool like the CertManager
  operator.

* **certificateFile**
  Filename of the certificate. Defaults to "tls.crt".

* **privateKeyFile**
  Filename of the private key. Defaults to "tls.key".

* **caFile**
  Optional filename to the CA certificate (e.g. "ca.crt"). Useful when the
  client connects with `sslmode=verify-ca` or `sslmode=verify-full`.
  Default is empty.

* **caSecretName**
  By setting the `caSecretName` value, the ca certificate file defined by the
  `caFile` will be fetched from this secret instead of `secretName` above.
  This secret has to hold a file with that name in its root.

  Optionally one can provide full path for any of them. By default it is
  relative to the "/tls/", which is mount path of the tls secret.
  If `caSecretName` is defined, the ca.crt path is relative to "/tlsca/",
  otherwise to the same "/tls/".

## Change data capture streams

This sections enables change data capture (CDC) streams via Postgres' 
[logical decoding](https://www.postgresql.org/docs/17/logicaldecoding.html)
feature and `pgoutput` plugin. While the Postgres operator takes responsibility
for providing the setup to publish change events, it relies on external tools
to consume them. At Zalando, we are using a workflow based on
[Debezium Connector](https://debezium.io/documentation/reference/stable/connectors/postgresql.html)
which can feed streams into Zalando’s distributed event broker [Nakadi](https://nakadi.io/)
among others.

The Postgres Operator creates custom resources for Zalando's internal CDC
operator which will be used to set up the consumer part. Each stream object
can have the following properties:

* **applicationId**
  The application name to which the database and CDC belongs to. For each
  set of streams with a distinct `applicationId` a separate stream resource as
  well as a separate logical replication slot will be created. This means there
  can be different streams in the same database and streams with the same
  `applicationId` are bundled in one stream resource. The stream resource will
  be called like the Postgres cluster plus "-<applicationId>" suffix. Required.

* **database**
  Name of the database from where events will be published via Postgres'
  logical decoding feature. The operator will take care of updating the
  database configuration (setting `wal_level: logical`, creating logical
  replication slots, using output plugin `pgoutput` and creating a dedicated
  replication user). Required.

* **tables**
  Defines a map of table names and their properties (`eventType`, `idColumn`
  and `payloadColumn`). Required.
  The CDC operator is following the [outbox pattern](https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/).
  The application is responsible for putting events into a (JSON/B or VARCHAR)
  payload column of the outbox table in the structure of the specified target
  event type. The operator will create a [PUBLICATION](https://www.postgresql.org/docs/17/logical-replication-publication.html)
  in Postgres for all tables specified for one `database` and `applicationId`.
  The CDC operator will consume from it shortly after transactions are
  committed to the outbox table. The `idColumn` will be used in telemetry for
  the CDC operator. The names for `idColumn` and `payloadColumn` can be
  configured. Defaults are `id` and `payload`. The target `eventType` has to
  be defined. One can also specify a `recoveryEventType` that will be used
  for a dead letter queue. By enabling `ignoreRecovery`, you can choose to
  ignore failing events.

* **filter**
  Streamed events can be filtered by a jsonpath expression for each table.
  Optional.

* **enableRecovery**
  Flag to enable a dead letter queue recovery for all streams tables.
  Alternatively, recovery can also be enable for single outbox tables by only
  specifying a `recoveryEventType` and no `enableRecovery` flag. When set to
  false or missing, events will be retried until consuming succeeded. You can
  use a `filter` expression to get rid of poison pills. Optional.

* **batchSize**
  Defines the size of batches in which events are consumed. Optional.
  Defaults to 1.

* **cpu**
  CPU requests to be set as an annotation on the stream resource. Optional.

* **memory**
  memory requests to be set as an annotation on the stream resource. Optional.


================================================
File: docs/reference/command_line_and_environment.md
================================================
# Command-line options

The following command-line options are supported for the operator:

* **-kubeconfig**
  the path to the kubeconfig file. Usually named config, it contains
  authorization information as well as the URL of the Kubernetes master.

* **-outofcluster**
  run the operator on a client machine, as opposed to a within the cluster.
  When running in this mode, the operator cannot connect to databases inside
  the cluster, as well as call URLs of in-cluster objects (i.e. teams api
  server). Mostly useful for debugging, it also requires setting the
  `OPERATOR_NAMESPACE` environment variable for the operator own namespace.

* **-nodatabaseaccess**
  disable database access from the operator. Equivalent to the
  `enable_database_access` set to off and can be overridden by the
  aforementioned operator configuration option.

* **-noteamsapi**
  disable access to the teams API. Equivalent to the `enable_teams_api` set to
  off can can be overridden by the aforementioned operator configuration
  option.

In addition to that, standard [glog
flags](https://godoc.org/github.com/golang/glog) are also supported. For
instance, one may want to add `-alsologtostderr` and `-v=8` to debug the
operator REST calls.

# Environment variables

The following environment variables are accepted by the operator:

* **CONFIG_MAP_NAME**
  name of the config map where the operator should look for its configuration.
  Must be present.

* **OPERATOR_NAMESPACE**
  name of the namespace the operator runs it. Overrides autodetection by the
  operator itself.

* **WATCHED_NAMESPACE**
  the name of the namespace the operator watches. Special '*' character denotes
  all namespaces. Empty value defaults to the operator namespace. Overrides the
  `watched_namespace` operator parameter.

* **SCALYR_API_KEY** (*deprecated*)
  the value of the Scalyr API key to supply to the pods. Overrides the
  `scalyr_api_key` operator parameter.

* **CRD_READY_WAIT_TIMEOUT**
  defines the timeout for the complete `postgresql` CRD creation. When not set
  default is 30s.

* **CRD_READY_WAIT_INTERVAL**
  defines the  interval between consecutive attempts waiting for the
  `postgresql` CRD to be created. The default is 5s.
  
* **ENABLE_JSON_LOGGING**
  Set to `true` for JSON formatted logging output.
  The default is false.


================================================
File: e2e/README.md
================================================
# Postgres Operator end-to-end tests

End-to-end tests shall ensure that the Postgres Operator does its job when
applying manifests against a Kubernetes (K8s) environment. A test runner
Dockerfile is provided to run e2e tests without the need to install K8s and
its runtime `kubectl` in advance. The test runner uses
[kind](https://kind.sigs.k8s.io/) to create a local K8s cluster which runs on
Docker.

## Prerequisites

Docker
Go

# Notice

The `manifest` folder in e2e tests folder is not commited to git, it comes from `/manifests`

## Build test runner

In the directory of the cloned Postgres Operator repository change to the e2e
folder and run:

```bash
make
```

This will build the `postgres-operator-e2e-tests` image and download the kind
runtime.

## Run tests

In the e2e folder you can invoke tests either with `make test` or with:

```bash
./run.sh main
```

To run both the build and test step you can invoke `make e2e` from the parent
directory.

To run the end 2 end test and keep the kind state execute:
```bash
NOCLEANUP=True ./run.sh main
```

## Run individual test

After having executed a normal E2E run with `NOCLEANUP=True` Kind still continues to run, allowing you subsequent test runs.

To run an individual test, run the following command in the `e2e` directory

```bash
NOCLEANUP=True ./run.sh main tests.test_e2e.EndToEndTestCase.test_lazy_spilo_upgrade
```

## Inspecting Kind

If you want to inspect Kind/Kubernetes cluster, switch `kubeconfig` file and context
```bash
# save the old config in case you have it
export KUBECONFIG_SAVED=$KUBECONFIG

# use the one created by e2e tests
export KUBECONFIG=/tmp/kind-config-postgres-operator-e2e-tests

# this kubeconfig defines a single context
kubectl config use-context kind-postgres-operator-e2e-tests
```

or use the following script to exec into the K8s setup and then use `kubectl`

```bash
./exec_into_env.sh

# use kubectl
kubectl get pods

# watch relevant objects
./scripts/watch_objects.sh

# get operator logs
./scripts/get_logs.sh
```

If you want to inspect the state of the `kind` cluster manually with a single command, add a `context` flag
```bash
kubectl get pods --context kind-kind
```
or set the context for a few commands at once



## Cleaning up Kind

To cleanup kind and start fresh

```bash
e2e/run.sh cleanup
```

That also helps in case you see the
```
ERROR: no nodes found for cluster "postgres-operator-e2e-tests"
```
that happens when the `kind` cluster was deleted manually but its configuraiton file was not.

## Covered use cases

The current tests are all bundled in [`test_e2e.py`](tests/test_e2e.py):

* support for multiple namespaces
* scale Postgres cluster up and down
* taint-based eviction of Postgres pods
* invoking logical backup cron job
* uniqueness of master pod
* custom service annotations


================================================
File: e2e/Dockerfile
================================================
# An image to run e2e tests.
# The image does not include the tests; all necessary files are bind-mounted when a container starts.
FROM ubuntu:20.04
LABEL maintainer="Team ACID @ Zalando <team-acid@zalando.de>"

ENV TERM xterm-256color

COPY requirements.txt ./

RUN apt-get update \
    && apt-get install --no-install-recommends -y \
           python3 \
           python3-setuptools \
           python3-pip \
           curl \
           vim \
    && pip3 install --no-cache-dir -r requirements.txt \
    && curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.24.3/bin/linux/amd64/kubectl \
    && chmod +x ./kubectl \
    && mv ./kubectl /usr/local/bin/kubectl \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# working line
# python3 -m unittest discover -v --failfast -k test_e2e.EndToEndTestCase.test_lazy_spilo_upgrade --start-directory tests
ENTRYPOINT ["python3", "-m", "unittest"]
CMD ["discover","-v","--failfast","--start-directory","/tests"]

================================================
File: e2e/Makefile
================================================
.PHONY: clean copy docker push tools test

BINARY ?= postgres-operator-e2e-tests-runner
BUILD_FLAGS ?= -v
CGO_ENABLED ?= 0
ifeq ($(RACE),1)
	BUILD_FLAGS += -race -a
    CGO_ENABLED=1
endif

LOCAL_BUILD_FLAGS ?= $(BUILD_FLAGS)
LDFLAGS ?= -X=main.version=$(VERSION)

IMAGE            ?= registry.opensource.zalan.do/acid/$(BINARY)
VERSION          ?= $(shell git describe --tags --always --dirty)
TAG              ?= $(VERSION)
GITHEAD          = $(shell git rev-parse --short HEAD)
GITURL           = $(shell git config --get remote.origin.url)
GITSTATU         = $(shell git status --porcelain || echo 'no changes')
TTYFLAGS         = $(shell test -t 0 && echo '-it')

ifndef GOPATH
	GOPATH := $(HOME)/go
endif

PATH := $(GOPATH)/bin:$(PATH)

default: tools

clean:
	rm -rf manifests
	rm -rf tls

copy: clean
	mkdir manifests
	cp -r ../manifests .
	mkdir tls

docker:
	docker build -t "$(IMAGE):$(TAG)" .

push: docker
	docker push "$(IMAGE):$(TAG)"

tools:
	# install pinned version of 'kind'
	# go install must run outside of a dir with a (module-based) Go project !
	# otherwise go install updates project's dependencies and/or behaves differently
	cd "/tmp" && GO111MODULE=on go install sigs.k8s.io/kind@v0.24.0

e2etest: tools copy clean
	./run.sh main

cleanup: clean
	./run.sh cleanup

================================================
File: e2e/exec.sh
================================================
#!/usr/bin/env bash
kubectl exec -i $1 -- sh -c "$2"


================================================
File: e2e/exec_into_env.sh
================================================
#!/bin/bash

export cluster_name="postgres-operator-e2e-tests"
export kubeconfig_path="/tmp/kind-config-${cluster_name}"
export operator_image="registry.opensource.zalan.do/acid/postgres-operator:latest"
export e2e_test_runner_image="registry.opensource.zalan.do/acid/postgres-operator-e2e-tests-runner:0.4"

docker run -it --entrypoint /bin/bash --network=host -e "TERM=xterm-256color" \
    --mount type=bind,source="$(readlink -f ${kubeconfig_path})",target=/root/.kube/config \
    --mount type=bind,source="$(readlink -f manifests)",target=/manifests \
    --mount type=bind,source="$(readlink -f tests)",target=/tests \
    --mount type=bind,source="$(readlink -f exec.sh)",target=/exec.sh \
    --mount type=bind,source="$(readlink -f scripts)",target=/scripts \
    -e OPERATOR_IMAGE="${operator_image}" "${e2e_test_runner_image}"


================================================
File: e2e/kind-cluster-postgres-operator-e2e-tests.yaml
================================================
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
featureGates:
  StatefulSetAutoDeletePVC: true


================================================
File: e2e/requirements.txt
================================================
kubernetes==29.2.0
timeout_decorator==0.5.0
pyyaml==6.0.1


================================================
File: e2e/run.sh
================================================
#!/usr/bin/env bash

# enable unofficial bash strict mode
set -o errexit
set -o nounset
set -o pipefail
IFS=$'\n\t'

readonly cluster_name="postgres-operator-e2e-tests"
readonly kubeconfig_path="/tmp/kind-config-${cluster_name}"
readonly spilo_image="registry.opensource.zalan.do/acid/spilo-17-e2e:0.3"
readonly e2e_test_runner_image="registry.opensource.zalan.do/acid/postgres-operator-e2e-tests-runner:0.4"

export GOPATH=${GOPATH-~/go}
export PATH=${GOPATH}/bin:$PATH

echo "Clustername: ${cluster_name}"
echo "Kubeconfig path: ${kubeconfig_path}"

function pull_images(){
  operator_tag=$(git describe --tags --always --dirty)
  if [[ -z $(docker images -q registry.opensource.zalan.do/acid/postgres-operator:${operator_tag}) ]]
  then
    docker pull registry.opensource.zalan.do/acid/postgres-operator:latest
  fi
  operator_image=$(docker images --filter=reference="registry.opensource.zalan.do/acid/postgres-operator" --format "{{.Repository}}:{{.Tag}}" | head -1)
}

function start_kind(){
  echo "Starting kind for e2e tests"
  # avoid interference with previous test runs
  if [[ $(kind get clusters | grep "^${cluster_name}*") != "" ]]
  then
    kind delete cluster --name ${cluster_name}
  fi

  export KUBECONFIG="${kubeconfig_path}"
  kind create cluster --name ${cluster_name} --config kind-cluster-postgres-operator-e2e-tests.yaml  
  docker pull "${spilo_image}"
  kind load docker-image "${spilo_image}" --name ${cluster_name}
}

function load_operator_image() {
  echo "Loading operator image"
  export KUBECONFIG="${kubeconfig_path}"
  kind load docker-image "${operator_image}" --name ${cluster_name}
}

function set_kind_api_server_ip(){
  echo "Setting up kind API server ip"
  # use the actual kubeconfig to connect to the 'kind' API server
  # but update the IP address of the API server to the one from the Docker 'bridge' network
  readonly local kind_api_server_port=6443 # well-known in the 'kind' codebase
  readonly local kind_api_server=$(docker inspect --format "{{ .NetworkSettings.Networks.kind.IPAddress }}:${kind_api_server_port}" "${cluster_name}"-control-plane)
  sed -i "s/server.*$/server: https:\/\/$kind_api_server/g" "${kubeconfig_path}"
}

function generate_certificate(){
  openssl req -x509 -nodes -newkey rsa:2048 -keyout tls/tls.key -out tls/tls.crt -subj "/CN=acid.zalan.do"
}

function run_tests(){
  echo "Running tests... image: ${e2e_test_runner_image}"
  # tests modify files in ./manifests, so we mount a copy of this directory done by the e2e Makefile

  docker run --rm --network=host -e "TERM=xterm-256color" \
  --mount type=bind,source="$(readlink -f ${kubeconfig_path})",target=/root/.kube/config \
  --mount type=bind,source="$(readlink -f manifests)",target=/manifests \
  --mount type=bind,source="$(readlink -f tls)",target=/tls \
  --mount type=bind,source="$(readlink -f tests)",target=/tests \
  --mount type=bind,source="$(readlink -f exec.sh)",target=/exec.sh \
  --mount type=bind,source="$(readlink -f scripts)",target=/scripts \
  -e OPERATOR_IMAGE="${operator_image}" "${e2e_test_runner_image}" ${E2E_TEST_CASE-} $@
}

function cleanup(){
  echo "Executing cleanup"
  unset KUBECONFIG
  kind delete cluster --name ${cluster_name}
  rm -rf ${kubeconfig_path}
}

function main(){
  echo "Entering main function..."
  [[ -z ${NOCLEANUP-} ]] && trap "cleanup" QUIT TERM EXIT
  pull_images
  [[ ! -f ${kubeconfig_path} ]] && start_kind
  load_operator_image
  set_kind_api_server_ip
  generate_certificate

  shift
  run_tests $@
  exit 0
}

"$1" $@


================================================
File: e2e/scripts/cleanup.sh
================================================
#!/bin/bash
kubectl delete postgresql acid-minimal-cluster
kubectl delete deployments -l application=db-connection-pooler,cluster-name=acid-minimal-cluster
kubectl delete statefulsets -l application=spilo,cluster-name=acid-minimal-cluster
kubectl delete services -l application=spilo,cluster-name=acid-minimal-cluster
kubectl delete configmap postgres-operator
kubectl delete deployment postgres-operator

================================================
File: e2e/scripts/get_logs.sh
================================================
#!/bin/bash
kubectl logs $(kubectl get pods -l name=postgres-operator --field-selector status.phase=Running -o jsonpath='{.items..metadata.name}')


================================================
File: e2e/scripts/watch_objects.sh
================================================
#!/bin/bash

watch -c "
kubectl get postgresql --all-namespaces
echo
echo -n 'Rolling upgrade pending: '
kubectl get pods -o jsonpath='{.items[].metadata.annotations.zalando-postgres-operator-rolling-update-required}'
echo
echo
echo 'Pods'
kubectl get pods -l application=spilo -o wide --all-namespaces
echo
kubectl get pods -l application=db-connection-pooler -o wide --all-namespaces
echo
echo 'Statefulsets'
kubectl get statefulsets --all-namespaces
echo
echo 'Deployments'
kubectl get deployments --all-namespaces -l application=db-connection-pooler
kubectl get deployments --all-namespaces -l application=postgres-operator
echo
echo
echo 'Step from operator deployment'
kubectl get pods -l name=postgres-operator -o jsonpath='{.items..metadata.annotations.step}'
echo
echo
echo 'Spilo Image in statefulset'
kubectl get pods -l application=spilo -o jsonpath='{.items..spec.containers..image}'
echo
echo
echo 'Queue Status'
kubectl exec -it \$(kubectl get pods -l name=postgres-operator -o jsonpath='{.items..metadata.name}') -- curl localhost:8080/workers/all/status/
echo"

================================================
File: e2e/tests/__init__.py
================================================
# This version is replaced during release process.
__version__ = '2019.0.dev1'


================================================
File: e2e/tests/k8s_api.py
================================================
import json
import time
import subprocess
import warnings

from kubernetes import client, config
from kubernetes.client.rest import ApiException


def to_selector(labels):
    return ",".join(["=".join(lbl) for lbl in labels.items()])


class K8sApi:

    def __init__(self):

        # https://github.com/kubernetes-client/python/issues/309
        warnings.simplefilter("ignore", ResourceWarning)

        self.config = config.load_kube_config()
        self.k8s_client = client.ApiClient()
        self.rbac_api = client.RbacAuthorizationV1Api()

        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.batch_v1 = client.BatchV1Api()
        self.custom_objects_api = client.CustomObjectsApi()
        self.policy_v1 = client.PolicyV1Api()
        self.storage_v1_api = client.StorageV1Api()


class K8s:
    '''
    Wraps around K8s api client and helper methods.
    '''

    RETRY_TIMEOUT_SEC = 1

    def __init__(self, labels='x=y', namespace='default'):
        self.api = K8sApi()
        self.labels = labels
        self.namespace = namespace

    def get_pg_nodes(self, pg_cluster_name, namespace='default'):
        master_pod_node = ''
        replica_pod_nodes = []
        podsList = self.api.core_v1.list_namespaced_pod(namespace, label_selector=pg_cluster_name)
        for pod in podsList.items:
            if pod.metadata.labels.get('spilo-role') == 'master':
                master_pod_node = pod.spec.node_name
            elif pod.metadata.labels.get('spilo-role') == 'replica':
                replica_pod_nodes.append(pod.spec.node_name)

        return master_pod_node, replica_pod_nodes

    def get_cluster_nodes(self, cluster_labels='application=spilo,cluster-name=acid-minimal-cluster', namespace='default'):
        m = []
        r = []
        podsList = self.api.core_v1.list_namespaced_pod(namespace, label_selector=cluster_labels)
        for pod in podsList.items:
            if pod.metadata.labels.get('spilo-role') == 'master' and pod.status.phase == 'Running':
                m.append(pod.spec.node_name)
            elif pod.metadata.labels.get('spilo-role') == 'replica' and pod.status.phase == 'Running':
                r.append(pod.spec.node_name)

        return m, r

    def wait_for_operator_pod_start(self):
        self.wait_for_pod_start("name=postgres-operator")
        # give operator time to subscribe to objects
        time.sleep(1)
        return True

    def get_operator_pod(self):
        pods = self.api.core_v1.list_namespaced_pod(
            'default', label_selector='name=postgres-operator'
        ).items

        pods = list(filter(lambda x: x.status.phase == 'Running', pods))

        if len(pods):
            return pods[0]

        return None

    def get_operator_log(self):
        operator_pod = self.get_operator_pod()
        pod_name = operator_pod.metadata.name
        return self.api.core_v1.read_namespaced_pod_log(
            name=pod_name,
            namespace='default'
        )

    def pg_get_status(self, name="acid-minimal-cluster", namespace="default"):
        pg = self.api.custom_objects_api.get_namespaced_custom_object(
            "acid.zalan.do", "v1", namespace, "postgresqls", name)
        return pg.get("status", {}).get("PostgresClusterStatus", None)

    def wait_for_pod_start(self, pod_labels, namespace='default'):
        pod_phase = 'No pod running'
        while pod_phase != 'Running':
            pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=pod_labels).items
            if pods:
                pod_phase = pods[0].status.phase

            time.sleep(self.RETRY_TIMEOUT_SEC)

    def get_service_type(self, svc_labels, namespace='default'):
        svc_type = ''
        svcs = self.api.core_v1.list_namespaced_service(namespace, label_selector=svc_labels, limit=1).items
        for svc in svcs:
            svc_type = svc.spec.type
        return svc_type

    def check_service_annotations(self, svc_labels, annotations, namespace='default'):
        svcs = self.api.core_v1.list_namespaced_service(namespace, label_selector=svc_labels, limit=1).items
        for svc in svcs:
            for key, value in annotations.items():
                if not svc.metadata.annotations or key not in svc.metadata.annotations or svc.metadata.annotations[key] != value:
                    print("Expected key {} not found in service annotations {}".format(key, svc.metadata.annotations))
                    return False
        return True

    def check_statefulset_annotations(self, sset_labels, annotations, namespace='default'):
        ssets = self.api.apps_v1.list_namespaced_stateful_set(namespace, label_selector=sset_labels, limit=1).items
        for sset in ssets:
            for key, value in annotations.items():
                if key not in sset.metadata.annotations or sset.metadata.annotations[key] != value:
                    print("Expected key {} not found in statefulset annotations {}".format(key, sset.metadata.annotations))
                    return False
        return True

    def scale_cluster(self, number_of_instances, name="acid-minimal-cluster", namespace="default"):
        body = {
            "spec": {
                "numberOfInstances": number_of_instances
            }
        }
        self.api.custom_objects_api.patch_namespaced_custom_object(
            "acid.zalan.do", "v1", namespace, "postgresqls", name, body)

    def wait_for_running_pods(self, labels, number, namespace=''):
        while self.count_pods_with_label(labels) != number:
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def wait_for_pods_to_stop(self, labels, namespace=''):
        while self.count_pods_with_label(labels) != 0:
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def wait_for_service(self, labels, namespace='default'):
        def get_services():
            return self.api.core_v1.list_namespaced_service(
                namespace, label_selector=labels
            ).items

        while not get_services():
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def count_pods_with_volume_mount(self, mount_name, labels, namespace='default'):
        pod_count = 0
        pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items
        for pod in pods:
            for mount in pod.spec.containers[0].volume_mounts:
                if mount.name == mount_name:
                    pod_count += 1

        return pod_count

    def count_pods_with_env_variable(self, env_variable_key, labels, namespace='default'):
        pod_count = 0
        pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items
        for pod in pods:
            for env in pod.spec.containers[0].env:
                if env.name == env_variable_key:
                    pod_count += 1

        return pod_count

    def count_pods_with_rolling_update_flag(self, labels, namespace='default'):
        pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items
        return len(list(filter(lambda x: "zalando-postgres-operator-rolling-update-required" in x.metadata.annotations, pods)))

    def count_pods_with_label(self, labels, namespace='default'):
        return len(self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items)

    def count_services_with_label(self, labels, namespace='default'):
        return len(self.api.core_v1.list_namespaced_service(namespace, label_selector=labels).items)

    def count_endpoints_with_label(self, labels, namespace='default'):
        return len(self.api.core_v1.list_namespaced_endpoints(namespace, label_selector=labels).items)

    def count_secrets_with_label(self, labels, namespace='default'):
        return len(self.api.core_v1.list_namespaced_secret(namespace, label_selector=labels).items)

    def count_statefulsets_with_label(self, labels, namespace='default'):
        return len(self.api.apps_v1.list_namespaced_stateful_set(namespace, label_selector=labels).items)

    def count_deployments_with_label(self, labels, namespace='default'):
        return len(self.api.apps_v1.list_namespaced_deployment(namespace, label_selector=labels).items)

    def count_pdbs_with_label(self, labels, namespace='default'):
        return len(self.api.policy_v1.list_namespaced_pod_disruption_budget(
            namespace, label_selector=labels).items)

    def count_pvcs_with_label(self, labels, namespace='default'):
        return len(self.api.core_v1.list_namespaced_persistent_volume_claim(namespace, label_selector=labels).items)

    def count_running_pods(self, labels='application=spilo,cluster-name=acid-minimal-cluster', namespace='default'):
        pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items
        return len(list(filter(lambda x: x.status.phase == 'Running', pods)))

    def count_pods_with_container_capabilities(self, capabilities, labels, namespace='default'):
        pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items
        return len(list(filter(lambda x: x.spec.containers[0].security_context.capabilities.add == capabilities, pods)))

    def wait_for_pod_failover(self, failover_targets, labels, namespace='default'):
        pod_phase = 'Failing over'
        new_pod_node = ''
        pods_with_update_flag = self.count_pods_with_rolling_update_flag(labels, namespace)
        while (pod_phase != 'Running') or (new_pod_node not in failover_targets):
            pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items
            if pods:
                new_pod_node = pods[0].spec.node_name
                pod_phase = pods[0].status.phase
            time.sleep(self.RETRY_TIMEOUT_SEC)
        
        while pods_with_update_flag != 0:
            pods_with_update_flag = self.count_pods_with_rolling_update_flag(labels, namespace)
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def wait_for_namespace_creation(self, namespace='default'):
        ns_found = False
        while ns_found != True:
            ns = self.api.core_v1.list_namespace().items
            for n in ns:
                if n.metadata.name == namespace:
                    ns_found = True
                    break
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def get_logical_backup_job(self, namespace='default'):
        return self.api.batch_v1.list_namespaced_cron_job(namespace, label_selector="application=spilo")

    def wait_for_logical_backup_job(self, expected_num_of_jobs):
        while (len(self.get_logical_backup_job().items) != expected_num_of_jobs):
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def wait_for_logical_backup_job_deletion(self):
        self.wait_for_logical_backup_job(expected_num_of_jobs=0)

    def wait_for_logical_backup_job_creation(self):
        self.wait_for_logical_backup_job(expected_num_of_jobs=1)

    def delete_operator_pod(self, step="Delete operator pod"):
        # patching the pod template in the deployment restarts the operator pod
        self.api.apps_v1.patch_namespaced_deployment("postgres-operator", "default", {"spec": {"template": {"metadata": {"annotations": {"step": "{}-{}".format(step, time.time())}}}}})
        self.wait_for_operator_pod_start()

    def update_config(self, config_map_patch, step="Updating operator deployment"):
        self.api.core_v1.patch_namespaced_config_map("postgres-operator", "default", config_map_patch)
        self.delete_operator_pod(step=step)

    def patch_pod(self, data, pod_name, namespace="default"):
        self.api.core_v1.patch_namespaced_pod(pod_name, namespace, data)

    def create_tls_secret_with_kubectl(self, secret_name):
        return subprocess.run(
            ["kubectl", "create", "secret", "tls", secret_name, "--key=tls/tls.key", "--cert=tls/tls.crt"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)

    def create_tls_ca_secret_with_kubectl(self, secret_name):
        return subprocess.run(
            ["kubectl", "create", "secret", "generic", secret_name, "--from-file=ca.crt=tls/ca.crt"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)

    def create_with_kubectl(self, path):
        return subprocess.run(
            ["kubectl", "apply", "-f", path],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)

    def exec_with_kubectl(self, pod, cmd):
        return subprocess.run(["./exec.sh", pod, cmd],
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE)

    def patroni_rest(self, pod, path):
        r = self.exec_with_kubectl(pod, "curl localhost:8008/" + path)
        if not r.returncode == 0 or not r.stdout.decode()[0:1] == "{":
            return None

        return json.loads(r.stdout.decode())

    def get_patroni_state(self, pod):
        r = self.exec_with_kubectl(pod, "patronictl list -f json")
        if not r.returncode == 0 or not r.stdout.decode()[0:1] == "[":
            return []
        return json.loads(r.stdout.decode())

    def get_operator_state(self):
        pod = self.get_operator_pod()
        if pod is None:
            return None
        pod = pod.metadata.name

        r = self.exec_with_kubectl(pod, "curl localhost:8080/workers/all/status/")
        if not r.returncode == 0 or not r.stdout.decode()[0:1] == "{":
            return None

        return json.loads(r.stdout.decode())

    def get_patroni_running_members(self, pod="acid-minimal-cluster-0"):
        result = self.get_patroni_state(pod)
        return list(filter(lambda x: "State" in x and x["State"] in ["running", "streaming"], result))

    def get_deployment_replica_count(self, name="acid-minimal-cluster-pooler", namespace="default"):
        try:
            deployment = self.api.apps_v1.read_namespaced_deployment(name, namespace)
            return deployment.spec.replicas
        except ApiException:
            return None

    def get_statefulset_image(self, label_selector="application=spilo,cluster-name=acid-minimal-cluster", namespace='default'):
        ssets = self.api.apps_v1.list_namespaced_stateful_set(namespace, label_selector=label_selector, limit=1)
        if len(ssets.items) == 0:
            return None
        return ssets.items[0].spec.template.spec.containers[0].image

    def get_effective_pod_image(self, pod_name, namespace='default'):
        '''
        Get the Spilo image pod currently uses. In case of lazy rolling updates
        it may differ from the one specified in the stateful set.
        '''
        pod = self.api.core_v1.list_namespaced_pod(
            namespace, label_selector="statefulset.kubernetes.io/pod-name=" + pod_name)

        if len(pod.items) == 0:
            return None
        return pod.items[0].spec.containers[0].image

    def get_cluster_pod(self, role, labels='application=spilo,cluster-name=acid-minimal-cluster', namespace='default'):
        labels = labels + ',spilo-role=' + role

        pods = self.api.core_v1.list_namespaced_pod(
                namespace, label_selector=labels).items

        if pods:
            return pods[0]

    def get_cluster_leader_pod(self, labels='application=spilo,cluster-name=acid-minimal-cluster', namespace='default'):
        return self.get_cluster_pod('master', labels, namespace)

    def get_cluster_replica_pod(self, labels='application=spilo,cluster-name=acid-minimal-cluster', namespace='default'):
        return self.get_cluster_pod('replica', labels, namespace)

    def get_secret(self, username, clustername='acid-minimal-cluster', namespace='default'):
        secret = self.api.core_v1.read_namespaced_secret(
                "{}.{}.credentials.postgresql.acid.zalan.do".format(username.replace("_","-"), clustername), namespace)
        secret.metadata.resource_version = None
        secret.metadata.uid = None
        return secret

    def create_secret(self, secret, namespace='default'):
        return self.api.core_v1.create_namespaced_secret(namespace, secret)

class K8sBase:
    '''
    K8s basic API wrapper class supposed to be inherited by other more specific classes for e2e tests
    '''

    RETRY_TIMEOUT_SEC = 1

    def __init__(self, labels='x=y', namespace='default'):
        self.api = K8sApi()
        self.labels = labels
        self.namespace = namespace

    def get_pg_nodes(self, pg_cluster_labels='cluster-name=acid-minimal-cluster', namespace='default'):
        master_pod_node = ''
        replica_pod_nodes = []
        podsList = self.api.core_v1.list_namespaced_pod(namespace, label_selector=pg_cluster_labels)
        for pod in podsList.items:
            if pod.metadata.labels.get('spilo-role') == 'master':
                master_pod_node = pod.spec.node_name
            elif pod.metadata.labels.get('spilo-role') == 'replica':
                replica_pod_nodes.append(pod.spec.node_name)

        return master_pod_node, replica_pod_nodes

    def get_cluster_nodes(self, cluster_labels='cluster-name=acid-minimal-cluster', namespace='default'):
        m = []
        r = []
        podsList = self.api.core_v1.list_namespaced_pod(namespace, label_selector=cluster_labels)
        for pod in podsList.items:
            if pod.metadata.labels.get('spilo-role') == 'master' and pod.status.phase == 'Running':
                m.append(pod.spec.node_name)
            elif pod.metadata.labels.get('spilo-role') == 'replica' and pod.status.phase == 'Running':
                r.append(pod.spec.node_name)

        return m, r

    def wait_for_operator_pod_start(self):
        self.wait_for_pod_start("name=postgres-operator")

    def get_operator_pod(self):
        pods = self.api.core_v1.list_namespaced_pod(
            'default', label_selector='name=postgres-operator'
        ).items

        if pods:
            return pods[0]

        return None

    def get_operator_log(self):
        operator_pod = self.get_operator_pod()
        pod_name = operator_pod.metadata.name
        return self.api.core_v1.read_namespaced_pod_log(
            name=pod_name,
            namespace='default'
        )

    def wait_for_pod_start(self, pod_labels, namespace='default'):
        pod_phase = 'No pod running'
        while pod_phase != 'Running':
            pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=pod_labels).items
            if pods:
                pod_phase = pods[0].status.phase

            time.sleep(self.RETRY_TIMEOUT_SEC)

    def get_service_type(self, svc_labels, namespace='default'):
        svc_type = ''
        svcs = self.api.core_v1.list_namespaced_service(namespace, label_selector=svc_labels, limit=1).items
        for svc in svcs:
            svc_type = svc.spec.type
        return svc_type

    def check_service_annotations(self, svc_labels, annotations, namespace='default'):
        svcs = self.api.core_v1.list_namespaced_service(namespace, label_selector=svc_labels, limit=1).items
        for svc in svcs:
            for key, value in annotations.items():
                if key not in svc.metadata.annotations or svc.metadata.annotations[key] != value:
                    print("Expected key {} not found in annotations {}".format(key, svc.metadata.annotation))
                    return False
        return True

    def check_statefulset_annotations(self, sset_labels, annotations, namespace='default'):
        ssets = self.api.apps_v1.list_namespaced_stateful_set(namespace, label_selector=sset_labels, limit=1).items
        for sset in ssets:
            for key, value in annotations.items():
                if key not in sset.metadata.annotations or sset.metadata.annotations[key] != value:
                    print("Expected key {} not found in annotations {}".format(key, sset.metadata.annotation))
                    return False
        return True

    def scale_cluster(self, number_of_instances, name="acid-minimal-cluster", namespace="default"):
        body = {
            "spec": {
                "numberOfInstances": number_of_instances
            }
        }
        self.api.custom_objects_api.patch_namespaced_custom_object(
            "acid.zalan.do", "v1", namespace, "postgresqls", name, body)

    def wait_for_running_pods(self, labels, number, namespace=''):
        while self.count_pods_with_label(labels) != number:
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def wait_for_pods_to_stop(self, labels, namespace=''):
        while self.count_pods_with_label(labels) != 0:
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def wait_for_service(self, labels, namespace='default'):
        def get_services():
            return self.api.core_v1.list_namespaced_service(
                namespace, label_selector=labels
            ).items

        while not get_services():
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def count_pods_with_rolling_update_flag(self, labels, namespace='default'):
        pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items
        return len(list(filter(lambda x: "zalando-postgres-operator-rolling-update-required" in x.metadata.annotations, pods)))

    def count_pods_with_label(self, labels, namespace='default'):
        return len(self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items)

    def count_services_with_label(self, labels, namespace='default'):
        return len(self.api.core_v1.list_namespaced_service(namespace, label_selector=labels).items)

    def count_endpoints_with_label(self, labels, namespace='default'):
        return len(self.api.core_v1.list_namespaced_endpoints(namespace, label_selector=labels).items)

    def count_secrets_with_label(self, labels, namespace='default'):
        return len(self.api.core_v1.list_namespaced_secret(namespace, label_selector=labels).items)

    def count_statefulsets_with_label(self, labels, namespace='default'):
        return len(self.api.apps_v1.list_namespaced_stateful_set(namespace, label_selector=labels).items)

    def count_deployments_with_label(self, labels, namespace='default'):
        return len(self.api.apps_v1.list_namespaced_deployment(namespace, label_selector=labels).items)

    def count_pdbs_with_label(self, labels, namespace='default'):
        return len(self.api.policy_v1.list_namespaced_pod_disruption_budget(
            namespace, label_selector=labels).items)

    def count_pvcs_with_label(self, labels, namespace='default'):
        return len(self.api.core_v1.list_namespaced_persistent_volume_claim(namespace, label_selector=labels).items)

    def count_running_pods(self, labels='application=spilo,cluster-name=acid-minimal-cluster', namespace='default'):
        pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items
        return len(list(filter(lambda x: x.status.phase == 'Running', pods)))

    def count_pods_with_container_capabilities(self, capabilities, labels, namespace='default'):
        pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items
        return len(list(filter(lambda x: x.spec.containers[0].security_context.capabilities.add == capabilities, pods)))

    def wait_for_pod_failover(self, failover_targets, labels, namespace='default'):
        pod_phase = 'Failing over'
        new_pod_node = ''
        pods_with_update_flag = self.count_pods_with_rolling_update_flag(labels, namespace)
        while (pod_phase != 'Running') or (new_pod_node not in failover_targets):
            pods = self.api.core_v1.list_namespaced_pod(namespace, label_selector=labels).items
            if pods:
                new_pod_node = pods[0].spec.node_name
                pod_phase = pods[0].status.phase
            time.sleep(self.RETRY_TIMEOUT_SEC)

        while pods_with_update_flag != 0:
            pods_with_update_flag = self.count_pods_with_rolling_update_flag(labels, namespace)
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def get_logical_backup_job(self, namespace='default'):
        return self.api.batch_v1.list_namespaced_cron_job(namespace, label_selector="application=spilo")

    def wait_for_logical_backup_job(self, expected_num_of_jobs):
        while (len(self.get_logical_backup_job().items) != expected_num_of_jobs):
            time.sleep(self.RETRY_TIMEOUT_SEC)

    def wait_for_logical_backup_job_deletion(self):
        self.wait_for_logical_backup_job(expected_num_of_jobs=0)

    def wait_for_logical_backup_job_creation(self):
        self.wait_for_logical_backup_job(expected_num_of_jobs=1)

    def delete_operator_pod(self, step="Delete operator deplyment"):
        self.api.apps_v1.patch_namespaced_deployment("postgres-operator","default", {"spec":{"template":{"metadata":{"annotations":{"step":"{}-{}".format(step, time.time())}}}}})
        self.wait_for_operator_pod_start()

    def update_config(self, config_map_patch, step="Updating operator deployment"):
        self.api.core_v1.patch_namespaced_config_map("postgres-operator", "default", config_map_patch)
        self.delete_operator_pod(step=step)

    def create_with_kubectl(self, path):
        return subprocess.run(
            ["kubectl", "apply", "-f", path],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)

    def exec_with_kubectl(self, pod, cmd):
        return subprocess.run(["./exec.sh", pod, cmd],
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE)

    def patroni_rest(self, pod, path):
        r = self.exec_with_kubectl(pod, "curl localhost:8008/" + path)
        if not r.returncode == 0 or not r.stdout.decode()[0:1] == "{":
            return None

        return json.loads(r.stdout.decode())

    def get_patroni_state(self, pod):
        r = self.exec_with_kubectl(pod, "patronictl list -f json")
        if not r.returncode == 0 or not r.stdout.decode()[0:1] == "[":
            return []
        return json.loads(r.stdout.decode())

    def get_patroni_running_members(self, pod):
        result = self.get_patroni_state(pod)
        return list(filter(lambda x: x["State"] in ["running", "streaming"], result))

    def get_statefulset_image(self, label_selector="application=spilo,cluster-name=acid-minimal-cluster", namespace='default'):
        ssets = self.api.apps_v1.list_namespaced_stateful_set(namespace, label_selector=label_selector, limit=1)
        if len(ssets.items) == 0:
            return None
        return ssets.items[0].spec.template.spec.containers[0].image

    def get_effective_pod_image(self, pod_name, namespace='default'):
        '''
        Get the Spilo image pod currently uses. In case of lazy rolling updates
        it may differ from the one specified in the stateful set.
        '''
        pod = self.api.core_v1.list_namespaced_pod(
            namespace, label_selector="statefulset.kubernetes.io/pod-name=" + pod_name)

        if len(pod.items) == 0:
            return None
        return pod.items[0].spec.containers[0].image


"""
  Inspiriational classes towards easier writing of end to end tests with one cluster per test case
"""


class K8sOperator(K8sBase):
    def __init__(self, labels="name=postgres-operator", namespace="default"):
        super().__init__(labels, namespace)


class K8sPostgres(K8sBase):
    def __init__(self, labels="cluster-name=acid-minimal-cluster", namespace="default"):
        super().__init__(labels, namespace)

    def get_pg_nodes(self):
        master_pod_node = ''
        replica_pod_nodes = []
        podsList = self.api.core_v1.list_namespaced_pod(self.namespace, label_selector=self.labels)
        for pod in podsList.items:
            if pod.metadata.labels.get('spilo-role') == 'master':
                master_pod_node = pod.spec.node_name
            elif pod.metadata.labels.get('spilo-role') == 'replica':
                replica_pod_nodes.append(pod.spec.node_name)

        return master_pod_node, replica_pod_nodes


================================================
File: hack/custom-boilerplate.go.txt
================================================
/*
Copyright YEAR Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/


================================================
File: hack/tools.go
================================================
// +build tools

/*
Copyright 2019 The Kubernetes Authors.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// This package imports things required by build scripts, to force `go mod` to see them as dependencies
package tools

import _ "k8s.io/code-generator"


================================================
File: hack/update-codegen.sh
================================================
#!/usr/bin/env bash

set -o errexit
set -o nounset
set -o pipefail

GENERATED_PACKAGE_ROOT="github.com"
OPERATOR_PACKAGE_ROOT="${GENERATED_PACKAGE_ROOT}/zalando/postgres-operator"
SCRIPT_ROOT=$(dirname ${BASH_SOURCE})/..
TARGET_CODE_DIR=${1-${SCRIPT_ROOT}/pkg}
CODEGEN_PKG=${CODEGEN_PKG:-$(cd "${SCRIPT_ROOT}"; ls -d -1 ./vendor/k8s.io/code-generator 2>/dev/null || echo "${GOPATH}"/src/k8s.io/code-generator)}

cleanup() {
    rm -rf "${GENERATED_PACKAGE_ROOT}"
}
trap "cleanup" EXIT SIGINT

bash "${CODEGEN_PKG}/generate-groups.sh" client,deepcopy,informer,lister \
  "${OPERATOR_PACKAGE_ROOT}/pkg/generated" "${OPERATOR_PACKAGE_ROOT}/pkg/apis" \
  "acid.zalan.do:v1 zalando.org:v1" \
  --go-header-file "${SCRIPT_ROOT}"/hack/custom-boilerplate.go.txt \
  -o ./

cp -r "${OPERATOR_PACKAGE_ROOT}"/pkg/* "${TARGET_CODE_DIR}"

cleanup


================================================
File: hack/verify-codegen.sh
================================================
#!/usr/bin/env bash

set -o errexit
set -o nounset
set -o pipefail

SCRIPT_ROOT=$(dirname "${BASH_SOURCE}")/..
DIFFROOT="${SCRIPT_ROOT}/pkg"
TMP_DIFFROOT="${SCRIPT_ROOT}/_tmp/pkg"
_tmp="${SCRIPT_ROOT}/_tmp"

cleanup() {
    rm -rf "${_tmp}"
}
trap "cleanup" EXIT SIGINT

cleanup

mkdir -p "${TMP_DIFFROOT}"
cp -a "${DIFFROOT}"/* "${TMP_DIFFROOT}"

"${SCRIPT_ROOT}/hack/update-codegen.sh" "${TMP_DIFFROOT}"
echo "diffing ${DIFFROOT} against freshly generated codegen"
ret=0
diff -Naupr "${DIFFROOT}" "${TMP_DIFFROOT}" || ret=$?
if [[ $ret -eq 0 ]]
then
    echo "${DIFFROOT} up to date."
else
    echo "${DIFFROOT} is out of date. Please run 'make codegen'"
    exit 1
fi


================================================
File: kubectl-pg/README.md
================================================
# Kubectl Plugin for Zalando's Postgres Operator

This plugin is a prototype developed as a part of **Google Summer of Code 2019** under the [Postgres Operator](https://summerofcode.withgoogle.com/archive/2019/organizations/6187982082539520/) organization.

## Installation of kubectl pg plugin

This project uses Go Modules for dependency management to build locally.
Install go and enable go modules with ```export GO111MODULE=on```.
From Go >=1.13 Go modules will be enabled by default.

```bash
# Assumes you have a working KUBECONFIG
$ GO111MODULE="on"
$ GOPATH/src/github.com/zalando/postgres-operator/kubectl-pg  go mod vendor
# This generate a vendor directory with all dependencies needed by the plugin.
$ $GOPATH/src/github.com/zalando/postgres-operator/kubectl-pg  go install
# This will place the kubectl-pg binary in your $GOPATH/bin
```

## Before using the kubectl pg plugin make sure to set KUBECONFIG env variable

Ideally KUBECONFIG is found in $HOME/.kube/config else specify the KUBECONFIG path here.
```export KUBECONFIG=$HOME/.kube/config```

## List all commands available in kubectl pg

```kubectl pg --help``` (or) ```kubectl pg```

## Check if Postgres Operator is installed and running

```kubectl pg check```

## Create a new cluster using manifest file

```kubectl pg create -f acid-minimal-cluster.yaml```

## List postgres resources

```kubectl pg list```

List clusters across namespaces
```kubectl pg list all```

## Update existing cluster using manifest file

```kubectl pg update -f acid-minimal-cluster.yaml```

## Delete existing cluster

Using the manifest file:
```kubectl pg delete -f acid-minimal-cluster.yaml```

Or by specifying the cluster name:
```kubectl pg delete acid-minimal-cluster```

Use `--namespace` or `-n` flag if your cluster is in a different namespace to where your current context is pointing to:
```kubectl pg delete acid-minimal-cluster -n namespace01```

## Adding manifest roles to an existing cluster

```kubectl pg add-user USER01 -p CREATEDB,LOGIN -c acid-minimal-cluster```

Privileges can only be [SUPERUSER, REPLICATION, INHERIT, LOGIN, NOLOGIN, CREATEROLE, CREATEDB, BYPASSRLS]
Note: By default, a LOGIN user is created (unless NOLOGIN is specified).

## Adding databases to an existing cluster

You have to specify an owner of the new database and this role must already exist in the cluster:
```kubectl pg add-db DB01 -o OWNER01 -c acid-minimal-cluster```

## Extend the volume of an existing pg cluster

```kubectl pg ext-volume 2Gi -c acid-minimal-cluster```

## Print the version of Postgres Operator and kubectl pg plugin

```kubectl pg version```

## Connect to the shell of a postgres pod

Connect to the master pod:
```kubectl pg connect -c CLUSTER -m```

Connect to a random replica pod:
```kubectl pg connect -c CLUSTER```

Connect to a certain replica pod:
```kubectl pg connect -c CLUSTER -r 0```

## Connect to a database via psql

Adding the `-p` flag allows you to directly connect to a given database with the psql client.
With `-u` you specify the user. If left out the name of the current OS user is taken.
`-d` lets you specify the database. If no database is specified, it will be the same as the user name.

Connect to `app_db` database on the master with role `app_user`:
```kubectl pg connect -c CLUSTER -m -p -u app_user -d app_db```

Connect to the `postgres` database on a random replica with role `postgres`:
```kubectl pg connect -c CLUSTER -p -u postgres```

Connect to a certain replica assuming name of OS user, database role and name are all the same:
```kubectl pg connect -c CLUSTER -r 0 -p```


## Access Postgres Operator logs

```kubectl pg logs -o```

## Access Patroni logs of different database pods

Fetch logs of master:
```kubectl pg logs -c CLUSTER -m```

Fetch logs of a random replica pod:
```kubectl pg logs -c CLUSTER```

Fetch logs of specified replica
```kubectl pg logs -c CLUSTER -r 2```

## Development

When making changes to the plugin make sure to change the (major/patch) version of plugin in `build.sh` script and run `./build.sh`.

## Google Summer of Code 2019

### GSoC Proposal

[kubectl pg proposal](https://docs.google.com/document/d/1-WMy9HkfZ1XnnMbzplMe9rCzKrRMGaMz4owLVXXPb7w/edit)

### Weekly Reports

https://github.com/VineethReddy02/GSoC-Kubectl-Plugin-for-Postgres-Operator-tracker

### Final Project Report

https://gist.github.com/VineethReddy02/159283bd368a710379eaf0f6bd60a40a


================================================
File: kubectl-pg/build.sh
================================================

VERSION=1.0
sed -i "s/KubectlPgVersion string = \"[^\"]*\"/KubectlPgVersion string = \"${VERSION}\"/" cmd/version.go
go install

================================================
File: kubectl-pg/go.mod
================================================
module github.com/zalando/postgres-operator/kubectl-pg

go 1.23.4

require (
	github.com/spf13/cobra v1.8.1
	github.com/spf13/viper v1.19.0
	github.com/zalando/postgres-operator v1.13.0
	k8s.io/api v0.30.4
	k8s.io/apiextensions-apiserver v0.25.9
	k8s.io/apimachinery v0.30.4
	k8s.io/client-go v0.30.4
)

require (
	github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc // indirect
	github.com/emicklei/go-restful/v3 v3.11.0 // indirect
	github.com/fsnotify/fsnotify v1.7.0 // indirect
	github.com/go-logr/logr v1.4.1 // indirect
	github.com/go-openapi/jsonpointer v0.19.6 // indirect
	github.com/go-openapi/jsonreference v0.20.2 // indirect
	github.com/go-openapi/swag v0.22.3 // indirect
	github.com/gogo/protobuf v1.3.2 // indirect
	github.com/golang/protobuf v1.5.4 // indirect
	github.com/google/gnostic-models v0.6.8 // indirect
	github.com/google/gofuzz v1.2.0 // indirect
	github.com/google/uuid v1.4.0 // indirect
	github.com/gorilla/websocket v1.5.0 // indirect
	github.com/hashicorp/hcl v1.0.0 // indirect
	github.com/imdario/mergo v0.3.6 // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/josharian/intern v1.0.0 // indirect
	github.com/json-iterator/go v1.1.12 // indirect
	github.com/kr/text v0.2.0 // indirect
	github.com/magiconair/properties v1.8.7 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/mitchellh/mapstructure v1.5.0 // indirect
	github.com/moby/spdystream v0.2.0 // indirect
	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
	github.com/modern-go/reflect2 v1.0.2 // indirect
	github.com/motomux/pretty v0.0.0-20161209205251-b2aad2c9a95d // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f // indirect
	github.com/pelletier/go-toml/v2 v2.2.2 // indirect
	github.com/sagikazarmark/locafero v0.4.0 // indirect
	github.com/sagikazarmark/slog-shim v0.1.0 // indirect
	github.com/sirupsen/logrus v1.9.3 // indirect
	github.com/sourcegraph/conc v0.3.0 // indirect
	github.com/spf13/afero v1.11.0 // indirect
	github.com/spf13/cast v1.6.0 // indirect
	github.com/spf13/pflag v1.0.5 // indirect
	github.com/subosito/gotenv v1.6.0 // indirect
	go.uber.org/multierr v1.11.0 // indirect
	golang.org/x/crypto v0.31.0 // indirect
	golang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3 // indirect
	golang.org/x/net v0.25.0 // indirect
	golang.org/x/oauth2 v0.18.0 // indirect
	golang.org/x/sys v0.28.0 // indirect
	golang.org/x/term v0.27.0 // indirect
	golang.org/x/text v0.21.0 // indirect
	golang.org/x/time v0.5.0 // indirect
	google.golang.org/appengine v1.6.8 // indirect
	google.golang.org/protobuf v1.33.0 // indirect
	gopkg.in/inf.v0 v0.9.1 // indirect
	gopkg.in/ini.v1 v1.67.0 // indirect
	gopkg.in/yaml.v2 v2.4.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
	k8s.io/klog/v2 v2.120.1 // indirect
	k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340 // indirect
	k8s.io/utils v0.0.0-20230726121419-3b25d923346b // indirect
	sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect
	sigs.k8s.io/structured-merge-diff/v4 v4.4.1 // indirect
	sigs.k8s.io/yaml v1.3.0 // indirect
)


================================================
File: kubectl-pg/go.sum
================================================
github.com/armon/go-socks5 v0.0.0-20160902184237-e75332964ef5 h1:0CwZNZbxp69SHPdPJAN/hZIm0C4OItdklCFmMRWYpio=
github.com/armon/go-socks5 v0.0.0-20160902184237-e75332964ef5/go.mod h1:wHh0iHkYZB8zMSxRWpUBQtwG5a7fFgvEO+odwuTv2gs=
github.com/cpuguy83/go-md2man/v2 v2.0.4/go.mod h1:tgQtvFlXSQOSOSIRvRPT7W67SCa46tRHOmNcaadrF8o=
github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc h1:U9qPSI2PIWSS1VwoXQT9A3Wy9MM3WgvqSxFWenqJduM=
github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/emicklei/go-restful/v3 v3.11.0 h1:rAQeMHw1c7zTmncogyy8VvRZwtkmkZ4FxERmMY4rD+g=
github.com/emicklei/go-restful/v3 v3.11.0/go.mod h1:6n3XBCmQQb25CM2LCACGz8ukIrRry+4bhvbpWn3mrbc=
github.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=
github.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=
github.com/fsnotify/fsnotify v1.7.0 h1:8JEhPFa5W2WU7YfeZzPNqzMP6Lwt7L2715Ggo0nosvA=
github.com/fsnotify/fsnotify v1.7.0/go.mod h1:40Bi/Hjc2AVfZrqy+aj+yEI+/bRxZnMJyTJwOpGvigM=
github.com/go-logr/logr v1.4.1 h1:pKouT5E8xu9zeFC39JXRDukb6JFQPXM5p5I91188VAQ=
github.com/go-logr/logr v1.4.1/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=
github.com/go-openapi/jsonpointer v0.19.6 h1:eCs3fxoIi3Wh6vtgmLTOjdhSpiqphQ+DaPn38N2ZdrE=
github.com/go-openapi/jsonpointer v0.19.6/go.mod h1:osyAmYz/mB/C3I+WsTTSgw1ONzaLJoLCyoi6/zppojs=
github.com/go-openapi/jsonreference v0.20.2 h1:3sVjiK66+uXK/6oQ8xgcRKcFgQ5KXa2KvnJRumpMGbE=
github.com/go-openapi/jsonreference v0.20.2/go.mod h1:Bl1zwGIM8/wsvqjsOQLJ/SH+En5Ap4rVB5KVcIDZG2k=
github.com/go-openapi/swag v0.22.3 h1:yMBqmnQ0gyZvEb/+KzuWZOXgllrXT4SADYbvDaXHv/g=
github.com/go-openapi/swag v0.22.3/go.mod h1:UzaqsxGiab7freDnrUUra0MwWfN/q7tE4j+VcZ0yl14=
github.com/go-task/slim-sprig v0.0.0-20230315185526-52ccab3ef572 h1:tfuBGBXKqDEevZMzYi5KSi8KkcZtzBcTgAUUtapy0OI=
github.com/go-task/slim-sprig v0.0.0-20230315185526-52ccab3ef572/go.mod h1:9Pwr4B2jHnOSGXyyzV8ROjYa2ojvAY6HCGYYfMoC3Ls=
github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=
github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=
github.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=
github.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=
github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
github.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=
github.com/google/gnostic-models v0.6.8 h1:yo/ABAfM5IMRsS1VnXjTBvUb61tFIHozhlYvRgGre9I=
github.com/google/gnostic-models v0.6.8/go.mod h1:5n7qKqH0f5wFt+aWF8CW6pZLLNOfYuF5OpfBSENuI8U=
github.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
github.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
github.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=
github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
github.com/google/gofuzz v1.2.0 h1:xRy4A+RhZaiKjJ1bPfwQ8sedCA+YS2YcCHW6ec7JMi0=
github.com/google/gofuzz v1.2.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
github.com/google/pprof v0.0.0-20210720184732-4bb14d4b1be1 h1:K6RDEckDVWvDI9JAJYCmNdQXq6neHJOYx3V6jnqNEec=
github.com/google/pprof v0.0.0-20210720184732-4bb14d4b1be1/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=
github.com/google/uuid v1.4.0 h1:MtMxsa51/r9yyhkyLsVeVt0B+BGQZzpQiTQ4eHZ8bc4=
github.com/google/uuid v1.4.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
github.com/gorilla/websocket v1.4.2/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=
github.com/gorilla/websocket v1.5.0 h1:PPwGk2jz7EePpoHN/+ClbZu8SPxiqlu12wZP/3sWmnc=
github.com/gorilla/websocket v1.5.0/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=
github.com/hashicorp/hcl v1.0.0 h1:0Anlzjpi4vEasTeNFn2mLJgTSwt0+6sfsiTG8qcWGx4=
github.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk+7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=
github.com/imdario/mergo v0.3.6 h1:xTNEAn+kxVO7dTZGu0CegyqKZmoWFI0rF8UxjlB2d28=
github.com/imdario/mergo v0.3.6/go.mod h1:2EnlNZ0deacrJVfApfmtdGgDfMuh/nq6Ok1EcJh5FfA=
github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
github.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
github.com/josharian/intern v1.0.0 h1:vlS4z54oSdjm0bgjRigI+G1HpF+tI+9rE5LLzOg8HmY=
github.com/josharian/intern v1.0.0/go.mod h1:5DoeVV0s6jJacbCEi61lwdGj/aVlrQvzHFFd8Hwg//Y=
github.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=
github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=
github.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=
github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=
github.com/kr/pretty v0.2.1/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=
github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=
github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/magiconair/properties v1.8.7 h1:IeQXZAiQcpL9mgcAe1Nu6cX9LLw6ExEHKjN0VQdvPDY=
github.com/magiconair/properties v1.8.7/go.mod h1:Dhd985XPs7jluiymwWYZ0G4Z61jb3vdS329zhj2hYo0=
github.com/mailru/easyjson v0.7.7 h1:UGYAvKxe3sBsEDzO8ZeWOSlIQfWFlxbzLZe7hwFURr0=
github.com/mailru/easyjson v0.7.7/go.mod h1:xzfreul335JAWq5oZzymOObrkdz5UnU4kGfJJLY9Nlc=
github.com/mitchellh/mapstructure v1.5.0 h1:jeMsZIYE/09sWLaz43PL7Gy6RuMjD2eJVyuac5Z2hdY=
github.com/mitchellh/mapstructure v1.5.0/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=
github.com/moby/spdystream v0.2.0 h1:cjW1zVyyoiM0T7b6UoySUFqzXMoqRckQtXwGPiBhOM8=
github.com/moby/spdystream v0.2.0/go.mod h1:f7i0iNDQJ059oMTcWxx8MA/zKFIuD/lY+0GqbN2Wy8c=
github.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=
github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
github.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9Gz0M=
github.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=
github.com/motomux/pretty v0.0.0-20161209205251-b2aad2c9a95d h1:LznySqW8MqVeFh+pW6rOkFdld9QQ7jRydBKKM6jyPVI=
github.com/motomux/pretty v0.0.0-20161209205251-b2aad2c9a95d/go.mod h1:u3hJ0kqCQu/cPpsu3RbCOPZ0d7V3IjPjv1adNRleM9I=
github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 h1:C3w9PqII01/Oq1c1nUAm88MOHcQC9l5mIlSMApZMrHA=
github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=
github.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f h1:y5//uYreIhSUg3J1GEMiLbxo1LJaP8RfCpH6pymGZus=
github.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f/go.mod h1:ZdcZmHo+o7JKHSa8/e818NopupXU1YMK5fe1lsApnBw=
github.com/onsi/ginkgo/v2 v2.15.0 h1:79HwNRBAZHOEwrczrgSOPy+eFTTlIGELKy5as+ClttY=
github.com/onsi/ginkgo/v2 v2.15.0/go.mod h1:HlxMHtYF57y6Dpf+mc5529KKmSq9h2FpCF+/ZkwUxKM=
github.com/onsi/gomega v1.31.0 h1:54UJxxj6cPInHS3a35wm6BK/F9nHYueZ1NVujHDrnXE=
github.com/onsi/gomega v1.31.0/go.mod h1:DW9aCi7U6Yi40wNVAvT6kzFnEVEI5n3DloYBiKiT6zk=
github.com/pelletier/go-toml/v2 v2.2.2 h1:aYUidT7k73Pcl9nb2gScu7NSrKCSHIDE89b3+6Wq+LM=
github.com/pelletier/go-toml/v2 v2.2.2/go.mod h1:1t835xjRzz80PqgE6HHgN2JOsmgYu/h4qDAS4n929Rs=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2 h1:Jamvg5psRIccs7FGNTlIRMkT8wgtp5eCXdBlqhYGL6U=
github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/rogpeppe/go-internal v1.10.0 h1:TMyTOH3F/DB16zRVcYyreMH6GnZZrwQVAoYjRBZyWFQ=
github.com/rogpeppe/go-internal v1.10.0/go.mod h1:UQnix2H7Ngw/k4C5ijL5+65zddjncjaFoBhdsK/akog=
github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
github.com/sagikazarmark/locafero v0.4.0 h1:HApY1R9zGo4DBgr7dqsTH/JJxLTTsOt7u6keLGt6kNQ=
github.com/sagikazarmark/locafero v0.4.0/go.mod h1:Pe1W6UlPYUk/+wc/6KFhbORCfqzgYEpgQ3O5fPuL3H4=
github.com/sagikazarmark/slog-shim v0.1.0 h1:diDBnUNK9N/354PgrxMywXnAwEr1QZcOr6gto+ugjYE=
github.com/sagikazarmark/slog-shim v0.1.0/go.mod h1:SrcSrq8aKtyuqEI1uvTDTK1arOWRIczQRv+GVI1AkeQ=
github.com/sirupsen/logrus v1.9.3 h1:dueUQJ1C2q9oE3F7wvmSGAaVtTmUizReu6fjN8uqzbQ=
github.com/sirupsen/logrus v1.9.3/go.mod h1:naHLuLoDiP4jHNo9R0sCBMtWGeIprob74mVsIT4qYEQ=
github.com/sourcegraph/conc v0.3.0 h1:OQTbbt6P72L20UqAkXXuLOj79LfEanQ+YQFNpLA9ySo=
github.com/sourcegraph/conc v0.3.0/go.mod h1:Sdozi7LEKbFPqYX2/J+iBAM6HpqSLTASQIKqDmF7Mt0=
github.com/spf13/afero v1.11.0 h1:WJQKhtpdm3v2IzqG8VMqrr6Rf3UYpEF239Jy9wNepM8=
github.com/spf13/afero v1.11.0/go.mod h1:GH9Y3pIexgf1MTIWtNGyogA5MwRIDXGUr+hbWNoBjkY=
github.com/spf13/cast v1.6.0 h1:GEiTHELF+vaR5dhz3VqZfFSzZjYbgeKDpBxQVS4GYJ0=
github.com/spf13/cast v1.6.0/go.mod h1:ancEpBxwJDODSW/UG4rDrAqiKolqNNh2DX3mk86cAdo=
github.com/spf13/cobra v1.8.1 h1:e5/vxKd/rZsfSJMUX1agtjeTDf+qv1/JdBF8gg5k9ZM=
github.com/spf13/cobra v1.8.1/go.mod h1:wHxEcudfqmLYa8iTfL+OuZPbBZkmvliBWKIezN3kD9Y=
github.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=
github.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/viper v1.19.0 h1:RWq5SEjt8o25SROyN3z2OrDB9l7RPd3lwTWU8EcEdcI=
github.com/spf13/viper v1.19.0/go.mod h1:GQUN9bilAbhU/jgc1bKs99f/suXKeUMct8Adx5+Ntkg=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=
github.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=
github.com/stretchr/objx v0.5.2/go.mod h1:FRsXN1f5AsAjCGJKqEizvkpNtU+EGNCLh3NxZ/8L+MA=
github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=
github.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=
github.com/stretchr/testify v1.8.4/go.mod h1:sz/lmYIOXD/1dqDmKjjqLyZ2RngseejIcXlSw2iwfAo=
github.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=
github.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
github.com/subosito/gotenv v1.6.0 h1:9NlTDc1FTs4qu0DDq7AEtTPNw6SVm7uBMsUCUjABIf8=
github.com/subosito/gotenv v1.6.0/go.mod h1:Dk4QP5c2W3ibzajGcXpNraDfq2IrhjMIvMSWPKKo0FU=
github.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
github.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
github.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=
github.com/zalando/postgres-operator v1.13.0 h1:T9Mb+ZRQyTxXbagIK66GLVGCwM3661aX2lOkNpax4s8=
github.com/zalando/postgres-operator v1.13.0/go.mod h1:WiMEKzUny2lJHYle+7+D/5BhlvPn8prl76rEDYLsQAg=
go.uber.org/multierr v1.11.0 h1:blXXJkSxSSfBVBlC76pxqeO+LN3aDfLQo+309xJstO0=
go.uber.org/multierr v1.11.0/go.mod h1:20+QtiLqy0Nd6FdQB9TLXag12DsQkrbs3htMFfDN80Y=
golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
golang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=
golang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=
golang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=
golang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3 h1:hNQpMuAJe5CtcUqCXaWga3FHu+kQvCqcsoVaQgSV60o=
golang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3/go.mod h1:idGWGoKP1toJGkd5/ig9ZLuPcZBC3ewk7SzmH0uou08=
golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
golang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
golang.org/x/mod v0.6.0-dev.0.20220419223038-86c51ed26bb4/go.mod h1:jJ57K6gSWd91VN4djpZkiMVwK6gcyfeH4XE8wZrZaV4=
golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
golang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=
golang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=
golang.org/x/net v0.25.0 h1:d/OCCoBEUq33pjydKrGQhw7IlUPI2Oylr+8qLx49kac=
golang.org/x/net v0.25.0/go.mod h1:JkAGAh7GEvH74S6FOH42FLoXpXbE/aqXSrIQjXgsiwM=
golang.org/x/oauth2 v0.18.0 h1:09qnuIAgzdx1XplqJvW6CQqMCtGZykZWcXzPMPUusvI=
golang.org/x/oauth2 v0.18.0/go.mod h1:Wf7knwG0MPoWIMMBgFlEaSUDaKskp0dCfrlJRJXbBi8=
golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.28.0 h1:Fksou7UEQUWlKvIdsqzJmUmCX3cZuD2+P3XyyzwMhlA=
golang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=
golang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=
golang.org/x/term v0.27.0 h1:WP60Sv1nlK1T6SupCHbXzSaN0b9wUmsPoRS9b61A23Q=
golang.org/x/term v0.27.0/go.mod h1:iMsnZpn0cago0GOrHO2+Y7u7JPn5AylBrcoWkElMTSM=
golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
golang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=
golang.org/x/text v0.3.8/go.mod h1:E6s5w1FMmriuDzIBO73fBruAKo1PCIq6d2Q6DHfQ8WQ=
golang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=
golang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=
golang.org/x/time v0.5.0 h1:o7cqy6amK/52YcAKIPlM3a+Fpj35zvRj2TP+e1xFSfk=
golang.org/x/time v0.5.0/go.mod h1:3BpzKBy/shNhVucY/MWOyx10tF3SFh9QdLuxbVysPQM=
golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=
golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=
golang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=
golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d h1:vU5i/LfpvrRCpgM/VPfJLg5KjxD3E+hfT1SH+d9zLwg=
golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d/go.mod h1:aiJjzUbINMkxbQROHiO6hDPo2LHcIPhhQsa9DLh0yGk=
golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
google.golang.org/appengine v1.6.8 h1:IhEN5q69dyKagZPYMSdIjS2HqprW324FRQZJcGqPAsM=
google.golang.org/appengine v1.6.8/go.mod h1:1jJ3jBArFh5pcgW8gCtRJnepW8FzD1V44FJffLiz/Ds=
google.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=
google.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=
google.golang.org/protobuf v1.33.0 h1:uNO2rsAINq/JlFpSdYEKIZ0uKD/R9cpdv0T+yoGwGmI=
google.golang.org/protobuf v1.33.0/go.mod h1:c6P6GXX6sHbq/GpV6MGZEdwhWPcYBgnhAHhKbcUYpos=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/inf.v0 v0.9.1 h1:73M5CoZyi3ZLMOyDlQh031Cx6N9NDJ2Vvfl76EDAgDc=
gopkg.in/inf.v0 v0.9.1/go.mod h1:cWUDdTG/fYaXco+Dcufb5Vnc6Gp2YChqWtbxRZE0mXw=
gopkg.in/ini.v1 v1.67.0 h1:Dgnx+6+nfE+IfzjUEISNeydPJh9AXNNsWbGP9KzCsOA=
gopkg.in/ini.v1 v1.67.0/go.mod h1:pNLf8WUiyNEtQjuu5G5vTm06TEv9tsIgeAvK8hOrP4k=
gopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=
gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
k8s.io/api v0.30.4 h1:XASIELmW8w8q0i1Y4124LqPoWMycLjyQti/fdYHYjCs=
k8s.io/api v0.30.4/go.mod h1:ZqniWRKu7WIeLijbbzetF4U9qZ03cg5IRwl8YVs8mX0=
k8s.io/apiextensions-apiserver v0.25.9 h1:Pycd6lm2auABp9wKQHCFSEPG+NPdFSTJXPST6NJFzB8=
k8s.io/apiextensions-apiserver v0.25.9/go.mod h1:ijGxmSG1GLOEaWhTuaEr0M7KUeia3mWCZa6FFQqpt1M=
k8s.io/apimachinery v0.30.4 h1:5QHQI2tInzr8LsT4kU/2+fSeibH1eIHswNx480cqIoY=
k8s.io/apimachinery v0.30.4/go.mod h1:iexa2somDaxdnj7bha06bhb43Zpa6eWH8N8dbqVjTUc=
k8s.io/client-go v0.30.4 h1:eculUe+HPQoPbixfwmaSZGsKcOf7D288tH6hDAdd+wY=
k8s.io/client-go v0.30.4/go.mod h1:IBS0R/Mt0LHkNHF4E6n+SUDPG7+m2po6RZU7YHeOpzc=
k8s.io/klog/v2 v2.120.1 h1:QXU6cPEOIslTGvZaXvFWiP9VKyeet3sawzTOvdXb4Vw=
k8s.io/klog/v2 v2.120.1/go.mod h1:3Jpz1GvMt720eyJH1ckRHK1EDfpxISzJ7I9OYgaDtPE=
k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340 h1:BZqlfIlq5YbRMFko6/PM7FjZpUb45WallggurYhKGag=
k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340/go.mod h1:yD4MZYeKMBwQKVht279WycxKyM84kkAx2DPrTXaeb98=
k8s.io/utils v0.0.0-20230726121419-3b25d923346b h1:sgn3ZU783SCgtaSJjpcVVlRqd6GSnlTLKgpAAttJvpI=
k8s.io/utils v0.0.0-20230726121419-3b25d923346b/go.mod h1:OLgZIPagt7ERELqWJFomSt595RzquPNLL48iOWgYOg0=
sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd h1:EDPBXCAspyGV4jQlpZSudPeMmr1bNJefnuqLsRAsHZo=
sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd/go.mod h1:B8JuhiUyNFVKdsE8h686QcCxMaH6HrOAZj4vswFpcB0=
sigs.k8s.io/structured-merge-diff/v4 v4.4.1 h1:150L+0vs/8DA78h1u02ooW1/fFq/Lwr+sGiqlzvrtq4=
sigs.k8s.io/structured-merge-diff/v4 v4.4.1/go.mod h1:N8hJocpFajUSSeSJ9bOZ77VzejKZaXsTtZo4/u7Io08=
sigs.k8s.io/yaml v1.3.0 h1:a2VclLzOGrwOHDiV8EfBGhvjHvP46CtW5j6POvhYGGo=
sigs.k8s.io/yaml v1.3.0/go.mod h1:GeOyir5tyXNByN85N/dRIT9es5UQNerPYEKK56eTBm8=


================================================
File: kubectl-pg/main.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package main

import (
	"github.com/zalando/postgres-operator/kubectl-pg/cmd"
)

func main() {
	cmd.Execute()
}


================================================
File: kubectl-pg/cmd/addDb.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"encoding/json"
	"fmt"
	"log"

	"github.com/spf13/cobra"
	PostgresqlLister "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
)

// addDbCmd represents the addDb command
var addDbCmd = &cobra.Command{
	Use:   "add-db",
	Short: "Adds a DB and its owner to a Postgres cluster. The owner role is created if it does not exist",
	Long:  `Adds a new DB to the Postgres cluster. Owner needs to be specified by the -o flag, cluster with -c flag.`,
	Run: func(cmd *cobra.Command, args []string) {
		if len(args) > 0 {
			dbName := args[0]
			dbOwner, _ := cmd.Flags().GetString("owner")
			clusterName, _ := cmd.Flags().GetString("cluster")
			addDb(dbName, dbOwner, clusterName)
		} else {
			fmt.Println("database name can't be empty. Use kubectl pg add-db [-h | --help] for more info")
		}

	},
	Example: `
kubectl pg add-db db01 -o owner01 -c cluster01
`,
}

// add db and it's owner to the cluster
func addDb(dbName string, dbOwner string, clusterName string) {
	config := getConfig()
	postgresConfig, err := PostgresqlLister.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	namespace := getCurrentNamespace()
	postgresql, err := postgresConfig.Postgresqls(namespace).Get(context.TODO(), clusterName, metav1.GetOptions{})
	if err != nil {
		log.Fatal(err)
	}

	var dbOwnerExists bool
	dbUsers := postgresql.Spec.Users
	for key := range dbUsers {
		if key == dbOwner {
			dbOwnerExists = true
		}
	}
	var patch []byte
	// validating reserved DB names
	if dbOwnerExists && dbName != "postgres" && dbName != "template0" && dbName != "template1" {
		patch = dbPatch(dbName, dbOwner)
	} else if !dbOwnerExists {
		log.Fatal("The provided db-owner doesn't exist")
	} else {
		log.Fatal("The provided db-name is reserved by postgres")
	}

	updatedPostgres, err := postgresConfig.Postgresqls(namespace).Patch(context.TODO(), postgresql.Name, types.MergePatchType, patch, metav1.PatchOptions{})
	if err != nil {
		log.Fatal(err)
	}

	if updatedPostgres.ResourceVersion != postgresql.ResourceVersion {
		fmt.Printf("Created new database %s with owner %s in PostgreSQL cluster %s.\n", dbName, dbOwner, updatedPostgres.Name)
	} else {
		fmt.Printf("postgresql %s is unchanged.\n", updatedPostgres.Name)
	}
}

func dbPatch(dbname string, owner string) []byte {
	ins := map[string]map[string]map[string]string{"spec": {"databases": {dbname: owner}}}
	patchInstances, err := json.Marshal(ins)
	if err != nil {
		log.Fatal(err, "unable to parse patch for add-db")
	}
	return patchInstances
}

func init() {
	addDbCmd.Flags().StringP("owner", "o", "", "provide owner of the database.")
	addDbCmd.Flags().StringP("cluster", "c", "", "provide a postgres cluster name.")
	rootCmd.AddCommand(addDbCmd)
}


================================================
File: kubectl-pg/cmd/addUser.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"strings"

	"github.com/spf13/cobra"
	PostgresqlLister "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
)

var allowedPrivileges = []string{"SUPERUSER", "REPLICATION", "INHERIT", "LOGIN", "NOLOGIN", "CREATEROLE", "CREATEDB", "BYPASSRLS"}

// addUserCmd represents the addUser command
var addUserCmd = &cobra.Command{
	Use:   "add-user",
	Short: "Adds a user to the postgres cluster with given privileges",
	Long:  `Adds a user to the postgres cluster. You can add privileges as well with -p flag.`,
	Run: func(cmd *cobra.Command, args []string) {
		clusterName, _ := cmd.Flags().GetString("cluster")
		privileges, _ := cmd.Flags().GetString("privileges")

		if len(args) > 0 {
			user := args[0]
			var permissions []string
			var perms []string

			if privileges != "" {
				parsedRoles := strings.Replace(privileges, ",", " ", -1)
				parsedRoles = strings.ToUpper(parsedRoles)
				permissions = strings.Fields(parsedRoles)
				var invalidPerms []string

				for _, userPrivilege := range permissions {
					validPerm := false
					for _, privilege := range allowedPrivileges {
						if privilege == userPrivilege {
							perms = append(perms, userPrivilege)
							validPerm = true
						}
					}
					if !validPerm {
						invalidPerms = append(invalidPerms, userPrivilege)
					}
				}

				if len(invalidPerms) > 0 {
					fmt.Printf("Invalid privileges %s\n", invalidPerms)
					return
				}
			}
			addUser(user, clusterName, perms)
		}
	},
	Example: `
kubectl pg add-user user01 -p login,createdb -c cluster01
`,
}

// add user to the cluster with provided permissions
func addUser(user string, clusterName string, permissions []string) {
	config := getConfig()
	postgresConfig, err := PostgresqlLister.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	namespace := getCurrentNamespace()
	postgresql, err := postgresConfig.Postgresqls(namespace).Get(context.TODO(), clusterName, metav1.GetOptions{})
	if err != nil {
		log.Fatal(err)
	}

	setUsers := make(map[string]bool)
	for _, k := range permissions {
		setUsers[k] = true
	}

	if existingRoles, key := postgresql.Spec.Users[user]; key {
		for _, k := range existingRoles {
			setUsers[k] = true
		}
	}

	Privileges := []string{}
	for keys, values := range setUsers {
		if values {
			Privileges = append(Privileges, keys)
		}
	}

	patch := applyUserPatch(user, Privileges)
	updatedPostgresql, err := postgresConfig.Postgresqls(namespace).Patch(context.TODO(), postgresql.Name, types.MergePatchType, patch, metav1.PatchOptions{})
	if err != nil {
		log.Fatal(err)
	}

	if updatedPostgresql.ResourceVersion != postgresql.ResourceVersion {
		fmt.Printf("postgresql %s is updated with new user %s and with privileges %s.\n", updatedPostgresql.Name, user, permissions)
	} else {
		fmt.Printf("postgresql %s is unchanged.\n", updatedPostgresql.Name)
	}
}

func applyUserPatch(user string, value []string) []byte {
	ins := map[string]map[string]map[string][]string{"spec": {"users": {user: value}}}
	patchInstances, err := json.Marshal(ins)
	if err != nil {
		log.Fatal(err, "unable to parse number of instances json")
	}
	return patchInstances
}

func init() {
	addUserCmd.Flags().StringP("cluster", "c", "", "add user to the provided cluster.")
	addUserCmd.Flags().StringP("privileges", "p", "", "add privileges separated by commas without spaces")
	rootCmd.AddCommand(addUserCmd)
}


================================================
File: kubectl-pg/cmd/check.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"fmt"
	"log"

	"github.com/spf13/cobra"
	postgresConstants "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	v1 "k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1"
	apiextv1 "k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset/typed/apiextensions/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// checkCmd represent kubectl pg check.
var checkCmd = &cobra.Command{
	Use:   "check",
	Short: "Checks the Postgres operator is installed in the k8s cluster",
	Long: `Checks that the Postgres CRD is registered in a k8s cluster.
This means that the operator pod was able to start normally.`,
	Run: func(cmd *cobra.Command, args []string) {
		check()
	},
	Example: `
kubectl pg check
`,
}

// check validates postgresql CRD registered or not.
func check() *v1.CustomResourceDefinition {
	config := getConfig()
	apiExtClient, err := apiextv1.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	crdInfo, err := apiExtClient.CustomResourceDefinitions().Get(context.TODO(), postgresConstants.PostgresCRDResouceName, metav1.GetOptions{})
	if err != nil {
		log.Fatal(err)
	}

	if crdInfo.Name == postgresConstants.PostgresCRDResouceName {
		fmt.Printf("Postgres Operator is installed in the k8s cluster.\n")
	} else {
		fmt.Printf("Postgres Operator is not installed in the k8s cluster.\n")
	}
	return crdInfo
}

func init() {
	rootCmd.AddCommand(checkCmd)
}


================================================
File: kubectl-pg/cmd/connect.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"log"
	"os"
	user "os/user"

	"github.com/spf13/cobra"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/remotecommand"
)

// connectCmd represents the kubectl pg connect command
var connectCmd = &cobra.Command{
	Use:   "connect",
	Short: "Connects to the shell prompt, psql prompt of postgres cluster",
	Long:  `Connects to the shell prompt, psql prompt of postgres cluster and also to specified replica or master.`,
	Run: func(cmd *cobra.Command, args []string) {
		clusterName, _ := cmd.Flags().GetString("cluster")
		master, _ := cmd.Flags().GetBool("master")
		replica, _ := cmd.Flags().GetString("replica")
		psql, _ := cmd.Flags().GetBool("psql")
		userName, _ := cmd.Flags().GetString("user")
		dbName, _ := cmd.Flags().GetString("database")

		if psql {
			if userName == "" {
				userInfo, err := user.Current()
				if err != nil {
					log.Fatal(err)
				}
				userName = userInfo.Username
			}
		}
		if dbName == "" {
			dbName = userName
		}

		connect(clusterName, master, replica, psql, userName, dbName)
	},
	Example: `
#connects to the master of postgres cluster
kubectl pg connect -c cluster -m

#connects to the random replica of postgres cluster
kubectl pg connect -c cluster

#connects to the provided replica number of postgres cluster
kubectl pg connect -c cluster -r 2

#connects to psql prompt of master for provided postgres cluster with current shell user
kubectl pg connect -c cluster -p -m

#connects to psql prompt of random replica for provided postgres cluster with provided user and db
kubectl pg connect -c cluster -p -u user01 -d db01
`,
}

func connect(clusterName string, master bool, replica string, psql bool, user string, dbName string) {
	config := getConfig()
	client, err := kubernetes.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	podName := getPodName(clusterName, master, replica)
	var execRequest *rest.Request

	if psql {
		execRequest = client.CoreV1().RESTClient().Post().Resource("pods").
			Name(podName).
			Namespace(getCurrentNamespace()).
			SubResource("exec").
			Param("container", "postgres").
			Param("command", "psql").
			Param("command", dbName).
			Param("command", user).
			Param("stdin", "true").
			Param("stdout", "true").
			Param("stderr", "true").
			Param("tty", "true")
	} else {
		execRequest = client.CoreV1().RESTClient().Post().Resource("pods").
			Name(podName).
			Namespace(getCurrentNamespace()).
			SubResource("exec").
			Param("container", "postgres").
			Param("command", "su").
			Param("command", "postgres").
			Param("stdin", "true").
			Param("stdout", "true").
			Param("stderr", "true").
			Param("tty", "true")
	}

	exec, err := remotecommand.NewSPDYExecutor(config, "POST", execRequest.URL())
	if err != nil {
		log.Fatal(err)
	}

	err = exec.Stream(remotecommand.StreamOptions{
		Stdin:  os.Stdin,
		Stdout: os.Stdout,
		Stderr: os.Stderr,
		Tty:    true,
	})
	if err != nil {
		log.Fatal(err)
	}
}

func init() {
	connectCmd.Flags().StringP("cluster", "c", "", "provide the cluster name.")
	connectCmd.Flags().BoolP("master", "m", false, "connect to master.")
	connectCmd.Flags().StringP("replica", "r", "", "connect to replica. Specify replica number.")
	connectCmd.Flags().BoolP("psql", "p", false, "connect to psql prompt.")
	connectCmd.Flags().StringP("user", "u", "", "provide user.")
	connectCmd.Flags().StringP("database", "d", "", "provide database name.")
	rootCmd.AddCommand(connectCmd)
}


================================================
File: kubectl-pg/cmd/create.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"fmt"
	"log"
	"os"

	"github.com/spf13/cobra"
	v1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	PostgresqlLister "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	"k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset/scheme"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// createCmd kubectl pg create.
var createCmd = &cobra.Command{
	Use:   "create",
	Short: "Creates postgres object using manifest file",
	Long:  `Creates postgres custom resource objects from a manifest file.`,
	Run: func(cmd *cobra.Command, args []string) {
		fileName, _ := cmd.Flags().GetString("file")
		create(fileName)
	},
	Example: `
kubectl pg create -f cluster-manifest.yaml
`,
}

// Create postgresql resources.
func create(fileName string) {
	config := getConfig()
	postgresConfig, err := PostgresqlLister.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}
	ymlFile, err := os.ReadFile(fileName)
	if err != nil {
		log.Fatal(err)
	}

	decode := scheme.Codecs.UniversalDeserializer().Decode
	obj, _, err := decode([]byte(ymlFile), nil, &v1.Postgresql{})
	if err != nil {
		log.Fatal(err)
	}

	postgresSql := obj.(*v1.Postgresql)
	_, err = postgresConfig.Postgresqls(postgresSql.Namespace).Create(context.TODO(), postgresSql, metav1.CreateOptions{})
	if err != nil {
		log.Fatal(err)
	}

	fmt.Printf("postgresql %s created.\n", postgresSql.Name)
}

func init() {
	createCmd.Flags().StringP("file", "f", "", "manifest file with the cluster definition.")
	rootCmd.AddCommand(createCmd)
}


================================================
File: kubectl-pg/cmd/delete.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"fmt"
	"log"
	"os"

	"github.com/spf13/cobra"
	v1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	PostgresqlLister "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	"k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset/scheme"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// deleteCmd represents kubectl pg delete.
var deleteCmd = &cobra.Command{
	Use:   "delete",
	Short: "Deletes postgresql object by cluster-name/manifest file",
	Long: `Deletes the postgres objects identified by a manifest file or cluster-name.
Deleting the manifest is sufficient to delete the cluster.`,
	Run: func(cmd *cobra.Command, args []string) {
		namespace, _ := cmd.Flags().GetString("namespace")
		file, _ := cmd.Flags().GetString("file")

		if file != "" {
			deleteByFile(file)
		} else if namespace != "" {
			if len(args) != 0 {
				clusterName := args[0]
				deleteByName(clusterName, namespace)
			} else {
				fmt.Println("cluster name can't be empty")
			}
		} else {
			fmt.Println("use the flag either -n or -f to delete a resource.")
		}
	},
	Example: `
#Deleting the postgres cluster using manifest file
kubectl pg delete -f cluster-manifest.yaml

#Deleting the postgres cluster using cluster name in current namespace.
kubectl pg delete cluster01

#Deleting the postgres cluster using cluster name in provided namespace
kubectl pg delete cluster01 -n namespace01
`,
}

func deleteByFile(file string) {
	config := getConfig()
	postgresConfig, err := PostgresqlLister.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	ymlFile, err := os.ReadFile(file)
	if err != nil {
		log.Fatal(err)
	}

	decode := scheme.Codecs.UniversalDeserializer().Decode
	obj, _, err := decode([]byte(ymlFile), nil, &v1.Postgresql{})
	if err != nil {
		log.Fatal(err)
	}

	postgresSql := obj.(*v1.Postgresql)
	_, err = postgresConfig.Postgresqls(postgresSql.Namespace).Get(context.TODO(), postgresSql.Name, metav1.GetOptions{})
	if err != nil {
		fmt.Printf("Postgresql %s not found with the provided namespace %s : %s \n", postgresSql.Name, postgresSql.Namespace, err)
		return
	}
	fmt.Printf("Are you sure you want to remove this PostgreSQL cluster? If so, please type (%s/%s) and hit Enter\n", postgresSql.Namespace, postgresSql.Name)

	confirmAction(postgresSql.Name, postgresSql.Namespace)
	err = postgresConfig.Postgresqls(postgresSql.Namespace).Delete(context.TODO(), postgresSql.Name, metav1.DeleteOptions{})
	if err != nil {
		log.Fatal(err)
	}
	fmt.Printf("Postgresql %s deleted from %s.\n", postgresSql.Name, postgresSql.Namespace)
}

func deleteByName(clusterName string, namespace string) {
	config := getConfig()
	postgresConfig, err := PostgresqlLister.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	_, err = postgresConfig.Postgresqls(namespace).Get(context.TODO(), clusterName, metav1.GetOptions{})
	if err != nil {
		fmt.Printf("Postgresql %s not found with the provided namespace %s : %s \n", clusterName, namespace, err)
		return
	}
	fmt.Printf("Are you sure you want to remove this PostgreSQL cluster? If so, please type (%s/%s) and hit Enter\n", namespace, clusterName)

	confirmAction(clusterName, namespace)
	err = postgresConfig.Postgresqls(namespace).Delete(context.TODO(), clusterName, metav1.DeleteOptions{})
	if err != nil {
		log.Fatal(err)
	}
	fmt.Printf("Postgresql %s deleted from %s.\n", clusterName, namespace)
}

func init() {
	namespace := getCurrentNamespace()
	deleteCmd.Flags().StringP("namespace", "n", namespace, "namespace of the cluster to be deleted.")
	deleteCmd.Flags().StringP("file", "f", "", "manifest file with the cluster definition.")
	rootCmd.AddCommand(deleteCmd)
}


================================================
File: kubectl-pg/cmd/extVolume.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"strconv"

	"github.com/spf13/cobra"
	PostgresqlLister "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
)

// extVolumeCmd represents the extVolume command
var extVolumeCmd = &cobra.Command{
	Use:   "ext-volume",
	Short: "Increases the volume size of a given Postgres cluster",
	Long:  `Extends the volume of the postgres cluster. But volume cannot be shrinked.`,
	Run: func(cmd *cobra.Command, args []string) {
		clusterName, _ := cmd.Flags().GetString("cluster")
		if len(args) > 0 {
			volume := args[0]
			extVolume(volume, clusterName)
		} else {
			fmt.Println("please enter the cluster name with -c flag & volume in desired units")
		}
	},
	Example: `
#Extending the volume size of provided cluster 
kubectl pg ext-volume 2Gi -c cluster01
`,
}

// extend volume with provided size & cluster name
func extVolume(increasedVolumeSize string, clusterName string) {
	config := getConfig()
	postgresConfig, err := PostgresqlLister.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	namespace := getCurrentNamespace()
	postgresql, err := postgresConfig.Postgresqls(namespace).Get(context.TODO(), clusterName, metav1.GetOptions{})
	if err != nil {
		log.Fatal(err)
	}

	oldSize, err := resource.ParseQuantity(postgresql.Spec.Volume.Size)
	if err != nil {
		log.Fatal(err)
	}

	newSize, err := resource.ParseQuantity(increasedVolumeSize)
	if err != nil {
		log.Fatal(err)
	}

	_, err = strconv.Atoi(newSize.String())
	if err == nil {
		fmt.Println("provide the valid volume size with respective units i.e Ki, Mi, Gi")
		return
	}

	if newSize.Value() > oldSize.Value() {
		patchInstances := volumePatch(newSize)
		response, err := postgresConfig.Postgresqls(namespace).Patch(context.TODO(), postgresql.Name, types.MergePatchType, patchInstances, metav1.PatchOptions{})
		if err != nil {
			log.Fatal(err)
		}
		if postgresql.ResourceVersion != response.ResourceVersion {
			fmt.Printf("%s volume is extended to %s.\n", response.Name, increasedVolumeSize)
		} else {
			fmt.Printf("%s volume %s is unchanged.\n", response.Name, postgresql.Spec.Volume.Size)
		}
	} else if newSize.Value() == oldSize.Value() {
		fmt.Println("volume already has the desired size.")
	} else {
		fmt.Printf("volume %s size cannot be shrinked.\n", postgresql.Spec.Volume.Size)
	}
}

func volumePatch(volume resource.Quantity) []byte {
	patchData := map[string]map[string]map[string]resource.Quantity{"spec": {"volume": {"size": volume}}}
	patch, err := json.Marshal(patchData)
	if err != nil {
		log.Fatal(err, "unable to parse patch to extend volume")
	}
	return patch
}

func init() {
	extVolumeCmd.Flags().StringP("cluster", "c", "", "provide cluster name.")
	rootCmd.AddCommand(extVolumeCmd)
}


================================================
File: kubectl-pg/cmd/list.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"fmt"
	"log"
	"strconv"
	"time"

	"github.com/spf13/cobra"
	v1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	PostgresqlLister "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

const (
	TrimCreateTimestamp = 6000000000
)

// listCmd represents kubectl pg list.
var listCmd = &cobra.Command{
	Use:   "list",
	Short: "Lists all the resources of kind postgresql",
	Long:  `Lists all the info specific to postgresql objects.`,
	Run: func(cmd *cobra.Command, args []string) {
		allNamespaces, _ := cmd.Flags().GetBool("all-namespaces")
		namespace, _ := cmd.Flags().GetString("namespace")
		if allNamespaces {
			list(allNamespaces, "")
		} else {
			list(allNamespaces, namespace)
		}

	},
	Example: `
#Lists postgres cluster in current namespace
kubectl pg list

#Lists postgres clusters in all namespaces
kubectl pg list -A
`,
}

// list command to list postgres.
func list(allNamespaces bool, namespace string) {
	config := getConfig()
	postgresConfig, err := PostgresqlLister.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	listPostgres, err := postgresConfig.Postgresqls(namespace).List(context.TODO(), metav1.ListOptions{})
	if err != nil {
		log.Fatal(err)
	}

	if len(listPostgres.Items) == 0 {
		if namespace != "" {
			fmt.Printf("No Postgresql clusters found in namespace: %v\n", namespace)
		} else {
			fmt.Println("No Postgresql clusters found in all namespaces")
		}
		return
	}

	if allNamespaces {
		listAll(listPostgres)
	} else {
		listWithNamespace(listPostgres)
	}
}

func listAll(listPostgres *v1.PostgresqlList) {
	template := "%-32s%-16s%-12s%-12s%-12s%-12s%-12s\n"
	fmt.Printf(template, "NAME", "STATUS", "INSTANCES", "VERSION", "AGE", "VOLUME", "NAMESPACE")
	for _, pgObjs := range listPostgres.Items {
		fmt.Printf(template, pgObjs.Name,
			pgObjs.Status.PostgresClusterStatus,
			strconv.Itoa(int(pgObjs.Spec.NumberOfInstances)),
			pgObjs.Spec.PostgresqlParam.PgVersion,
			time.Since(pgObjs.CreationTimestamp.Time).Truncate(TrimCreateTimestamp),
			pgObjs.Spec.Size, pgObjs.Namespace)
	}
}

func listWithNamespace(listPostgres *v1.PostgresqlList) {
	template := "%-32s%-16s%-12s%-12s%-12s%-12s\n"
	fmt.Printf(template, "NAME", "STATUS", "INSTANCES", "VERSION", "AGE", "VOLUME")
	for _, pgObjs := range listPostgres.Items {
		fmt.Printf(template, pgObjs.Name,
			pgObjs.Status.PostgresClusterStatus,
			strconv.Itoa(int(pgObjs.Spec.NumberOfInstances)),
			pgObjs.Spec.PostgresqlParam.PgVersion,
			time.Since(pgObjs.CreationTimestamp.Time).Truncate(TrimCreateTimestamp),
			pgObjs.Spec.Size)
	}
}

func init() {
	listCmd.Flags().BoolP("all-namespaces", "A", false, "list pg resources across all namespaces.")
	listCmd.Flags().StringP("namespace", "n", getCurrentNamespace(), "provide the namespace")
	rootCmd.AddCommand(listCmd)
}


================================================
File: kubectl-pg/cmd/logs.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"io"
	"log"
	"os"

	"github.com/spf13/cobra"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
)

// logsCmd represents the logs command
var logsCmd = &cobra.Command{
	Use:   "logs",
	Short: "This will fetch the logs of the specified postgres cluster & postgres operator",
	Long:  `Fetches the logs of the postgres cluster (i.e master( with -m flag) & replica with (-r 1 pod number) and without -m or -r connects to random replica`,
	Run: func(cmd *cobra.Command, args []string) {
		opLogs, _ := cmd.Flags().GetBool("operator")
		clusterName, _ := cmd.Flags().GetString("cluster")
		master, _ := cmd.Flags().GetBool("master")
		replica, _ := cmd.Flags().GetString("replica")

		if opLogs {
			operatorLogs()
		} else {
			clusterLogs(clusterName, master, replica)
		}
	},
	Example: `
#Fetch the logs of the postgres operator
kubectl pg logs -o

#Fetch the logs of the master for provided cluster
kubectl pg logs -c cluster01 -m

#Fetch the logs of the random replica for provided cluster
kubectl pg logs -c cluster01

#Fetch the logs of the provided replica number of the cluster
kubectl pg logs -c cluster01 -r 3
`,
}

func operatorLogs() {
	config := getConfig()
	client, err := kubernetes.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	operator := getPostgresOperator(client)
	allPods, err := client.CoreV1().Pods(operator.Namespace).List(context.TODO(), metav1.ListOptions{})
	if err != nil {
		log.Fatal(err)
	}

	var operatorPodName string
	for _, pod := range allPods.Items {
		for key, value := range pod.Labels {
			if (key == "name" && value == OperatorName) || (key == "app.kubernetes.io/name" && value == OperatorName) {
				operatorPodName = pod.Name
				break
			}
		}
	}

	execRequest := client.CoreV1().RESTClient().Get().Namespace(operator.Namespace).
		Name(operatorPodName).
		Resource("pods").
		SubResource("log").
		Param("follow", "--follow").
		Param("container", OperatorName)

	readCloser, err := execRequest.Stream(context.TODO())
	if err != nil {
		log.Fatal(err)
	}

	defer readCloser.Close()
	_, err = io.Copy(os.Stdout, readCloser)
	if err != nil {
		log.Fatal(err)
	}
}

func clusterLogs(clusterName string, master bool, replica string) {
	config := getConfig()
	client, err := kubernetes.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	podName := getPodName(clusterName, master, replica)
	execRequest := client.CoreV1().RESTClient().Get().Namespace(getCurrentNamespace()).
		Name(podName).
		Resource("pods").
		SubResource("log").
		Param("follow", "--follow").
		Param("container", "postgres")

	readCloser, err := execRequest.Stream(context.TODO())
	if err != nil {
		log.Fatal(err)
	}

	defer readCloser.Close()
	_, err = io.Copy(os.Stdout, readCloser)
	if err != nil {
		log.Fatal(err)
	}
}

func init() {
	rootCmd.AddCommand(logsCmd)
	logsCmd.Flags().BoolP("operator", "o", false, "logs of operator")
	logsCmd.Flags().StringP("cluster", "c", "", "logs for the provided cluster")
	logsCmd.Flags().BoolP("master", "m", false, "Patroni logs of master")
	logsCmd.Flags().StringP("replica", "r", "", "Patroni logs of replica. Specify replica number.")
}


================================================
File: kubectl-pg/cmd/root.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
	"github.com/spf13/viper"
)

var rootCmd = &cobra.Command{
	Use:   "kubectl-pg",
	Short: "kubectl plugin for the Zalando Postgres operator.",
	Long:  `kubectl pg plugin for interaction with Zalando postgres operator.`,
}

// Execute adds all child commands to the root command and sets flags appropriately.
// This is called by main.main(). It only needs to happen once to the rootCmd.
func Execute() {
	if err := rootCmd.Execute(); err != nil {
		fmt.Println(err)
		os.Exit(1)
	}
}

func init() {
	viper.SetDefault("author", "Vineeth Pothulapati <vineethpothulapati@outlook.com>")
	viper.SetDefault("license", "mit")
}


================================================
File: kubectl-pg/cmd/scale.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"strconv"

	"github.com/spf13/cobra"
	PostgresqlLister "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
)

// scaleCmd represents the scale command
var scaleCmd = &cobra.Command{
	Use:   "scale",
	Short: "Add/remove pods to a Postgres cluster",
	Long: `Scales the postgres objects using cluster-name.
Scaling to 0 leads to down time.`,
	Run: func(cmd *cobra.Command, args []string) {
		clusterName, err := cmd.Flags().GetString("cluster")
		if err != nil {
			log.Fatal(err)
		}
		namespace, err := cmd.Flags().GetString("namespace")
		if err != nil {
			log.Fatal(err)
		}

		if len(args) > 0 {
			numberOfInstances, err := strconv.Atoi(args[0])
			if err != nil {
				log.Fatal(err)
			}
			scale(int32(numberOfInstances), clusterName, namespace)
		} else {
			fmt.Println("Please enter number of instances to scale.")
		}

	},
	Example: `
#Usage
kubectl pg scale [NUMBER-OF-INSTANCES] -c [CLUSTER-NAME] -n [NAMESPACE]

#Scales the number of instances of the provided cluster
kubectl pg scale 5 -c cluster01
`,
}

func scale(numberOfInstances int32, clusterName string, namespace string) {
	config := getConfig()
	postgresConfig, err := PostgresqlLister.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	postgresql, err := postgresConfig.Postgresqls(namespace).Get(context.TODO(), clusterName, metav1.GetOptions{})
	if err != nil {
		log.Fatal(err)
	}

	minInstances, maxInstances := allowedMinMaxInstances(config)

	if minInstances == -1 && maxInstances == -1 {
		postgresql.Spec.NumberOfInstances = numberOfInstances
	} else if numberOfInstances <= maxInstances && numberOfInstances >= minInstances {
		postgresql.Spec.NumberOfInstances = numberOfInstances
	} else if minInstances == -1 && numberOfInstances < postgresql.Spec.NumberOfInstances ||
		maxInstances == -1 && numberOfInstances > postgresql.Spec.NumberOfInstances {
		postgresql.Spec.NumberOfInstances = numberOfInstances
	} else {
		log.Fatalf("cannot scale to the provided instances as they don't adhere to MIN_INSTANCES: %v and MAX_INSTANCES: %v provided in configmap or operatorconfiguration", maxInstances, minInstances)
	}

	if numberOfInstances == 0 {
		fmt.Printf("Scaling to zero leads to down time. please type %s/%s and hit Enter this serves to confirm the action\n", namespace, clusterName)
		confirmAction(clusterName, namespace)
	}

	patchInstances := scalePatch(numberOfInstances)
	UpdatedPostgres, err := postgresConfig.Postgresqls(namespace).Patch(context.TODO(), postgresql.Name, types.MergePatchType, patchInstances, metav1.PatchOptions{})
	if err != nil {
		log.Fatal(err)
	}

	if UpdatedPostgres.ResourceVersion != postgresql.ResourceVersion {
		fmt.Printf("scaled postgresql %s/%s to %d instances\n", UpdatedPostgres.Namespace, UpdatedPostgres.Name, UpdatedPostgres.Spec.NumberOfInstances)
		return
	}
	fmt.Printf("postgresql %s is unchanged.\n", postgresql.Name)
}

func scalePatch(value int32) []byte {
	instances := map[string]map[string]int32{"spec": {"numberOfInstances": value}}
	patchInstances, err := json.Marshal(instances)
	if err != nil {
		log.Fatal(err, "unable to parse patch for scale")
	}
	return patchInstances
}

func allowedMinMaxInstances(config *rest.Config) (int32, int32) {
	k8sClient, err := kubernetes.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	operator := getPostgresOperator(k8sClient)

	operatorContainer := operator.Spec.Template.Spec.Containers
	var configMapName, operatorConfigName string
	// -1 indicates no limitations for min/max instances
	minInstances := -1
	maxInstances := -1
	for _, envData := range operatorContainer[0].Env {
		if envData.Name == "CONFIG_MAP_NAME" {
			configMapName = envData.Value
		}
		if envData.Name == "POSTGRES_OPERATOR_CONFIGURATION_OBJECT" {
			operatorConfigName = envData.Value
		}
	}

	if operatorConfigName == "" {
		configMap, err := k8sClient.CoreV1().ConfigMaps(operator.Namespace).Get(context.TODO(), configMapName, metav1.GetOptions{})
		if err != nil {
			log.Fatal(err)
		}

		configMapData := configMap.Data
		for key, value := range configMapData {
			if key == "min_instances" {
				minInstances, err = strconv.Atoi(value)
				if err != nil {
					log.Fatalf("invalid min instances in configmap %v", err)
				}
			}

			if key == "max_instances" {
				maxInstances, err = strconv.Atoi(value)
				if err != nil {
					log.Fatalf("invalid max instances in configmap %v", err)
				}
			}
		}
	} else if configMapName == "" {
		pgClient, err := PostgresqlLister.NewForConfig(config)
		if err != nil {
			log.Fatal(err)
		}

		operatorConfig, err := pgClient.OperatorConfigurations(operator.Namespace).Get(context.TODO(), operatorConfigName, metav1.GetOptions{})
		if err != nil {
			log.Fatalf("unable to read operator configuration %v", err)
		}

		minInstances = int(operatorConfig.Configuration.MinInstances)
		maxInstances = int(operatorConfig.Configuration.MaxInstances)
	}
	return int32(minInstances), int32(maxInstances)
}

func init() {
	namespace := getCurrentNamespace()
	scaleCmd.Flags().StringP("namespace", "n", namespace, "namespace of the cluster to be scaled")
	scaleCmd.Flags().StringP("cluster", "c", "", "provide the cluster name.")
	rootCmd.AddCommand(scaleCmd)
}


================================================
File: kubectl-pg/cmd/update.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"fmt"
	"log"
	"os"

	"github.com/spf13/cobra"
	v1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	PostgresqlLister "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	"k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset/scheme"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// updateCmd represents kubectl pg update
var updateCmd = &cobra.Command{
	Use:   "update",
	Short: "Updates postgresql object using manifest file",
	Long:  `Updates the state of cluster using manifest file to reflect the changes on the cluster.`,
	Run: func(cmd *cobra.Command, args []string) {
		fileName, _ := cmd.Flags().GetString("file")
		updatePgResources(fileName)
	},
	Example: `
#usage
kubectl pg update -f [File-NAME]

#update the postgres cluster with updated manifest file
kubectl pg update -f cluster-manifest.yaml
`,
}

// Update postgresql resources.
func updatePgResources(fileName string) {
	config := getConfig()
	postgresConfig, err := PostgresqlLister.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}
	ymlFile, err := os.ReadFile(fileName)
	if err != nil {
		log.Fatal(err)
	}

	decode := scheme.Codecs.UniversalDeserializer().Decode
	obj, _, err := decode([]byte(ymlFile), nil, &v1.Postgresql{})
	if err != nil {
		log.Fatal(err)
	}

	newPostgresObj := obj.(*v1.Postgresql)
	oldPostgresObj, err := postgresConfig.Postgresqls(newPostgresObj.Namespace).Get(context.TODO(), newPostgresObj.Name, metav1.GetOptions{})
	if err != nil {
		log.Fatal(err)
	}

	newPostgresObj.ResourceVersion = oldPostgresObj.ResourceVersion
	response, err := postgresConfig.Postgresqls(newPostgresObj.Namespace).Update(context.TODO(), newPostgresObj, metav1.UpdateOptions{})
	if err != nil {
		log.Fatal(err)
	}

	if newPostgresObj.ResourceVersion != response.ResourceVersion {
		fmt.Printf("postgresql %s updated.\n", response.Name)
	} else {
		fmt.Printf("postgresql %s is unchanged.\n", response.Name)
	}
}

func init() {
	updateCmd.Flags().StringP("file", "f", "", "manifest file with the cluster definition.")
	rootCmd.AddCommand(updateCmd)
}


================================================
File: kubectl-pg/cmd/util.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"context"
	"flag"
	"fmt"
	"log"
	"os"
	"os/exec"
	"path/filepath"
	"strconv"
	"strings"

	PostgresqlLister "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	v1 "k8s.io/api/apps/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	restclient "k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/client-go/util/homedir"
)

const (
	OperatorName     = "postgres-operator"
	DefaultNamespace = "default"
)

func getConfig() *restclient.Config {
	var kubeconfig *string
	var config *restclient.Config
	envKube := os.Getenv("KUBECONFIG")
	if envKube != "" {
		kubeconfig = &envKube
	} else {
		if home := homedir.HomeDir(); home != "" {
			kubeconfig = flag.String("kubeconfig", filepath.Join(home, ".kube", "config"), "(optional) absolute path to the kubeconfig file")
		} else {
			kubeconfig = flag.String("kubeconfig", "", "absolute path to the kubeconfig file")
		}
	}
	flag.Parse()
	var err error
	config, err = clientcmd.BuildConfigFromFlags("", *kubeconfig)
	if err != nil {
		log.Fatal(err)
	}
	return config
}

func getCurrentNamespace() string {
	namespace, err := exec.Command("kubectl", "config", "view", "--minify", "--output", "jsonpath={..namespace}").CombinedOutput()
	if err != nil {
		log.Fatal(err)
	}
	currentNamespace := string(namespace)
	if currentNamespace == "" {
		currentNamespace = DefaultNamespace
	}
	return currentNamespace
}

func confirmAction(clusterName string, namespace string) {
	for {
		confirmClusterDetails := ""
		_, err := fmt.Scan(&confirmClusterDetails)
		if err != nil {
			log.Fatalf("couldn't get confirmation from the user %v", err)
		}
		clusterDetails := strings.Split(confirmClusterDetails, "/")
		if clusterDetails[0] != namespace || clusterDetails[1] != clusterName {
			fmt.Printf("cluster name or namespace does not match. Please re-enter %s/%s\nHint: Press (ctrl+c) to exit\n", namespace, clusterName)
		} else {
			return
		}
	}
}

func getPodName(clusterName string, master bool, replicaNumber string) string {
	config := getConfig()
	client, err := kubernetes.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	postgresConfig, err := PostgresqlLister.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	postgresCluster, err := postgresConfig.Postgresqls(getCurrentNamespace()).Get(context.TODO(), clusterName, metav1.GetOptions{})
	if err != nil {
		log.Fatal(err)
	}

	numOfInstances := postgresCluster.Spec.NumberOfInstances
	var podName string
	var podRole string
	replica := clusterName + "-" + replicaNumber

	for ins := 0; ins < int(numOfInstances); ins++ {
		pod, err := client.CoreV1().Pods(getCurrentNamespace()).Get(context.TODO(), clusterName+"-"+strconv.Itoa(ins), metav1.GetOptions{})
		if err != nil {
			log.Fatal(err)
		}

		podRole = pod.Labels["spilo-role"]
		if podRole == "master" && master {
			podName = pod.Name
			fmt.Printf("connected to %s with pod name as %s\n", podRole, podName)
			break
		} else if podRole == "replica" && !master && (pod.Name == replica || replicaNumber == "") {
			podName = pod.Name
			fmt.Printf("connected to %s with pod name as %s\n", podRole, podName)
			break
		}
	}
	if podName == "" {
		log.Fatal("Provided replica doesn't exist")
	}
	return podName
}

func getPostgresOperator(k8sClient *kubernetes.Clientset) *v1.Deployment {
	var operator *v1.Deployment
	operator, err := k8sClient.AppsV1().Deployments(getCurrentNamespace()).Get(context.TODO(), OperatorName, metav1.GetOptions{})
	if err == nil {
		return operator
	}

	allDeployments := k8sClient.AppsV1().Deployments("")
	listDeployments, err := allDeployments.List(context.TODO(), metav1.ListOptions{})
	if err != nil {
		log.Fatal(err)
	}

	for _, deployment := range listDeployments.Items {
		if deployment.Name == OperatorName {
			operator = deployment.DeepCopy()
			break
		} else {
			for key, value := range deployment.Labels {
				if key == "app.kubernetes.io/name" && value == OperatorName {
					operator = deployment.DeepCopy()
					break
				}
			}
		}
	}
	return operator
}


================================================
File: kubectl-pg/cmd/version.go
================================================
/*
Copyright © 2019 Vineeth Pothulapati <vineethpothulapati@outlook.com>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

package cmd

import (
	"fmt"
	"log"
	"strings"

	"github.com/spf13/cobra"
	"k8s.io/client-go/kubernetes"
)

var KubectlPgVersion string = "1.0"

// versionCmd represents the version command
var versionCmd = &cobra.Command{
	Use:   "version",
	Short: "version of kubectl-pg & postgres-operator",
	Long:  `version of kubectl-pg and current running postgres-operator`,
	Run: func(cmd *cobra.Command, args []string) {
		namespace, err := cmd.Flags().GetString("namespace")
		if err != nil {
			log.Fatal(err)
		}
		version(namespace)
	},
	Example: `
#Lists the version of kubectl pg plugin and postgres operator in current namespace
kubectl pg version

#Lists the version of kubectl pg plugin and postgres operator in provided namespace
kubectl pg version -n namespace01
`,
}

func version(namespace string) {
	fmt.Printf("kubectl-pg: %s\n", KubectlPgVersion)

	config := getConfig()
	client, err := kubernetes.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	operatorDeployment := getPostgresOperator(client)
	if operatorDeployment.Name == "" {
		log.Fatal("make sure zalando's postgres operator is running")
	}
	operatorImage := operatorDeployment.Spec.Template.Spec.Containers[0].Image
	imageDetails := strings.Split(operatorImage, ":")
	imageSplit := len(imageDetails)
	imageVersion := imageDetails[imageSplit-1]
	fmt.Printf("Postgres-Operator: %s\n", imageVersion)
}

func init() {
	rootCmd.AddCommand(versionCmd)
	versionCmd.Flags().StringP("namespace", "n", DefaultNamespace, "provide the namespace.")
}


================================================
File: logical-backup/Dockerfile
================================================
ARG BASE_IMAGE=registry.opensource.zalan.do/library/ubuntu-22.04:latest
FROM ${BASE_IMAGE}
LABEL maintainer="Team ACID @ Zalando <team-acid@zalando.de>"

SHELL ["/bin/bash", "-o", "pipefail", "-c"]
RUN apt-get update     \
    && apt-get install --no-install-recommends -y \
        apt-utils \
        ca-certificates \
        lsb-release \
        pigz \
        python3-pip \
        python3-setuptools \
        curl \
        jq \
        gnupg \
        gcc \
        libffi-dev \
    && curl -sL https://aka.ms/InstallAzureCLIDeb | bash \
    && pip3 install --upgrade pip \
    && pip3 install --no-cache-dir awscli --upgrade \
    && pip3 install --no-cache-dir gsutil --upgrade \
    && echo "deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list \
    && cat /etc/apt/sources.list.d/pgdg.list \
    && curl --silent https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add - \
    && apt-get update \
    && apt-get install --no-install-recommends -y  \
        postgresql-client-17  \
        postgresql-client-16  \
        postgresql-client-15  \
        postgresql-client-14  \
        postgresql-client-13  \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

COPY dump.sh ./

ENV PG_DIR=/usr/lib/postgresql

ENTRYPOINT ["/dump.sh"]


================================================
File: logical-backup/dump.sh
================================================
#! /usr/bin/env bash

# enable unofficial bash strict mode
set -o errexit
set -o nounset
set -o pipefail
IFS=$'\n\t'

ALL_DB_SIZE_QUERY="select sum(pg_database_size(datname)::numeric) from pg_database;"
PG_BIN=$PG_DIR/$PG_VERSION/bin
DUMP_SIZE_COEFF=5
ERRORCOUNT=0

TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
KUBERNETES_SERVICE_PORT=${KUBERNETES_SERVICE_PORT:-443}
if [ "$KUBERNETES_SERVICE_HOST" != "${KUBERNETES_SERVICE_HOST#*[0-9].[0-9]}" ]; then
    echo "IPv4"
    K8S_API_URL=https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1
elif [ "$KUBERNETES_SERVICE_HOST" != "${KUBERNETES_SERVICE_HOST#*:[0-9a-fA-F]}" ]; then
    echo "IPv6"
    K8S_API_URL=https://[$KUBERNETES_SERVICE_HOST]:$KUBERNETES_SERVICE_PORT/api/v1
elif [ -n "$KUBERNETES_SERVICE_HOST" ]; then
    echo "Hostname"
    K8S_API_URL=https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1
else
  echo "KUBERNETES_SERVICE_HOST was not set"
fi
echo "API Endpoint: ${K8S_API_URL}"
CERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt

LOGICAL_BACKUP_PROVIDER=${LOGICAL_BACKUP_PROVIDER:="s3"}
LOGICAL_BACKUP_S3_RETENTION_TIME=${LOGICAL_BACKUP_S3_RETENTION_TIME:=""}

function estimate_size {
    "$PG_BIN"/psql -tqAc "${ALL_DB_SIZE_QUERY}"
}

function dump {
    # settings are taken from the environment
    "$PG_BIN"/pg_dumpall
}

function compress {
    pigz
}

function az_upload {
    PATH_TO_BACKUP=$LOGICAL_BACKUP_S3_BUCKET"/"$LOGICAL_BACKUP_S3_BUCKET_PREFIX"/"$SCOPE$LOGICAL_BACKUP_S3_BUCKET_SCOPE_SUFFIX"/logical_backups/"$(date +%s).sql.gz

    az storage blob upload --file "$1" --account-name "$LOGICAL_BACKUP_AZURE_STORAGE_ACCOUNT_NAME" --account-key "$LOGICAL_BACKUP_AZURE_STORAGE_ACCOUNT_KEY" -c "$LOGICAL_BACKUP_AZURE_STORAGE_CONTAINER" -n "$PATH_TO_BACKUP"
}

function aws_delete_objects {
    args=(
      "--bucket=$LOGICAL_BACKUP_S3_BUCKET"
    )

    [[ ! -z "$LOGICAL_BACKUP_S3_ENDPOINT" ]] && args+=("--endpoint-url=$LOGICAL_BACKUP_S3_ENDPOINT")
    [[ ! -z "$LOGICAL_BACKUP_S3_REGION" ]] && args+=("--region=$LOGICAL_BACKUP_S3_REGION")

    aws s3api delete-objects "${args[@]}" --delete Objects=["$(printf {Key=%q}, "$@")"],Quiet=true
}
export -f aws_delete_objects

function aws_delete_outdated {
    if [[ -z "$LOGICAL_BACKUP_S3_RETENTION_TIME" ]] ; then
        echo "no retention time configured: skip cleanup of outdated backups"
        return 0
    fi

    # define cutoff date for outdated backups (day precision)
    cutoff_date=$(date -d "$LOGICAL_BACKUP_S3_RETENTION_TIME ago" +%F)

    # mimic bucket setup from Spilo
    prefix=$LOGICAL_BACKUP_S3_BUCKET_PREFIX"/"$SCOPE$LOGICAL_BACKUP_S3_BUCKET_SCOPE_SUFFIX"/logical_backups/"

    args=(
      "--no-paginate"
      "--output=text"
      "--prefix=$prefix"
      "--bucket=$LOGICAL_BACKUP_S3_BUCKET"
    )

    [[ ! -z "$LOGICAL_BACKUP_S3_ENDPOINT" ]] && args+=("--endpoint-url=$LOGICAL_BACKUP_S3_ENDPOINT")
    [[ ! -z "$LOGICAL_BACKUP_S3_REGION" ]] && args+=("--region=$LOGICAL_BACKUP_S3_REGION")

    # list objects older than the cutoff date
    aws s3api list-objects "${args[@]}" --query="Contents[?LastModified<='$cutoff_date'].[Key]" > /tmp/outdated-backups

    # spare the last backup
    sed -i '$d' /tmp/outdated-backups

    count=$(wc -l < /tmp/outdated-backups)
    if [[ $count == 0 ]] ; then
      echo "no outdated backups to delete"
      return 0
    fi
    echo "deleting $count outdated backups created before $cutoff_date"

    # deleted outdated files in batches with 100 at a time
    tr '\n' '\0'  < /tmp/outdated-backups | xargs -0 -P1 -n100 bash -c 'aws_delete_objects "$@"' _
}

function aws_upload {
    declare -r EXPECTED_SIZE="$1"

    # mimic bucket setup from Spilo
    # to keep logical backups at the same path as WAL
    # NB: $LOGICAL_BACKUP_S3_BUCKET_SCOPE_SUFFIX already contains the leading "/" when set by the Postgres Operator
    PATH_TO_BACKUP=s3://$LOGICAL_BACKUP_S3_BUCKET"/"$LOGICAL_BACKUP_S3_BUCKET_PREFIX"/"$SCOPE$LOGICAL_BACKUP_S3_BUCKET_SCOPE_SUFFIX"/logical_backups/"$(date +%s).sql.gz

    args=()

    [[ ! -z "$EXPECTED_SIZE" ]] && args+=("--expected-size=$EXPECTED_SIZE")
    [[ ! -z "$LOGICAL_BACKUP_S3_ENDPOINT" ]] && args+=("--endpoint-url=$LOGICAL_BACKUP_S3_ENDPOINT")
    [[ ! -z "$LOGICAL_BACKUP_S3_REGION" ]] && args+=("--region=$LOGICAL_BACKUP_S3_REGION")
    [[ ! -z "$LOGICAL_BACKUP_S3_SSE" ]] && args+=("--sse=$LOGICAL_BACKUP_S3_SSE")

    aws s3 cp - "$PATH_TO_BACKUP" "${args[@]//\'/}"
}

function gcs_upload {
    PATH_TO_BACKUP=gs://$LOGICAL_BACKUP_S3_BUCKET"/"$LOGICAL_BACKUP_S3_BUCKET_PREFIX"/"$SCOPE$LOGICAL_BACKUP_S3_BUCKET_SCOPE_SUFFIX"/logical_backups/"$(date +%s).sql.gz

    gsutil -o Credentials:gs_service_key_file=$LOGICAL_BACKUP_GOOGLE_APPLICATION_CREDENTIALS cp - "$PATH_TO_BACKUP"
}

function upload {
    case $LOGICAL_BACKUP_PROVIDER in
        "gcs")
            gcs_upload
            ;;
        "s3")
            aws_upload $(($(estimate_size) / DUMP_SIZE_COEFF))
            aws_delete_outdated
            ;;
    esac
}

function get_pods {
    declare -r SELECTOR="$1"

    curl "${K8S_API_URL}/namespaces/${POD_NAMESPACE}/pods?$SELECTOR" \
      --cacert $CERT \
      -H "Authorization: Bearer ${TOKEN}" | jq .items[].status.podIP -r
}

function get_current_pod {
    curl "${K8S_API_URL}/namespaces/${POD_NAMESPACE}/pods?fieldSelector=metadata.name%3D${HOSTNAME}" \
      --cacert $CERT \
      -H "Authorization: Bearer ${TOKEN}"
}

declare -a search_strategy=(
    list_all_replica_pods_current_node
    list_all_replica_pods_any_node
    get_master_pod
)

function list_all_replica_pods_current_node {
    get_pods "labelSelector=${CLUSTER_NAME_LABEL}%3D${SCOPE},spilo-role%3Dreplica&fieldSelector=spec.nodeName%3D${CURRENT_NODENAME}" | tee | head -n 1
}

function list_all_replica_pods_any_node {
    get_pods "labelSelector=${CLUSTER_NAME_LABEL}%3D${SCOPE},spilo-role%3Dreplica" | tee | head -n 1
}

function get_master_pod {
    get_pods "labelSelector=${CLUSTER_NAME_LABEL}%3D${SCOPE},spilo-role%3Dmaster" | tee | head -n 1
}

CURRENT_NODENAME=$(get_current_pod | jq .items[].spec.nodeName --raw-output)
export CURRENT_NODENAME

for search in "${search_strategy[@]}"; do

    PGHOST=$(eval "$search")
    export PGHOST

    if [ -n "$PGHOST" ]; then
        break
    fi

done

set -x
if [ "$LOGICAL_BACKUP_PROVIDER" == "az" ]; then
    dump | compress > /tmp/azure-backup.sql.gz
    az_upload /tmp/azure-backup.sql.gz
else
    dump | compress | upload
    [[ ${PIPESTATUS[0]} != 0 || ${PIPESTATUS[1]} != 0 || ${PIPESTATUS[2]} != 0 ]] && (( ERRORCOUNT += 1 ))
    set +x

    exit $ERRORCOUNT
fi


================================================
File: manifests/api-service.yaml
================================================
apiVersion: v1
kind: Service
metadata:
  name: postgres-operator
spec:
  type: ClusterIP
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    name: postgres-operator


================================================
File: manifests/complete-postgres-manifest.yaml
================================================
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-test-cluster
#  labels:
#    application: test-app
#    environment: demo
#  annotations:
#    "acid.zalan.do/controller": "second-operator"
#    "delete-date": "2020-08-31"  # can only be deleted on that day if "delete-date "key is configured
#    "delete-clustername": "acid-test-cluster"  # can only be deleted when name matches if "delete-clustername" key is configured
spec:
  dockerImage: ghcr.io/zalando/spilo-17:4.0-p2
  teamId: "acid"
  numberOfInstances: 2
  users:  # Application/Robot users
    zalando:
    - superuser
    - createdb
    foo_user: []
#    flyway: []
#  usersIgnoringSecretRotation:
#  - bar_user
#  usersWithSecretRotation:
#  - foo_user
#  usersWithInPlaceSecretRotation:
#  - flyway
#  - bar_owner_user
  enableMasterLoadBalancer: false
  enableReplicaLoadBalancer: false
  enableConnectionPooler: false # enable/disable connection pooler deployment
  enableReplicaConnectionPooler: false # set to enable connectionPooler for replica service
  enableMasterPoolerLoadBalancer: false
  enableReplicaPoolerLoadBalancer: false
  allowedSourceRanges:  # load balancers' source ranges for both master and replica services
  - 127.0.0.1/32
  databases:
    foo: zalando
  preparedDatabases:
    bar:
      defaultUsers: true
      extensions:
        pg_partman: public
        pgcrypto: public
      schemas:
        data: {}
        history:
          defaultRoles: true
          defaultUsers: false
  postgresql:
    version: "17"
    parameters:  # Expert section
      shared_buffers: "32MB"
      max_connections: "10"
      log_statement: "all"
#  env:
#  - name: wal_s3_bucket
#    value: my-custom-bucket

  volume:
    size: 1Gi
#    storageClass: my-sc
#    iops: 1000  # for EBS gp3
#    throughput: 250  # in MB/s for EBS gp3
#    selector:
#      matchExpressions:
#        - { key: flavour, operator: In, values: [ "banana", "chocolate" ] }
#      matchLabels:
#        environment: dev
#        service: postgres
#     subPath: $(NODE_NAME)/$(POD_NAME)
#     isSubPathExpr: true
  additionalVolumes:
    - name: empty
      mountPath: /opt/empty
      targetContainers:
        - all
      volumeSource:
        emptyDir: {}
#    - name: data
#      mountPath: /home/postgres/pgdata/partitions
#      targetContainers:
#        - postgres
#      volumeSource:
#        PersistentVolumeClaim:
#          claimName: pvc-postgresql-data-partitions
#          readyOnly: false
#    - name: data
#      mountPath: /home/postgres/pgdata/partitions
#      subPath: $(NODE_NAME)/$(POD_NAME)
#      isSubPathExpr: true
#      targetContainers:
#        - postgres
#      volumeSource:
#        PersistentVolumeClaim:
#          claimName: pvc-postgresql-data-partitions
#          readyOnly: false
#    - name: conf
#      mountPath: /etc/telegraf
#      subPath: telegraf.conf
#      targetContainers:
#        - telegraf-sidecar
#      volumeSource:
#        configMap:
#          name: my-config-map

  enableShmVolume: true
#  spiloRunAsUser: 101
#  spiloRunAsGroup: 103
#  spiloFSGroup: 103
#  podAnnotations:
#    annotation.key: value
#  serviceAnnotations:
#    annotation.key: value
#  podPriorityClassName: "spilo-pod-priority"
#  tolerations:
#  - key: postgres
#    operator: Exists
#    effect: NoSchedule
  resources:
    requests:
      cpu: 10m
      memory: 100Mi
#      hugepages-2Mi: 128Mi
#      hugepages-1Gi: 1Gi
    limits:
      cpu: 500m
      memory: 500Mi
#      hugepages-2Mi: 128Mi
#      hugepages-1Gi: 1Gi
  patroni:
    failsafe_mode: false
    initdb:
      encoding: "UTF8"
      locale: "en_US.UTF-8"
      data-checksums: "true"
#    pg_hba:
#      - hostssl all all 0.0.0.0/0 md5
#      - host    all all 0.0.0.0/0 md5
#    slots:
#      permanent_physical_1:
#        type: physical
#      permanent_logical_1:
#        type: logical
#        database: foo
#        plugin: pgoutput
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    synchronous_mode: false
    synchronous_mode_strict: false
    synchronous_node_count: 1
    maximum_lag_on_failover: 33554432

# restore a Postgres DB with point-in-time-recovery
# with a non-empty timestamp, clone from an S3 bucket using the latest backup before the timestamp
# with an empty/absent timestamp, clone from an existing alive cluster using pg_basebackup
#  clone:
#    uid: "efd12e58-5786-11e8-b5a7-06148230260c"
#    cluster: "acid-minimal-cluster"
#    timestamp: "2017-12-19T12:40:33+01:00"  # timezone required (offset relative to UTC, see RFC 3339 section 5.6)
#    s3_wal_path: "s3://custom/path/to/bucket"

# run periodic backups with k8s cron jobs
#  enableLogicalBackup: true
#  logicalBackupRetention: "3 months"
#  logicalBackupSchedule: "30 00 * * *"

#  maintenanceWindows:
#  - 01:00-06:00  #UTC
#  - Sat:00:00-04:00

# overwrite custom properties for connection pooler deployments
#  connectionPooler:
#    numberOfInstances: 2
#    mode: "transaction"
#    schema: "pooler"
#    user: "pooler"
#    maxDBConnections: 60
#    resources:
#      requests:
#        cpu: 300m
#        memory: 100Mi
#      limits:
#        cpu: "1"
#        memory: 100Mi

  initContainers:
  - name: date
    image: busybox
    command: [ "/bin/date" ]
#  sidecars:
#   - name: "telegraf-sidecar"
#     image: "telegraf:latest"
#     ports:
#     - name: metrics
#       containerPort: 8094
#       protocol: TCP
#     resources:
#       limits:
#         cpu: 500m
#         memory: 500Mi
#       requests:
#         cpu: 100m
#         memory: 100Mi
#     env:
#       - name: "USEFUL_VAR"
#         value: "perhaps-true"

# Custom TLS certificate. Disabled unless tls.secretName has a value.
  tls:
    secretName: ""  # should correspond to a Kubernetes Secret resource to load
    certificateFile: "tls.crt"
    privateKeyFile: "tls.key"
    caFile: ""  # optionally configure Postgres with a CA certificate
    caSecretName: "" # optionally the ca.crt can come from this secret instead.
# file names can be also defined with absolute path, and will no longer be relative
# to the "/tls/" path where the secret is being mounted by default, and "/tlsca/"
# where the caSecret is mounted by default.
# When TLS is enabled, also set spiloFSGroup parameter above to the relevant value.
# if unknown, set it to 103 which is the usual value in the default spilo images.
# In Openshift, there is no need to set spiloFSGroup/spilo_fsgroup.

# Add node affinity support by allowing postgres pods to schedule only on nodes that
# have label: "postgres-operator:enabled" set.
#  nodeAffinity:
#    requiredDuringSchedulingIgnoredDuringExecution:
#      nodeSelectorTerms:
#        - matchExpressions:
#            - key: postgres-operator
#              operator: In
#              values:
#                - enabled

# Enables change data capture streams for defined database tables
#  streams:
#  - applicationId: test-app
#    database: foo
#    tables:
#      data.state_pending_outbox:
#        eventType: test-app.status-pending
#      data.state_approved_outbox:
#        eventType: test-app.status-approved
#      data.orders_outbox:
#        eventType: test-app.order-completed
#        idColumn: o_id
#        payloadColumn: o_payload
#    # Optional. Filter ignores events before a certain txnId and lsn. Can be used to skip bad events
#    filter:
#      data.orders_outbox: "[?(@.source.txId > 500 && @.source.lsn > 123456)]"
#    batchSize: 1000


================================================
File: manifests/configmap.yaml
================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-operator
data:
  # additional_owner_roles: "cron_admin"
  # additional_pod_capabilities: "SYS_NICE"
  # additional_secret_mount: "some-secret-name"
  # additional_secret_mount_path: "/some/dir"
  api_port: "8080"
  aws_region: eu-central-1
  cluster_domain: cluster.local
  cluster_history_entries: "1000"
  cluster_labels: application:spilo
  cluster_name_label: cluster-name
  connection_pooler_default_cpu_limit: "1"
  connection_pooler_default_cpu_request: "500m"
  connection_pooler_default_memory_limit: 100Mi
  connection_pooler_default_memory_request: 100Mi
  connection_pooler_image: "registry.opensource.zalan.do/acid/pgbouncer:master-32"
  connection_pooler_max_db_connections: "60"
  connection_pooler_mode: "transaction"
  connection_pooler_number_of_instances: "2"
  connection_pooler_schema: "pooler"
  connection_pooler_user: "pooler"
  crd_categories: "all"
  # custom_service_annotations: "keyx:valuez,keya:valuea"
  # custom_pod_annotations: "keya:valuea,keyb:valueb"
  db_hosted_zone: db.example.com
  debug_logging: "true"
  default_cpu_limit: "1"
  default_cpu_request: 100m
  default_memory_limit: 500Mi
  default_memory_request: 100Mi
  # delete_annotation_date_key: delete-date
  # delete_annotation_name_key: delete-clustername
  docker_image: ghcr.io/zalando/spilo-17:4.0-p2
  # downscaler_annotations: "deployment-time,downscaler/*"
  enable_admin_role_for_users: "true"
  enable_crd_registration: "true"
  enable_crd_validation: "true"
  enable_cross_namespace_secret: "false"
  enable_finalizers: "false"
  enable_database_access: "true"
  enable_ebs_gp3_migration: "false"
  enable_ebs_gp3_migration_max_size: "1000"
  enable_init_containers: "true"
  enable_lazy_spilo_upgrade: "false"
  enable_master_load_balancer: "false"
  enable_master_pooler_load_balancer: "false"
  enable_password_rotation: "false"
  enable_patroni_failsafe_mode: "false"
  enable_owner_references: "false"
  enable_persistent_volume_claim_deletion: "true"
  enable_pgversion_env_var: "true"
  enable_pod_antiaffinity: "false"
  enable_pod_disruption_budget: "true"
  enable_postgres_team_crd: "false"
  enable_postgres_team_crd_superusers: "false"
  enable_readiness_probe: "false"
  enable_replica_load_balancer: "false"
  enable_replica_pooler_load_balancer: "false"
  enable_secrets_deletion: "true"
  enable_shm_volume: "true"
  enable_sidecars: "true"
  enable_spilo_wal_path_compat: "true"
  enable_team_id_clustername_prefix: "false"
  enable_team_member_deprecation: "false"
  enable_team_superuser: "false"
  enable_teams_api: "false"
  etcd_host: ""
  external_traffic_policy: "Cluster"
  # gcp_credentials: ""
  # ignored_annotations: ""
  # infrastructure_roles_secret_name: "postgresql-infrastructure-roles"
  # infrastructure_roles_secrets: "secretname:monitoring-roles,userkey:user,passwordkey:password,rolekey:inrole"
  # ignore_instance_limits_annotation_key: ""
  # inherited_annotations: owned-by
  # inherited_labels: application,environment
  # kube_iam_role: ""
  kubernetes_use_configmaps: "false"
  # log_s3_bucket: ""
  # logical_backup_azure_storage_account_name: ""
  # logical_backup_azure_storage_container: ""
  # logical_backup_azure_storage_account_key: ""
  # logical_backup_cpu_limit: ""
  # logical_backup_cpu_request: ""
  logical_backup_cronjob_environment_secret: ""
  logical_backup_docker_image: "ghcr.io/zalando/postgres-operator/logical-backup:v1.14.0"
  # logical_backup_google_application_credentials: ""
  logical_backup_job_prefix: "logical-backup-"
  # logical_backup_memory_limit: ""
  # logical_backup_memory_request: ""
  logical_backup_provider: "s3"
  logical_backup_s3_access_key_id: ""
  logical_backup_s3_bucket: "my-bucket-url"
  logical_backup_s3_bucket_prefix: "spilo"
  logical_backup_s3_region: ""
  logical_backup_s3_endpoint: ""
  logical_backup_s3_secret_access_key: ""
  logical_backup_s3_sse: "AES256"
  logical_backup_s3_retention_time: ""
  logical_backup_schedule: "30 00 * * *"
  major_version_upgrade_mode: "manual"
  # major_version_upgrade_team_allow_list: ""
  master_dns_name_format: "{cluster}.{namespace}.{hostedzone}"
  master_legacy_dns_name_format: "{cluster}.{team}.{hostedzone}"
  master_pod_move_timeout: 20m
  # max_cpu_request: "1"
  max_instances: "-1"
  # max_memory_request: 4Gi
  min_cpu_limit: 250m
  min_instances: "-1"
  min_memory_limit: 250Mi
  minimal_major_version: "13"
  # node_readiness_label: "status:ready"
  # node_readiness_label_merge: "OR"
  oauth_token_secret_name: postgresql-operator
  pam_configuration: "https://info.example.com/oauth2/tokeninfo?access_token= uid realm=/employees"
  pam_role_name: zalandos
  patroni_api_check_interval: "1s"
  patroni_api_check_timeout: "5s"
  password_rotation_interval: "90"
  password_rotation_user_retention: "180"
  pdb_master_label_selector: "true"
  pdb_name_format: "postgres-{cluster}-pdb"
  persistent_volume_claim_retention_policy: "when_deleted:retain,when_scaled:retain"
  pod_antiaffinity_preferred_during_scheduling: "false"
  pod_antiaffinity_topology_key: "kubernetes.io/hostname"
  pod_deletion_wait_timeout: 10m
  # pod_environment_configmap: "default/my-custom-config"
  # pod_environment_secret: "my-custom-secret"
  pod_label_wait_timeout: 10m
  pod_management_policy: "ordered_ready"
  # pod_priority_class_name: "postgres-pod-priority"
  pod_role_label: spilo-role
  pod_service_account_definition: ""
  pod_service_account_name: "postgres-pod"
  pod_service_account_role_binding_definition: ""
  pod_terminate_grace_period: 5m
  postgres_superuser_teams: "postgres_superusers"
  protected_role_names: "admin,cron_admin"
  ready_wait_interval: 3s
  ready_wait_timeout: 30s
  repair_period: 5m
  replica_dns_name_format: "{cluster}-repl.{namespace}.{hostedzone}"
  replica_legacy_dns_name_format: "{cluster}-repl.{team}.{hostedzone}"
  replication_username: standby
  resource_check_interval: 3s
  resource_check_timeout: 10m
  resync_period: 30m
  ring_log_lines: "100"
  role_deletion_suffix: "_deleted"
  secret_name_template: "{username}.{cluster}.credentials.{tprkind}.{tprgroup}"
  share_pgsocket_with_sidecars: "false"
  # sidecar_docker_images: ""
  set_memory_request_to_limit: "false"
  spilo_allow_privilege_escalation: "true"
  # spilo_runasuser: 101
  # spilo_runasgroup: 103
  # spilo_fsgroup: 103
  spilo_privileged: "false"
  storage_resize_mode: "pvc"
  super_username: postgres
  target_major_version: "17"
  team_admin_role: "admin"
  team_api_role_configuration: "log_statement:all"
  teams_api_url: http://fake-teams-api.default.svc.cluster.local
  # toleration: "key:db-only,operator:Exists,effect:NoSchedule"
  # wal_az_storage_account: ""
  # wal_gs_bucket: ""
  # wal_s3_bucket: ""
  watched_namespace: "*"  # listen to all namespaces
  workers: "8"


================================================
File: manifests/custom-team-membership.yaml
================================================
apiVersion: "acid.zalan.do/v1"
kind: PostgresTeam
metadata:
  name: custom-team-membership
spec:
  additionalSuperuserTeams:
    acid:
    - "postgres_superusers"
  additionalTeams:
    acid: []
  additionalMembers:
    acid:
    - "elephant"


================================================
File: manifests/e2e-storage-class.yaml
================================================
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/host-path


================================================
File: manifests/fake-teams-api.yaml
================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fake-teams-api
spec:
  replicas: 1
  selector:
    matchLabels:
      name: fake-teams-api
  template:
    metadata:
      labels:
        name: fake-teams-api
    spec:
      containers:
      - name: fake-teams-api
        image: ikitiki/fake-teams-api:latest

---

apiVersion: v1
kind: Service
metadata:
  name: fake-teams-api
spec:
  selector:
    name: fake-teams-api
  ports:
  - name: server
    port: 80
    protocol: TCP
    targetPort: 80
  type: NodePort

---

apiVersion: v1
kind: Secret
metadata:
  name: postgresql-operator
  namespace: default
type: Opaque
data:
  read-only-token-secret: dGVzdHRva2Vu
  read-only-token-type: QmVhcmVy


================================================
File: manifests/fes.crd.yaml
================================================
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: fabriceventstreams.zalando.org
spec:
  group: zalando.org
  names:
    kind: FabricEventStream
    listKind: FabricEventStreamList
    plural: fabriceventstreams
    singular: fabriceventstream
    shortNames:
    - fes
    categories:
    - all
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object


================================================
File: manifests/infrastructure-roles-configmap.yaml
================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgresql-infrastructure-roles
data:
  batman: |
    inrole: [admin]  # following roles will be assigned to the new user
    user_flags:
      - createdb
    db_parameters:  # db parameters, applyed for this particular user
      log_statement: all


================================================
File: manifests/infrastructure-roles-new.yaml
================================================
apiVersion: v1
data:
  # infrastructure role definition in the new format
  # robot_zmon_acid_monitoring_new
  user: cm9ib3Rfem1vbl9hY2lkX21vbml0b3JpbmdfbmV3
  # foobar_new
  password: Zm9vYmFyX25ldw==
kind: Secret
metadata:
  name: postgresql-infrastructure-roles-new
type: Opaque


================================================
File: manifests/infrastructure-roles.yaml
================================================
apiVersion: v1
data:
  # required format (w/o quotes): 'propertyNumber: value'
  # allowed properties: 'user', 'password', 'inrole'
  # numbers >= 1 are mandatory
  # alternatively, supply the user: password pairs and
  # provide other options in the configmap.
  # robot_zmon_acid_monitoring
  user1: cm9ib3Rfem1vbl9hY2lkX21vbml0b3Jpbmc=
  # foobar
  password1: Zm9vYmFy
  # robot_zmon
  inrole1: cm9ib3Rfem1vbg==
  # testuser
  user2: dGVzdHVzZXI=
  # testpassword
  password2: dGVzdHBhc3N3b3Jk
  # user batman with the password justice
  # look for other fields in the infrastructure roles configmap
  batman: anVzdGljZQ==
kind: Secret
metadata:
  name: postgresql-infrastructure-roles
type: Opaque


================================================
File: manifests/kustomization.yaml
================================================
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- configmap.yaml
- operator-service-account-rbac.yaml
- postgres-operator.yaml
- api-service.yaml


================================================
File: manifests/minimal-fake-pooler-deployment.yaml
================================================
# will not run but is good enough for tests to fail
apiVersion: apps/v1
kind: Deployment
metadata:
  name: acid-minimal-cluster-pooler
  labels:
    application: db-connection-pooler
    connection-pooler: acid-minimal-cluster-pooler
spec:
  replicas: 1
  selector:
    matchLabels:
      application: db-connection-pooler
      connection-pooler: acid-minimal-cluster-pooler
      cluster-name: acid-minimal-cluster
  template:
    metadata:
      labels:
        application: db-connection-pooler
        connection-pooler: acid-minimal-cluster-pooler
        cluster-name: acid-minimal-cluster
    spec:
      serviceAccountName: postgres-operator
      containers:
      - name: postgres-operator
        image: registry.opensource.zalan.do/acid/pgbouncer:master-32
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            cpu: 100m
            memory: 250Mi
          limits:
            cpu: 500m
            memory: 500Mi
        env: []


================================================
File: manifests/minimal-master-replica-svcmonitor.yaml
================================================
# Here we use https://github.com/prometheus-community/helm-charts/charts/kube-prometheus-stack
# Please keep the ServiceMonitor's label same as the Helm release name of kube-prometheus-stack 

apiVersion: v1
kind: Namespace
metadata:
  name: test-pg
---
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-minimal-cluster
  namespace: test-pg
  labels:
    app: test-pg
spec:
  teamId: "acid"
  volume:
    size: 1Gi
  numberOfInstances: 2
  users:
    zalando:  # database owner
    - superuser
    - createdb
    foo_user: []  # role for application foo
  databases:
    foo: zalando  # dbname: owner
  preparedDatabases:
    bar: {}
  postgresql:
    version: "13"
  sidecars:
    - name: "exporter"
      image: "quay.io/prometheuscommunity/postgres-exporter:v0.15.0"
      ports:
        - name: exporter
          containerPort: 9187
          protocol: TCP
      env:
       - name: DATA_SOURCE_URI
         value: ":5432/?sslmode=disable"
       - name: DATA_SOURCE_USER
         value: "postgres"
       - name: DATA_SOURCE_PASS
         valueFrom:
           secretKeyRef:
             name: postgres.test-pg.credentials.postgresql.acid.zalan.do
             key: password
      resources:
        limits:
          cpu: 500m
          memory: 256M
        requests:
          cpu: 100m
          memory: 200M
---
apiVersion: v1
kind: Service
metadata:
  name: acid-minimal-cluster-svc-metrics-master
  namespace: test-pg
  labels:
    app: test-pg
    spilo-role: master
  annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9187"
spec:
  type: ClusterIP
  ports:
    - name: exporter
      port: 9187
      targetPort: exporter
  selector:
    application: spilo
    cluster-name: acid-minimal-cluster
    spilo-role: master
---
apiVersion: v1
kind: Service
metadata:
  name: acid-minimal-cluster-svc-metrics-replica
  namespace: test-pg
  labels:
    app: test-pg
    spilo-role: replica
  annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9187"
spec:
  type: ClusterIP
  ports:
    - name: exporter
      port: 9187
      targetPort: exporter
  selector:
    application: spilo
    cluster-name: acid-minimal-cluster
    spilo-role: replica
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: acid-minimal-cluster-svcm-master
  namespace: test-pg
  labels:
    app: test-pg
    spilo-role: master
spec:
  endpoints:
    - port: exporter
      interval: 15s
      scrapeTimeout: 10s
  namespaceSelector:
    matchNames:
      - test-pg
  selector:
    matchLabels:
      app: test-pg
      spilo-role: master
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: acid-minimal-cluster-svcm-replica
  namespace: test-pg
  labels:
    app: test-pg
    spilo-role: replica
spec:
  endpoints:
    - port: exporter
      interval: 15s
      scrapeTimeout: 10s
  namespaceSelector:
    matchNames:
      - test-pg
  selector:
    matchLabels:
      app: test-pg
      spilo-role: replica


================================================
File: manifests/minimal-postgres-lowest-version-manifest.yaml
================================================
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-upgrade-test
spec:
  teamId: "acid"
  volume:
    size: 1Gi
  numberOfInstances: 2
  users:
    zalando:  # database owner
    - superuser
    - createdb
    foo_user: []  # role for application foo
  databases:
    foo: zalando  # dbname: owner
  preparedDatabases:
    bar: {}
  postgresql:
    version: "13"


================================================
File: manifests/minimal-postgres-manifest.yaml
================================================
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-minimal-cluster
spec:
  teamId: "acid"
  volume:
    size: 1Gi
  numberOfInstances: 2
  users:
    zalando:  # database owner
    - superuser
    - createdb
    foo_user: []  # role for application foo
  databases:
    foo: zalando  # dbname: owner
  preparedDatabases:
    bar: {}
  postgresql:
    version: "17"


================================================
File: manifests/operator-service-account-rbac-openshift.yaml
================================================
apiVersion: v1
kind: ServiceAccount
metadata:
  name: postgres-operator
  namespace: default

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: postgres-operator
rules:
# all verbs allowed for custom operator resources
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  - postgresqls/status
  - operatorconfigurations
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
# operator only reads PostgresTeams
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresteams
  verbs:
  - get
  - list
  - watch
# all verbs allowed for event streams (Zalando-internal feature)
# - apiGroups:
#   - zalando.org
#   resources:
#   - fabriceventstreams
#   verbs:
#   - create
#   - delete
#   - deletecollection
#   - get
#   - list
#   - patch
#   - update
#   - watch
# to create or get/update CRDs when starting up
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - create
  - get
  - patch
  - update
# to read configuration and manage ConfigMaps used by Patroni
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
# to send events to the CRs
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
# to CRUD secrets for database access
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - create
  - delete
  - get
  - patch
  - update
# to check nodes for node readiness label
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
# to read or delete existing PVCs. Creation via StatefulSet
- apiGroups:
  - ""
  resources:
  - persistentvolumeclaims
  verbs:
  - delete
  - get
  - list
  - patch
  - update
 # to read existing PVs. Creation should be done via dynamic provisioning
- apiGroups:
  - ""
  resources:
  - persistentvolumes
  verbs:
  - get
  - list
  - update  # only for resizing AWS volumes
# to watch Spilo pods and do rolling updates. Creation via StatefulSet
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - delete
  - get
  - list
  - patch
  - update
  - watch
# to resize the filesystem in Spilo pods when increasing volume size
- apiGroups:
  - ""
  resources:
  - pods/exec
  verbs:
  - create
# to CRUD services to point to Postgres cluster instances
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - create
  - delete
  - get
  - patch
  - update
# to CRUD the StatefulSet which controls the Postgres cluster instances
- apiGroups:
  - apps
  resources:
  - statefulsets
  - deployments
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
# to CRUD cron jobs for logical backups
- apiGroups:
  - batch
  resources:
  - cronjobs
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
# to get namespaces operator resources can run in
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
# to define PDBs. Update happens via delete/create
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - create
  - delete
  - get
# to create ServiceAccounts in each namespace the operator watches
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - get
  - create
# to create role bindings to the postgres-pod service account
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  verbs:
  - get
  - create
# to grant privilege to run privileged pods (not needed by default)
#- apiGroups:
#  - extensions
#  resources:
#  - podsecuritypolicies
#  resourceNames:
#  - privileged
#  verbs:
#  - use

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: postgres-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: postgres-operator
subjects:
- kind: ServiceAccount
  name: postgres-operator
  namespace: default

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: postgres-pod
rules:
# Patroni needs to watch and manage config maps
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
# Patroni needs to watch pods
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - patch
  - update
  - watch
# to let Patroni create a headless service
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - create
# to grant privilege to run privileged pods (not needed by default)
#- apiGroups:
#  - extensions
#  resources:
#  - podsecuritypolicies
#  resourceNames:
#  - privileged
#  verbs:
#  - use


================================================
File: manifests/operator-service-account-rbac.yaml
================================================
apiVersion: v1
kind: ServiceAccount
metadata:
  name: postgres-operator
  namespace: default

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: postgres-operator
rules:
# all verbs allowed for custom operator resources
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  - postgresqls/status
  - operatorconfigurations
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
# operator only reads PostgresTeams
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresteams
  verbs:
  - get
  - list
  - watch
# all verbs allowed for event streams (Zalando-internal feature)
# - apiGroups:
#   - zalando.org
#   resources:
#   - fabriceventstreams
#   verbs:
#   - create
#   - delete
#   - deletecollection
#   - get
#   - list
#   - patch
#   - update
#   - watch
# to create or get/update CRDs when starting up
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - create
  - get
  - patch
  - update
# to read configuration from ConfigMaps
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
# to send events to the CRs
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
# to manage endpoints which are also used by Patroni
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
# to CRUD secrets for database access
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - create
  - delete
  - get
  - update
  - patch
# to check nodes for node readiness label
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
# to read or delete existing PVCs. Creation via StatefulSet
- apiGroups:
  - ""
  resources:
  - persistentvolumeclaims
  verbs:
  - delete
  - get
  - list
  - patch
  - update
 # to read existing PVs. Creation should be done via dynamic provisioning
- apiGroups:
  - ""
  resources:
  - persistentvolumes
  verbs:
  - get
  - list
  - update  # only for resizing AWS volumes
# to watch Spilo pods and do rolling updates. Creation via StatefulSet
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - delete
  - get
  - list
  - patch
  - update
  - watch
# to resize the filesystem in Spilo pods when increasing volume size
- apiGroups:
  - ""
  resources:
  - pods/exec
  verbs:
  - create
# to CRUD services to point to Postgres cluster instances
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - create
  - delete
  - get
  - patch
  - update
# to CRUD the StatefulSet which controls the Postgres cluster instances
- apiGroups:
  - apps
  resources:
  - statefulsets
  - deployments
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
# to CRUD cron jobs for logical backups
- apiGroups:
  - batch
  resources:
  - cronjobs
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
# to get namespaces operator resources can run in
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
# to define PDBs. Update happens via delete/create
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - create
  - delete
  - get
# to create ServiceAccounts in each namespace the operator watches
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - get
  - create
# to create role bindings to the postgres-pod service account
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  verbs:
  - get
  - create
# to grant privilege to run privileged pods (not needed by default)
#- apiGroups:
#  - extensions
#  resources:
#  - podsecuritypolicies
#  resourceNames:
#  - privileged
#  verbs:
#  - use

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: postgres-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: postgres-operator
subjects:
- kind: ServiceAccount
  name: postgres-operator
  namespace: default

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: postgres-pod
rules:
# Patroni needs to watch and manage endpoints
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
# Patroni needs to watch pods
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - patch
  - update
  - watch
# to let Patroni create a headless service
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - create
# to grant privilege to run privileged pods (not needed by default)
#- apiGroups:
#  - extensions
#  resources:
#  - podsecuritypolicies
#  resourceNames:
#  - privileged
#  verbs:
#  - use


================================================
File: manifests/operatorconfiguration.crd.yaml
================================================
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: operatorconfigurations.acid.zalan.do
spec:
  group: acid.zalan.do
  names:
    kind: OperatorConfiguration
    listKind: OperatorConfigurationList
    plural: operatorconfigurations
    singular: operatorconfiguration
    shortNames:
    - opconfig
    categories:
    - all
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    subresources:
      status: {}
    additionalPrinterColumns:
    - name: Image
      type: string
      description: Spilo image to be used for Pods
      jsonPath: .configuration.docker_image
    - name: Cluster-Label
      type: string
      description: Label for K8s resources created by operator
      jsonPath: .configuration.kubernetes.cluster_name_label
    - name: Service-Account
      type: string
      description: Name of service account to be used
      jsonPath: .configuration.kubernetes.pod_service_account_name
    - name: Min-Instances
      type: integer
      description: Minimum number of instances per Postgres cluster
      jsonPath: .configuration.min_instances
    - name: Age
      type: date
      jsonPath: .metadata.creationTimestamp
    schema:
      openAPIV3Schema:
        type: object
        required:
          - kind
          - apiVersion
          - configuration
        properties:
          kind:
            type: string
            enum:
            - OperatorConfiguration
          apiVersion:
            type: string
            enum:
            - acid.zalan.do/v1
          configuration:
            type: object
            properties:
              crd_categories:
                type: array
                nullable: true
                items:
                  type: string
              docker_image:
                type: string
                default: "ghcr.io/zalando/spilo-17:4.0-p2"
              enable_crd_registration:
                type: boolean
                default: true
              enable_crd_validation:
                type: boolean
                description: deprecated
                default: true
              enable_lazy_spilo_upgrade:
                type: boolean
                default: false
              enable_pgversion_env_var:
                type: boolean
                default: true
              enable_shm_volume:
                type: boolean
                default: true
              enable_spilo_wal_path_compat:
                type: boolean
                default: false
              enable_team_id_clustername_prefix:
                type: boolean
                default: false
              etcd_host:
                type: string
                default: ""
              ignore_instance_limits_annotation_key:
                type: string
              kubernetes_use_configmaps:
                type: boolean
                default: false
              max_instances:
                type: integer
                description: "-1 = disabled"
                minimum: -1
                default: -1
              min_instances:
                type: integer
                description: "-1 = disabled"
                minimum: -1
                default: -1
              resync_period:
                type: string
                default: "30m"
              repair_period:
                type: string
                default: "5m"
              set_memory_request_to_limit:
                type: boolean
                default: false
              sidecar_docker_images:
                type: object
                additionalProperties:
                  type: string
              sidecars:
                type: array
                nullable: true
                items:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              workers:
                type: integer
                minimum: 1
                default: 8
              users:
                type: object
                properties:
                  additional_owner_roles:
                    type: array
                    nullable: true
                    items:
                      type: string
                  enable_password_rotation:
                    type: boolean
                    default: false
                  password_rotation_interval:
                    type: integer
                    default: 90
                  password_rotation_user_retention:
                    type: integer
                    default: 180
                  replication_username:
                     type: string
                     default: standby
                  super_username:
                     type: string
                     default: postgres
              major_version_upgrade:
                type: object
                properties:
                  major_version_upgrade_mode:
                    type: string
                    default: "manual"
                  major_version_upgrade_team_allow_list:
                    type: array
                    items:
                      type: string
                  minimal_major_version:
                    type: string
                    default: "13"
                  target_major_version:
                    type: string
                    default: "17"
              kubernetes:
                type: object
                properties:
                  additional_pod_capabilities:
                    type: array
                    items:
                      type: string
                  cluster_domain:
                    type: string
                    default: "cluster.local"
                  cluster_labels:
                    type: object
                    additionalProperties:
                      type: string
                    default:
                      application: spilo
                  cluster_name_label:
                    type: string
                    default: "cluster-name"
                  custom_pod_annotations:
                    type: object
                    additionalProperties:
                      type: string
                  delete_annotation_date_key:
                    type: string
                  delete_annotation_name_key:
                    type: string
                  downscaler_annotations:
                    type: array
                    items:
                      type: string
                  enable_cross_namespace_secret:
                    type: boolean
                    default: false
                  enable_finalizers:
                    type: boolean
                    default: false
                  enable_init_containers:
                    type: boolean
                    default: true
                  enable_owner_references:
                    type: boolean
                    default: false
                  enable_persistent_volume_claim_deletion:
                    type: boolean
                    default: true
                  enable_pod_antiaffinity:
                    type: boolean
                    default: false
                  enable_pod_disruption_budget:
                    type: boolean
                    default: true
                  enable_readiness_probe:
                    type: boolean
                    default: false
                  enable_secrets_deletion:
                    type: boolean
                    default: true
                  enable_sidecars:
                    type: boolean
                    default: true
                  ignored_annotations:
                    type: array
                    items:
                      type: string
                  infrastructure_roles_secret_name:
                    type: string
                  infrastructure_roles_secrets:
                    type: array
                    nullable: true
                    items:
                      type: object
                      required:
                        - secretname
                        - userkey
                        - passwordkey
                      properties:
                        secretname:
                          type: string
                        userkey:
                          type: string
                        passwordkey:
                          type: string
                        rolekey:
                          type: string
                        defaultuservalue:
                          type: string
                        defaultrolevalue:
                          type: string
                        details:
                          type: string
                        template:
                          type: boolean
                  inherited_annotations:
                    type: array
                    items:
                      type: string
                  inherited_labels:
                    type: array
                    items:
                      type: string
                  master_pod_move_timeout:
                    type: string
                    default: "20m"
                  node_readiness_label:
                    type: object
                    additionalProperties:
                      type: string
                  node_readiness_label_merge:
                    type: string
                    enum:
                      - "AND"
                      - "OR"
                  oauth_token_secret_name:
                    type: string
                    default: "postgresql-operator"
                  pdb_master_label_selector:
                    type: boolean
                    default: true
                  pdb_name_format:
                    type: string
                    default: "postgres-{cluster}-pdb"
                  persistent_volume_claim_retention_policy:
                    type: object
                    properties:
                      when_deleted:
                        type: string
                        enum:
                          - "delete"
                          - "retain"
                      when_scaled:
                        type: string
                        enum:
                          - "delete"
                          - "retain"
                  pod_antiaffinity_preferred_during_scheduling:
                    type: boolean
                    default: false
                  pod_antiaffinity_topology_key:
                    type: string
                    default: "kubernetes.io/hostname"
                  pod_environment_configmap:
                    type: string
                  pod_environment_secret:
                    type: string
                  pod_management_policy:
                    type: string
                    enum:
                      - "ordered_ready"
                      - "parallel"
                    default: "ordered_ready"
                  pod_priority_class_name:
                    type: string
                  pod_role_label:
                    type: string
                    default: "spilo-role"
                  pod_service_account_definition:
                    type: string
                    default: ""
                  pod_service_account_name:
                    type: string
                    default: "postgres-pod"
                  pod_service_account_role_binding_definition:
                    type: string
                    default: ""
                  pod_terminate_grace_period:
                    type: string
                    default: "5m"
                  secret_name_template:
                    type: string
                    default: "{username}.{cluster}.credentials.{tprkind}.{tprgroup}"
                  share_pgsocket_with_sidecars:
                    type: boolean
                    default: false
                  spilo_allow_privilege_escalation:
                    type: boolean
                    default: true
                  spilo_runasuser:
                    type: integer
                  spilo_runasgroup:
                    type: integer
                  spilo_fsgroup:
                    type: integer
                  spilo_privileged:
                    type: boolean
                    default: false
                  storage_resize_mode:
                    type: string
                    enum:
                      - "ebs"
                      - "mixed"
                      - "pvc"
                      - "off"
                    default: "pvc"
                  toleration:
                    type: object
                    additionalProperties:
                      type: string
                  watched_namespace:
                    type: string
              postgres_pod_resources:
                type: object
                properties:
                  default_cpu_limit:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$|^$'
                  default_cpu_request:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$|^$'
                  default_memory_limit:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$|^$'
                  default_memory_request:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$|^$'
                  max_cpu_request:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$|^$'
                  max_memory_request:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$|^$'
                  min_cpu_limit:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$|^$'
                  min_memory_limit:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$|^$'
              timeouts:
                type: object
                properties:
                  patroni_api_check_interval:
                    type: string
                    default: "1s"
                  patroni_api_check_timeout:
                    type: string
                    default: "5s"
                  pod_label_wait_timeout:
                    type: string
                    default: "10m"
                  pod_deletion_wait_timeout:
                    type: string
                    default: "10m"
                  ready_wait_interval:
                    type: string
                    default: "4s"
                  ready_wait_timeout:
                    type: string
                    default: "30s"
                  resource_check_interval:
                    type: string
                    default: "3s"
                  resource_check_timeout:
                    type: string
                    default: "10m"
              load_balancer:
                type: object
                properties:
                  custom_service_annotations:
                    type: object
                    additionalProperties:
                      type: string
                  db_hosted_zone:
                    type: string
                    default: "db.example.com"
                  enable_master_load_balancer:
                    type: boolean
                    default: true
                  enable_master_pooler_load_balancer:
                    type: boolean
                    default: false
                  enable_replica_load_balancer:
                    type: boolean
                    default: false
                  enable_replica_pooler_load_balancer:
                    type: boolean
                    default: false
                  external_traffic_policy:
                    type: string
                    enum:
                      - "Cluster"
                      - "Local"
                    default: "Cluster"
                  master_dns_name_format:
                    type: string
                    default: "{cluster}.{namespace}.{hostedzone}"
                  master_legacy_dns_name_format:
                    type: string
                    default: "{cluster}.{team}.{hostedzone}"
                  replica_dns_name_format:
                    type: string
                    default: "{cluster}-repl.{namespace}.{hostedzone}"
                  replica_legacy_dns_name_format:
                    type: string
                    default: "{cluster}-repl.{team}.{hostedzone}"
              aws_or_gcp:
                type: object
                properties:
                  additional_secret_mount:
                    type: string
                  additional_secret_mount_path:
                    type: string
                  aws_region:
                    type: string
                    default: "eu-central-1"
                  enable_ebs_gp3_migration:
                    type: boolean
                    default: false
                  enable_ebs_gp3_migration_max_size:
                    type: integer
                    default: 1000
                  gcp_credentials:
                    type: string
                  kube_iam_role:
                    type: string
                  log_s3_bucket:
                    type: string
                  wal_az_storage_account:
                    type: string
                  wal_gs_bucket:
                    type: string
                  wal_s3_bucket:
                    type: string
              logical_backup:
                type: object
                properties:
                  logical_backup_azure_storage_account_name:
                    type: string
                  logical_backup_azure_storage_container:
                    type: string
                  logical_backup_azure_storage_account_key:
                    type: string
                  logical_backup_cpu_limit:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                  logical_backup_cpu_request:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                  logical_backup_docker_image:
                    type: string
                    default: "ghcr.io/zalando/postgres-operator/logical-backup:v1.14.0"
                  logical_backup_google_application_credentials:
                    type: string
                  logical_backup_job_prefix:
                    type: string
                    default: "logical-backup-"
                  logical_backup_memory_limit:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                  logical_backup_memory_request:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                  logical_backup_provider:
                    type: string
                    enum:
                      - "az"
                      - "gcs"
                      - "s3"
                    default: "s3"
                  logical_backup_s3_access_key_id:
                    type: string
                  logical_backup_s3_bucket:
                    type: string
                  logical_backup_s3_bucket_prefix:
                    type: string
                  logical_backup_s3_endpoint:
                    type: string
                  logical_backup_s3_region:
                    type: string
                  logical_backup_s3_secret_access_key:
                    type: string
                  logical_backup_s3_sse:
                    type: string
                  logical_backup_s3_retention_time:
                    type: string
                  logical_backup_schedule:
                    type: string
                    pattern: '^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$'
                    default: "30 00 * * *"
                  logical_backup_cronjob_environment_secret:
                    type: string
              debug:
                type: object
                properties:
                  debug_logging:
                    type: boolean
                    default: true
                  enable_database_access:
                    type: boolean
                    default: true
              teams_api:
                type: object
                properties:
                  enable_admin_role_for_users:
                    type: boolean
                    default: true
                  enable_postgres_team_crd:
                    type: boolean
                    default: true
                  enable_postgres_team_crd_superusers:
                    type: boolean
                    default: false
                  enable_team_member_deprecation:
                    type: boolean
                    default: false
                  enable_team_superuser:
                    type: boolean
                    default: false
                  enable_teams_api:
                    type: boolean
                    default: true
                  pam_configuration:
                    type: string
                    default: "https://info.example.com/oauth2/tokeninfo?access_token= uid realm=/employees"
                  pam_role_name:
                    type: string
                    default: "zalandos"
                  postgres_superuser_teams:
                    type: array
                    items:
                      type: string
                  protected_role_names:
                    type: array
                    items:
                      type: string
                    default:
                    - admin
                    - cron_admin
                  role_deletion_suffix:
                    type: string
                    default: "_deleted"
                  team_admin_role:
                    type: string
                    default: "admin"
                  team_api_role_configuration:
                    type: object
                    additionalProperties:
                      type: string
                    default:
                      log_statement: all
                  teams_api_url:
                    type: string
                    default: "https://teams.example.com/api/"
              logging_rest_api:
                type: object
                properties:
                  api_port:
                    type: integer
                    default: 8080
                  cluster_history_entries:
                    type: integer
                    default: 1000
                  ring_log_lines:
                    type: integer
                    default: 100
              scalyr:  # deprecated
                type: object
                properties:
                  scalyr_api_key:
                    type: string
                  scalyr_cpu_limit:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                    default: "1"
                  scalyr_cpu_request:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                    default: "100m"
                  scalyr_image:
                    type: string
                  scalyr_memory_limit:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                    default: "500Mi"
                  scalyr_memory_request:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                    default: "50Mi"
                  scalyr_server_url:
                    type: string
                    default: "https://upload.eu.scalyr.com"
              connection_pooler:
                type: object
                properties:
                  connection_pooler_schema:
                    type: string
                    default: "pooler"
                  connection_pooler_user:
                    type: string
                    default: "pooler"
                  connection_pooler_image:
                    type: string
                    default: "registry.opensource.zalan.do/acid/pgbouncer:master-32"
                  connection_pooler_max_db_connections:
                    type: integer
                    default: 60
                  connection_pooler_mode:
                    type: string
                    enum:
                      - "session"
                      - "transaction"
                    default: "transaction"
                  connection_pooler_number_of_instances:
                    type: integer
                    minimum: 1
                    default: 2
                  connection_pooler_default_cpu_limit:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                  connection_pooler_default_cpu_request:
                    type: string
                    pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                  connection_pooler_default_memory_limit:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                  connection_pooler_default_memory_request:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
              patroni:
                type: object
                properties:
                  enable_patroni_failsafe_mode:
                    type: boolean
                    default: false
          status:
            type: object
            additionalProperties:
              type: string


================================================
File: manifests/platform-credentials.yaml
================================================
apiVersion: "zalando.org/v1"
kind: PlatformCredentialsSet
metadata:
  name: postgresql-operator
spec:
  application: postgresql-operator
  tokens:
    read-only:
      privileges:
    cluster-registry-rw:
      privileges:
    cluster-rw:
      privileges:


================================================
File: manifests/postgres-operator.yaml
================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-operator
  labels:
    application: postgres-operator
spec:
  replicas: 1
  strategy:
    type: "Recreate"
  selector:
    matchLabels:
      name: postgres-operator
  template:
    metadata:
      labels:
        name: postgres-operator
    spec:
      serviceAccountName: postgres-operator
      containers:
      - name: postgres-operator
        image: ghcr.io/zalando/postgres-operator:v1.14.0
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            cpu: 100m
            memory: 250Mi
          limits:
            cpu: 500m
            memory: 500Mi
        securityContext:
          runAsUser: 1000
          runAsNonRoot: true
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
        env:
        # provided additional ENV vars can overwrite individual config map entries
        - name: CONFIG_MAP_NAME
          value: "postgres-operator"
        # In order to use the CRD OperatorConfiguration instead, uncomment these lines and comment out the two lines above
        # - name: POSTGRES_OPERATOR_CONFIGURATION_OBJECT
        #  value: postgresql-operator-default-configuration
        # Define an ID to isolate controllers from each other
        # - name: CONTROLLER_ID
        #   value: "second-operator"


================================================
File: manifests/postgres-pod-priority-class.yaml
================================================
apiVersion: scheduling.k8s.io/v1
description: 'This priority class must be used only for databases controlled by the
  Postgres operator'
kind: PriorityClass
metadata:
  labels:
    application: postgres-operator
  name: postgres-pod-priority
preemptionPolicy: PreemptLowerPriority
globalDefault: false
value: 1000000


================================================
File: manifests/postgresql-operator-default-configuration.yaml
================================================
apiVersion: "acid.zalan.do/v1"
kind: OperatorConfiguration
metadata:
  name: postgresql-operator-default-configuration
configuration:
  docker_image: ghcr.io/zalando/spilo-17:4.0-p2
  # enable_crd_registration: true
  # crd_categories:
  # - all
  # enable_lazy_spilo_upgrade: false
  enable_pgversion_env_var: true
  # enable_shm_volume: true
  enable_spilo_wal_path_compat: false
  enable_team_id_clustername_prefix: false
  etcd_host: ""
  # ignore_instance_limits_annotation_key: ""
  # kubernetes_use_configmaps: false
  max_instances: -1
  min_instances: -1
  resync_period: 30m
  repair_period: 5m
  # set_memory_request_to_limit: false
  # sidecars:
  # - image: image:123
  #   name: global-sidecar-1
  #   ports:
  #   - containerPort: 80
  #     protocol: TCP
  workers: 8
  users:
    # additional_owner_roles: 
    # - cron_admin
    enable_password_rotation: false
    password_rotation_interval: 90
    password_rotation_user_retention: 180
    replication_username: standby
    super_username: postgres
  major_version_upgrade:
    major_version_upgrade_mode: "manual"
    # major_version_upgrade_team_allow_list:
    # - acid
    minimal_major_version: "13"
    target_major_version: "17"
  kubernetes:
    # additional_pod_capabilities:
    # - "SYS_NICE"
    cluster_domain: cluster.local
    cluster_labels:
      application: spilo
    cluster_name_label: cluster-name
    # custom_pod_annotations:
    #   keya: valuea
    #   keyb: valueb
    # delete_annotation_date_key: delete-date
    # delete_annotation_name_key: delete-clustername
    # downscaler_annotations:
    # - deployment-time
    # - downscaler/*
    # enable_cross_namespace_secret: "false"
    enable_finalizers: false
    enable_init_containers: true
    enable_owner_references: false
    enable_persistent_volume_claim_deletion: true
    enable_pod_antiaffinity: false
    enable_pod_disruption_budget: true
    enable_readiness_probe: false
    enable_secrets_deletion: true
    enable_sidecars: true
    # ignored_annotations:
    # - k8s.v1.cni.cncf.io/network-status
    # infrastructure_roles_secret_name: "postgresql-infrastructure-roles"
    # infrastructure_roles_secrets:
    # - secretname: "monitoring-roles"
    #   userkey: "user"
    #   passwordkey: "password"
    #   rolekey: "inrole"
    # - secretname: "other-infrastructure-role"
    #   userkey: "other-user-key"
    #   passwordkey: "other-password-key"
    # inherited_annotations:
    # - owned-by
    # inherited_labels:
    # - application
    # - environment
    master_pod_move_timeout: 20m
    # node_readiness_label:
    #   status: ready
    # node_readiness_label_merge: "OR"
    oauth_token_secret_name: postgresql-operator
    pdb_master_label_selector: true
    pdb_name_format: "postgres-{cluster}-pdb"
    persistent_volume_claim_retention_policy: 
      when_deleted: "retain"
      when_scaled: "retain"
    pod_antiaffinity_preferred_during_scheduling: false
    pod_antiaffinity_topology_key: "kubernetes.io/hostname"
    # pod_environment_configmap: "default/my-custom-config"
    # pod_environment_secret: "my-custom-secret"
    pod_management_policy: "ordered_ready"
    # pod_priority_class_name: "postgres-pod-priority"
    pod_role_label: spilo-role
    # pod_service_account_definition: ""
    pod_service_account_name: postgres-pod
    # pod_service_account_role_binding_definition: ""
    pod_terminate_grace_period: 5m
    secret_name_template: "{username}.{cluster}.credentials.{tprkind}.{tprgroup}"
    share_pgsocket_with_sidecars: false
    spilo_allow_privilege_escalation: true
    # spilo_runasuser: 101
    # spilo_runasgroup: 103
    # spilo_fsgroup: 103
    spilo_privileged: false
    storage_resize_mode: pvc
    # toleration:
    #   key: db-only
    #   operator: Exists
    #   effect: NoSchedule
    # watched_namespace: ""
  postgres_pod_resources:
    default_cpu_limit: "1"
    default_cpu_request: 100m
    default_memory_limit: 500Mi
    default_memory_request: 100Mi
    # max_cpu_request: "1"
    # max_memory_request: 4Gi
    # min_cpu_limit: 250m
    # min_memory_limit: 250Mi
  timeouts:
    patroni_api_check_interval: 1s
    patroni_api_check_timeout: 5s
    pod_label_wait_timeout: 10m
    pod_deletion_wait_timeout: 10m
    ready_wait_interval: 4s
    ready_wait_timeout: 30s
    resource_check_interval: 3s
    resource_check_timeout: 10m
  load_balancer:
    # custom_service_annotations:
    #   keyx: valuex
    #   keyy: valuey
    # db_hosted_zone: ""
    enable_master_load_balancer: false
    enable_master_pooler_load_balancer: false
    enable_replica_load_balancer: false
    enable_replica_pooler_load_balancer: false
    external_traffic_policy: "Cluster"
    master_dns_name_format: "{cluster}.{namespace}.{hostedzone}"
    # master_legacy_dns_name_format: "{cluster}.{team}.{hostedzone}"
    replica_dns_name_format: "{cluster}-repl.{namespace}.{hostedzone}"
    # replica_dns_old_name_format: "{cluster}-repl.{team}.{hostedzone}"
  aws_or_gcp:
    # additional_secret_mount: "some-secret-name"
    # additional_secret_mount_path: "/some/dir"
    aws_region: eu-central-1
    enable_ebs_gp3_migration: false
    # enable_ebs_gp3_migration_max_size: 1000
    # gcp_credentials: ""
    # kube_iam_role: ""
    # log_s3_bucket: ""
    # wal_az_storage_account: ""
    # wal_gs_bucket: ""
    # wal_s3_bucket: ""
  logical_backup:
    # logical_backup_azure_storage_account_name: ""
    # logical_backup_azure_storage_container: ""
    # logical_backup_azure_storage_account_key: ""
    # logical_backup_cpu_limit: ""
    # logical_backup_cpu_request: ""
    # logical_backup_memory_limit: ""
    # logical_backup_memory_request: ""
    logical_backup_docker_image: "ghcr.io/zalando/postgres-operator/logical-backup:v1.14.0"
    # logical_backup_google_application_credentials: ""
    logical_backup_job_prefix: "logical-backup-"
    logical_backup_provider: "s3"
    # logical_backup_s3_access_key_id: ""
    logical_backup_s3_bucket: "my-bucket-url"
    # logical_backup_s3_bucket_prefix: "spilo"
    # logical_backup_s3_endpoint: ""
    # logical_backup_s3_region: ""
    # logical_backup_s3_secret_access_key: ""
    logical_backup_s3_sse: "AES256"
    # logical_backup_s3_retention_time: ""
    logical_backup_schedule: "30 00 * * *"
    # logical_backup_cronjob_environment_secret: ""
  debug:
    debug_logging: true
    enable_database_access: true
  teams_api:
    # enable_admin_role_for_users: true
    # enable_postgres_team_crd: false
    # enable_postgres_team_crd_superusers: false
    enable_team_member_deprecation: false
    enable_team_superuser: false
    enable_teams_api: false
    # pam_configuration: ""
    pam_role_name: zalandos
    # postgres_superuser_teams:
    # - postgres_superusers
    protected_role_names:
    - admin
    - cron_admin
    role_deletion_suffix: "_deleted"
    team_admin_role: admin
    team_api_role_configuration:
      log_statement: all
    # teams_api_url: ""
  logging_rest_api:
    api_port: 8080
    cluster_history_entries: 1000
    ring_log_lines: 100
  connection_pooler:
    connection_pooler_default_cpu_limit: "1"
    connection_pooler_default_cpu_request: "500m"
    connection_pooler_default_memory_limit: 100Mi
    connection_pooler_default_memory_request: 100Mi
    connection_pooler_image: "registry.opensource.zalan.do/acid/pgbouncer:master-32"
    # connection_pooler_max_db_connections: 60
    connection_pooler_mode: "transaction"
    connection_pooler_number_of_instances: 2
    # connection_pooler_schema: "pooler"
    # connection_pooler_user: "pooler"
  patroni:
    enable_patroni_failsafe_mode: false


================================================
File: manifests/postgresql.crd.yaml
================================================
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: postgresqls.acid.zalan.do
spec:
  group: acid.zalan.do
  names:
    kind: postgresql
    listKind: postgresqlList
    plural: postgresqls
    singular: postgresql
    shortNames:
    - pg
    categories:
    - all
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    subresources:
      status: {}
    additionalPrinterColumns:
    - name: Team
      type: string
      description: Team responsible for Postgres cluster
      jsonPath: .spec.teamId
    - name: Version
      type: string
      description: PostgreSQL version
      jsonPath: .spec.postgresql.version
    - name: Pods
      type: integer
      description: Number of Pods per Postgres cluster
      jsonPath: .spec.numberOfInstances
    - name: Volume
      type: string
      description: Size of the bound volume
      jsonPath: .spec.volume.size
    - name: CPU-Request
      type: string
      description: Requested CPU for Postgres containers
      jsonPath: .spec.resources.requests.cpu
    - name: Memory-Request
      type: string
      description: Requested memory for Postgres containers
      jsonPath: .spec.resources.requests.memory
    - name: Age
      type: date
      jsonPath: .metadata.creationTimestamp
    - name: Status
      type: string
      description: Current sync status of postgresql resource
      jsonPath: .status.PostgresClusterStatus
    schema:
      openAPIV3Schema:
        type: object
        required:
          - kind
          - apiVersion
          - spec
        properties:
          kind:
            type: string
            enum:
              - postgresql
          apiVersion:
            type: string
            enum:
              - acid.zalan.do/v1
          spec:
            type: object
            required:
              - numberOfInstances
              - teamId
              - postgresql
              - volume
            properties:
              additionalVolumes:
                type: array
                items:
                  type: object
                  required:
                    - name
                    - mountPath
                    - volumeSource
                  properties:
                    isSubPathExpr:
                      type: boolean
                    name:
                      type: string
                    mountPath:
                      type: string
                    subPath:
                      type: string
                    targetContainers:
                      type: array
                      nullable: true
                      items:
                        type: string
                    volumeSource:
                      type: object
                      x-kubernetes-preserve-unknown-fields: true
              allowedSourceRanges:
                type: array
                nullable: true
                items:
                  type: string
                  pattern: '^(\d|[1-9]\d|1\d\d|2[0-4]\d|25[0-5])\.(\d|[1-9]\d|1\d\d|2[0-4]\d|25[0-5])\.(\d|[1-9]\d|1\d\d|2[0-4]\d|25[0-5])\.(\d|[1-9]\d|1\d\d|2[0-4]\d|25[0-5])\/(\d|[1-2]\d|3[0-2])$'
              clone:
                type: object
                required:
                  - cluster
                properties:
                  cluster:
                    type: string
                  s3_endpoint:
                    type: string
                  s3_access_key_id:
                    type: string
                  s3_secret_access_key:
                    type: string
                  s3_force_path_style:
                    type: boolean
                  s3_wal_path:
                    type: string
                  timestamp:
                    type: string
                    pattern: '^([0-9]+)-(0[1-9]|1[012])-(0[1-9]|[12][0-9]|3[01])[Tt]([01][0-9]|2[0-3]):([0-5][0-9]):([0-5][0-9]|60)(\.[0-9]+)?(([+-]([01][0-9]|2[0-3]):[0-5][0-9]))$'
                    # The regexp matches the date-time format (RFC 3339 Section 5.6) that specifies a timezone as an offset relative to UTC
                    # Example: 1996-12-19T16:39:57-08:00
                    # Note: this field requires a timezone
                  uid:
                    format: uuid
                    type: string
              connectionPooler:
                type: object
                properties:
                  dockerImage:
                    type: string
                  maxDBConnections:
                    type: integer
                  mode:
                    type: string
                    enum:
                      - "session"
                      - "transaction"
                  numberOfInstances:
                    type: integer
                    minimum: 1
                  resources:
                    type: object
                    properties:
                      limits:
                        type: object
                        properties:
                          cpu:
                            type: string
                            pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                          memory:
                            type: string
                            pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                      requests:
                        type: object
                        properties:
                          cpu:
                            type: string
                            pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                          memory:
                            type: string
                            pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                  schema:
                    type: string
                  user:
                    type: string
              databases:
                type: object
                additionalProperties:
                  type: string
                # Note: usernames specified here as database owners must be declared in the users key of the spec key.
              dockerImage:
                type: string
              enableConnectionPooler:
                type: boolean
              enableReplicaConnectionPooler:
                type: boolean
              enableLogicalBackup:
                type: boolean
              enableMasterLoadBalancer:
                type: boolean
              enableMasterPoolerLoadBalancer:
                type: boolean
              enableReplicaLoadBalancer:
                type: boolean
              enableReplicaPoolerLoadBalancer:
                type: boolean
              enableShmVolume:
                type: boolean
              env:
                type: array
                nullable: true
                items:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              init_containers:
                type: array
                description: deprecated
                nullable: true
                items:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              initContainers:
                type: array
                nullable: true
                items:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              logicalBackupRetention:
                type: string
              logicalBackupSchedule:
                type: string
                pattern: '^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$'
              maintenanceWindows:
                type: array
                items:
                  type: string
                  pattern: '^\ *((Mon|Tue|Wed|Thu|Fri|Sat|Sun):(2[0-3]|[01]?\d):([0-5]?\d)|(2[0-3]|[01]?\d):([0-5]?\d))-((2[0-3]|[01]?\d):([0-5]?\d)|(2[0-3]|[01]?\d):([0-5]?\d))\ *$'
              masterServiceAnnotations:
                type: object
                additionalProperties:
                  type: string
              nodeAffinity:
                type: object
                properties:
                  preferredDuringSchedulingIgnoredDuringExecution:
                    type: array
                    items:
                      type: object
                      required:
                      - preference
                      - weight
                      properties:
                        preference:
                          type: object
                          properties:
                            matchExpressions:
                              type: array
                              items:
                                type: object
                                required:
                                - key
                                - operator
                                properties:
                                  key:
                                    type: string
                                  operator:
                                    type: string
                                  values:
                                    type: array
                                    items:
                                      type: string
                            matchFields:
                              type: array
                              items:
                                type: object
                                required:
                                - key
                                - operator
                                properties:
                                  key:
                                    type: string
                                  operator:
                                    type: string
                                  values:
                                    type: array
                                    items:
                                      type: string
                        weight:
                          format: int32
                          type: integer
                  requiredDuringSchedulingIgnoredDuringExecution:
                    type: object
                    required:
                    - nodeSelectorTerms
                    properties:
                      nodeSelectorTerms:
                        type: array
                        items:
                          type: object
                          properties:
                            matchExpressions:
                              type: array
                              items:
                                type: object
                                required:
                                - key
                                - operator
                                properties:
                                  key:
                                    type: string
                                  operator:
                                    type: string
                                  values:
                                    type: array
                                    items:
                                      type: string
                            matchFields:
                              type: array
                              items:
                                type: object
                                required:
                                - key
                                - operator
                                properties:
                                  key:
                                    type: string
                                  operator:
                                    type: string
                                  values:
                                    type: array
                                    items:
                                      type: string
              numberOfInstances:
                type: integer
                minimum: 0
              patroni:
                type: object
                properties:
                  failsafe_mode:
                    type: boolean
                  initdb:
                    type: object
                    additionalProperties:
                      type: string
                  loop_wait:
                    type: integer
                  maximum_lag_on_failover:
                    type: integer
                  pg_hba:
                    type: array
                    items:
                      type: string
                  retry_timeout:
                    type: integer
                  slots:
                    type: object
                    additionalProperties:
                      type: object
                      additionalProperties:
                        type: string
                  synchronous_mode:
                    type: boolean
                  synchronous_mode_strict:
                    type: boolean
                  synchronous_node_count:
                    type: integer
                  ttl:
                    type: integer
              podAnnotations:
                type: object
                additionalProperties:
                  type: string
              pod_priority_class_name:
                type: string
                description: deprecated
              podPriorityClassName:
                type: string
              postgresql:
                type: object
                required:
                  - version
                properties:
                  version:
                    type: string
                    enum:
                      - "13"
                      - "14"
                      - "15"
                      - "16"
                      - "17"
                  parameters:
                    type: object
                    additionalProperties:
                      type: string
              preparedDatabases:
                type: object
                additionalProperties:
                  type: object
                  properties:
                    defaultUsers:
                      type: boolean
                    extensions:
                      type: object
                      additionalProperties:
                        type: string
                    schemas:
                      type: object
                      additionalProperties:
                        type: object
                        properties:
                          defaultUsers:
                            type: boolean
                          defaultRoles:
                            type: boolean
                    secretNamespace:
                      type: string
              replicaLoadBalancer:
                type: boolean
                description: deprecated
              replicaServiceAnnotations:
                type: object
                additionalProperties:
                  type: string
              resources:
                type: object
                properties:
                  limits:
                    type: object
                    properties:
                      cpu:
                        type: string
                        # Decimal natural followed by m, or decimal natural followed by
                        # dot followed by up to three decimal digits.
                        #
                        # This is because the Kubernetes CPU resource has millis as the
                        # maximum precision.  The actual values are checked in code
                        # because the regular expression would be huge and horrible and
                        # not very helpful in validation error messages; this one checks
                        # only the format of the given number.
                        #
                        # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu
                        pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                        # Note: the value specified here must not be zero or be lower
                        # than the corresponding request.
                      memory:
                        type: string
                        # You can express memory as a plain integer or as a fixed-point
                        # integer using one of these suffixes: E, P, T, G, M, k. You can
                        # also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki
                        #
                        # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                        # Note: the value specified here must not be zero or be higher
                        # than the corresponding limit.
                      hugepages-2Mi:
                        type: string
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                      hugepages-1Gi:
                        type: string
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                  requests:
                    type: object
                    properties:
                      cpu:
                        type: string
                        pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                      memory:
                        type: string
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                      hugepages-2Mi:
                        type: string
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                      hugepages-1Gi:
                        type: string
                        pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
              schedulerName:
                type: string
              serviceAnnotations:
                type: object
                additionalProperties:
                  type: string
              sidecars:
                type: array
                nullable: true
                items:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              spiloRunAsUser:
                type: integer
              spiloRunAsGroup:
                type: integer
              spiloFSGroup:
                type: integer
              standby:
                type: object
                properties:
                  s3_wal_path:
                    type: string
                  gs_wal_path:
                    type: string
                  standby_host:
                    type: string
                  standby_port:
                    type: string
                oneOf:
                - required:
                  - s3_wal_path
                - required:
                  - gs_wal_path
                - required:
                  - standby_host
              streams:
                type: array
                items:
                  type: object
                  required:
                    - applicationId
                    - database
                    - tables
                  properties:
                    applicationId:
                      type: string
                    batchSize:
                      type: integer
                    cpu:
                      type: string
                      pattern: '^(\d+m|\d+(\.\d{1,3})?)$'
                    database:
                      type: string
                    enableRecovery:
                      type: boolean
                    filter:
                      type: object
                      additionalProperties:
                        type: string
                    memory:
                      type: string
                      pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                    tables:
                      type: object
                      additionalProperties:
                        type: object
                        required:
                          - eventType
                        properties:
                          eventType:
                            type: string
                          idColumn:
                            type: string
                          ignoreRecovery:
                            type: boolean
                          payloadColumn:
                            type: string
                          recoveryEventType:
                            type: string
              teamId:
                type: string
              tls:
                type: object
                required:
                  - secretName
                properties:
                  secretName:
                    type: string
                  certificateFile:
                    type: string
                  privateKeyFile:
                    type: string
                  caFile:
                    type: string
                  caSecretName:
                    type: string
              tolerations:
                type: array
                items:
                  type: object
                  properties:
                    key:
                      type: string
                    operator:
                      type: string
                      enum:
                        - Equal
                        - Exists
                    value:
                      type: string
                    effect:
                      type: string
                      enum:
                        - NoExecute
                        - NoSchedule
                        - PreferNoSchedule
                    tolerationSeconds:
                      type: integer
              useLoadBalancer:
                type: boolean
                description: deprecated
              users:
                type: object
                additionalProperties:
                  type: array
                  nullable: true
                  items:
                    type: string
                    enum:
                    - bypassrls
                    - BYPASSRLS
                    - nobypassrls
                    - NOBYPASSRLS
                    - createdb
                    - CREATEDB
                    - nocreatedb
                    - NOCREATEDB
                    - createrole
                    - CREATEROLE
                    - nocreaterole
                    - NOCREATEROLE
                    - inherit
                    - INHERIT
                    - noinherit
                    - NOINHERIT
                    - login
                    - LOGIN
                    - nologin
                    - NOLOGIN
                    - replication
                    - REPLICATION
                    - noreplication
                    - NOREPLICATION
                    - superuser
                    - SUPERUSER
                    - nosuperuser
                    - NOSUPERUSER
              usersIgnoringSecretRotation:
                type: array
                nullable: true
                items:
                  type: string
              usersWithInPlaceSecretRotation:
                type: array
                nullable: true
                items:
                  type: string
              usersWithSecretRotation:
                type: array
                nullable: true
                items:
                  type: string
              volume:
                type: object
                required:
                  - size
                properties:
                  isSubPathExpr:
                    type: boolean
                  iops:
                    type: integer
                  selector:
                    type: object
                    properties:
                      matchExpressions:
                        type: array
                        items:
                          type: object
                          required:
                            - key
                            - operator
                          properties:
                            key:
                              type: string
                            operator:
                              type: string
                              enum:
                                - DoesNotExist
                                - Exists
                                - In
                                - NotIn
                            values:
                              type: array
                              items:
                                type: string
                      matchLabels:
                        type: object
                        x-kubernetes-preserve-unknown-fields: true
                  size:
                    type: string
                    pattern: '^(\d+(e\d+)?|\d+(\.\d+)?(e\d+)?[EPTGMK]i?)$'
                    # Note: the value specified here must not be zero.
                  storageClass:
                    type: string
                  subPath:
                    type: string
                  throughput:
                    type: integer
          status:
            type: object
            additionalProperties:
              type: string


================================================
File: manifests/postgresteam.crd.yaml
================================================
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: postgresteams.acid.zalan.do
spec:
  group: acid.zalan.do
  names:
    kind: PostgresTeam
    listKind: PostgresTeamList
    plural: postgresteams
    singular: postgresteam
    shortNames:
    - pgteam
    categories:
    - all
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    subresources:
      status: {}
    schema:
      openAPIV3Schema:
        type: object
        required:
          - kind
          - apiVersion
          - spec
        properties:
          kind:
            type: string
            enum:
              - PostgresTeam
          apiVersion:
            type: string
            enum:
              - acid.zalan.do/v1
          spec:
            type: object
            properties:
              additionalSuperuserTeams:
                type: object
                description: "Map for teamId and associated additional superuser teams"
                additionalProperties:
                  type: array
                  nullable: true
                  description: "List of teams to become Postgres superusers"
                  items:
                    type: string
              additionalTeams:
                type: object
                description: "Map for teamId and associated additional teams"
                additionalProperties:
                  type: array
                  nullable: true
                  description: "List of teams whose members will also be added to the Postgres cluster"
                  items:
                    type: string
              additionalMembers:
                type: object
                description: "Map for teamId and associated additional users"
                additionalProperties:
                  type: array
                  nullable: true
                  description: "List of users who will also be added to the Postgres cluster"
                  items:
                    type: string


================================================
File: manifests/standby-manifest.yaml
================================================
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-standby-cluster
spec:
  teamId: "acid"
  volume:
    size: 1Gi
  numberOfInstances: 1
  postgresql:
    version: "17"
  # Make this a standby cluster and provide either the s3 bucket path of source cluster or the remote primary host for continuous streaming.
  standby:
    # s3_wal_path: "s3://mybucket/spilo/acid-minimal-cluster/abcd1234-2a4b-4b2a-8c9c-c1234defg567/wal/14/"
    standby_host: "acid-minimal-cluster.default"
    # standby_port: "5432"


================================================
File: manifests/user-facing-clusterroles.yaml
================================================
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
  name: zalando-postgres-operator:users:admin
rules:
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  - postgresqls/status
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
  name: zalando-postgres-operator:users:edit
rules:
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  verbs:
  - create
  - update
  - patch
  - delete

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: zalando-postgres-operator:users:view
rules:
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  - postgresqls/status
  verbs:
  - get
  - list
  - watch


================================================
File: mocks/mocks.go
================================================
package mocks


================================================
File: pkg/apis/acid.zalan.do/register.go
================================================
package acidzalando

const (
	// GroupName is the group name for the operator CRDs
	GroupName = "acid.zalan.do"
)


================================================
File: pkg/apis/acid.zalan.do/v1/const.go
================================================
package v1

// ClusterStatusUnknown etc : status of a Postgres cluster known to the operator
const (
	ClusterStatusUnknown      = ""
	ClusterStatusCreating     = "Creating"
	ClusterStatusUpdating     = "Updating"
	ClusterStatusUpdateFailed = "UpdateFailed"
	ClusterStatusSyncFailed   = "SyncFailed"
	ClusterStatusAddFailed    = "CreateFailed"
	ClusterStatusRunning      = "Running"
	ClusterStatusInvalid      = "Invalid"
)

const (
	serviceNameMaxLength   = 63
	clusterNameMaxLength   = serviceNameMaxLength - len("-repl")
	serviceNameRegexString = `^[a-z]([-a-z0-9]*[a-z0-9])?$`
)


================================================
File: pkg/apis/acid.zalan.do/v1/doc.go
================================================
// Package v1 is the v1 version of the API.
// +k8s:deepcopy-gen=package,register

// +groupName=acid.zalan.do

package v1


================================================
File: pkg/apis/acid.zalan.do/v1/marshal.go
================================================
package v1

import (
	"encoding/json"
	"fmt"
	"strings"
	"time"
)

type postgresqlCopy Postgresql
type postgresStatusCopy PostgresStatus

// MarshalJSON converts a maintenance window definition to JSON.
func (m *MaintenanceWindow) MarshalJSON() ([]byte, error) {
	if m.Everyday {
		return []byte(fmt.Sprintf("\"%s-%s\"",
			m.StartTime.Format("15:04"),
			m.EndTime.Format("15:04"))), nil
	}

	return []byte(fmt.Sprintf("\"%s:%s-%s\"",
		m.Weekday.String()[:3],
		m.StartTime.Format("15:04"),
		m.EndTime.Format("15:04"))), nil
}

// UnmarshalJSON converts a JSON to the maintenance window definition.
func (m *MaintenanceWindow) UnmarshalJSON(data []byte) error {
	var (
		got MaintenanceWindow
		err error
	)

	parts := strings.Split(string(data[1:len(data)-1]), "-")
	if len(parts) != 2 {
		return fmt.Errorf("incorrect maintenance window format")
	}

	fromParts := strings.Split(parts[0], ":")
	switch len(fromParts) {
	case 3:
		got.Everyday = false
		got.Weekday, err = parseWeekday(fromParts[0])
		if err != nil {
			return fmt.Errorf("could not parse weekday: %v", err)
		}

		got.StartTime, err = parseTime(fromParts[1] + ":" + fromParts[2])
	case 2:
		got.Everyday = true
		got.StartTime, err = parseTime(fromParts[0] + ":" + fromParts[1])
	default:
		return fmt.Errorf("incorrect maintenance window format")
	}
	if err != nil {
		return fmt.Errorf("could not parse start time: %v", err)
	}

	got.EndTime, err = parseTime(parts[1])
	if err != nil {
		return fmt.Errorf("could not parse end time: %v", err)
	}

	if got.EndTime.Before(&got.StartTime) {
		return fmt.Errorf("'From' time must be prior to the 'To' time")
	}

	*m = got

	return nil
}

// UnmarshalJSON converts a JSON to the status subresource definition.
func (ps *PostgresStatus) UnmarshalJSON(data []byte) error {
	var (
		tmp    postgresStatusCopy
		status string
	)

	err := json.Unmarshal(data, &tmp)
	if err != nil {
		metaErr := json.Unmarshal(data, &status)
		if metaErr != nil {
			return fmt.Errorf("could not parse status: %v; err %v", string(data), metaErr)
		}
		tmp.PostgresClusterStatus = status
	}
	*ps = PostgresStatus(tmp)

	return nil
}

// UnmarshalJSON converts a JSON into the PostgreSQL object.
func (p *Postgresql) UnmarshalJSON(data []byte) error {
	var tmp postgresqlCopy

	err := json.Unmarshal(data, &tmp)
	if err != nil {
		metaErr := json.Unmarshal(data, &tmp.ObjectMeta)
		if metaErr != nil {
			return err
		}

		tmp.Error = err.Error()
		tmp.Status.PostgresClusterStatus = ClusterStatusInvalid

		*p = Postgresql(tmp)

		return nil
	}
	tmp2 := Postgresql(tmp)

	if err := validateCloneClusterDescription(tmp2.Spec.Clone); err != nil {
		tmp2.Error = err.Error()
		tmp2.Status.PostgresClusterStatus = ClusterStatusInvalid
	}

	*p = tmp2

	return nil
}

// UnmarshalJSON convert to Duration from byte slice of json
func (d *Duration) UnmarshalJSON(b []byte) error {
	var (
		v   interface{}
		err error
	)
	if err = json.Unmarshal(b, &v); err != nil {
		return err
	}
	switch val := v.(type) {
	case string:
		t, err := time.ParseDuration(val)
		if err != nil {
			return err
		}
		*d = Duration(t)
		return nil
	case float64:
		t := time.Duration(val)
		*d = Duration(t)
		return nil
	default:
		return fmt.Errorf("could not recognize type %T as a valid type to unmarshal to Duration", val)
	}
}


================================================
File: pkg/apis/acid.zalan.do/v1/operator_configuration_type.go
================================================
package v1

// Operator configuration CRD definition, please use snake_case for field names.

import (
	"github.com/zalando/postgres-operator/pkg/util/config"

	"time"

	"github.com/zalando/postgres-operator/pkg/spec"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// +genclient
// +genclient:onlyVerbs=get
// +genclient:noStatus
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// OperatorConfiguration defines the specification for the OperatorConfiguration.
type OperatorConfiguration struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata"`

	Configuration OperatorConfigurationData `json:"configuration"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// OperatorConfigurationList is used in the k8s API calls
type OperatorConfigurationList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata"`

	Items []OperatorConfiguration `json:"items"`
}

// PostgresUsersConfiguration defines the system users of Postgres.
type PostgresUsersConfiguration struct {
	SuperUsername                 string   `json:"super_username,omitempty"`
	ReplicationUsername           string   `json:"replication_username,omitempty"`
	AdditionalOwnerRoles          []string `json:"additional_owner_roles,omitempty"`
	EnablePasswordRotation        bool     `json:"enable_password_rotation,omitempty"`
	PasswordRotationInterval      uint32   `json:"password_rotation_interval,omitempty"`
	PasswordRotationUserRetention uint32   `json:"password_rotation_user_retention,omitempty"`
}

// MajorVersionUpgradeConfiguration defines how to execute major version upgrades of Postgres.
type MajorVersionUpgradeConfiguration struct {
	MajorVersionUpgradeMode          string   `json:"major_version_upgrade_mode" default:"manual"` // off - no actions, manual - manifest triggers action, full - manifest and minimal version violation trigger upgrade
	MajorVersionUpgradeTeamAllowList []string `json:"major_version_upgrade_team_allow_list,omitempty"`
	MinimalMajorVersion              string   `json:"minimal_major_version" default:"13"`
	TargetMajorVersion               string   `json:"target_major_version" default:"17"`
}

// KubernetesMetaConfiguration defines k8s conf required for all Postgres clusters and the operator itself
type KubernetesMetaConfiguration struct {
	EnableOwnerReferences *bool  `json:"enable_owner_references,omitempty"`
	PodServiceAccountName string `json:"pod_service_account_name,omitempty"`
	// TODO: change it to the proper json
	PodServiceAccountDefinition            string                       `json:"pod_service_account_definition,omitempty"`
	PodServiceAccountRoleBindingDefinition string                       `json:"pod_service_account_role_binding_definition,omitempty"`
	PodTerminateGracePeriod                Duration                     `json:"pod_terminate_grace_period,omitempty"`
	SpiloPrivileged                        bool                         `json:"spilo_privileged,omitempty"`
	SpiloAllowPrivilegeEscalation          *bool                        `json:"spilo_allow_privilege_escalation,omitempty"`
	SpiloRunAsUser                         *int64                       `json:"spilo_runasuser,omitempty"`
	SpiloRunAsGroup                        *int64                       `json:"spilo_runasgroup,omitempty"`
	SpiloFSGroup                           *int64                       `json:"spilo_fsgroup,omitempty"`
	AdditionalPodCapabilities              []string                     `json:"additional_pod_capabilities,omitempty"`
	WatchedNamespace                       string                       `json:"watched_namespace,omitempty"`
	PDBNameFormat                          config.StringTemplate        `json:"pdb_name_format,omitempty"`
	PDBMasterLabelSelector                 *bool                        `json:"pdb_master_label_selector,omitempty"`
	EnablePodDisruptionBudget              *bool                        `json:"enable_pod_disruption_budget,omitempty"`
	StorageResizeMode                      string                       `json:"storage_resize_mode,omitempty"`
	EnableInitContainers                   *bool                        `json:"enable_init_containers,omitempty"`
	EnableSidecars                         *bool                        `json:"enable_sidecars,omitempty"`
	SharePgSocketWithSidecars              *bool                        `json:"share_pgsocket_with_sidecars,omitempty"`
	SecretNameTemplate                     config.StringTemplate        `json:"secret_name_template,omitempty"`
	ClusterDomain                          string                       `json:"cluster_domain,omitempty"`
	OAuthTokenSecretName                   spec.NamespacedName          `json:"oauth_token_secret_name,omitempty"`
	InfrastructureRolesSecretName          spec.NamespacedName          `json:"infrastructure_roles_secret_name,omitempty"`
	InfrastructureRolesDefs                []*config.InfrastructureRole `json:"infrastructure_roles_secrets,omitempty"`
	PodRoleLabel                           string                       `json:"pod_role_label,omitempty"`
	ClusterLabels                          map[string]string            `json:"cluster_labels,omitempty"`
	InheritedLabels                        []string                     `json:"inherited_labels,omitempty"`
	InheritedAnnotations                   []string                     `json:"inherited_annotations,omitempty"`
	DownscalerAnnotations                  []string                     `json:"downscaler_annotations,omitempty"`
	IgnoredAnnotations                     []string                     `json:"ignored_annotations,omitempty"`
	ClusterNameLabel                       string                       `json:"cluster_name_label,omitempty"`
	DeleteAnnotationDateKey                string                       `json:"delete_annotation_date_key,omitempty"`
	DeleteAnnotationNameKey                string                       `json:"delete_annotation_name_key,omitempty"`
	NodeReadinessLabel                     map[string]string            `json:"node_readiness_label,omitempty"`
	NodeReadinessLabelMerge                string                       `json:"node_readiness_label_merge,omitempty"`
	CustomPodAnnotations                   map[string]string            `json:"custom_pod_annotations,omitempty"`
	// TODO: use a proper toleration structure?
	PodToleration                            map[string]string   `json:"toleration,omitempty"`
	PodEnvironmentConfigMap                  spec.NamespacedName `json:"pod_environment_configmap,omitempty"`
	PodEnvironmentSecret                     string              `json:"pod_environment_secret,omitempty"`
	PodPriorityClassName                     string              `json:"pod_priority_class_name,omitempty"`
	MasterPodMoveTimeout                     Duration            `json:"master_pod_move_timeout,omitempty"`
	EnablePodAntiAffinity                    bool                `json:"enable_pod_antiaffinity,omitempty"`
	PodAntiAffinityPreferredDuringScheduling bool                `json:"pod_antiaffinity_preferred_during_scheduling,omitempty"`
	PodAntiAffinityTopologyKey               string              `json:"pod_antiaffinity_topology_key,omitempty"`
	PodManagementPolicy                      string              `json:"pod_management_policy,omitempty"`
	PersistentVolumeClaimRetentionPolicy     map[string]string   `json:"persistent_volume_claim_retention_policy,omitempty"`
	EnableSecretsDeletion                    *bool               `json:"enable_secrets_deletion,omitempty"`
	EnablePersistentVolumeClaimDeletion      *bool               `json:"enable_persistent_volume_claim_deletion,omitempty"`
	EnableReadinessProbe                     bool                `json:"enable_readiness_probe,omitempty"`
	EnableCrossNamespaceSecret               bool                `json:"enable_cross_namespace_secret,omitempty"`
	EnableFinalizers                         *bool               `json:"enable_finalizers,omitempty"`
}

// PostgresPodResourcesDefaults defines the spec of default resources
type PostgresPodResourcesDefaults struct {
	DefaultCPURequest    string `json:"default_cpu_request,omitempty"`
	DefaultMemoryRequest string `json:"default_memory_request,omitempty"`
	DefaultCPULimit      string `json:"default_cpu_limit,omitempty"`
	DefaultMemoryLimit   string `json:"default_memory_limit,omitempty"`
	MinCPULimit          string `json:"min_cpu_limit,omitempty"`
	MinMemoryLimit       string `json:"min_memory_limit,omitempty"`
	MaxCPURequest        string `json:"max_cpu_request,omitempty"`
	MaxMemoryRequest     string `json:"max_memory_request,omitempty"`
}

// OperatorTimeouts defines the timeout of ResourceCheck, PodWait, ReadyWait
type OperatorTimeouts struct {
	ResourceCheckInterval   Duration `json:"resource_check_interval,omitempty"`
	ResourceCheckTimeout    Duration `json:"resource_check_timeout,omitempty"`
	PodLabelWaitTimeout     Duration `json:"pod_label_wait_timeout,omitempty"`
	PodDeletionWaitTimeout  Duration `json:"pod_deletion_wait_timeout,omitempty"`
	ReadyWaitInterval       Duration `json:"ready_wait_interval,omitempty"`
	ReadyWaitTimeout        Duration `json:"ready_wait_timeout,omitempty"`
	PatroniAPICheckInterval Duration `json:"patroni_api_check_interval,omitempty"`
	PatroniAPICheckTimeout  Duration `json:"patroni_api_check_timeout,omitempty"`
}

// LoadBalancerConfiguration defines the LB configuration
type LoadBalancerConfiguration struct {
	DbHostedZone                    string                `json:"db_hosted_zone,omitempty"`
	EnableMasterLoadBalancer        bool                  `json:"enable_master_load_balancer,omitempty"`
	EnableMasterPoolerLoadBalancer  bool                  `json:"enable_master_pooler_load_balancer,omitempty"`
	EnableReplicaLoadBalancer       bool                  `json:"enable_replica_load_balancer,omitempty"`
	EnableReplicaPoolerLoadBalancer bool                  `json:"enable_replica_pooler_load_balancer,omitempty"`
	CustomServiceAnnotations        map[string]string     `json:"custom_service_annotations,omitempty"`
	MasterDNSNameFormat             config.StringTemplate `json:"master_dns_name_format,omitempty"`
	MasterLegacyDNSNameFormat       config.StringTemplate `json:"master_legacy_dns_name_format,omitempty"`
	ReplicaDNSNameFormat            config.StringTemplate `json:"replica_dns_name_format,omitempty"`
	ReplicaLegacyDNSNameFormat      config.StringTemplate `json:"replica_legacy_dns_name_format,omitempty"`
	ExternalTrafficPolicy           string                `json:"external_traffic_policy" default:"Cluster"`
}

// AWSGCPConfiguration defines the configuration for AWS
// TODO complete Google Cloud Platform (GCP) configuration
type AWSGCPConfiguration struct {
	WALES3Bucket                 string `json:"wal_s3_bucket,omitempty"`
	AWSRegion                    string `json:"aws_region,omitempty"`
	WALGSBucket                  string `json:"wal_gs_bucket,omitempty"`
	GCPCredentials               string `json:"gcp_credentials,omitempty"`
	WALAZStorageAccount          string `json:"wal_az_storage_account,omitempty"`
	LogS3Bucket                  string `json:"log_s3_bucket,omitempty"`
	KubeIAMRole                  string `json:"kube_iam_role,omitempty"`
	AdditionalSecretMount        string `json:"additional_secret_mount,omitempty"`
	AdditionalSecretMountPath    string `json:"additional_secret_mount_path,omitempty"`
	EnableEBSGp3Migration        bool   `json:"enable_ebs_gp3_migration" default:"false"`
	EnableEBSGp3MigrationMaxSize int64  `json:"enable_ebs_gp3_migration_max_size" default:"1000"`
}

// OperatorDebugConfiguration defines options for the debug mode
type OperatorDebugConfiguration struct {
	DebugLogging   bool `json:"debug_logging,omitempty"`
	EnableDBAccess bool `json:"enable_database_access,omitempty"`
}

// TeamsAPIConfiguration defines the configuration of TeamsAPI
type TeamsAPIConfiguration struct {
	EnableTeamsAPI                  bool              `json:"enable_teams_api,omitempty"`
	TeamsAPIUrl                     string            `json:"teams_api_url,omitempty"`
	TeamAPIRoleConfiguration        map[string]string `json:"team_api_role_configuration,omitempty"`
	EnableTeamSuperuser             bool              `json:"enable_team_superuser,omitempty"`
	EnableAdminRoleForUsers         bool              `json:"enable_admin_role_for_users,omitempty"`
	TeamAdminRole                   string            `json:"team_admin_role,omitempty"`
	PamRoleName                     string            `json:"pam_role_name,omitempty"`
	PamConfiguration                string            `json:"pam_configuration,omitempty"`
	ProtectedRoles                  []string          `json:"protected_role_names,omitempty"`
	PostgresSuperuserTeams          []string          `json:"postgres_superuser_teams,omitempty"`
	EnablePostgresTeamCRD           bool              `json:"enable_postgres_team_crd,omitempty"`
	EnablePostgresTeamCRDSuperusers bool              `json:"enable_postgres_team_crd_superusers,omitempty"`
	EnableTeamMemberDeprecation     bool              `json:"enable_team_member_deprecation,omitempty"`
	RoleDeletionSuffix              string            `json:"role_deletion_suffix,omitempty"`
}

// LoggingRESTAPIConfiguration defines Logging API conf
type LoggingRESTAPIConfiguration struct {
	APIPort               int `json:"api_port,omitempty"`
	RingLogLines          int `json:"ring_log_lines,omitempty"`
	ClusterHistoryEntries int `json:"cluster_history_entries,omitempty"`
}

// ScalyrConfiguration defines the configuration for ScalyrAPI
type ScalyrConfiguration struct {
	ScalyrAPIKey        string `json:"scalyr_api_key,omitempty"`
	ScalyrImage         string `json:"scalyr_image,omitempty"`
	ScalyrServerURL     string `json:"scalyr_server_url,omitempty"`
	ScalyrCPURequest    string `json:"scalyr_cpu_request,omitempty"`
	ScalyrMemoryRequest string `json:"scalyr_memory_request,omitempty"`
	ScalyrCPULimit      string `json:"scalyr_cpu_limit,omitempty"`
	ScalyrMemoryLimit   string `json:"scalyr_memory_limit,omitempty"`
}

// ConnectionPoolerConfiguration defines default configuration for connection pooler
type ConnectionPoolerConfiguration struct {
	NumberOfInstances    *int32 `json:"connection_pooler_number_of_instances,omitempty"`
	Schema               string `json:"connection_pooler_schema,omitempty"`
	User                 string `json:"connection_pooler_user,omitempty"`
	Image                string `json:"connection_pooler_image,omitempty"`
	Mode                 string `json:"connection_pooler_mode,omitempty"`
	MaxDBConnections     *int32 `json:"connection_pooler_max_db_connections,omitempty"`
	DefaultCPURequest    string `json:"connection_pooler_default_cpu_request,omitempty"`
	DefaultMemoryRequest string `json:"connection_pooler_default_memory_request,omitempty"`
	DefaultCPULimit      string `json:"connection_pooler_default_cpu_limit,omitempty"`
	DefaultMemoryLimit   string `json:"connection_pooler_default_memory_limit,omitempty"`
}

// OperatorLogicalBackupConfiguration defines configuration for logical backup
type OperatorLogicalBackupConfiguration struct {
	Schedule                     string `json:"logical_backup_schedule,omitempty"`
	DockerImage                  string `json:"logical_backup_docker_image,omitempty"`
	BackupProvider               string `json:"logical_backup_provider,omitempty"`
	AzureStorageAccountName      string `json:"logical_backup_azure_storage_account_name,omitempty"`
	AzureStorageContainer        string `json:"logical_backup_azure_storage_container,omitempty"`
	AzureStorageAccountKey       string `json:"logical_backup_azure_storage_account_key,omitempty"`
	S3Bucket                     string `json:"logical_backup_s3_bucket,omitempty"`
	S3BucketPrefix               string `json:"logical_backup_s3_bucket_prefix,omitempty"`
	S3Region                     string `json:"logical_backup_s3_region,omitempty"`
	S3Endpoint                   string `json:"logical_backup_s3_endpoint,omitempty"`
	S3AccessKeyID                string `json:"logical_backup_s3_access_key_id,omitempty"`
	S3SecretAccessKey            string `json:"logical_backup_s3_secret_access_key,omitempty"`
	S3SSE                        string `json:"logical_backup_s3_sse,omitempty"`
	RetentionTime                string `json:"logical_backup_s3_retention_time,omitempty"`
	GoogleApplicationCredentials string `json:"logical_backup_google_application_credentials,omitempty"`
	JobPrefix                    string `json:"logical_backup_job_prefix,omitempty"`
	CronjobEnvironmentSecret     string `json:"logical_backup_cronjob_environment_secret,omitempty"`
	CPURequest                   string `json:"logical_backup_cpu_request,omitempty"`
	MemoryRequest                string `json:"logical_backup_memory_request,omitempty"`
	CPULimit                     string `json:"logical_backup_cpu_limit,omitempty"`
	MemoryLimit                  string `json:"logical_backup_memory_limit,omitempty"`
}

// PatroniConfiguration defines configuration for Patroni
type PatroniConfiguration struct {
	FailsafeMode *bool `json:"enable_patroni_failsafe_mode,omitempty"`
}

// OperatorConfigurationData defines the operation config
type OperatorConfigurationData struct {
	EnableCRDRegistration         *bool                              `json:"enable_crd_registration,omitempty"`
	EnableCRDValidation           *bool                              `json:"enable_crd_validation,omitempty"`
	CRDCategories                 []string                           `json:"crd_categories,omitempty"`
	EnableLazySpiloUpgrade        bool                               `json:"enable_lazy_spilo_upgrade,omitempty"`
	EnablePgVersionEnvVar         bool                               `json:"enable_pgversion_env_var,omitempty"`
	EnableSpiloWalPathCompat      bool                               `json:"enable_spilo_wal_path_compat,omitempty"`
	EnableTeamIdClusternamePrefix bool                               `json:"enable_team_id_clustername_prefix,omitempty"`
	EtcdHost                      string                             `json:"etcd_host,omitempty"`
	KubernetesUseConfigMaps       bool                               `json:"kubernetes_use_configmaps,omitempty"`
	DockerImage                   string                             `json:"docker_image,omitempty"`
	Workers                       uint32                             `json:"workers,omitempty"`
	ResyncPeriod                  Duration                           `json:"resync_period,omitempty"`
	RepairPeriod                  Duration                           `json:"repair_period,omitempty"`
	SetMemoryRequestToLimit       bool                               `json:"set_memory_request_to_limit,omitempty"`
	ShmVolume                     *bool                              `json:"enable_shm_volume,omitempty"`
	SidecarImages                 map[string]string                  `json:"sidecar_docker_images,omitempty"` // deprecated in favour of SidecarContainers
	SidecarContainers             []v1.Container                     `json:"sidecars,omitempty"`
	PostgresUsersConfiguration    PostgresUsersConfiguration         `json:"users"`
	MajorVersionUpgrade           MajorVersionUpgradeConfiguration   `json:"major_version_upgrade"`
	Kubernetes                    KubernetesMetaConfiguration        `json:"kubernetes"`
	PostgresPodResources          PostgresPodResourcesDefaults       `json:"postgres_pod_resources"`
	Timeouts                      OperatorTimeouts                   `json:"timeouts"`
	LoadBalancer                  LoadBalancerConfiguration          `json:"load_balancer"`
	AWSGCP                        AWSGCPConfiguration                `json:"aws_or_gcp"`
	OperatorDebug                 OperatorDebugConfiguration         `json:"debug"`
	TeamsAPI                      TeamsAPIConfiguration              `json:"teams_api"`
	LoggingRESTAPI                LoggingRESTAPIConfiguration        `json:"logging_rest_api"`
	Scalyr                        ScalyrConfiguration                `json:"scalyr"`
	LogicalBackup                 OperatorLogicalBackupConfiguration `json:"logical_backup"`
	ConnectionPooler              ConnectionPoolerConfiguration      `json:"connection_pooler"`
	Patroni                       PatroniConfiguration               `json:"patroni"`

	MinInstances                      int32  `json:"min_instances,omitempty"`
	MaxInstances                      int32  `json:"max_instances,omitempty"`
	IgnoreInstanceLimitsAnnotationKey string `json:"ignore_instance_limits_annotation_key,omitempty"`
}

// Duration shortens this frequently used name
type Duration time.Duration


================================================
File: pkg/apis/acid.zalan.do/v1/postgres_team_type.go
================================================
package v1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// +genclient
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// PostgresTeam defines Custom Resource Definition Object for team management.
type PostgresTeam struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec PostgresTeamSpec `json:"spec"`
}

// PostgresTeamSpec defines the specification for the PostgresTeam TPR.
type PostgresTeamSpec struct {
	AdditionalSuperuserTeams map[string][]string `json:"additionalSuperuserTeams,omitempty"`
	AdditionalTeams          map[string][]string `json:"additionalTeams,omitempty"`
	AdditionalMembers        map[string][]string `json:"additionalMembers,omitempty"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// PostgresTeamList defines a list of PostgresTeam definitions.
type PostgresTeamList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata"`

	Items []PostgresTeam `json:"items"`
}


================================================
File: pkg/apis/acid.zalan.do/v1/postgresql_type.go
================================================
package v1

// Postgres CRD definition, please use CamelCase for field names.

import (
	"time"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// +genclient
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// Postgresql defines PostgreSQL Custom Resource Definition Object.
type Postgresql struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   PostgresSpec   `json:"spec"`
	Status PostgresStatus `json:"status"`
	Error  string         `json:"-"`
}

// PostgresSpec defines the specification for the PostgreSQL TPR.
type PostgresSpec struct {
	PostgresqlParam `json:"postgresql"`
	Volume          `json:"volume,omitempty"`
	Patroni         `json:"patroni,omitempty"`
	*Resources      `json:"resources,omitempty"`

	EnableConnectionPooler        *bool             `json:"enableConnectionPooler,omitempty"`
	EnableReplicaConnectionPooler *bool             `json:"enableReplicaConnectionPooler,omitempty"`
	ConnectionPooler              *ConnectionPooler `json:"connectionPooler,omitempty"`

	TeamID      string `json:"teamId"`
	DockerImage string `json:"dockerImage,omitempty"`

	// deprecated field storing cluster name without teamId prefix
	ClusterName string `json:"-"`

	SpiloRunAsUser  *int64 `json:"spiloRunAsUser,omitempty"`
	SpiloRunAsGroup *int64 `json:"spiloRunAsGroup,omitempty"`
	SpiloFSGroup    *int64 `json:"spiloFSGroup,omitempty"`

	// vars that enable load balancers are pointers because it is important to know if any of them is omitted from the Postgres manifest
	// in that case the var evaluates to nil and the value is taken from the operator config
	EnableMasterLoadBalancer        *bool `json:"enableMasterLoadBalancer,omitempty"`
	EnableMasterPoolerLoadBalancer  *bool `json:"enableMasterPoolerLoadBalancer,omitempty"`
	EnableReplicaLoadBalancer       *bool `json:"enableReplicaLoadBalancer,omitempty"`
	EnableReplicaPoolerLoadBalancer *bool `json:"enableReplicaPoolerLoadBalancer,omitempty"`

	// deprecated load balancer settings maintained for backward compatibility
	// see "Load balancers" operator docs
	UseLoadBalancer     *bool `json:"useLoadBalancer,omitempty"`
	ReplicaLoadBalancer *bool `json:"replicaLoadBalancer,omitempty"`

	// load balancers' source ranges are the same for master and replica services
	AllowedSourceRanges []string `json:"allowedSourceRanges"`

	Users                          map[string]UserFlags `json:"users,omitempty"`
	UsersIgnoringSecretRotation    []string             `json:"usersIgnoringSecretRotation,omitempty"`
	UsersWithSecretRotation        []string             `json:"usersWithSecretRotation,omitempty"`
	UsersWithInPlaceSecretRotation []string             `json:"usersWithInPlaceSecretRotation,omitempty"`

	NumberOfInstances      int32                       `json:"numberOfInstances"`
	MaintenanceWindows     []MaintenanceWindow         `json:"maintenanceWindows,omitempty"`
	Clone                  *CloneDescription           `json:"clone,omitempty"`
	Databases              map[string]string           `json:"databases,omitempty"`
	PreparedDatabases      map[string]PreparedDatabase `json:"preparedDatabases,omitempty"`
	SchedulerName          *string                     `json:"schedulerName,omitempty"`
	NodeAffinity           *v1.NodeAffinity            `json:"nodeAffinity,omitempty"`
	Tolerations            []v1.Toleration             `json:"tolerations,omitempty"`
	Sidecars               []Sidecar                   `json:"sidecars,omitempty"`
	InitContainers         []v1.Container              `json:"initContainers,omitempty"`
	PodPriorityClassName   string                      `json:"podPriorityClassName,omitempty"`
	ShmVolume              *bool                       `json:"enableShmVolume,omitempty"`
	EnableLogicalBackup    bool                        `json:"enableLogicalBackup,omitempty"`
	LogicalBackupRetention string                      `json:"logicalBackupRetention,omitempty"`
	LogicalBackupSchedule  string                      `json:"logicalBackupSchedule,omitempty"`
	StandbyCluster         *StandbyDescription         `json:"standby,omitempty"`
	PodAnnotations         map[string]string           `json:"podAnnotations,omitempty"`
	ServiceAnnotations     map[string]string           `json:"serviceAnnotations,omitempty"`
	// MasterServiceAnnotations takes precedence over ServiceAnnotations for master role if not empty
	MasterServiceAnnotations map[string]string `json:"masterServiceAnnotations,omitempty"`
	// ReplicaServiceAnnotations takes precedence over ServiceAnnotations for replica role if not empty
	ReplicaServiceAnnotations map[string]string  `json:"replicaServiceAnnotations,omitempty"`
	TLS                       *TLSDescription    `json:"tls,omitempty"`
	AdditionalVolumes         []AdditionalVolume `json:"additionalVolumes,omitempty"`
	Streams                   []Stream           `json:"streams,omitempty"`
	Env                       []v1.EnvVar        `json:"env,omitempty"`

	// deprecated json tags
	InitContainersOld       []v1.Container `json:"init_containers,omitempty"`
	PodPriorityClassNameOld string         `json:"pod_priority_class_name,omitempty"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// PostgresqlList defines a list of PostgreSQL clusters.
type PostgresqlList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata"`

	Items []Postgresql `json:"items"`
}

// PreparedDatabase describes elements to be bootstrapped
type PreparedDatabase struct {
	PreparedSchemas map[string]PreparedSchema `json:"schemas,omitempty"`
	DefaultUsers    bool                      `json:"defaultUsers,omitempty" defaults:"false"`
	Extensions      map[string]string         `json:"extensions,omitempty"`
	SecretNamespace string                    `json:"secretNamespace,omitempty"`
}

// PreparedSchema describes elements to be bootstrapped per schema
type PreparedSchema struct {
	DefaultRoles *bool `json:"defaultRoles,omitempty" defaults:"true"`
	DefaultUsers bool  `json:"defaultUsers,omitempty" defaults:"false"`
}

// MaintenanceWindow describes the time window when the operator is allowed to do maintenance on a cluster.
type MaintenanceWindow struct {
	Everyday  bool         `json:"everyday,omitempty"`
	Weekday   time.Weekday `json:"weekday,omitempty"`
	StartTime metav1.Time  `json:"startTime,omitempty"`
	EndTime   metav1.Time  `json:"endTime,omitempty"`
}

// Volume describes a single volume in the manifest.
type Volume struct {
	Selector      *metav1.LabelSelector `json:"selector,omitempty"`
	Size          string                `json:"size"`
	StorageClass  string                `json:"storageClass,omitempty"`
	SubPath       string                `json:"subPath,omitempty"`
	IsSubPathExpr *bool                 `json:"isSubPathExpr,omitempty"`
	Iops          *int64                `json:"iops,omitempty"`
	Throughput    *int64                `json:"throughput,omitempty"`
	VolumeType    string                `json:"type,omitempty"`
}

// AdditionalVolume specs additional optional volumes for statefulset
type AdditionalVolume struct {
	Name             string          `json:"name"`
	MountPath        string          `json:"mountPath"`
	SubPath          string          `json:"subPath,omitempty"`
	IsSubPathExpr    *bool           `json:"isSubPathExpr,omitempty"`
	TargetContainers []string        `json:"targetContainers"`
	VolumeSource     v1.VolumeSource `json:"volumeSource"`
}

// PostgresqlParam describes PostgreSQL version and pairs of configuration parameter name - values.
type PostgresqlParam struct {
	PgVersion  string            `json:"version"`
	Parameters map[string]string `json:"parameters,omitempty"`
}

// ResourceDescription describes CPU and memory resources defined for a cluster.
type ResourceDescription struct {
	CPU          *string `json:"cpu,omitempty"`
	Memory       *string `json:"memory,omitempty"`
	HugePages2Mi *string `json:"hugepages-2Mi,omitempty"`
	HugePages1Gi *string `json:"hugepages-1Gi,omitempty"`
}

// Resources describes requests and limits for the cluster resouces.
type Resources struct {
	ResourceRequests ResourceDescription `json:"requests,omitempty"`
	ResourceLimits   ResourceDescription `json:"limits,omitempty"`
}

// Patroni contains Patroni-specific configuration
type Patroni struct {
	InitDB                map[string]string            `json:"initdb,omitempty"`
	PgHba                 []string                     `json:"pg_hba,omitempty"`
	TTL                   uint32                       `json:"ttl,omitempty"`
	LoopWait              uint32                       `json:"loop_wait,omitempty"`
	RetryTimeout          uint32                       `json:"retry_timeout,omitempty"`
	MaximumLagOnFailover  float32                      `json:"maximum_lag_on_failover,omitempty"` // float32 because https://github.com/kubernetes/kubernetes/issues/30213
	Slots                 map[string]map[string]string `json:"slots,omitempty"`
	SynchronousMode       bool                         `json:"synchronous_mode,omitempty"`
	SynchronousModeStrict bool                         `json:"synchronous_mode_strict,omitempty"`
	SynchronousNodeCount  uint32                       `json:"synchronous_node_count,omitempty" defaults:"1"`
	FailsafeMode          *bool                        `json:"failsafe_mode,omitempty"`
}

// StandbyDescription contains remote primary config or s3/gs wal path
type StandbyDescription struct {
	S3WalPath   string `json:"s3_wal_path,omitempty"`
	GSWalPath   string `json:"gs_wal_path,omitempty"`
	StandbyHost string `json:"standby_host,omitempty"`
	StandbyPort string `json:"standby_port,omitempty"`
}

// TLSDescription specs TLS properties
type TLSDescription struct {
	SecretName      string `json:"secretName,omitempty"`
	CertificateFile string `json:"certificateFile,omitempty"`
	PrivateKeyFile  string `json:"privateKeyFile,omitempty"`
	CAFile          string `json:"caFile,omitempty"`
	CASecretName    string `json:"caSecretName,omitempty"`
}

// CloneDescription describes which cluster the new should clone and up to which point in time
type CloneDescription struct {
	ClusterName       string `json:"cluster,omitempty"`
	UID               string `json:"uid,omitempty"`
	EndTimestamp      string `json:"timestamp,omitempty"`
	S3WalPath         string `json:"s3_wal_path,omitempty"`
	S3Endpoint        string `json:"s3_endpoint,omitempty"`
	S3AccessKeyId     string `json:"s3_access_key_id,omitempty"`
	S3SecretAccessKey string `json:"s3_secret_access_key,omitempty"`
	S3ForcePathStyle  *bool  `json:"s3_force_path_style,omitempty" defaults:"false"`
}

// Sidecar defines a container to be run in the same pod as the Postgres container.
type Sidecar struct {
	*Resources  `json:"resources,omitempty"`
	Name        string             `json:"name,omitempty"`
	DockerImage string             `json:"image,omitempty"`
	Ports       []v1.ContainerPort `json:"ports,omitempty"`
	Env         []v1.EnvVar        `json:"env,omitempty"`
	Command     []string           `json:"command,omitempty"`
}

// UserFlags defines flags (such as superuser, nologin) that could be assigned to individual users
type UserFlags []string

// PostgresStatus contains status of the PostgreSQL cluster (running, creation failed etc.)
type PostgresStatus struct {
	PostgresClusterStatus string `json:"PostgresClusterStatus"`
}

// ConnectionPooler Options for connection pooler
//
// TODO: prepared snippets of configuration, one can choose via type, e.g.
// pgbouncer-large (with higher resources) or odyssey-small (with smaller
// resources)
// Type              string `json:"type,omitempty"`
//
// TODO: figure out what other important parameters of the connection pooler it
// makes sense to expose. E.g. pool size (min/max boundaries), max client
// connections etc.
type ConnectionPooler struct {
	NumberOfInstances *int32 `json:"numberOfInstances,omitempty"`
	Schema            string `json:"schema,omitempty"`
	User              string `json:"user,omitempty"`
	Mode              string `json:"mode,omitempty"`
	DockerImage       string `json:"dockerImage,omitempty"`
	MaxDBConnections  *int32 `json:"maxDBConnections,omitempty"`

	*Resources `json:"resources,omitempty"`
}

// Stream defines properties for creating FabricEventStream resources
type Stream struct {
	ApplicationId  string                 `json:"applicationId"`
	Database       string                 `json:"database"`
	Tables         map[string]StreamTable `json:"tables"`
	Filter         map[string]*string     `json:"filter,omitempty"`
	BatchSize      *uint32                `json:"batchSize,omitempty"`
	CPU            *string                `json:"cpu,omitempty"`
	Memory         *string                `json:"memory,omitempty"`
	EnableRecovery *bool                  `json:"enableRecovery,omitempty"`
}

// StreamTable defines properties of outbox tables for FabricEventStreams
type StreamTable struct {
	EventType         string  `json:"eventType"`
	RecoveryEventType string  `json:"recoveryEventType,omitempty"`
	IgnoreRecovery    *bool   `json:"ignoreRecovery,omitempty"`
	IdColumn          *string `json:"idColumn,omitempty"`
	PayloadColumn     *string `json:"payloadColumn,omitempty"`
}


================================================
File: pkg/apis/acid.zalan.do/v1/register.go
================================================
package v1

import (
	acidzalando "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
)

// APIVersion of the `postgresql` and `operator` CRDs
const (
	APIVersion = "v1"
)

var (
	// localSchemeBuilder and AddToScheme will stay in k8s.io/kubernetes.

	// SchemeBuilder : An instance of runtime.SchemeBuilder, global for this package
	SchemeBuilder      runtime.SchemeBuilder
	localSchemeBuilder = &SchemeBuilder
	//AddToScheme is localSchemeBuilder.AddToScheme
	AddToScheme = localSchemeBuilder.AddToScheme
	//SchemeGroupVersion has GroupName and APIVersion
	SchemeGroupVersion = schema.GroupVersion{Group: acidzalando.GroupName, Version: APIVersion}
)

func init() {
	// We only register manually written functions here. The registration of the
	// generated functions takes place in the generated files. The separation
	// makes the code compile even when the generated files are missing.
	localSchemeBuilder.Register(addKnownTypes)
}

// Resource takes an unqualified resource and returns a Group qualified GroupResource
func Resource(resource string) schema.GroupResource {
	return SchemeGroupVersion.WithResource(resource).GroupResource()
}

// Adds the list of known types to api.Scheme.
func addKnownTypes(scheme *runtime.Scheme) error {
	// AddKnownType assumes derives the type kind from the type name, which is always uppercase.
	// For our CRDs we use lowercase names historically, therefore we have to supply the name separately.
	// TODO: User uppercase CRDResourceKind of our types in the next major API version
	scheme.AddKnownTypeWithName(SchemeGroupVersion.WithKind("postgresql"), &Postgresql{})
	scheme.AddKnownTypeWithName(SchemeGroupVersion.WithKind("postgresqlList"), &PostgresqlList{})
	scheme.AddKnownTypeWithName(SchemeGroupVersion.WithKind("PostgresTeam"), &PostgresTeam{})
	scheme.AddKnownTypeWithName(SchemeGroupVersion.WithKind("PostgresTeamList"), &PostgresTeamList{})
	scheme.AddKnownTypeWithName(SchemeGroupVersion.WithKind("OperatorConfiguration"),
		&OperatorConfiguration{})
	scheme.AddKnownTypeWithName(SchemeGroupVersion.WithKind("OperatorConfigurationList"),
		&OperatorConfigurationList{})
	metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
	return nil
}


================================================
File: pkg/apis/acid.zalan.do/v1/util.go
================================================
package v1

import (
	"fmt"
	"regexp"
	"strings"
	"time"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

var (
	weekdays         = map[string]int{"Sun": 0, "Mon": 1, "Tue": 2, "Wed": 3, "Thu": 4, "Fri": 5, "Sat": 6}
	serviceNameRegex = regexp.MustCompile(serviceNameRegexString)
)

// Clone convenience wrapper around DeepCopy
func (p *Postgresql) Clone() *Postgresql {
	if p == nil {
		return nil
	}
	return p.DeepCopy()
}

func parseTime(s string) (metav1.Time, error) {
	parts := strings.Split(s, ":")
	if len(parts) != 2 {
		return metav1.Time{}, fmt.Errorf("incorrect time format")
	}
	timeLayout := "15:04"

	tp, err := time.Parse(timeLayout, s)
	if err != nil {
		return metav1.Time{}, err
	}

	return metav1.Time{Time: tp.UTC()}, nil
}

func parseWeekday(s string) (time.Weekday, error) {
	weekday, ok := weekdays[s]
	if !ok {
		return time.Weekday(0), fmt.Errorf("incorrect weekday")
	}

	return time.Weekday(weekday), nil
}

func ExtractClusterName(clusterName string, teamName string) (string, error) {
	teamNameLen := len(teamName)
	if len(clusterName) < teamNameLen+2 {
		return "", fmt.Errorf("cluster name must match {TEAM}-{NAME} format. Got cluster name '%v', team name '%v'", clusterName, teamName)
	}

	if teamNameLen == 0 {
		return "", fmt.Errorf("team name is empty")
	}

	if strings.ToLower(clusterName[:teamNameLen+1]) != strings.ToLower(teamName)+"-" {
		return "", fmt.Errorf("name must match {TEAM}-{NAME} format")
	}
	if len(clusterName) > clusterNameMaxLength {
		return "", fmt.Errorf("name cannot be longer than %d characters", clusterNameMaxLength)
	}
	if !serviceNameRegex.MatchString(clusterName) {
		return "", fmt.Errorf("name must confirm to DNS-1035, regex used for validation is %q",
			serviceNameRegexString)
	}

	return clusterName[teamNameLen+1:], nil
}

func validateCloneClusterDescription(clone *CloneDescription) error {
	// when cloning from the basebackup (no end timestamp) check that the cluster name is a valid service name
	if clone != nil && clone.ClusterName != "" && clone.EndTimestamp == "" {
		if !serviceNameRegex.MatchString(clone.ClusterName) {
			return fmt.Errorf("clone cluster name must confirm to DNS-1035, regex used for validation is %q",
				serviceNameRegexString)
		}
		if len(clone.ClusterName) > serviceNameMaxLength {
			return fmt.Errorf("clone cluster name must be no longer than %d characters", serviceNameMaxLength)
		}
	}
	return nil
}

// Success of the current Status
func (postgresStatus PostgresStatus) Success() bool {
	return postgresStatus.PostgresClusterStatus != ClusterStatusAddFailed &&
		postgresStatus.PostgresClusterStatus != ClusterStatusUpdateFailed &&
		postgresStatus.PostgresClusterStatus != ClusterStatusSyncFailed
}

// Running status of cluster
func (postgresStatus PostgresStatus) Running() bool {
	return postgresStatus.PostgresClusterStatus == ClusterStatusRunning
}

// Creating status of cluster
func (postgresStatus PostgresStatus) Creating() bool {
	return postgresStatus.PostgresClusterStatus == ClusterStatusCreating
}

func (postgresStatus PostgresStatus) String() string {
	return postgresStatus.PostgresClusterStatus
}


================================================
File: pkg/apis/acid.zalan.do/v1/util_test.go
================================================
package v1

import (
	"bytes"
	"encoding/json"
	"errors"
	"reflect"
	"testing"
	"time"

	"github.com/zalando/postgres-operator/pkg/util"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

var parseTimeTests = []struct {
	about string
	in    string
	out   metav1.Time
	err   error
}{
	{"parse common time with minutes", "16:08", mustParseTime("16:08"), nil},
	{"parse time with zeroed minutes", "11:00", mustParseTime("11:00"), nil},
	{"parse corner case last minute of the day", "23:59", mustParseTime("23:59"), nil},

	{"expect error as hour is out of range", "26:09", metav1.Now(), errors.New(`parsing time "26:09": hour out of range`)},
	{"expect error as minute is out of range", "23:69", metav1.Now(), errors.New(`parsing time "23:69": minute out of range`)},
}

func stringToPointer(str string) *string {
	return &str
}

var parseWeekdayTests = []struct {
	about string
	in    string
	out   time.Weekday
	err   error
}{
	{"parse common weekday", "Wed", time.Wednesday, nil},
	{"expect error as weekday is invalid", "Sunday", time.Weekday(0), errors.New("incorrect weekday")},
	{"expect error as weekday is empty", "", time.Weekday(0), errors.New("incorrect weekday")},
}

var clusterNames = []struct {
	about       string
	in          string
	inTeam      string
	clusterName string
	err         error
}{
	{"common team and cluster name", "acid-test", "acid", "test", nil},
	{"cluster name with hyphen", "test-my-name", "test", "my-name", nil},
	{"cluster and team name with hyphen", "my-team-another-test", "my-team", "another-test", nil},
	{"expect error as cluster name is just hyphens", "------strange-team-cluster", "-----", "strange-team-cluster",
		errors.New(`name must confirm to DNS-1035, regex used for validation is "^[a-z]([-a-z0-9]*[a-z0-9])?$"`)},
	{"expect error as cluster name is too long", "fooobar-fooobarfooobarfooobarfooobarfooobarfooobarfooobarfooobar", "fooobar", "",
		errors.New("name cannot be longer than 58 characters")},
	{"expect error as cluster name does not match {TEAM}-{NAME} format", "acid-test", "test", "", errors.New("name must match {TEAM}-{NAME} format")},
	{"expect error as team and cluster name are empty", "-test", "", "", errors.New("team name is empty")},
	{"expect error as cluster name is empty and team name is a hyphen", "-test", "-", "", errors.New("name must match {TEAM}-{NAME} format")},
	{"expect error as cluster name is empty, team name is a hyphen and cluster name is empty", "", "-", "", errors.New("cluster name must match {TEAM}-{NAME} format. Got cluster name '', team name '-'")},
	{"expect error as cluster and team name are hyphens", "-", "-", "", errors.New("cluster name must match {TEAM}-{NAME} format. Got cluster name '-', team name '-'")},
	// user may specify the team part of the full cluster name differently from the team name returned by the Teams API
	// in the case the actual Teams API name is long enough, this will fail the check
	{"expect error as team name does not match", "foo-bar", "qwerty", "", errors.New("cluster name must match {TEAM}-{NAME} format. Got cluster name 'foo-bar', team name 'qwerty'")},
}

var cloneClusterDescriptions = []struct {
	about string
	in    *CloneDescription
	err   error
}{
	{"cluster name invalid but EndTimeSet is not empty", &CloneDescription{"foo+bar", "", "NotEmpty", "", "", "", "", nil}, nil},
	{"expect error as cluster name does not match DNS-1035", &CloneDescription{"foo+bar", "", "", "", "", "", "", nil},
		errors.New(`clone cluster name must confirm to DNS-1035, regex used for validation is "^[a-z]([-a-z0-9]*[a-z0-9])?$"`)},
	{"expect error as cluster name is too long", &CloneDescription{"foobar123456789012345678901234567890123456789012345678901234567890", "", "", "", "", "", "", nil},
		errors.New("clone cluster name must be no longer than 63 characters")},
	{"common cluster name", &CloneDescription{"foobar", "", "", "", "", "", "", nil}, nil},
}

var maintenanceWindows = []struct {
	about string
	in    []byte
	out   MaintenanceWindow
	err   error
}{{"regular scenario",
	[]byte(`"Tue:10:00-20:00"`),
	MaintenanceWindow{
		Everyday:  false,
		Weekday:   time.Tuesday,
		StartTime: mustParseTime("10:00"),
		EndTime:   mustParseTime("20:00"),
	}, nil},
	{"starts and ends at the same time",
		[]byte(`"Mon:10:00-10:00"`),
		MaintenanceWindow{
			Everyday:  false,
			Weekday:   time.Monday,
			StartTime: mustParseTime("10:00"),
			EndTime:   mustParseTime("10:00"),
		}, nil},
	{"starts and ends 00:00 on sunday",
		[]byte(`"Sun:00:00-00:00"`),
		MaintenanceWindow{
			Everyday:  false,
			Weekday:   time.Sunday,
			StartTime: mustParseTime("00:00"),
			EndTime:   mustParseTime("00:00"),
		}, nil},
	{"without day indication should define to sunday",
		[]byte(`"01:00-10:00"`),
		MaintenanceWindow{
			Everyday:  true,
			Weekday:   time.Sunday,
			StartTime: mustParseTime("01:00"),
			EndTime:   mustParseTime("10:00"),
		}, nil},
	{"expect error as 'From' is later than 'To'", []byte(`"Mon:12:00-11:00"`), MaintenanceWindow{}, errors.New(`'From' time must be prior to the 'To' time`)},
	{"expect error as 'From' is later than 'To' with 00:00 corner case", []byte(`"Mon:10:00-00:00"`), MaintenanceWindow{}, errors.New(`'From' time must be prior to the 'To' time`)},
	{"expect error as 'From' time is not valid", []byte(`"Wed:33:00-00:00"`), MaintenanceWindow{}, errors.New(`could not parse start time: parsing time "33:00": hour out of range`)},
	{"expect error as 'To' time is not valid", []byte(`"Wed:00:00-26:00"`), MaintenanceWindow{}, errors.New(`could not parse end time: parsing time "26:00": hour out of range`)},
	{"expect error as weekday is not valid", []byte(`"Sunday:00:00-00:00"`), MaintenanceWindow{}, errors.New(`could not parse weekday: incorrect weekday`)},
	{"expect error as weekday is empty", []byte(`":00:00-10:00"`), MaintenanceWindow{}, errors.New(`could not parse weekday: incorrect weekday`)},
	{"expect error as maintenance window set seconds", []byte(`"Mon:00:00:00-10:00:00"`), MaintenanceWindow{}, errors.New(`incorrect maintenance window format`)},
	{"expect error as 'To' time set seconds", []byte(`"Mon:00:00-00:00:00"`), MaintenanceWindow{}, errors.New("could not parse end time: incorrect time format")},
	// ideally, should be implemented
	{"expect error as 'To' has a weekday", []byte(`"Mon:00:00-Fri:00:00"`), MaintenanceWindow{}, errors.New("could not parse end time: incorrect time format")},
	{"expect error as 'To' time is missing", []byte(`"Mon:00:00"`), MaintenanceWindow{}, errors.New("incorrect maintenance window format")}}

var postgresStatus = []struct {
	about string
	in    []byte
	out   PostgresStatus
	err   error
}{
	{"cluster running", []byte(`{"PostgresClusterStatus":"Running"}`),
		PostgresStatus{PostgresClusterStatus: ClusterStatusRunning}, nil},
	{"cluster status undefined", []byte(`{"PostgresClusterStatus":""}`),
		PostgresStatus{PostgresClusterStatus: ClusterStatusUnknown}, nil},
	{"cluster running without full JSON format", []byte(`"Running"`),
		PostgresStatus{PostgresClusterStatus: ClusterStatusRunning}, nil},
	{"cluster status empty", []byte(`""`),
		PostgresStatus{PostgresClusterStatus: ClusterStatusUnknown}, nil}}

var tmp postgresqlCopy
var unmarshalCluster = []struct {
	about   string
	in      []byte
	out     Postgresql
	marshal []byte
	err     error
}{
	{
		about: "example with simple status field",
		in: []byte(`{
	  "kind": "Postgresql","apiVersion": "acid.zalan.do/v1",
	  "metadata": {"name": "acid-testcluster1"}, "spec": {"teamId": 100}}`),
		out: Postgresql{
			TypeMeta: metav1.TypeMeta{
				Kind:       "Postgresql",
				APIVersion: "acid.zalan.do/v1",
			},
			ObjectMeta: metav1.ObjectMeta{
				Name: "acid-testcluster1",
			},
			Status: PostgresStatus{PostgresClusterStatus: ClusterStatusInvalid},
			// This error message can vary between Go versions, so compute it for the current version.
			Error: json.Unmarshal([]byte(`{
				"kind": "Postgresql","apiVersion": "acid.zalan.do/v1",
				"metadata": {"name": "acid-testcluster1"}, "spec": {"teamId": 100}}`), &tmp).Error(),
		},
		marshal: []byte(`{"kind":"Postgresql","apiVersion":"acid.zalan.do/v1","metadata":{"name":"acid-testcluster1","creationTimestamp":null},"spec":{"postgresql":{"version":"","parameters":null},"volume":{"size":"","storageClass":""},"patroni":{"initdb":null,"pg_hba":null,"ttl":0,"loop_wait":0,"retry_timeout":0,"maximum_lag_on_failover":0,"slots":null},"teamId":"","allowedSourceRanges":null,"numberOfInstances":0,"users":null,"clone":null},"status":"Invalid"}`),
		err:     nil},
	{
		about: "example with /status subresource",
		in: []byte(`{
	  "kind": "Postgresql","apiVersion": "acid.zalan.do/v1",
	  "metadata": {"name": "acid-testcluster1"}, "spec": {"teamId": 100}}`),
		out: Postgresql{
			TypeMeta: metav1.TypeMeta{
				Kind:       "Postgresql",
				APIVersion: "acid.zalan.do/v1",
			},
			ObjectMeta: metav1.ObjectMeta{
				Name: "acid-testcluster1",
			},
			Status: PostgresStatus{PostgresClusterStatus: ClusterStatusInvalid},
			// This error message can vary between Go versions, so compute it for the current version.
			Error: json.Unmarshal([]byte(`{
				"kind": "Postgresql","apiVersion": "acid.zalan.do/v1",
				"metadata": {"name": "acid-testcluster1"}, "spec": {"teamId": 100}}`), &tmp).Error(),
		},
		marshal: []byte(`{"kind":"Postgresql","apiVersion":"acid.zalan.do/v1","metadata":{"name":"acid-testcluster1","creationTimestamp":null},"spec":{"postgresql":{"version":"","parameters":null},"volume":{"size":"","storageClass":""},"patroni":{"initdb":null,"pg_hba":null,"ttl":0,"loop_wait":0,"retry_timeout":0,"maximum_lag_on_failover":0,"slots":null},"teamId":"","allowedSourceRanges":null,"numberOfInstances":0,"users":null,"clone":null},"status":{"PostgresClusterStatus":"Invalid"}}`),
		err:     nil},
	{
		about: "example with detailed input manifest and deprecated pod_priority_class_name -> podPriorityClassName",
		in: []byte(`{
	  "kind": "Postgresql",
	  "apiVersion": "acid.zalan.do/v1",
	  "metadata": {
	    "name": "acid-testcluster1"
	  },
	  "spec": {
	    "teamId": "acid",
		"pod_priority_class_name": "spilo-pod-priority",
	    "volume": {
	      "size": "5Gi",
	      "storageClass": "SSD",
	      "subPath": "subdir"
	    },
	    "numberOfInstances": 2,
	    "users": {
	      "zalando": [
	        "superuser",
	        "createdb"
	      ]
	    },
	    "allowedSourceRanges": [
	      "127.0.0.1/32"
	    ],
	    "postgresql": {
	      "version": "17",
	      "parameters": {
	        "shared_buffers": "32MB",
	        "max_connections": "10",
	        "log_statement": "all"
	      }
	    },
	    "resources": {
	      "requests": {
	        "cpu": "10m",
	        "memory": "50Mi"
	      },
	      "limits": {
	        "cpu": "300m",
	        "memory": "3000Mi"
	      }
	    },
	    "clone" : {
	     "cluster": "acid-batman"
	     },
		"enableShmVolume": false,
	    "patroni": {
	      "initdb": {
	        "encoding": "UTF8",
	        "locale": "en_US.UTF-8",
	        "data-checksums": "true"
	      },
	      "pg_hba": [
	        "hostssl all all 0.0.0.0/0 md5",
	        "host    all all 0.0.0.0/0 md5"
	      ],
	      "ttl": 30,
	      "loop_wait": 10,
	      "retry_timeout": 10,
		    "maximum_lag_on_failover": 33554432,
			  "slots" : {
				  "permanent_logical_1" : {
					  "type"     : "logical",
					  "database" : "foo",
					  "plugin"   : "pgoutput"
			       }
			  }
	  	},
	  	"maintenanceWindows": [
	    	"Mon:01:00-06:00",
	    	"Sat:00:00-04:00",
	    	"05:00-05:15"
	  	]
	  }
		}`),
		out: Postgresql{
			TypeMeta: metav1.TypeMeta{
				Kind:       "Postgresql",
				APIVersion: "acid.zalan.do/v1",
			},
			ObjectMeta: metav1.ObjectMeta{
				Name: "acid-testcluster1",
			},
			Spec: PostgresSpec{
				PostgresqlParam: PostgresqlParam{
					PgVersion: "17",
					Parameters: map[string]string{
						"shared_buffers":  "32MB",
						"max_connections": "10",
						"log_statement":   "all",
					},
				},
				PodPriorityClassNameOld: "spilo-pod-priority",
				Volume: Volume{
					Size:         "5Gi",
					StorageClass: "SSD",
					SubPath:      "subdir",
				},
				ShmVolume: util.False(),
				Patroni: Patroni{
					InitDB: map[string]string{
						"encoding":       "UTF8",
						"locale":         "en_US.UTF-8",
						"data-checksums": "true",
					},
					PgHba:                []string{"hostssl all all 0.0.0.0/0 md5", "host    all all 0.0.0.0/0 md5"},
					TTL:                  30,
					LoopWait:             10,
					RetryTimeout:         10,
					MaximumLagOnFailover: 33554432,
					Slots:                map[string]map[string]string{"permanent_logical_1": {"type": "logical", "database": "foo", "plugin": "pgoutput"}},
				},
				Resources: &Resources{
					ResourceRequests: ResourceDescription{CPU: stringToPointer("10m"), Memory: stringToPointer("50Mi")},
					ResourceLimits:   ResourceDescription{CPU: stringToPointer("300m"), Memory: stringToPointer("3000Mi")},
				},

				TeamID:              "acid",
				AllowedSourceRanges: []string{"127.0.0.1/32"},
				NumberOfInstances:   2,
				Users:               map[string]UserFlags{"zalando": {"superuser", "createdb"}},
				MaintenanceWindows: []MaintenanceWindow{{
					Everyday:  false,
					Weekday:   time.Monday,
					StartTime: mustParseTime("01:00"),
					EndTime:   mustParseTime("06:00"),
				}, {
					Everyday:  false,
					Weekday:   time.Saturday,
					StartTime: mustParseTime("00:00"),
					EndTime:   mustParseTime("04:00"),
				},
					{
						Everyday:  true,
						Weekday:   time.Sunday,
						StartTime: mustParseTime("05:00"),
						EndTime:   mustParseTime("05:15"),
					},
				},
				Clone: &CloneDescription{
					ClusterName: "acid-batman",
				},
			},
			Error: "",
		},
		marshal: []byte(`{"kind":"Postgresql","apiVersion":"acid.zalan.do/v1","metadata":{"name":"acid-testcluster1","creationTimestamp":null},"spec":{"postgresql":{"version":"17","parameters":{"log_statement":"all","max_connections":"10","shared_buffers":"32MB"}},"pod_priority_class_name":"spilo-pod-priority","volume":{"size":"5Gi","storageClass":"SSD", "subPath": "subdir"},"enableShmVolume":false,"patroni":{"initdb":{"data-checksums":"true","encoding":"UTF8","locale":"en_US.UTF-8"},"pg_hba":["hostssl all all 0.0.0.0/0 md5","host    all all 0.0.0.0/0 md5"],"ttl":30,"loop_wait":10,"retry_timeout":10,"maximum_lag_on_failover":33554432,"slots":{"permanent_logical_1":{"database":"foo","plugin":"pgoutput","type":"logical"}}},"resources":{"requests":{"cpu":"10m","memory":"50Mi"},"limits":{"cpu":"300m","memory":"3000Mi"}},"teamId":"acid","allowedSourceRanges":["127.0.0.1/32"],"numberOfInstances":2,"users":{"zalando":["superuser","createdb"]},"maintenanceWindows":["Mon:01:00-06:00","Sat:00:00-04:00","05:00-05:15"],"clone":{"cluster":"acid-batman"}},"status":{"PostgresClusterStatus":""}}`),
		err:     nil},
	{
		about: "example with clone",
		in:    []byte(`{"kind": "Postgresql","apiVersion": "acid.zalan.do/v1","metadata": {"name": "acid-testcluster1"}, "spec": {"teamId": "acid", "clone": {"cluster": "team-batman"}}}`),
		out: Postgresql{
			TypeMeta: metav1.TypeMeta{
				Kind:       "Postgresql",
				APIVersion: "acid.zalan.do/v1",
			},
			ObjectMeta: metav1.ObjectMeta{
				Name: "acid-testcluster1",
			},
			Spec: PostgresSpec{
				TeamID: "acid",
				Clone: &CloneDescription{
					ClusterName: "team-batman",
				},
			},
			Error: "",
		},
		marshal: []byte(`{"kind":"Postgresql","apiVersion":"acid.zalan.do/v1","metadata":{"name":"acid-testcluster1","creationTimestamp":null},"spec":{"postgresql":{"version":"","parameters":null},"volume":{"size":"","storageClass":""},"patroni":{"initdb":null,"pg_hba":null,"ttl":0,"loop_wait":0,"retry_timeout":0,"maximum_lag_on_failover":0,"slots":null},"teamId":"acid","allowedSourceRanges":null,"numberOfInstances":0,"users":null,"clone":{"cluster":"team-batman"}},"status":{"PostgresClusterStatus":""}}`),
		err:     nil},
	{
		about: "standby example",
		in:    []byte(`{"kind": "Postgresql","apiVersion": "acid.zalan.do/v1","metadata": {"name": "acid-testcluster1"}, "spec": {"teamId": "acid", "standby": {"s3_wal_path": "s3://custom/path/to/bucket/"}}}`),
		out: Postgresql{
			TypeMeta: metav1.TypeMeta{
				Kind:       "Postgresql",
				APIVersion: "acid.zalan.do/v1",
			},
			ObjectMeta: metav1.ObjectMeta{
				Name: "acid-testcluster1",
			},
			Spec: PostgresSpec{
				TeamID: "acid",
				StandbyCluster: &StandbyDescription{
					S3WalPath: "s3://custom/path/to/bucket/",
				},
			},
			Error: "",
		},
		marshal: []byte(`{"kind":"Postgresql","apiVersion":"acid.zalan.do/v1","metadata":{"name":"acid-testcluster1","creationTimestamp":null},"spec":{"postgresql":{"version":"","parameters":null},"volume":{"size":"","storageClass":""},"patroni":{"initdb":null,"pg_hba":null,"ttl":0,"loop_wait":0,"retry_timeout":0,"maximum_lag_on_failover":0,"slots":null},"teamId":"acid","allowedSourceRanges":null,"numberOfInstances":0,"users":null,"standby":{"s3_wal_path":"s3://custom/path/to/bucket/"}},"status":{"PostgresClusterStatus":""}}`),
		err:     nil},
	{
		about:   "expect error on malformatted JSON",
		in:      []byte(`{"kind": "Postgresql","apiVersion": "acid.zalan.do/v1"`),
		out:     Postgresql{},
		marshal: []byte{},
		err:     errors.New("unexpected end of JSON input")},
	{
		about:   "expect error on JSON with field's value malformatted",
		in:      []byte(`{"kind":"Postgresql","apiVersion":"acid.zalan.do/v1","metadata":{"name":"acid-testcluster","creationTimestamp":qaz},"spec":{"postgresql":{"version":"","parameters":null},"volume":{"size":"","storageClass":""},"patroni":{"initdb":null,"pg_hba":null,"ttl":0,"loop_wait":0,"retry_timeout":0,"maximum_lag_on_failover":0,"slots":null},"resources":{"requests":{"cpu":"","memory":""},"limits":{"cpu":"","memory":""}},"teamId":"acid","allowedSourceRanges":null,"numberOfInstances":0,"users":null,"clone":null},"status":{"PostgresClusterStatus":"Invalid"}}`),
		out:     Postgresql{},
		marshal: []byte{},
		err:     errors.New("invalid character 'q' looking for beginning of value"),
	},
}

var postgresqlList = []struct {
	about string
	in    []byte
	out   PostgresqlList
	err   error
}{
	{"expect success", []byte(`{"apiVersion":"v1","items":[{"apiVersion":"acid.zalan.do/v1","kind":"Postgresql","metadata":{"labels":{"team":"acid"},"name":"acid-testcluster42","namespace":"default","resourceVersion":"30446957","selfLink":"/apis/acid.zalan.do/v1/namespaces/default/postgresqls/acid-testcluster42","uid":"857cd208-33dc-11e7-b20a-0699041e4b03"},"spec":{"allowedSourceRanges":["185.85.220.0/22"],"numberOfInstances":1,"postgresql":{"version":"17"},"teamId":"acid","volume":{"size":"10Gi"}},"status":{"PostgresClusterStatus":"Running"}}],"kind":"List","metadata":{},"resourceVersion":"","selfLink":""}`),
		PostgresqlList{
			TypeMeta: metav1.TypeMeta{
				Kind:       "List",
				APIVersion: "v1",
			},
			Items: []Postgresql{{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Postgresql",
					APIVersion: "acid.zalan.do/v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:            "acid-testcluster42",
					Namespace:       "default",
					Labels:          map[string]string{"team": "acid"},
					ResourceVersion: "30446957",
					SelfLink:        "/apis/acid.zalan.do/v1/namespaces/default/postgresqls/acid-testcluster42",
					UID:             "857cd208-33dc-11e7-b20a-0699041e4b03",
				},
				Spec: PostgresSpec{
					ClusterName:         "testcluster42",
					PostgresqlParam:     PostgresqlParam{PgVersion: "17"},
					Volume:              Volume{Size: "10Gi"},
					TeamID:              "acid",
					AllowedSourceRanges: []string{"185.85.220.0/22"},
					NumberOfInstances:   1,
				},
				Status: PostgresStatus{
					PostgresClusterStatus: ClusterStatusRunning,
				},
				Error: "",
			}},
		},
		nil},
	{"expect error on malformatted JSON", []byte(`{"apiVersion":"v1","items":[{"apiVersion":"acid.zalan.do/v1","kind":"Postgresql","metadata":{"labels":{"team":"acid"},"name":"acid-testcluster42","namespace"`),
		PostgresqlList{},
		errors.New("unexpected end of JSON input")}}

var podAnnotations = []struct {
	about       string
	in          []byte
	annotations map[string]string
	err         error
}{{
	about: "common annotations",
	in: []byte(`{
		"kind": "Postgresql",
		"apiVersion": "acid.zalan.do/v1",
		"metadata": {
			"name": "acid-testcluster1"
		},
		"spec": {
			"podAnnotations": {
				"foo": "bar"
			},
			"teamId": "acid",
			"clone": {
				"cluster": "team-batman"
			}
		}
	}`),
	annotations: map[string]string{"foo": "bar"},
	err:         nil},
}

var serviceAnnotations = []struct {
	about       string
	in          []byte
	annotations map[string]string
	err         error
}{
	{
		about: "common single annotation",
		in: []byte(`{
			"kind": "Postgresql",
			"apiVersion": "acid.zalan.do/v1",
			"metadata": {
				"name": "acid-testcluster1"
			},
			"spec": {
				"serviceAnnotations": {
					"foo": "bar"
				},
				"teamId": "acid",
				"clone": {
					"cluster": "team-batman"
				}
			}
		}`),
		annotations: map[string]string{"foo": "bar"},
		err:         nil,
	},
	{
		about: "common two annotations",
		in: []byte(`{
			"kind": "Postgresql",
			"apiVersion": "acid.zalan.do/v1",
			"metadata": {
				"name": "acid-testcluster1"
			},
			"spec": {
				"serviceAnnotations": {
					"foo": "bar",
					"post": "gres"
				},
				"teamId": "acid",
				"clone": {
					"cluster": "team-batman"
				}
			}
		}`),
		annotations: map[string]string{"foo": "bar", "post": "gres"},
		err:         nil,
	},
}

func mustParseTime(s string) metav1.Time {
	v, err := time.Parse("15:04", s)
	if err != nil {
		panic(err)
	}

	return metav1.Time{Time: v.UTC()}
}

func TestParseTime(t *testing.T) {
	for _, tt := range parseTimeTests {
		t.Run(tt.about, func(t *testing.T) {
			aTime, err := parseTime(tt.in)
			if err != nil {
				if tt.err == nil || err.Error() != tt.err.Error() {
					t.Errorf("ParseTime expected error: %v, got: %v", tt.err, err)
				}
				return
			} else if tt.err != nil {
				t.Errorf("Expected error: %v", tt.err)
			}

			if aTime != tt.out {
				t.Errorf("Expected time: %v, got: %v", tt.out, aTime)
			}
		})
	}
}

func TestWeekdayTime(t *testing.T) {
	for _, tt := range parseWeekdayTests {
		t.Run(tt.about, func(t *testing.T) {
			aTime, err := parseWeekday(tt.in)
			if err != nil {
				if tt.err == nil || err.Error() != tt.err.Error() {
					t.Errorf("ParseWeekday expected error: %v, got: %v", tt.err, err)
				}
				return
			} else if tt.err != nil {
				t.Errorf("Expected error: %v", tt.err)
			}

			if aTime != tt.out {
				t.Errorf("Expected weekday: %v, got: %v", tt.out, aTime)
			}
		})
	}
}

func TestPodAnnotations(t *testing.T) {
	for _, tt := range podAnnotations {
		t.Run(tt.about, func(t *testing.T) {
			var cluster Postgresql
			err := cluster.UnmarshalJSON(tt.in)
			if err != nil {
				if tt.err == nil || err.Error() != tt.err.Error() {
					t.Errorf("Unable to marshal cluster with podAnnotations: expected %v got %v", tt.err, err)
				}
				return
			}
			for k, v := range cluster.Spec.PodAnnotations {
				found, expected := v, tt.annotations[k]
				if found != expected {
					t.Errorf("Didn't find correct value for key %v in for podAnnotations: Expected %v found %v", k, expected, found)
				}
			}
		})
	}
}

func TestServiceAnnotations(t *testing.T) {
	for _, tt := range serviceAnnotations {
		t.Run(tt.about, func(t *testing.T) {
			var cluster Postgresql
			err := cluster.UnmarshalJSON(tt.in)
			if err != nil {
				if tt.err == nil || err.Error() != tt.err.Error() {
					t.Errorf("Unable to marshal cluster with serviceAnnotations: expected %v got %v", tt.err, err)
				}
				return
			}
			for k, v := range cluster.Spec.ServiceAnnotations {
				found, expected := v, tt.annotations[k]
				if found != expected {
					t.Errorf("Didn't find correct value for key %v in for serviceAnnotations: Expected %v found %v", k, expected, found)
				}
			}
		})
	}
}

func TestClusterName(t *testing.T) {
	for _, tt := range clusterNames {
		t.Run(tt.about, func(t *testing.T) {
			name, err := ExtractClusterName(tt.in, tt.inTeam)
			if err != nil {
				if tt.err == nil || err.Error() != tt.err.Error() {
					t.Errorf("ExtractClusterName expected error: %v, got: %v", tt.err, err)
				}
				return
			} else if tt.err != nil {
				t.Errorf("Expected error: %v", tt.err)
			}
			if name != tt.clusterName {
				t.Errorf("Expected cluserName: %q, got: %q", tt.clusterName, name)
			}
		})
	}
}

func TestCloneClusterDescription(t *testing.T) {
	for _, tt := range cloneClusterDescriptions {
		t.Run(tt.about, func(t *testing.T) {
			if err := validateCloneClusterDescription(tt.in); err != nil {
				if tt.err == nil || err.Error() != tt.err.Error() {
					t.Errorf("testCloneClusterDescription expected error: %v, got: %v", tt.err, err)
				}
			} else if tt.err != nil {
				t.Errorf("Expected error: %v", tt.err)
			}
		})
	}
}

func TestUnmarshalMaintenanceWindow(t *testing.T) {
	for _, tt := range maintenanceWindows {
		t.Run(tt.about, func(t *testing.T) {
			var m MaintenanceWindow
			err := m.UnmarshalJSON(tt.in)
			if err != nil {
				if tt.err == nil || err.Error() != tt.err.Error() {
					t.Errorf("MaintenanceWindow unmarshal expected error: %v, got %v", tt.err, err)
				}
				return
			} else if tt.err != nil {
				t.Errorf("Expected error: %v", tt.err)
			}

			if !reflect.DeepEqual(m, tt.out) {
				t.Errorf("Expected maintenance window: %#v, got: %#v", tt.out, m)
			}
		})
	}
}

func TestMarshalMaintenanceWindow(t *testing.T) {
	for _, tt := range maintenanceWindows {
		t.Run(tt.about, func(t *testing.T) {
			if tt.err != nil {
				return
			}

			s, err := tt.out.MarshalJSON()
			if err != nil {
				t.Errorf("Marshal Error: %v", err)
			}

			if !bytes.Equal(s, tt.in) {
				t.Errorf("Expected Marshal: %q, got: %q", string(tt.in), string(s))
			}
		})
	}
}

func TestUnmarshalPostgresStatus(t *testing.T) {
	for _, tt := range postgresStatus {
		t.Run(tt.about, func(t *testing.T) {

			var ps PostgresStatus
			err := ps.UnmarshalJSON(tt.in)
			if err != nil {
				if tt.err == nil || err.Error() != tt.err.Error() {
					t.Errorf("CR status unmarshal expected error: %v, got %v", tt.err, err)
				}
				return
			}

			if !reflect.DeepEqual(ps, tt.out) {
				t.Errorf("Expected status: %#v, got: %#v", tt.out, ps)
			}
		})
	}
}

func TestPostgresUnmarshal(t *testing.T) {
	for _, tt := range unmarshalCluster {
		t.Run(tt.about, func(t *testing.T) {
			var cluster Postgresql
			err := cluster.UnmarshalJSON(tt.in)
			if err != nil {
				if tt.err == nil || err.Error() != tt.err.Error() {
					t.Errorf("Unmarshal expected error: %v, got: %v", tt.err, err)
				}
				return
			} else if tt.err != nil {
				t.Errorf("Expected error: %v", tt.err)
			}

			if !reflect.DeepEqual(cluster, tt.out) {
				t.Errorf("Expected Postgresql: %#v, got %#v", tt.out, cluster)
			}
		})
	}
}

func TestMarshal(t *testing.T) {
	for _, tt := range unmarshalCluster {
		t.Run(tt.about, func(t *testing.T) {

			if tt.err != nil {
				return
			}

			// Unmarshal and marshal example to capture api changes
			var cluster Postgresql
			err := cluster.UnmarshalJSON(tt.marshal)
			if err != nil {
				if tt.err == nil || err.Error() != tt.err.Error() {
					t.Errorf("Backwards compatibility unmarshal expected error: %v, got: %v", tt.err, err)
				}
				return
			}
			expected, err := json.Marshal(cluster)
			if err != nil {
				t.Errorf("Backwards compatibility marshal error: %v", err)
			}

			m, err := json.Marshal(tt.out)
			if err != nil {
				t.Errorf("Marshal error: %v", err)
			}
			if !bytes.Equal(m, expected) {
				t.Errorf("Marshal Postgresql \nexpected: %q, \ngot:      %q", string(expected), string(m))
			}
		})
	}
}

func TestPostgresMeta(t *testing.T) {
	for _, tt := range unmarshalCluster {
		t.Run(tt.about, func(t *testing.T) {

			if a := tt.out.GetObjectKind(); a != &tt.out.TypeMeta {
				t.Errorf("GetObjectKindMeta \nexpected: %v, \ngot:       %v", tt.out.TypeMeta, a)
			}

			if a := tt.out.GetObjectMeta(); reflect.DeepEqual(a, tt.out.ObjectMeta) {
				t.Errorf("GetObjectMeta \nexpected: %v, \ngot:       %v", tt.out.ObjectMeta, a)
			}
		})
	}
}

func TestPostgresListMeta(t *testing.T) {
	for _, tt := range postgresqlList {
		t.Run(tt.about, func(t *testing.T) {
			if tt.err != nil {
				return
			}

			if a := tt.out.GetObjectKind(); a != &tt.out.TypeMeta {
				t.Errorf("GetObjectKindMeta expected: %v, got: %v", tt.out.TypeMeta, a)
			}

			if a := tt.out.GetListMeta(); reflect.DeepEqual(a, tt.out.ListMeta) {
				t.Errorf("GetObjectMeta expected: %v, got: %v", tt.out.ListMeta, a)
			}

			return
		})
	}
}

func TestPostgresqlClone(t *testing.T) {
	for _, tt := range unmarshalCluster {
		t.Run(tt.about, func(t *testing.T) {
			cp := &tt.out
			cp.Error = ""
			clone := cp.Clone()
			if !reflect.DeepEqual(clone, cp) {
				t.Errorf("TestPostgresqlClone expected: \n%#v\n, got \n%#v", cp, clone)
			}
		})
	}
}


================================================
File: pkg/apis/acid.zalan.do/v1/zz_generated.deepcopy.go
================================================
//go:build !ignore_autogenerated
// +build !ignore_autogenerated

/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by deepcopy-gen. DO NOT EDIT.

package v1

import (
	config "github.com/zalando/postgres-operator/pkg/util/config"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
)

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *AWSGCPConfiguration) DeepCopyInto(out *AWSGCPConfiguration) {
	*out = *in
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AWSGCPConfiguration.
func (in *AWSGCPConfiguration) DeepCopy() *AWSGCPConfiguration {
	if in == nil {
		return nil
	}
	out := new(AWSGCPConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *AdditionalVolume) DeepCopyInto(out *AdditionalVolume) {
	*out = *in
	if in.IsSubPathExpr != nil {
		in, out := &in.IsSubPathExpr, &out.IsSubPathExpr
		*out = new(bool)
		**out = **in
	}
	if in.TargetContainers != nil {
		in, out := &in.TargetContainers, &out.TargetContainers
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	in.VolumeSource.DeepCopyInto(&out.VolumeSource)
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AdditionalVolume.
func (in *AdditionalVolume) DeepCopy() *AdditionalVolume {
	if in == nil {
		return nil
	}
	out := new(AdditionalVolume)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CloneDescription) DeepCopyInto(out *CloneDescription) {
	*out = *in
	if in.S3ForcePathStyle != nil {
		in, out := &in.S3ForcePathStyle, &out.S3ForcePathStyle
		*out = new(bool)
		**out = **in
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CloneDescription.
func (in *CloneDescription) DeepCopy() *CloneDescription {
	if in == nil {
		return nil
	}
	out := new(CloneDescription)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ConnectionPooler) DeepCopyInto(out *ConnectionPooler) {
	*out = *in
	if in.NumberOfInstances != nil {
		in, out := &in.NumberOfInstances, &out.NumberOfInstances
		*out = new(int32)
		**out = **in
	}
	if in.MaxDBConnections != nil {
		in, out := &in.MaxDBConnections, &out.MaxDBConnections
		*out = new(int32)
		**out = **in
	}
	if in.Resources != nil {
		in, out := &in.Resources, &out.Resources
		*out = new(Resources)
		(*in).DeepCopyInto(*out)
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ConnectionPooler.
func (in *ConnectionPooler) DeepCopy() *ConnectionPooler {
	if in == nil {
		return nil
	}
	out := new(ConnectionPooler)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ConnectionPoolerConfiguration) DeepCopyInto(out *ConnectionPoolerConfiguration) {
	*out = *in
	if in.NumberOfInstances != nil {
		in, out := &in.NumberOfInstances, &out.NumberOfInstances
		*out = new(int32)
		**out = **in
	}
	if in.MaxDBConnections != nil {
		in, out := &in.MaxDBConnections, &out.MaxDBConnections
		*out = new(int32)
		**out = **in
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ConnectionPoolerConfiguration.
func (in *ConnectionPoolerConfiguration) DeepCopy() *ConnectionPoolerConfiguration {
	if in == nil {
		return nil
	}
	out := new(ConnectionPoolerConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *KubernetesMetaConfiguration) DeepCopyInto(out *KubernetesMetaConfiguration) {
	*out = *in
	if in.EnableOwnerReferences != nil {
		in, out := &in.EnableOwnerReferences, &out.EnableOwnerReferences
		*out = new(bool)
		**out = **in
	}
	if in.SpiloAllowPrivilegeEscalation != nil {
		in, out := &in.SpiloAllowPrivilegeEscalation, &out.SpiloAllowPrivilegeEscalation
		*out = new(bool)
		**out = **in
	}
	if in.SpiloRunAsUser != nil {
		in, out := &in.SpiloRunAsUser, &out.SpiloRunAsUser
		*out = new(int64)
		**out = **in
	}
	if in.SpiloRunAsGroup != nil {
		in, out := &in.SpiloRunAsGroup, &out.SpiloRunAsGroup
		*out = new(int64)
		**out = **in
	}
	if in.SpiloFSGroup != nil {
		in, out := &in.SpiloFSGroup, &out.SpiloFSGroup
		*out = new(int64)
		**out = **in
	}
	if in.AdditionalPodCapabilities != nil {
		in, out := &in.AdditionalPodCapabilities, &out.AdditionalPodCapabilities
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.PDBMasterLabelSelector != nil {
		in, out := &in.PDBMasterLabelSelector, &out.PDBMasterLabelSelector
		*out = new(bool)
		**out = **in
	}
	if in.EnablePodDisruptionBudget != nil {
		in, out := &in.EnablePodDisruptionBudget, &out.EnablePodDisruptionBudget
		*out = new(bool)
		**out = **in
	}
	if in.EnableInitContainers != nil {
		in, out := &in.EnableInitContainers, &out.EnableInitContainers
		*out = new(bool)
		**out = **in
	}
	if in.EnableSidecars != nil {
		in, out := &in.EnableSidecars, &out.EnableSidecars
		*out = new(bool)
		**out = **in
	}
	if in.SharePgSocketWithSidecars != nil {
		in, out := &in.SharePgSocketWithSidecars, &out.SharePgSocketWithSidecars
		*out = new(bool)
		**out = **in
	}
	out.OAuthTokenSecretName = in.OAuthTokenSecretName
	out.InfrastructureRolesSecretName = in.InfrastructureRolesSecretName
	if in.InfrastructureRolesDefs != nil {
		in, out := &in.InfrastructureRolesDefs, &out.InfrastructureRolesDefs
		*out = make([]*config.InfrastructureRole, len(*in))
		for i := range *in {
			if (*in)[i] != nil {
				in, out := &(*in)[i], &(*out)[i]
				*out = new(config.InfrastructureRole)
				**out = **in
			}
		}
	}
	if in.ClusterLabels != nil {
		in, out := &in.ClusterLabels, &out.ClusterLabels
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.InheritedLabels != nil {
		in, out := &in.InheritedLabels, &out.InheritedLabels
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.InheritedAnnotations != nil {
		in, out := &in.InheritedAnnotations, &out.InheritedAnnotations
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.DownscalerAnnotations != nil {
		in, out := &in.DownscalerAnnotations, &out.DownscalerAnnotations
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.IgnoredAnnotations != nil {
		in, out := &in.IgnoredAnnotations, &out.IgnoredAnnotations
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.NodeReadinessLabel != nil {
		in, out := &in.NodeReadinessLabel, &out.NodeReadinessLabel
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.CustomPodAnnotations != nil {
		in, out := &in.CustomPodAnnotations, &out.CustomPodAnnotations
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.PodToleration != nil {
		in, out := &in.PodToleration, &out.PodToleration
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	out.PodEnvironmentConfigMap = in.PodEnvironmentConfigMap
	if in.PersistentVolumeClaimRetentionPolicy != nil {
		in, out := &in.PersistentVolumeClaimRetentionPolicy, &out.PersistentVolumeClaimRetentionPolicy
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.EnableSecretsDeletion != nil {
		in, out := &in.EnableSecretsDeletion, &out.EnableSecretsDeletion
		*out = new(bool)
		**out = **in
	}
	if in.EnablePersistentVolumeClaimDeletion != nil {
		in, out := &in.EnablePersistentVolumeClaimDeletion, &out.EnablePersistentVolumeClaimDeletion
		*out = new(bool)
		**out = **in
	}
	if in.EnableFinalizers != nil {
		in, out := &in.EnableFinalizers, &out.EnableFinalizers
		*out = new(bool)
		**out = **in
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new KubernetesMetaConfiguration.
func (in *KubernetesMetaConfiguration) DeepCopy() *KubernetesMetaConfiguration {
	if in == nil {
		return nil
	}
	out := new(KubernetesMetaConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *LoadBalancerConfiguration) DeepCopyInto(out *LoadBalancerConfiguration) {
	*out = *in
	if in.CustomServiceAnnotations != nil {
		in, out := &in.CustomServiceAnnotations, &out.CustomServiceAnnotations
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new LoadBalancerConfiguration.
func (in *LoadBalancerConfiguration) DeepCopy() *LoadBalancerConfiguration {
	if in == nil {
		return nil
	}
	out := new(LoadBalancerConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *LoggingRESTAPIConfiguration) DeepCopyInto(out *LoggingRESTAPIConfiguration) {
	*out = *in
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new LoggingRESTAPIConfiguration.
func (in *LoggingRESTAPIConfiguration) DeepCopy() *LoggingRESTAPIConfiguration {
	if in == nil {
		return nil
	}
	out := new(LoggingRESTAPIConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *MaintenanceWindow) DeepCopyInto(out *MaintenanceWindow) {
	*out = *in
	in.StartTime.DeepCopyInto(&out.StartTime)
	in.EndTime.DeepCopyInto(&out.EndTime)
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new MaintenanceWindow.
func (in *MaintenanceWindow) DeepCopy() *MaintenanceWindow {
	if in == nil {
		return nil
	}
	out := new(MaintenanceWindow)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *MajorVersionUpgradeConfiguration) DeepCopyInto(out *MajorVersionUpgradeConfiguration) {
	*out = *in
	if in.MajorVersionUpgradeTeamAllowList != nil {
		in, out := &in.MajorVersionUpgradeTeamAllowList, &out.MajorVersionUpgradeTeamAllowList
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new MajorVersionUpgradeConfiguration.
func (in *MajorVersionUpgradeConfiguration) DeepCopy() *MajorVersionUpgradeConfiguration {
	if in == nil {
		return nil
	}
	out := new(MajorVersionUpgradeConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *OperatorConfiguration) DeepCopyInto(out *OperatorConfiguration) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ObjectMeta.DeepCopyInto(&out.ObjectMeta)
	in.Configuration.DeepCopyInto(&out.Configuration)
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new OperatorConfiguration.
func (in *OperatorConfiguration) DeepCopy() *OperatorConfiguration {
	if in == nil {
		return nil
	}
	out := new(OperatorConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *OperatorConfiguration) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *OperatorConfigurationData) DeepCopyInto(out *OperatorConfigurationData) {
	*out = *in
	if in.EnableCRDRegistration != nil {
		in, out := &in.EnableCRDRegistration, &out.EnableCRDRegistration
		*out = new(bool)
		**out = **in
	}
	if in.EnableCRDValidation != nil {
		in, out := &in.EnableCRDValidation, &out.EnableCRDValidation
		*out = new(bool)
		**out = **in
	}
	if in.CRDCategories != nil {
		in, out := &in.CRDCategories, &out.CRDCategories
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.ShmVolume != nil {
		in, out := &in.ShmVolume, &out.ShmVolume
		*out = new(bool)
		**out = **in
	}
	if in.SidecarImages != nil {
		in, out := &in.SidecarImages, &out.SidecarImages
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.SidecarContainers != nil {
		in, out := &in.SidecarContainers, &out.SidecarContainers
		*out = make([]corev1.Container, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	in.PostgresUsersConfiguration.DeepCopyInto(&out.PostgresUsersConfiguration)
	in.MajorVersionUpgrade.DeepCopyInto(&out.MajorVersionUpgrade)
	in.Kubernetes.DeepCopyInto(&out.Kubernetes)
	out.PostgresPodResources = in.PostgresPodResources
	out.Timeouts = in.Timeouts
	in.LoadBalancer.DeepCopyInto(&out.LoadBalancer)
	out.AWSGCP = in.AWSGCP
	out.OperatorDebug = in.OperatorDebug
	in.TeamsAPI.DeepCopyInto(&out.TeamsAPI)
	out.LoggingRESTAPI = in.LoggingRESTAPI
	out.Scalyr = in.Scalyr
	out.LogicalBackup = in.LogicalBackup
	in.ConnectionPooler.DeepCopyInto(&out.ConnectionPooler)
	in.Patroni.DeepCopyInto(&out.Patroni)
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new OperatorConfigurationData.
func (in *OperatorConfigurationData) DeepCopy() *OperatorConfigurationData {
	if in == nil {
		return nil
	}
	out := new(OperatorConfigurationData)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *OperatorConfigurationList) DeepCopyInto(out *OperatorConfigurationList) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ListMeta.DeepCopyInto(&out.ListMeta)
	if in.Items != nil {
		in, out := &in.Items, &out.Items
		*out = make([]OperatorConfiguration, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new OperatorConfigurationList.
func (in *OperatorConfigurationList) DeepCopy() *OperatorConfigurationList {
	if in == nil {
		return nil
	}
	out := new(OperatorConfigurationList)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *OperatorConfigurationList) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *OperatorDebugConfiguration) DeepCopyInto(out *OperatorDebugConfiguration) {
	*out = *in
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new OperatorDebugConfiguration.
func (in *OperatorDebugConfiguration) DeepCopy() *OperatorDebugConfiguration {
	if in == nil {
		return nil
	}
	out := new(OperatorDebugConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *OperatorLogicalBackupConfiguration) DeepCopyInto(out *OperatorLogicalBackupConfiguration) {
	*out = *in
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new OperatorLogicalBackupConfiguration.
func (in *OperatorLogicalBackupConfiguration) DeepCopy() *OperatorLogicalBackupConfiguration {
	if in == nil {
		return nil
	}
	out := new(OperatorLogicalBackupConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *OperatorTimeouts) DeepCopyInto(out *OperatorTimeouts) {
	*out = *in
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new OperatorTimeouts.
func (in *OperatorTimeouts) DeepCopy() *OperatorTimeouts {
	if in == nil {
		return nil
	}
	out := new(OperatorTimeouts)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Patroni) DeepCopyInto(out *Patroni) {
	*out = *in
	if in.InitDB != nil {
		in, out := &in.InitDB, &out.InitDB
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.PgHba != nil {
		in, out := &in.PgHba, &out.PgHba
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.Slots != nil {
		in, out := &in.Slots, &out.Slots
		*out = make(map[string]map[string]string, len(*in))
		for key, val := range *in {
			var outVal map[string]string
			if val == nil {
				(*out)[key] = nil
			} else {
				in, out := &val, &outVal
				*out = make(map[string]string, len(*in))
				for key, val := range *in {
					(*out)[key] = val
				}
			}
			(*out)[key] = outVal
		}
	}
	if in.FailsafeMode != nil {
		in, out := &in.FailsafeMode, &out.FailsafeMode
		*out = new(bool)
		**out = **in
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Patroni.
func (in *Patroni) DeepCopy() *Patroni {
	if in == nil {
		return nil
	}
	out := new(Patroni)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PatroniConfiguration) DeepCopyInto(out *PatroniConfiguration) {
	*out = *in
	if in.FailsafeMode != nil {
		in, out := &in.FailsafeMode, &out.FailsafeMode
		*out = new(bool)
		**out = **in
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PatroniConfiguration.
func (in *PatroniConfiguration) DeepCopy() *PatroniConfiguration {
	if in == nil {
		return nil
	}
	out := new(PatroniConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PostgresPodResourcesDefaults) DeepCopyInto(out *PostgresPodResourcesDefaults) {
	*out = *in
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PostgresPodResourcesDefaults.
func (in *PostgresPodResourcesDefaults) DeepCopy() *PostgresPodResourcesDefaults {
	if in == nil {
		return nil
	}
	out := new(PostgresPodResourcesDefaults)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PostgresSpec) DeepCopyInto(out *PostgresSpec) {
	*out = *in
	in.PostgresqlParam.DeepCopyInto(&out.PostgresqlParam)
	in.Volume.DeepCopyInto(&out.Volume)
	in.Patroni.DeepCopyInto(&out.Patroni)
	if in.Resources != nil {
		in, out := &in.Resources, &out.Resources
		*out = new(Resources)
		(*in).DeepCopyInto(*out)
	}
	if in.EnableConnectionPooler != nil {
		in, out := &in.EnableConnectionPooler, &out.EnableConnectionPooler
		*out = new(bool)
		**out = **in
	}
	if in.EnableReplicaConnectionPooler != nil {
		in, out := &in.EnableReplicaConnectionPooler, &out.EnableReplicaConnectionPooler
		*out = new(bool)
		**out = **in
	}
	if in.ConnectionPooler != nil {
		in, out := &in.ConnectionPooler, &out.ConnectionPooler
		*out = new(ConnectionPooler)
		(*in).DeepCopyInto(*out)
	}
	if in.SpiloRunAsUser != nil {
		in, out := &in.SpiloRunAsUser, &out.SpiloRunAsUser
		*out = new(int64)
		**out = **in
	}
	if in.SpiloRunAsGroup != nil {
		in, out := &in.SpiloRunAsGroup, &out.SpiloRunAsGroup
		*out = new(int64)
		**out = **in
	}
	if in.SpiloFSGroup != nil {
		in, out := &in.SpiloFSGroup, &out.SpiloFSGroup
		*out = new(int64)
		**out = **in
	}
	if in.EnableMasterLoadBalancer != nil {
		in, out := &in.EnableMasterLoadBalancer, &out.EnableMasterLoadBalancer
		*out = new(bool)
		**out = **in
	}
	if in.EnableMasterPoolerLoadBalancer != nil {
		in, out := &in.EnableMasterPoolerLoadBalancer, &out.EnableMasterPoolerLoadBalancer
		*out = new(bool)
		**out = **in
	}
	if in.EnableReplicaLoadBalancer != nil {
		in, out := &in.EnableReplicaLoadBalancer, &out.EnableReplicaLoadBalancer
		*out = new(bool)
		**out = **in
	}
	if in.EnableReplicaPoolerLoadBalancer != nil {
		in, out := &in.EnableReplicaPoolerLoadBalancer, &out.EnableReplicaPoolerLoadBalancer
		*out = new(bool)
		**out = **in
	}
	if in.UseLoadBalancer != nil {
		in, out := &in.UseLoadBalancer, &out.UseLoadBalancer
		*out = new(bool)
		**out = **in
	}
	if in.ReplicaLoadBalancer != nil {
		in, out := &in.ReplicaLoadBalancer, &out.ReplicaLoadBalancer
		*out = new(bool)
		**out = **in
	}
	if in.AllowedSourceRanges != nil {
		in, out := &in.AllowedSourceRanges, &out.AllowedSourceRanges
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.Users != nil {
		in, out := &in.Users, &out.Users
		*out = make(map[string]UserFlags, len(*in))
		for key, val := range *in {
			var outVal []string
			if val == nil {
				(*out)[key] = nil
			} else {
				in, out := &val, &outVal
				*out = make(UserFlags, len(*in))
				copy(*out, *in)
			}
			(*out)[key] = outVal
		}
	}
	if in.UsersIgnoringSecretRotation != nil {
		in, out := &in.UsersIgnoringSecretRotation, &out.UsersIgnoringSecretRotation
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.UsersWithSecretRotation != nil {
		in, out := &in.UsersWithSecretRotation, &out.UsersWithSecretRotation
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.UsersWithInPlaceSecretRotation != nil {
		in, out := &in.UsersWithInPlaceSecretRotation, &out.UsersWithInPlaceSecretRotation
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.MaintenanceWindows != nil {
		in, out := &in.MaintenanceWindows, &out.MaintenanceWindows
		*out = make([]MaintenanceWindow, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.Clone != nil {
		in, out := &in.Clone, &out.Clone
		*out = new(CloneDescription)
		(*in).DeepCopyInto(*out)
	}
	if in.Databases != nil {
		in, out := &in.Databases, &out.Databases
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.PreparedDatabases != nil {
		in, out := &in.PreparedDatabases, &out.PreparedDatabases
		*out = make(map[string]PreparedDatabase, len(*in))
		for key, val := range *in {
			(*out)[key] = *val.DeepCopy()
		}
	}
	if in.SchedulerName != nil {
		in, out := &in.SchedulerName, &out.SchedulerName
		*out = new(string)
		**out = **in
	}
	if in.NodeAffinity != nil {
		in, out := &in.NodeAffinity, &out.NodeAffinity
		*out = new(corev1.NodeAffinity)
		(*in).DeepCopyInto(*out)
	}
	if in.Tolerations != nil {
		in, out := &in.Tolerations, &out.Tolerations
		*out = make([]corev1.Toleration, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.Sidecars != nil {
		in, out := &in.Sidecars, &out.Sidecars
		*out = make([]Sidecar, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.InitContainers != nil {
		in, out := &in.InitContainers, &out.InitContainers
		*out = make([]corev1.Container, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.ShmVolume != nil {
		in, out := &in.ShmVolume, &out.ShmVolume
		*out = new(bool)
		**out = **in
	}
	if in.StandbyCluster != nil {
		in, out := &in.StandbyCluster, &out.StandbyCluster
		*out = new(StandbyDescription)
		**out = **in
	}
	if in.PodAnnotations != nil {
		in, out := &in.PodAnnotations, &out.PodAnnotations
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.ServiceAnnotations != nil {
		in, out := &in.ServiceAnnotations, &out.ServiceAnnotations
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.MasterServiceAnnotations != nil {
		in, out := &in.MasterServiceAnnotations, &out.MasterServiceAnnotations
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.ReplicaServiceAnnotations != nil {
		in, out := &in.ReplicaServiceAnnotations, &out.ReplicaServiceAnnotations
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.TLS != nil {
		in, out := &in.TLS, &out.TLS
		*out = new(TLSDescription)
		**out = **in
	}
	if in.AdditionalVolumes != nil {
		in, out := &in.AdditionalVolumes, &out.AdditionalVolumes
		*out = make([]AdditionalVolume, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.Streams != nil {
		in, out := &in.Streams, &out.Streams
		*out = make([]Stream, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.Env != nil {
		in, out := &in.Env, &out.Env
		*out = make([]corev1.EnvVar, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.InitContainersOld != nil {
		in, out := &in.InitContainersOld, &out.InitContainersOld
		*out = make([]corev1.Container, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PostgresSpec.
func (in *PostgresSpec) DeepCopy() *PostgresSpec {
	if in == nil {
		return nil
	}
	out := new(PostgresSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PostgresStatus) DeepCopyInto(out *PostgresStatus) {
	*out = *in
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PostgresStatus.
func (in *PostgresStatus) DeepCopy() *PostgresStatus {
	if in == nil {
		return nil
	}
	out := new(PostgresStatus)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PostgresTeam) DeepCopyInto(out *PostgresTeam) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ObjectMeta.DeepCopyInto(&out.ObjectMeta)
	in.Spec.DeepCopyInto(&out.Spec)
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PostgresTeam.
func (in *PostgresTeam) DeepCopy() *PostgresTeam {
	if in == nil {
		return nil
	}
	out := new(PostgresTeam)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *PostgresTeam) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PostgresTeamList) DeepCopyInto(out *PostgresTeamList) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ListMeta.DeepCopyInto(&out.ListMeta)
	if in.Items != nil {
		in, out := &in.Items, &out.Items
		*out = make([]PostgresTeam, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PostgresTeamList.
func (in *PostgresTeamList) DeepCopy() *PostgresTeamList {
	if in == nil {
		return nil
	}
	out := new(PostgresTeamList)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *PostgresTeamList) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PostgresTeamSpec) DeepCopyInto(out *PostgresTeamSpec) {
	*out = *in
	if in.AdditionalSuperuserTeams != nil {
		in, out := &in.AdditionalSuperuserTeams, &out.AdditionalSuperuserTeams
		*out = make(map[string][]string, len(*in))
		for key, val := range *in {
			var outVal []string
			if val == nil {
				(*out)[key] = nil
			} else {
				in, out := &val, &outVal
				*out = make([]string, len(*in))
				copy(*out, *in)
			}
			(*out)[key] = outVal
		}
	}
	if in.AdditionalTeams != nil {
		in, out := &in.AdditionalTeams, &out.AdditionalTeams
		*out = make(map[string][]string, len(*in))
		for key, val := range *in {
			var outVal []string
			if val == nil {
				(*out)[key] = nil
			} else {
				in, out := &val, &outVal
				*out = make([]string, len(*in))
				copy(*out, *in)
			}
			(*out)[key] = outVal
		}
	}
	if in.AdditionalMembers != nil {
		in, out := &in.AdditionalMembers, &out.AdditionalMembers
		*out = make(map[string][]string, len(*in))
		for key, val := range *in {
			var outVal []string
			if val == nil {
				(*out)[key] = nil
			} else {
				in, out := &val, &outVal
				*out = make([]string, len(*in))
				copy(*out, *in)
			}
			(*out)[key] = outVal
		}
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PostgresTeamSpec.
func (in *PostgresTeamSpec) DeepCopy() *PostgresTeamSpec {
	if in == nil {
		return nil
	}
	out := new(PostgresTeamSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PostgresUsersConfiguration) DeepCopyInto(out *PostgresUsersConfiguration) {
	*out = *in
	if in.AdditionalOwnerRoles != nil {
		in, out := &in.AdditionalOwnerRoles, &out.AdditionalOwnerRoles
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PostgresUsersConfiguration.
func (in *PostgresUsersConfiguration) DeepCopy() *PostgresUsersConfiguration {
	if in == nil {
		return nil
	}
	out := new(PostgresUsersConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Postgresql) DeepCopyInto(out *Postgresql) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ObjectMeta.DeepCopyInto(&out.ObjectMeta)
	in.Spec.DeepCopyInto(&out.Spec)
	out.Status = in.Status
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Postgresql.
func (in *Postgresql) DeepCopy() *Postgresql {
	if in == nil {
		return nil
	}
	out := new(Postgresql)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *Postgresql) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PostgresqlList) DeepCopyInto(out *PostgresqlList) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ListMeta.DeepCopyInto(&out.ListMeta)
	if in.Items != nil {
		in, out := &in.Items, &out.Items
		*out = make([]Postgresql, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PostgresqlList.
func (in *PostgresqlList) DeepCopy() *PostgresqlList {
	if in == nil {
		return nil
	}
	out := new(PostgresqlList)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *PostgresqlList) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PostgresqlParam) DeepCopyInto(out *PostgresqlParam) {
	*out = *in
	if in.Parameters != nil {
		in, out := &in.Parameters, &out.Parameters
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PostgresqlParam.
func (in *PostgresqlParam) DeepCopy() *PostgresqlParam {
	if in == nil {
		return nil
	}
	out := new(PostgresqlParam)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PreparedDatabase) DeepCopyInto(out *PreparedDatabase) {
	*out = *in
	if in.PreparedSchemas != nil {
		in, out := &in.PreparedSchemas, &out.PreparedSchemas
		*out = make(map[string]PreparedSchema, len(*in))
		for key, val := range *in {
			(*out)[key] = *val.DeepCopy()
		}
	}
	if in.Extensions != nil {
		in, out := &in.Extensions, &out.Extensions
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PreparedDatabase.
func (in *PreparedDatabase) DeepCopy() *PreparedDatabase {
	if in == nil {
		return nil
	}
	out := new(PreparedDatabase)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PreparedSchema) DeepCopyInto(out *PreparedSchema) {
	*out = *in
	if in.DefaultRoles != nil {
		in, out := &in.DefaultRoles, &out.DefaultRoles
		*out = new(bool)
		**out = **in
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PreparedSchema.
func (in *PreparedSchema) DeepCopy() *PreparedSchema {
	if in == nil {
		return nil
	}
	out := new(PreparedSchema)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ResourceDescription) DeepCopyInto(out *ResourceDescription) {
	*out = *in
	if in.CPU != nil {
		in, out := &in.CPU, &out.CPU
		*out = new(string)
		**out = **in
	}
	if in.Memory != nil {
		in, out := &in.Memory, &out.Memory
		*out = new(string)
		**out = **in
	}
	if in.HugePages2Mi != nil {
		in, out := &in.HugePages2Mi, &out.HugePages2Mi
		*out = new(string)
		**out = **in
	}
	if in.HugePages1Gi != nil {
		in, out := &in.HugePages1Gi, &out.HugePages1Gi
		*out = new(string)
		**out = **in
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ResourceDescription.
func (in *ResourceDescription) DeepCopy() *ResourceDescription {
	if in == nil {
		return nil
	}
	out := new(ResourceDescription)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Resources) DeepCopyInto(out *Resources) {
	*out = *in
	in.ResourceRequests.DeepCopyInto(&out.ResourceRequests)
	in.ResourceLimits.DeepCopyInto(&out.ResourceLimits)
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Resources.
func (in *Resources) DeepCopy() *Resources {
	if in == nil {
		return nil
	}
	out := new(Resources)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ScalyrConfiguration) DeepCopyInto(out *ScalyrConfiguration) {
	*out = *in
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ScalyrConfiguration.
func (in *ScalyrConfiguration) DeepCopy() *ScalyrConfiguration {
	if in == nil {
		return nil
	}
	out := new(ScalyrConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Sidecar) DeepCopyInto(out *Sidecar) {
	*out = *in
	if in.Resources != nil {
		in, out := &in.Resources, &out.Resources
		*out = new(Resources)
		(*in).DeepCopyInto(*out)
	}
	if in.Ports != nil {
		in, out := &in.Ports, &out.Ports
		*out = make([]corev1.ContainerPort, len(*in))
		copy(*out, *in)
	}
	if in.Env != nil {
		in, out := &in.Env, &out.Env
		*out = make([]corev1.EnvVar, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.Command != nil {
		in, out := &in.Command, &out.Command
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Sidecar.
func (in *Sidecar) DeepCopy() *Sidecar {
	if in == nil {
		return nil
	}
	out := new(Sidecar)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *StandbyDescription) DeepCopyInto(out *StandbyDescription) {
	*out = *in
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new StandbyDescription.
func (in *StandbyDescription) DeepCopy() *StandbyDescription {
	if in == nil {
		return nil
	}
	out := new(StandbyDescription)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Stream) DeepCopyInto(out *Stream) {
	*out = *in
	if in.Tables != nil {
		in, out := &in.Tables, &out.Tables
		*out = make(map[string]StreamTable, len(*in))
		for key, val := range *in {
			(*out)[key] = *val.DeepCopy()
		}
	}
	if in.Filter != nil {
		in, out := &in.Filter, &out.Filter
		*out = make(map[string]*string, len(*in))
		for key, val := range *in {
			var outVal *string
			if val == nil {
				(*out)[key] = nil
			} else {
				in, out := &val, &outVal
				*out = new(string)
				**out = **in
			}
			(*out)[key] = outVal
		}
	}
	if in.BatchSize != nil {
		in, out := &in.BatchSize, &out.BatchSize
		*out = new(uint32)
		**out = **in
	}
	if in.CPU != nil {
		in, out := &in.CPU, &out.CPU
		*out = new(string)
		**out = **in
	}
	if in.Memory != nil {
		in, out := &in.Memory, &out.Memory
		*out = new(string)
		**out = **in
	}
	if in.EnableRecovery != nil {
		in, out := &in.EnableRecovery, &out.EnableRecovery
		*out = new(bool)
		**out = **in
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Stream.
func (in *Stream) DeepCopy() *Stream {
	if in == nil {
		return nil
	}
	out := new(Stream)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *StreamTable) DeepCopyInto(out *StreamTable) {
	*out = *in
	if in.IgnoreRecovery != nil {
		in, out := &in.IgnoreRecovery, &out.IgnoreRecovery
		*out = new(bool)
		**out = **in
	}
	if in.IdColumn != nil {
		in, out := &in.IdColumn, &out.IdColumn
		*out = new(string)
		**out = **in
	}
	if in.PayloadColumn != nil {
		in, out := &in.PayloadColumn, &out.PayloadColumn
		*out = new(string)
		**out = **in
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new StreamTable.
func (in *StreamTable) DeepCopy() *StreamTable {
	if in == nil {
		return nil
	}
	out := new(StreamTable)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *TLSDescription) DeepCopyInto(out *TLSDescription) {
	*out = *in
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new TLSDescription.
func (in *TLSDescription) DeepCopy() *TLSDescription {
	if in == nil {
		return nil
	}
	out := new(TLSDescription)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *TeamsAPIConfiguration) DeepCopyInto(out *TeamsAPIConfiguration) {
	*out = *in
	if in.TeamAPIRoleConfiguration != nil {
		in, out := &in.TeamAPIRoleConfiguration, &out.TeamAPIRoleConfiguration
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.ProtectedRoles != nil {
		in, out := &in.ProtectedRoles, &out.ProtectedRoles
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.PostgresSuperuserTeams != nil {
		in, out := &in.PostgresSuperuserTeams, &out.PostgresSuperuserTeams
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new TeamsAPIConfiguration.
func (in *TeamsAPIConfiguration) DeepCopy() *TeamsAPIConfiguration {
	if in == nil {
		return nil
	}
	out := new(TeamsAPIConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in UserFlags) DeepCopyInto(out *UserFlags) {
	{
		in := &in
		*out = make(UserFlags, len(*in))
		copy(*out, *in)
		return
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new UserFlags.
func (in UserFlags) DeepCopy() UserFlags {
	if in == nil {
		return nil
	}
	out := new(UserFlags)
	in.DeepCopyInto(out)
	return *out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Volume) DeepCopyInto(out *Volume) {
	*out = *in
	if in.Selector != nil {
		in, out := &in.Selector, &out.Selector
		*out = new(metav1.LabelSelector)
		(*in).DeepCopyInto(*out)
	}
	if in.IsSubPathExpr != nil {
		in, out := &in.IsSubPathExpr, &out.IsSubPathExpr
		*out = new(bool)
		**out = **in
	}
	if in.Iops != nil {
		in, out := &in.Iops, &out.Iops
		*out = new(int64)
		**out = **in
	}
	if in.Throughput != nil {
		in, out := &in.Throughput, &out.Throughput
		*out = new(int64)
		**out = **in
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Volume.
func (in *Volume) DeepCopy() *Volume {
	if in == nil {
		return nil
	}
	out := new(Volume)
	in.DeepCopyInto(out)
	return out
}


================================================
File: pkg/apis/zalando.org/register.go
================================================
package zalando

const (
	// GroupName is the group name for the operator CRDs
	GroupName = "zalando.org"
)


================================================
File: pkg/apis/zalando.org/v1/fabriceventstream.go
================================================
package v1

import (
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// +genclient
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// FabricEventStream defines FabricEventStream Custom Resource Definition Object.
type FabricEventStream struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec FabricEventStreamSpec `json:"spec"`
}

// FabricEventStreamSpec defines the specification for the FabricEventStream TPR.
type FabricEventStreamSpec struct {
	ApplicationId string        `json:"applicationId"`
	EventStreams  []EventStream `json:"eventStreams"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// FabricEventStreamList defines a list of FabricEventStreams .
type FabricEventStreamList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata"`

	Items []FabricEventStream `json:"items"`
}

// EventStream defines the source, flow and sink of the event stream
type EventStream struct {
	EventStreamFlow     EventStreamFlow     `json:"flow"`
	EventStreamSink     EventStreamSink     `json:"sink"`
	EventStreamSource   EventStreamSource   `json:"source"`
	EventStreamRecovery EventStreamRecovery `json:"recovery"`
}

// EventStreamFlow defines the flow characteristics of the event stream
type EventStreamFlow struct {
	Type          string  `json:"type"`
	PayloadColumn *string `json:"payloadColumn,omitempty"`
}

// EventStreamSink defines the target of the event stream
type EventStreamSink struct {
	Type         string  `json:"type"`
	EventType    string  `json:"eventType,omitempty"`
	MaxBatchSize *uint32 `json:"maxBatchSize,omitempty"`
}

// EventStreamRecovery defines the target of dead letter queue
type EventStreamRecovery struct {
	Type string           `json:"type"`
	Sink *EventStreamSink `json:"sink"`
}

// EventStreamSource defines the source of the event stream and connection for FES operator
type EventStreamSource struct {
	Type             string           `json:"type"`
	Schema           string           `json:"schema,omitempty" defaults:"public"`
	EventStreamTable EventStreamTable `json:"table"`
	Filter           *string          `json:"filter,omitempty"`
	Connection       Connection       `json:"jdbcConnection"`
}

// EventStreamTable defines the name and ID column to be used for streaming
type EventStreamTable struct {
	Name     string  `json:"name"`
	IDColumn *string `json:"idColumn,omitempty"`
}

// Connection to be used for allowing the FES operator to connect to a database
type Connection struct {
	Url             string  `json:"jdbcUrl"`
	SlotName        string  `json:"slotName"`
	PluginType      string  `json:"pluginType,omitempty"`
	PublicationName *string `json:"publicationName,omitempty"`
	DBAuth          DBAuth  `json:"databaseAuthentication"`
}

// DBAuth specifies the credentials to be used for connecting with the database
type DBAuth struct {
	Type        string `json:"type"`
	Name        string `json:"name,omitempty"`
	UserKey     string `json:"userKey,omitempty"`
	PasswordKey string `json:"passwordKey,omitempty"`
}

type Slot struct {
	Slot        map[string]string             `json:"slot"`
	Publication map[string]acidv1.StreamTable `json:"publication"`
}


================================================
File: pkg/apis/zalando.org/v1/register.go
================================================
package v1

import (
	"github.com/zalando/postgres-operator/pkg/apis/zalando.org"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/kubernetes/scheme"
)

// APIVersion of the `fabriceventstream` CRD
const (
	APIVersion = "v1"
)

var (
	schemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)
	AddToScheme   = schemeBuilder.AddToScheme
)

func init() {
	err := AddToScheme(scheme.Scheme)
	if err != nil {
		panic(err)
	}
}

// SchemeGroupVersion is the group version used to register these objects.
var SchemeGroupVersion = schema.GroupVersion{Group: zalando.GroupName, Version: APIVersion}

// Resource takes an unqualified resource and returns a Group-qualified GroupResource.
func Resource(resource string) schema.GroupResource {
	return SchemeGroupVersion.WithResource(resource).GroupResource()
}

// addKnownTypes adds the set of types defined in this package to the supplied scheme.
func addKnownTypes(scheme *runtime.Scheme) error {
	scheme.AddKnownTypes(SchemeGroupVersion,
		&FabricEventStream{},
		&FabricEventStreamList{},
	)
	metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
	return nil
}


================================================
File: pkg/apis/zalando.org/v1/zz_generated.deepcopy.go
================================================
// +build !ignore_autogenerated

/*
Copyright 2021 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by deepcopy-gen. DO NOT EDIT.

package v1

import (
	runtime "k8s.io/apimachinery/pkg/runtime"
)

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Connection) DeepCopyInto(out *Connection) {
	*out = *in
	if in.PublicationName != nil {
		in, out := &in.PublicationName, &out.PublicationName
		*out = new(string)
		**out = **in
	}
	in.DBAuth.DeepCopyInto(&out.DBAuth)
	return
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Connection) DeepCopy() *Connection {
	if in == nil {
		return nil
	}
	out := new(Connection)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *DBAuth) DeepCopyInto(out *DBAuth) {
	*out = *in
	return
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *DBAuth) DeepCopy() *DBAuth {
	if in == nil {
		return nil
	}
	out := new(DBAuth)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStream) DeepCopyInto(out *EventStream) {
	*out = *in
	in.EventStreamFlow.DeepCopyInto(&out.EventStreamFlow)
	in.EventStreamRecovery.DeepCopyInto(&out.EventStreamRecovery)
	in.EventStreamSink.DeepCopyInto(&out.EventStreamSink)
	in.EventStreamSource.DeepCopyInto(&out.EventStreamSource)
	return
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStream) DeepCopy() *EventStream {
	if in == nil {
		return nil
	}
	out := new(EventStream)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStreamFlow) DeepCopyInto(out *EventStreamFlow) {
	*out = *in
	if in.PayloadColumn != nil {
		in, out := &in.PayloadColumn, &out.PayloadColumn
		*out = new(string)
		**out = **in
	}
	return
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStreamFlow) DeepCopy() *EventStreamFlow {
	if in == nil {
		return nil
	}
	out := new(EventStreamFlow)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStreamRecovery) DeepCopyInto(out *EventStreamRecovery) {
	*out = *in
	if in.Sink != nil {
		in, out := &in.Sink, &out.Sink
		*out = new(EventStreamSink)
		(*in).DeepCopyInto(*out)
	}
	return
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStreamRecovery) DeepCopy() *EventStreamRecovery {
	if in == nil {
		return nil
	}
	out := new(EventStreamRecovery)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStreamSink) DeepCopyInto(out *EventStreamSink) {
	*out = *in
	if in.MaxBatchSize != nil {
		in, out := &in.MaxBatchSize, &out.MaxBatchSize
		*out = new(uint32)
		**out = **in
	}
	return
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStreamSink) DeepCopy() *EventStreamSink {
	if in == nil {
		return nil
	}
	out := new(EventStreamSink)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStreamSource) DeepCopyInto(out *EventStreamSource) {
	*out = *in
	in.Connection.DeepCopyInto(&out.Connection)
	if in.Filter != nil {
		in, out := &in.Filter, &out.Filter
		*out = new(string)
		**out = **in
	}
	in.EventStreamTable.DeepCopyInto(&out.EventStreamTable)
	return
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStreamSource) DeepCopy() *EventStreamSource {
	if in == nil {
		return nil
	}
	out := new(EventStreamSource)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStreamTable) DeepCopyInto(out *EventStreamTable) {
	*out = *in
	if in.IDColumn != nil {
		in, out := &in.IDColumn, &out.IDColumn
		*out = new(string)
		**out = **in
	}
	return
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *EventStreamTable) DeepCopy() *EventStreamTable {
	if in == nil {
		return nil
	}
	out := new(EventStreamTable)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *FabricEventStreamSpec) DeepCopyInto(out *FabricEventStreamSpec) {
	*out = *in
	if in.EventStreams != nil {
		in, out := &in.EventStreams, &out.EventStreams
		*out = make([]EventStream, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}

	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new FabricEventStreamSpec.
func (in *FabricEventStreamSpec) DeepCopy() *FabricEventStreamSpec {
	if in == nil {
		return nil
	}
	out := new(FabricEventStreamSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *FabricEventStream) DeepCopyInto(out *FabricEventStream) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ObjectMeta.DeepCopyInto(&out.ObjectMeta)
	in.Spec.DeepCopyInto(&out.Spec)
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new FabricEventStream.
func (in *FabricEventStream) DeepCopy() *FabricEventStream {
	if in == nil {
		return nil
	}
	out := new(FabricEventStream)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *FabricEventStream) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *FabricEventStreamList) DeepCopyInto(out *FabricEventStreamList) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ListMeta.DeepCopyInto(&out.ListMeta)
	if in.Items != nil {
		in, out := &in.Items, &out.Items
		*out = make([]FabricEventStream, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new FabricEventStreamList.
func (in *FabricEventStreamList) DeepCopy() *FabricEventStreamList {
	if in == nil {
		return nil
	}
	out := new(FabricEventStreamList)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *FabricEventStreamList) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}


================================================
File: pkg/apiserver/apiserver.go
================================================
package apiserver

import (
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"net/http/pprof"
	"regexp"
	"strconv"
	"sync"
	"time"

	"github.com/sirupsen/logrus"
	"github.com/zalando/postgres-operator/pkg/cluster"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/config"
)

const (
	httpAPITimeout  = time.Minute * 1
	shutdownTimeout = time.Second * 10
	httpReadTimeout = time.Millisecond * 100
)

// ControllerInformer describes stats methods of a controller
type controllerInformer interface {
	GetConfig() *spec.ControllerConfig
	GetOperatorConfig() *config.Config
	GetStatus() *spec.ControllerStatus
	TeamClusterList() map[string][]spec.NamespacedName
	ClusterStatus(namespace, cluster string) (*cluster.ClusterStatus, error)
	ClusterLogs(namespace, cluster string) ([]*spec.LogEntry, error)
	ClusterHistory(namespace, cluster string) ([]*spec.Diff, error)
	ClusterDatabasesMap() map[string][]string
	WorkerLogs(workerID uint32) ([]*spec.LogEntry, error)
	ListQueue(workerID uint32) (*spec.QueueDump, error)
	GetWorkersCnt() uint32
	WorkerStatus(workerID uint32) (*cluster.WorkerStatus, error)
}

// Server describes HTTP API server
type Server struct {
	logger     *logrus.Entry
	http       http.Server
	controller controllerInformer
}

const (
	teamRe      = `(?P<team>[a-zA-Z][a-zA-Z0-9\-_]*)`
	namespaceRe = `(?P<namespace>[a-z0-9]([-a-z0-9\-_]*[a-z0-9])?)`
	clusterRe   = `(?P<cluster>[a-zA-Z][a-zA-Z0-9\-_]*)`
)

var (
	clusterStatusRe  = fmt.Sprintf(`^/clusters/%s/%s/?$`, namespaceRe, clusterRe)
	clusterLogsRe    = fmt.Sprintf(`^/clusters/%s/%s/logs/?$`, namespaceRe, clusterRe)
	clusterHistoryRe = fmt.Sprintf(`^/clusters/%s/%s/history/?$`, namespaceRe, clusterRe)
	teamURLRe        = fmt.Sprintf(`^/clusters/%s/?$`, teamRe)

	clusterStatusURL     = regexp.MustCompile(clusterStatusRe)
	clusterLogsURL       = regexp.MustCompile(clusterLogsRe)
	clusterHistoryURL    = regexp.MustCompile(clusterHistoryRe)
	teamURL              = regexp.MustCompile(teamURLRe)
	workerLogsURL        = regexp.MustCompile(`^/workers/(?P<id>\d+)/logs/?$`)
	workerEventsQueueURL = regexp.MustCompile(`^/workers/(?P<id>\d+)/queue/?$`)
	workerStatusURL      = regexp.MustCompile(`^/workers/(?P<id>\d+)/status/?$`)
	workerAllQueue       = regexp.MustCompile(`^/workers/all/queue/?$`)
	workerAllStatus      = regexp.MustCompile(`^/workers/all/status/?$`)
	clustersURL          = "/clusters/"
)

// New creates new HTTP API server
func New(controller controllerInformer, port int, logger *logrus.Logger) *Server {
	s := &Server{
		logger:     logger.WithField("pkg", "apiserver"),
		controller: controller,
	}
	mux := http.NewServeMux()

	mux.Handle("/debug/pprof/", http.HandlerFunc(pprof.Index))
	mux.Handle("/debug/pprof/cmdline", http.HandlerFunc(pprof.Cmdline))
	mux.Handle("/debug/pprof/profile", http.HandlerFunc(pprof.Profile))
	mux.Handle("/debug/pprof/symbol", http.HandlerFunc(pprof.Symbol))
	mux.Handle("/debug/pprof/trace", http.HandlerFunc(pprof.Trace))

	mux.Handle("/status/", http.HandlerFunc(s.controllerStatus))
	mux.Handle("/readyz/", http.HandlerFunc(s.controllerReady))
	mux.Handle("/config/", http.HandlerFunc(s.operatorConfig))

	mux.HandleFunc("/clusters/", s.clusters)
	mux.HandleFunc("/workers/", s.workers)
	mux.HandleFunc("/databases/", s.databases)

	s.http = http.Server{
		Addr:        fmt.Sprintf(":%d", port),
		Handler:     http.TimeoutHandler(mux, httpAPITimeout, ""),
		ReadTimeout: httpReadTimeout,
	}

	return s
}

// Run starts the HTTP server
func (s *Server) Run(stopCh <-chan struct{}, wg *sync.WaitGroup) {

	var err error

	defer wg.Done()

	go func() {
		if err2 := s.http.ListenAndServe(); err2 != http.ErrServerClosed {
			s.logger.Fatalf("Could not start http server: %v", err2)
		}
	}()
	s.logger.Infof("listening on %s", s.http.Addr)

	<-stopCh

	ctx, cancel := context.WithTimeout(context.Background(), shutdownTimeout)
	defer cancel()
	if err = s.http.Shutdown(ctx); err == nil {
		s.logger.Infoln("Http server shut down")
		return
	}
	if err == context.DeadlineExceeded {
		s.logger.Warningf("Shutdown timeout exceeded. closing http server")
		if err = s.http.Close(); err != nil {
			s.logger.Errorf("could not close http connection: %v", err)
		}
		return
	}
	s.logger.Errorf("Could not shutdown http server: %v", err)
}

func (s *Server) respond(obj interface{}, err error, w http.ResponseWriter) {
	w.Header().Set("Content-Type", "application/json")
	if err != nil {
		w.WriteHeader(http.StatusInternalServerError)
		if err2 := json.NewEncoder(w).Encode(map[string]interface{}{"error": err.Error()}); err2 != nil {
			s.logger.Errorf("could not encode error response %q: %v", err, err2)
		}
		return
	}

	err = json.NewEncoder(w).Encode(obj)
	if err != nil {
		w.WriteHeader(http.StatusInternalServerError)
		s.logger.Errorf("Could not encode: %v", err)
	}
}

func (s *Server) controllerStatus(w http.ResponseWriter, req *http.Request) {
	s.respond(s.controller.GetStatus(), nil, w)
}

func (s *Server) controllerReady(w http.ResponseWriter, req *http.Request) {
	s.respond("OK", nil, w)
}

func (s *Server) operatorConfig(w http.ResponseWriter, req *http.Request) {
	s.respond(map[string]interface{}{
		"controller": s.controller.GetConfig(),
		"operator":   s.controller.GetOperatorConfig(),
	}, nil, w)
}

func (s *Server) clusters(w http.ResponseWriter, req *http.Request) {
	var (
		resp interface{}
		err  error
	)

	if matches := util.FindNamedStringSubmatch(clusterStatusURL, req.URL.Path); matches != nil {
		namespace := matches["namespace"]
		resp, err = s.controller.ClusterStatus(namespace, matches["cluster"])
	} else if matches := util.FindNamedStringSubmatch(teamURL, req.URL.Path); matches != nil {
		teamClusters := s.controller.TeamClusterList()
		clusters, found := teamClusters[matches["team"]]
		if !found {
			s.respond(nil, fmt.Errorf("could not find clusters for the team"), w)
			return
		}

		clusterNames := make([]string, 0)
		for _, cluster := range clusters {
			clusterNames = append(clusterNames, cluster.Name)
		}

		resp, err = clusterNames, nil
	} else if matches := util.FindNamedStringSubmatch(clusterLogsURL, req.URL.Path); matches != nil {
		namespace := matches["namespace"]
		resp, err = s.controller.ClusterLogs(namespace, matches["cluster"])
	} else if matches := util.FindNamedStringSubmatch(clusterHistoryURL, req.URL.Path); matches != nil {
		namespace := matches["namespace"]
		resp, err = s.controller.ClusterHistory(namespace, matches["cluster"])
	} else if req.URL.Path == clustersURL {
		clusterNamesPerTeam := make(map[string][]string)
		for team, clusters := range s.controller.TeamClusterList() {
			for _, cluster := range clusters {
				clusterNamesPerTeam[team] = append(clusterNamesPerTeam[team], cluster.Name)
			}
		}
		resp, err = clusterNamesPerTeam, nil
	} else {
		resp, err = nil, fmt.Errorf("page not found")
	}

	s.respond(resp, err, w)
}

func mustConvertToUint32(s string) uint32 {
	result, err := strconv.Atoi(s)
	if err != nil {
		panic(fmt.Errorf("mustConvertToUint32 called for %s: %v", s, err))
	}
	return uint32(result)
}

func (s *Server) workers(w http.ResponseWriter, req *http.Request) {
	var (
		resp interface{}
		err  error
	)

	if workerAllQueue.MatchString(req.URL.Path) {
		s.allQueues(w, req)
		return
	}
	if workerAllStatus.MatchString(req.URL.Path) {
		s.allWorkers(w, req)
		return
	}

	err = fmt.Errorf("page not found")

	if matches := util.FindNamedStringSubmatch(workerLogsURL, req.URL.Path); matches != nil {
		workerID := mustConvertToUint32(matches["id"])
		resp, err = s.controller.WorkerLogs(workerID)

	} else if matches := util.FindNamedStringSubmatch(workerEventsQueueURL, req.URL.Path); matches != nil {
		workerID := mustConvertToUint32(matches["id"])
		resp, err = s.controller.ListQueue(workerID)

	} else if matches := util.FindNamedStringSubmatch(workerStatusURL, req.URL.Path); matches != nil {
		var workerStatus *cluster.WorkerStatus

		workerID := mustConvertToUint32(matches["id"])
		resp = "idle"
		if workerStatus, err = s.controller.WorkerStatus(workerID); workerStatus != nil {
			resp = workerStatus
		}
	}

	s.respond(resp, err, w)
}

func (s *Server) databases(w http.ResponseWriter, req *http.Request) {

	databaseNamesPerCluster := s.controller.ClusterDatabasesMap()
	s.respond(databaseNamesPerCluster, nil, w)
}

func (s *Server) allQueues(w http.ResponseWriter, r *http.Request) {
	workersCnt := s.controller.GetWorkersCnt()
	resp := make(map[uint32]*spec.QueueDump, workersCnt)
	for i := uint32(0); i < workersCnt; i++ {
		queueDump, err := s.controller.ListQueue(i)
		if err != nil {
			s.respond(nil, err, w)
			return
		}

		resp[i] = queueDump
	}

	s.respond(resp, nil, w)
}

func (s *Server) allWorkers(w http.ResponseWriter, r *http.Request) {
	workersCnt := s.controller.GetWorkersCnt()
	resp := make(map[uint32]interface{}, workersCnt)
	for i := uint32(0); i < workersCnt; i++ {
		status, err := s.controller.WorkerStatus(i)
		if err != nil {
			s.respond(nil, err, w)
			continue
		}

		if status == nil {
			resp[i] = "idle"
		} else {
			resp[i] = status
		}
	}

	s.respond(resp, nil, w)
}


================================================
File: pkg/apiserver/apiserver_test.go
================================================
package apiserver

import (
	"testing"
)

const (
	clusterStatusTest        = "/clusters/test-namespace/testcluster/"
	clusterStatusNumericTest = "/clusters/test-namespace-1/testcluster/"
	clusterLogsTest          = "/clusters/test-namespace/testcluster/logs/"
	teamTest                 = "/clusters/test-id/"
)

func TestUrlRegexps(t *testing.T) {
	if clusterStatusURL.FindStringSubmatch(clusterStatusTest) == nil {
		t.Errorf("clusterStatusURL can't match %s", clusterStatusTest)
	}

	if clusterStatusURL.FindStringSubmatch(clusterStatusNumericTest) == nil {
		t.Errorf("clusterStatusURL can't match %s", clusterStatusNumericTest)
	}

	if clusterLogsURL.FindStringSubmatch(clusterLogsTest) == nil {
		t.Errorf("clusterLogsURL can't match %s", clusterLogsTest)
	}

	if teamURL.FindStringSubmatch(teamTest) == nil {
		t.Errorf("teamURL can't match %s", teamTest)
	}
}


================================================
File: pkg/cluster/connection_pooler.go
================================================
package cluster

import (
	"context"
	"encoding/json"
	"fmt"
	"reflect"
	"strings"
	"time"

	"github.com/r3labs/diff"
	"github.com/sirupsen/logrus"
	acidzalando "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do"
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	appsv1 "k8s.io/api/apps/v1"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/util/intstr"

	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"github.com/zalando/postgres-operator/pkg/util/retryutil"
)

var poolerRunAsUser = int64(100)
var poolerRunAsGroup = int64(101)

// ConnectionPoolerObjects K8s objects that are belong to connection pooler
type ConnectionPoolerObjects struct {
	Deployment  *appsv1.Deployment
	Service     *v1.Service
	Name        string
	ClusterName string
	Namespace   string
	Role        PostgresRole
	// It could happen that a connection pooler was enabled, but the operator
	// was not able to properly process a corresponding event or was restarted.
	// In this case we will miss missing/require situation and a lookup function
	// will not be installed. To avoid synchronizing it all the time to prevent
	// this, we can remember the result in memory at least until the next
	// restart.
	LookupFunction bool
	// Careful with referencing cluster.spec this object pointer changes
	// during runtime and lifetime of cluster
}

func (c *Cluster) connectionPoolerName(role PostgresRole) string {
	name := fmt.Sprintf("%s-%s", c.Name, constants.ConnectionPoolerResourceSuffix)
	if role == Replica {
		name = fmt.Sprintf("%s-%s", name, "repl")
	}
	return name
}

// isConnectionPoolerEnabled
func needConnectionPooler(spec *acidv1.PostgresSpec) bool {
	return needMasterConnectionPoolerWorker(spec) ||
		needReplicaConnectionPoolerWorker(spec)
}

func needMasterConnectionPooler(spec *acidv1.PostgresSpec) bool {
	return needMasterConnectionPoolerWorker(spec)
}

func needMasterConnectionPoolerWorker(spec *acidv1.PostgresSpec) bool {
	return (spec.EnableConnectionPooler != nil && *spec.EnableConnectionPooler) ||
		(spec.ConnectionPooler != nil && spec.EnableConnectionPooler == nil)
}

func needReplicaConnectionPooler(spec *acidv1.PostgresSpec) bool {
	return needReplicaConnectionPoolerWorker(spec)
}

func needReplicaConnectionPoolerWorker(spec *acidv1.PostgresSpec) bool {
	return spec.EnableReplicaConnectionPooler != nil &&
		*spec.EnableReplicaConnectionPooler
}

func (c *Cluster) needConnectionPoolerUser(oldSpec, newSpec *acidv1.PostgresSpec) bool {
	// return true if pooler is needed AND was not disabled before OR user name differs
	return (needMasterConnectionPoolerWorker(newSpec) || needReplicaConnectionPoolerWorker(newSpec)) &&
		((!needMasterConnectionPoolerWorker(oldSpec) &&
			!needReplicaConnectionPoolerWorker(oldSpec)) ||
			c.poolerUser(oldSpec) != c.poolerUser(newSpec))
}

func (c *Cluster) poolerUser(spec *acidv1.PostgresSpec) string {
	connectionPoolerSpec := spec.ConnectionPooler
	if connectionPoolerSpec == nil {
		connectionPoolerSpec = &acidv1.ConnectionPooler{}
	}
	// Using superuser as pooler user is not a good idea. First of all it's
	// not going to be synced correctly with the current implementation,
	// and second it's a bad practice.
	username := c.OpConfig.ConnectionPooler.User

	isSuperUser := connectionPoolerSpec.User == c.OpConfig.SuperUsername
	isProtectedUser := c.shouldAvoidProtectedOrSystemRole(
		connectionPoolerSpec.User, "connection pool role")

	if !isSuperUser && !isProtectedUser {
		username = util.Coalesce(
			connectionPoolerSpec.User,
			c.OpConfig.ConnectionPooler.User)
	}

	return username
}

// when listing pooler k8s objects
func (c *Cluster) poolerLabelsSet(addExtraLabels bool) labels.Set {
	poolerLabels := c.labelsSet(addExtraLabels)

	// TODO should be config values
	poolerLabels["application"] = "db-connection-pooler"

	return poolerLabels
}

// Return connection pooler labels selector, which should from one point of view
// inherit most of the labels from the cluster itself, but at the same time
// have e.g. different `application` label, so that recreatePod operation will
// not interfere with it (it lists all the pods via labels, and if there would
// be no difference, it will recreate also pooler pods).
func (c *Cluster) connectionPoolerLabels(role PostgresRole, addExtraLabels bool) *metav1.LabelSelector {
	poolerLabelsSet := c.poolerLabelsSet(addExtraLabels)

	// TODO should be config values
	poolerLabelsSet["connection-pooler"] = c.connectionPoolerName(role)

	if addExtraLabels {
		extraLabels := map[string]string{}
		extraLabels[c.OpConfig.PodRoleLabel] = string(role)

		poolerLabelsSet = labels.Merge(poolerLabelsSet, extraLabels)
	}

	return &metav1.LabelSelector{
		MatchLabels:      poolerLabelsSet,
		MatchExpressions: nil,
	}
}

// Prepare the database for connection pooler to be used, i.e. install lookup
// function (do it first, because it should be fast and if it didn't succeed,
// it doesn't makes sense to create more K8S objects. At this moment we assume
// that necessary connection pooler user exists.
//
// After that create all the objects for connection pooler, namely a deployment
// with a chosen pooler and a service to expose it.

// have connectionpooler name in the cp object to have it immutable name
// add these cp related functions to a new cp file
// opConfig, cluster, and database name
func (c *Cluster) createConnectionPooler(LookupFunction InstallFunction) (SyncReason, error) {
	var reason SyncReason
	c.setProcessName("creating connection pooler")

	//this is essentially sync with nil as oldSpec
	if reason, err := c.syncConnectionPooler(&acidv1.Postgresql{}, &c.Postgresql, LookupFunction); err != nil {
		return reason, err
	}
	return reason, nil
}

// Generate pool size related environment variables.
//
// MAX_DB_CONN would specify the global maximum for connections to a target
//
//	database.
//
// MAX_CLIENT_CONN is not configurable at the moment, just set it high enough.
//
// DEFAULT_SIZE is a pool size per db/user (having in mind the use case when
//
//	most of the queries coming through a connection pooler are from the same
//	user to the same db). In case if we want to spin up more connection pooler
//	instances, take this into account and maintain the same number of
//	connections.
//
// MIN_SIZE is a pool's minimal size, to prevent situation when sudden workload
//
//	have to wait for spinning up a new connections.
//
// RESERVE_SIZE is how many additional connections to allow for a pooler.

func (c *Cluster) getConnectionPoolerEnvVars() []v1.EnvVar {
	spec := &c.Spec
	connectionPoolerSpec := spec.ConnectionPooler
	if connectionPoolerSpec == nil {
		connectionPoolerSpec = &acidv1.ConnectionPooler{}
	}
	effectiveMode := util.Coalesce(
		connectionPoolerSpec.Mode,
		c.OpConfig.ConnectionPooler.Mode)

	numberOfInstances := connectionPoolerSpec.NumberOfInstances
	if numberOfInstances == nil {
		numberOfInstances = util.CoalesceInt32(
			c.OpConfig.ConnectionPooler.NumberOfInstances,
			k8sutil.Int32ToPointer(1))
	}

	effectiveMaxDBConn := util.CoalesceInt32(
		connectionPoolerSpec.MaxDBConnections,
		c.OpConfig.ConnectionPooler.MaxDBConnections)

	if effectiveMaxDBConn == nil {
		effectiveMaxDBConn = k8sutil.Int32ToPointer(
			constants.ConnectionPoolerMaxDBConnections)
	}

	maxDBConn := *effectiveMaxDBConn / *numberOfInstances

	defaultSize := maxDBConn / 2
	minSize := defaultSize / 2
	reserveSize := minSize

	return []v1.EnvVar{
		{
			Name:  "CONNECTION_POOLER_PORT",
			Value: fmt.Sprint(pgPort),
		},
		{
			Name:  "CONNECTION_POOLER_MODE",
			Value: effectiveMode,
		},
		{
			Name:  "CONNECTION_POOLER_DEFAULT_SIZE",
			Value: fmt.Sprint(defaultSize),
		},
		{
			Name:  "CONNECTION_POOLER_MIN_SIZE",
			Value: fmt.Sprint(minSize),
		},
		{
			Name:  "CONNECTION_POOLER_RESERVE_SIZE",
			Value: fmt.Sprint(reserveSize),
		},
		{
			Name:  "CONNECTION_POOLER_MAX_CLIENT_CONN",
			Value: fmt.Sprint(constants.ConnectionPoolerMaxClientConnections),
		},
		{
			Name:  "CONNECTION_POOLER_MAX_DB_CONN",
			Value: fmt.Sprint(maxDBConn),
		},
	}
}

func (c *Cluster) generateConnectionPoolerPodTemplate(role PostgresRole) (
	*v1.PodTemplateSpec, error) {
	spec := &c.Spec
	connectionPoolerSpec := spec.ConnectionPooler
	if connectionPoolerSpec == nil {
		connectionPoolerSpec = &acidv1.ConnectionPooler{}
	}
	gracePeriod := int64(c.OpConfig.PodTerminateGracePeriod.Seconds())
	resources, err := c.generateResourceRequirements(
		connectionPoolerSpec.Resources,
		makeDefaultConnectionPoolerResources(&c.OpConfig),
		connectionPoolerContainer)

	if err != nil {
		return nil, fmt.Errorf("could not generate resource requirements: %v", err)
	}

	effectiveDockerImage := util.Coalesce(
		connectionPoolerSpec.DockerImage,
		c.OpConfig.ConnectionPooler.Image)

	effectiveSchema := util.Coalesce(
		connectionPoolerSpec.Schema,
		c.OpConfig.ConnectionPooler.Schema)

	secretSelector := func(key string) *v1.SecretKeySelector {
		effectiveUser := util.Coalesce(
			connectionPoolerSpec.User,
			c.OpConfig.ConnectionPooler.User)

		return &v1.SecretKeySelector{
			LocalObjectReference: v1.LocalObjectReference{
				Name: c.credentialSecretName(effectiveUser),
			},
			Key: key,
		}
	}

	envVars := []v1.EnvVar{
		{
			Name:  "PGHOST",
			Value: c.serviceAddress(role),
		},
		{
			Name:  "PGPORT",
			Value: fmt.Sprint(c.servicePort(role)),
		},
		{
			Name: "PGUSER",
			ValueFrom: &v1.EnvVarSource{
				SecretKeyRef: secretSelector("username"),
			},
		},
		// the convention is to use the same schema name as
		// connection pooler username
		{
			Name:  "PGSCHEMA",
			Value: effectiveSchema,
		},
		{
			Name: "PGPASSWORD",
			ValueFrom: &v1.EnvVarSource{
				SecretKeyRef: secretSelector("password"),
			},
		},
	}
	envVars = append(envVars, c.getConnectionPoolerEnvVars()...)

	poolerContainer := v1.Container{
		Name:            connectionPoolerContainer,
		Image:           effectiveDockerImage,
		ImagePullPolicy: v1.PullIfNotPresent,
		Resources:       *resources,
		Ports: []v1.ContainerPort{
			{
				ContainerPort: pgPort,
				Protocol:      v1.ProtocolTCP,
			},
		},
		ReadinessProbe: &v1.Probe{
			ProbeHandler: v1.ProbeHandler{
				TCPSocket: &v1.TCPSocketAction{
					Port: intstr.IntOrString{IntVal: pgPort},
				},
			},
		},
		SecurityContext: &v1.SecurityContext{
			AllowPrivilegeEscalation: util.False(),
		},
	}

	// If the cluster has custom TLS certificates configured, we do the following:
	//  1. Add environment variables to tell pgBouncer where to find the TLS certificates
	//  2. Reference the secret in a volume
	//  3. Mount the volume to the container at /tls
	var poolerVolumes []v1.Volume
	var volumeMounts []v1.VolumeMount
	if spec.TLS != nil && spec.TLS.SecretName != "" {
		getPoolerTLSEnv := func(k string) string {
			keyName := ""
			switch k {
			case "tls.crt":
				keyName = "CONNECTION_POOLER_CLIENT_TLS_CRT"
			case "tls.key":
				keyName = "CONNECTION_POOLER_CLIENT_TLS_KEY"
			case "tls.ca":
				keyName = "CONNECTION_POOLER_CLIENT_CA_FILE"
			default:
				panic(fmt.Sprintf("TLS env key for pooler unknown %s", k))
			}

			return keyName
		}
		tlsEnv, tlsVolumes := generateTlsMounts(spec, getPoolerTLSEnv)
		envVars = append(envVars, tlsEnv...)
		for _, vol := range tlsVolumes {
			poolerVolumes = append(poolerVolumes, v1.Volume{
				Name:         vol.Name,
				VolumeSource: vol.VolumeSource,
			})
			volumeMounts = append(volumeMounts, v1.VolumeMount{
				Name:      vol.Name,
				MountPath: vol.MountPath,
			})
		}
	}

	poolerContainer.Env = envVars
	poolerContainer.VolumeMounts = volumeMounts
	tolerationsSpec := tolerations(&spec.Tolerations, c.OpConfig.PodToleration)
	securityContext := v1.PodSecurityContext{}

	// determine the User, Group and FSGroup for the pooler pod
	securityContext.RunAsUser = &poolerRunAsUser
	securityContext.RunAsGroup = &poolerRunAsGroup

	effectiveFSGroup := c.OpConfig.Resources.SpiloFSGroup
	if spec.SpiloFSGroup != nil {
		effectiveFSGroup = spec.SpiloFSGroup
	}
	if effectiveFSGroup != nil {
		securityContext.FSGroup = effectiveFSGroup
	}

	podTemplate := &v1.PodTemplateSpec{
		ObjectMeta: metav1.ObjectMeta{
			Labels:      c.connectionPoolerLabels(role, true).MatchLabels,
			Namespace:   c.Namespace,
			Annotations: c.annotationsSet(c.generatePodAnnotations(spec)),
		},
		Spec: v1.PodSpec{
			TerminationGracePeriodSeconds: &gracePeriod,
			Containers:                    []v1.Container{poolerContainer},
			Tolerations:                   tolerationsSpec,
			Volumes:                       poolerVolumes,
			SecurityContext:               &securityContext,
			ServiceAccountName:            c.OpConfig.PodServiceAccountName,
		},
	}

	nodeAffinity := c.nodeAffinity(c.OpConfig.NodeReadinessLabel, spec.NodeAffinity)
	if c.OpConfig.EnablePodAntiAffinity {
		labelsSet := labels.Set(c.connectionPoolerLabels(role, false).MatchLabels)
		podTemplate.Spec.Affinity = podAffinity(
			labelsSet,
			c.OpConfig.PodAntiAffinityTopologyKey,
			nodeAffinity,
			c.OpConfig.PodAntiAffinityPreferredDuringScheduling,
			true,
		)
	} else if nodeAffinity != nil {
		podTemplate.Spec.Affinity = nodeAffinity
	}

	return podTemplate, nil
}

func (c *Cluster) generateConnectionPoolerDeployment(connectionPooler *ConnectionPoolerObjects) (
	*appsv1.Deployment, error) {
	spec := &c.Spec

	// there are two ways to enable connection pooler, either to specify a
	// connectionPooler section or enableConnectionPooler. In the second case
	// spec.connectionPooler will be nil, so to make it easier to calculate
	// default values, initialize it to an empty structure. It could be done
	// anywhere, but here is the earliest common entry point between sync and
	// create code, so init here.
	connectionPoolerSpec := spec.ConnectionPooler
	if connectionPoolerSpec == nil {
		connectionPoolerSpec = &acidv1.ConnectionPooler{}
	}
	podTemplate, err := c.generateConnectionPoolerPodTemplate(connectionPooler.Role)

	numberOfInstances := connectionPoolerSpec.NumberOfInstances
	if numberOfInstances == nil {
		numberOfInstances = util.CoalesceInt32(
			c.OpConfig.ConnectionPooler.NumberOfInstances,
			k8sutil.Int32ToPointer(1))
	}

	if *numberOfInstances < constants.ConnectionPoolerMinInstances {
		msg := "adjusted number of connection pooler instances from %d to %d"
		c.logger.Warningf(msg, *numberOfInstances, constants.ConnectionPoolerMinInstances)

		*numberOfInstances = constants.ConnectionPoolerMinInstances
	}

	if err != nil {
		return nil, err
	}

	deployment := &appsv1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:        connectionPooler.Name,
			Namespace:   connectionPooler.Namespace,
			Labels:      c.connectionPoolerLabels(connectionPooler.Role, true).MatchLabels,
			Annotations: c.AnnotationsToPropagate(c.annotationsSet(nil)),
			// make StatefulSet object its owner to represent the dependency.
			// By itself StatefulSet is being deleted with "Orphaned"
			// propagation policy, which means that it's deletion will not
			// clean up this deployment, but there is a hope that this object
			// will be garbage collected if something went wrong and operator
			// didn't deleted it.
			OwnerReferences: c.ownerReferences(),
		},
		Spec: appsv1.DeploymentSpec{
			Replicas: numberOfInstances,
			Selector: c.connectionPoolerLabels(connectionPooler.Role, false),
			Template: *podTemplate,
		},
	}

	return deployment, nil
}

func (c *Cluster) generateConnectionPoolerService(connectionPooler *ConnectionPoolerObjects) *v1.Service {
	spec := &c.Spec
	poolerRole := connectionPooler.Role
	serviceSpec := v1.ServiceSpec{
		Ports: []v1.ServicePort{
			{
				Name:       connectionPooler.Name,
				Port:       pgPort,
				TargetPort: intstr.IntOrString{IntVal: c.servicePort(poolerRole)},
			},
		},
		Type: v1.ServiceTypeClusterIP,
		Selector: map[string]string{
			"connection-pooler": c.connectionPoolerName(poolerRole),
		},
	}

	if c.shouldCreateLoadBalancerForPoolerService(poolerRole, spec) {
		c.configureLoadBalanceService(&serviceSpec, spec.AllowedSourceRanges)
	}

	service := &v1.Service{
		ObjectMeta: metav1.ObjectMeta{
			Name:        connectionPooler.Name,
			Namespace:   connectionPooler.Namespace,
			Labels:      c.connectionPoolerLabels(connectionPooler.Role, false).MatchLabels,
			Annotations: c.annotationsSet(c.generatePoolerServiceAnnotations(poolerRole, spec)),
			// make StatefulSet object its owner to represent the dependency.
			// By itself StatefulSet is being deleted with "Orphaned"
			// propagation policy, which means that it's deletion will not
			// clean up this service, but there is a hope that this object will
			// be garbage collected if something went wrong and operator didn't
			// deleted it.
			OwnerReferences: c.ownerReferences(),
		},
		Spec: serviceSpec,
	}

	return service
}

func (c *Cluster) generatePoolerServiceAnnotations(role PostgresRole, spec *acidv1.PostgresSpec) map[string]string {
	var dnsString string
	annotations := c.getCustomServiceAnnotations(role, spec)

	if c.shouldCreateLoadBalancerForPoolerService(role, spec) {
		// set ELB Timeout annotation with default value
		if _, ok := annotations[constants.ElbTimeoutAnnotationName]; !ok {
			annotations[constants.ElbTimeoutAnnotationName] = constants.ElbTimeoutAnnotationValue
		}
		// -repl suffix will be added by replicaDNSName
		clusterNameWithPoolerSuffix := c.connectionPoolerName(Master)
		if role == Master {
			dnsString = c.masterDNSName(clusterNameWithPoolerSuffix)
		} else {
			dnsString = c.replicaDNSName(clusterNameWithPoolerSuffix)
		}
		annotations[constants.ZalandoDNSNameAnnotation] = dnsString
	}

	if len(annotations) == 0 {
		return nil
	}

	return annotations
}

func (c *Cluster) shouldCreateLoadBalancerForPoolerService(role PostgresRole, spec *acidv1.PostgresSpec) bool {

	switch role {

	case Replica:
		// if the value is explicitly set in a Postgresql manifest, follow this setting
		if spec.EnableReplicaPoolerLoadBalancer != nil {
			return *spec.EnableReplicaPoolerLoadBalancer
		}
		// otherwise, follow the operator configuration
		return c.OpConfig.EnableReplicaPoolerLoadBalancer

	case Master:
		if spec.EnableMasterPoolerLoadBalancer != nil {
			return *spec.EnableMasterPoolerLoadBalancer
		}
		return c.OpConfig.EnableMasterPoolerLoadBalancer

	default:
		panic(fmt.Sprintf("Unknown role %v", role))
	}
}

func (c *Cluster) listPoolerPods(listOptions metav1.ListOptions) ([]v1.Pod, error) {
	pods, err := c.KubeClient.Pods(c.Namespace).List(context.TODO(), listOptions)
	if err != nil {
		return nil, fmt.Errorf("could not get list of pooler pods: %v", err)
	}
	return pods.Items, nil
}

// delete connection pooler
func (c *Cluster) deleteConnectionPooler(role PostgresRole) (err error) {
	c.logger.Infof("deleting connection pooler spilo-role=%s", role)

	// Lack of connection pooler objects is not a fatal error, just log it if
	// it was present before in the manifest
	if c.ConnectionPooler[role] == nil || role == "" {
		c.logger.Debug("no connection pooler to delete")
		return nil
	}

	// Clean up the deployment object. If deployment resource we've remembered
	// is somehow empty, try to delete based on what would we generate
	deployment := c.ConnectionPooler[role].Deployment
	policy := metav1.DeletePropagationForeground
	options := metav1.DeleteOptions{PropagationPolicy: &policy}

	if deployment != nil {

		// set delete propagation policy to foreground, so that replica set will be
		// also deleted.

		err = c.KubeClient.
			Deployments(c.Namespace).
			Delete(context.TODO(), deployment.Name, options)

		if k8sutil.ResourceNotFound(err) {
			c.logger.Debugf("connection pooler deployment %s for role %s has already been deleted", deployment.Name, role)
		} else if err != nil {
			return fmt.Errorf("could not delete connection pooler deployment: %v", err)
		}

		c.logger.Infof("connection pooler deployment %s has been deleted for role %s", deployment.Name, role)
	}

	// Repeat the same for the service object
	service := c.ConnectionPooler[role].Service
	if service == nil {
		c.logger.Debug("no connection pooler service object to delete")
	} else {

		err = c.KubeClient.
			Services(c.Namespace).
			Delete(context.TODO(), service.Name, options)

		if k8sutil.ResourceNotFound(err) {
			c.logger.Debugf("connection pooler service %s for role %s has already been already deleted", service.Name, role)
		} else if err != nil {
			return fmt.Errorf("could not delete connection pooler service: %v", err)
		}

		c.logger.Infof("connection pooler service %s has been deleted for role %s", service.Name, role)
	}

	c.ConnectionPooler[role].Deployment = nil
	c.ConnectionPooler[role].Service = nil
	return nil
}

// delete connection pooler
func (c *Cluster) deleteConnectionPoolerSecret() (err error) {
	// Repeat the same for the secret object
	secretName := c.credentialSecretName(c.OpConfig.ConnectionPooler.User)

	secret, err := c.KubeClient.
		Secrets(c.Namespace).
		Get(context.TODO(), secretName, metav1.GetOptions{})

	if err != nil {
		c.logger.Debugf("could not get connection pooler secret %s: %v", secretName, err)
	} else {
		if err = c.deleteSecret(secret.UID); err != nil {
			return fmt.Errorf("could not delete pooler secret: %v", err)
		}
	}
	return nil
}

// Perform actual patching of a connection pooler deployment, assuming that all
// the check were already done before.
func updateConnectionPoolerDeployment(KubeClient k8sutil.KubernetesClient, newDeployment *appsv1.Deployment, doUpdate bool) (*appsv1.Deployment, error) {
	if newDeployment == nil {
		return nil, fmt.Errorf("there is no connection pooler in the cluster")
	}

	if doUpdate {
		updatedDeployment, err := KubeClient.Deployments(newDeployment.Namespace).Update(context.TODO(), newDeployment, metav1.UpdateOptions{})
		if err != nil {
			return nil, fmt.Errorf("could not update pooler deployment to match desired state: %v", err)
		}
		return updatedDeployment, nil
	}

	patchData, err := specPatch(newDeployment.Spec)
	if err != nil {
		return nil, fmt.Errorf("could not form patch for the connection pooler deployment: %v", err)
	}

	// An update probably requires RetryOnConflict, but since only one operator
	// worker at one time will try to update it chances of conflicts are
	// minimal.
	deployment, err := KubeClient.
		Deployments(newDeployment.Namespace).Patch(
		context.TODO(),
		newDeployment.Name,
		types.MergePatchType,
		patchData,
		metav1.PatchOptions{},
		"")
	if err != nil {
		return nil, fmt.Errorf("could not patch connection pooler deployment: %v", err)
	}

	return deployment, nil
}

// patchConnectionPoolerAnnotations updates the annotations of connection pooler deployment
func patchConnectionPoolerAnnotations(KubeClient k8sutil.KubernetesClient, deployment *appsv1.Deployment, annotations map[string]string) (*appsv1.Deployment, error) {
	patchData, err := metaAnnotationsPatch(annotations)
	if err != nil {
		return nil, fmt.Errorf("could not form patch for the connection pooler deployment metadata: %v", err)
	}
	result, err := KubeClient.Deployments(deployment.Namespace).Patch(
		context.TODO(),
		deployment.Name,
		types.MergePatchType,
		[]byte(patchData),
		metav1.PatchOptions{},
		"")
	if err != nil {
		return nil, fmt.Errorf("could not patch connection pooler annotations %q: %v", patchData, err)
	}
	return result, nil

}

// Test if two connection pooler configuration needs to be synced. For simplicity
// compare not the actual K8S objects, but the configuration itself and request
// sync if there is any difference.
func needSyncConnectionPoolerSpecs(oldSpec, newSpec *acidv1.ConnectionPooler, logger *logrus.Entry) (sync bool, reasons []string) {
	reasons = []string{}
	sync = false

	changelog, err := diff.Diff(oldSpec, newSpec)
	if err != nil {
		logger.Infof("cannot get diff, do not do anything, %+v", err)
		return false, reasons
	}

	if len(changelog) > 0 {
		sync = true
	}

	for _, change := range changelog {
		msg := fmt.Sprintf("%s %+v from '%+v' to '%+v'",
			change.Type, change.Path, change.From, change.To)
		reasons = append(reasons, msg)
	}

	return sync, reasons
}

// Check if we need to synchronize connection pooler deployment due to new
// defaults, that are different from what we see in the DeploymentSpec
func (c *Cluster) needSyncConnectionPoolerDefaults(Config *Config, spec *acidv1.ConnectionPooler, deployment *appsv1.Deployment) (sync bool, reasons []string) {

	reasons = []string{}
	sync = false

	config := Config.OpConfig.ConnectionPooler
	podTemplate := deployment.Spec.Template
	poolerContainer := podTemplate.Spec.Containers[constants.ConnectionPoolerContainer]

	if spec == nil {
		spec = &acidv1.ConnectionPooler{}
	}

	if spec.NumberOfInstances == nil &&
		*deployment.Spec.Replicas != *config.NumberOfInstances {

		sync = true
		msg := fmt.Sprintf("numberOfInstances is different (having %d, required %d)",
			*deployment.Spec.Replicas, *config.NumberOfInstances)
		reasons = append(reasons, msg)
	}

	if spec.DockerImage == "" &&
		poolerContainer.Image != config.Image {

		sync = true
		msg := fmt.Sprintf("dockerImage is different (having %s, required %s)",
			poolerContainer.Image, config.Image)
		reasons = append(reasons, msg)
	}

	expectedResources, err := c.generateResourceRequirements(spec.Resources,
		makeDefaultConnectionPoolerResources(&Config.OpConfig),
		connectionPoolerContainer)

	// An error to generate expected resources means something is not quite
	// right, but for the purpose of robustness do not panic here, just report
	// and ignore resources comparison (in the worst case there will be no
	// updates for new resource values).
	if err == nil && syncResources(&poolerContainer.Resources, expectedResources) {
		sync = true
		msg := fmt.Sprintf("resources are different (having %+v, required %+v)",
			poolerContainer.Resources, expectedResources)
		reasons = append(reasons, msg)
	}

	if err != nil {
		return false, reasons
	}

	for _, env := range poolerContainer.Env {
		if spec.User == "" && env.Name == "PGUSER" {
			ref := env.ValueFrom.SecretKeyRef.LocalObjectReference
			secretName := Config.OpConfig.SecretNameTemplate.Format(
				"username", strings.Replace(config.User, "_", "-", -1),
				"cluster", c.Name,
				"tprkind", acidv1.PostgresCRDResourceKind,
				"tprgroup", acidzalando.GroupName)

			if ref.Name != secretName {
				sync = true
				msg := fmt.Sprintf("pooler user and secret are different (having %s, required %s)",
					ref.Name, secretName)
				reasons = append(reasons, msg)
			}
		}

		if spec.Schema == "" && env.Name == "PGSCHEMA" && env.Value != config.Schema {
			sync = true
			msg := fmt.Sprintf("pooler schema is different (having %s, required %s)",
				env.Value, config.Schema)
			reasons = append(reasons, msg)
		}
	}

	return sync, reasons
}

// Generate default resource section for connection pooler deployment, to be
// used if nothing custom is specified in the manifest
func makeDefaultConnectionPoolerResources(config *config.Config) acidv1.Resources {

	defaultRequests := acidv1.ResourceDescription{
		CPU:    &config.ConnectionPooler.ConnectionPoolerDefaultCPURequest,
		Memory: &config.ConnectionPooler.ConnectionPoolerDefaultMemoryRequest,
	}
	defaultLimits := acidv1.ResourceDescription{
		CPU:    &config.ConnectionPooler.ConnectionPoolerDefaultCPULimit,
		Memory: &config.ConnectionPooler.ConnectionPoolerDefaultMemoryLimit,
	}

	return acidv1.Resources{
		ResourceRequests: defaultRequests,
		ResourceLimits:   defaultLimits,
	}
}

func logPoolerEssentials(log *logrus.Entry, oldSpec, newSpec *acidv1.Postgresql) {
	var v []string
	var input []*bool

	newMasterConnectionPoolerEnabled := needMasterConnectionPoolerWorker(&newSpec.Spec)
	if oldSpec == nil {
		input = []*bool{nil, nil, &newMasterConnectionPoolerEnabled, newSpec.Spec.EnableReplicaConnectionPooler}
	} else {
		oldMasterConnectionPoolerEnabled := needMasterConnectionPoolerWorker(&oldSpec.Spec)
		input = []*bool{&oldMasterConnectionPoolerEnabled, oldSpec.Spec.EnableReplicaConnectionPooler, &newMasterConnectionPoolerEnabled, newSpec.Spec.EnableReplicaConnectionPooler}
	}

	for _, b := range input {
		if b == nil {
			v = append(v, "nil")
		} else {
			v = append(v, fmt.Sprintf("%v", *b))
		}
	}

	log.Debugf("syncing connection pooler (master, replica) from (%v, %v) to (%v, %v)", v[0], v[1], v[2], v[3])
}

func (c *Cluster) syncConnectionPooler(oldSpec, newSpec *acidv1.Postgresql, LookupFunction InstallFunction) (SyncReason, error) {

	var reason SyncReason
	var err error
	var connectionPoolerNeeded bool

	logPoolerEssentials(c.logger, oldSpec, newSpec)

	// Check and perform the sync requirements for each of the roles.
	for _, role := range [2]PostgresRole{Master, Replica} {

		if role == Master {
			connectionPoolerNeeded = needMasterConnectionPoolerWorker(&newSpec.Spec)
		} else {
			connectionPoolerNeeded = needReplicaConnectionPoolerWorker(&newSpec.Spec)
		}

		// if the call is via createConnectionPooler, then it is required to initialize
		// the structure
		if c.ConnectionPooler == nil {
			c.ConnectionPooler = map[PostgresRole]*ConnectionPoolerObjects{}
		}
		if c.ConnectionPooler[role] == nil {
			c.ConnectionPooler[role] = &ConnectionPoolerObjects{
				Deployment:     nil,
				Service:        nil,
				Name:           c.connectionPoolerName(role),
				ClusterName:    c.Name,
				Namespace:      c.Namespace,
				LookupFunction: false,
				Role:           role,
			}
		}

		if connectionPoolerNeeded {
			// Try to sync in any case. If we didn't needed connection pooler before,
			// it means we want to create it. If it was already present, still sync
			// since it could happen that there is no difference in specs, and all
			// the resources are remembered, but the deployment was manually deleted
			// in between

			// in this case also do not forget to install lookup function
			// skip installation in standby clusters, since they are read-only
			if !c.ConnectionPooler[role].LookupFunction && c.Spec.StandbyCluster == nil {
				connectionPooler := c.Spec.ConnectionPooler
				specSchema := ""
				specUser := ""

				if connectionPooler != nil {
					specSchema = connectionPooler.Schema
					specUser = connectionPooler.User
				}

				schema := util.Coalesce(
					specSchema,
					c.OpConfig.ConnectionPooler.Schema)

				user := util.Coalesce(
					specUser,
					c.OpConfig.ConnectionPooler.User)

				if err = LookupFunction(schema, user); err != nil {
					return NoSync, err
				}
				c.ConnectionPooler[role].LookupFunction = true
			}

			if reason, err = c.syncConnectionPoolerWorker(oldSpec, newSpec, role); err != nil {
				c.logger.Errorf("could not sync connection pooler: %v", err)
				return reason, err
			}
		} else {
			// delete and cleanup resources if they are already detected
			if c.ConnectionPooler[role] != nil &&
				(c.ConnectionPooler[role].Deployment != nil ||
					c.ConnectionPooler[role].Service != nil) {

				if err = c.deleteConnectionPooler(role); err != nil {
					c.logger.Warningf("could not remove connection pooler: %v", err)
				}
			}
		}
	}
	if (needMasterConnectionPoolerWorker(&oldSpec.Spec) || needReplicaConnectionPoolerWorker(&oldSpec.Spec)) &&
		!needMasterConnectionPoolerWorker(&newSpec.Spec) && !needReplicaConnectionPoolerWorker(&newSpec.Spec) {
		if err = c.deleteConnectionPoolerSecret(); err != nil {
			c.logger.Warningf("could not remove connection pooler secret: %v", err)
		}
	}

	return reason, nil
}

// Synchronize connection pooler resources. Effectively we're interested only in
// synchronizing the corresponding deployment, but in case of deployment or
// service is missing, create it. After checking, also remember an object for
// the future references.
func (c *Cluster) syncConnectionPoolerWorker(oldSpec, newSpec *acidv1.Postgresql, role PostgresRole) (
	SyncReason, error) {

	var (
		deployment    *appsv1.Deployment
		newDeployment *appsv1.Deployment
		pods          []v1.Pod
		service       *v1.Service
		newService    *v1.Service
		err           error
	)

	updatedPodAnnotations := map[string]*string{}
	syncReason := make([]string, 0)
	deployment, err = c.KubeClient.
		Deployments(c.Namespace).
		Get(context.TODO(), c.connectionPoolerName(role), metav1.GetOptions{})

	if err != nil && k8sutil.ResourceNotFound(err) {
		c.logger.Warningf("deployment %s for connection pooler synchronization is not found, create it", c.connectionPoolerName(role))

		newDeployment, err = c.generateConnectionPoolerDeployment(c.ConnectionPooler[role])
		if err != nil {
			return NoSync, fmt.Errorf("could not generate deployment for connection pooler: %v", err)
		}

		deployment, err = c.KubeClient.
			Deployments(newDeployment.Namespace).
			Create(context.TODO(), newDeployment, metav1.CreateOptions{})

		if err != nil {
			return NoSync, err
		}
		c.ConnectionPooler[role].Deployment = deployment
	} else if err != nil {
		return NoSync, fmt.Errorf("could not get connection pooler deployment to sync: %v", err)
	} else {
		c.ConnectionPooler[role].Deployment = deployment
		// actual synchronization

		var oldConnectionPooler *acidv1.ConnectionPooler

		if oldSpec != nil {
			oldConnectionPooler = oldSpec.Spec.ConnectionPooler
		}

		newConnectionPooler := newSpec.Spec.ConnectionPooler
		// sync implementation below assumes that both old and new specs are
		// not nil, but it can happen. To avoid any confusion like updating a
		// deployment because the specification changed from nil to an empty
		// struct (that was initialized somewhere before) replace any nil with
		// an empty spec.
		if oldConnectionPooler == nil {
			oldConnectionPooler = &acidv1.ConnectionPooler{}
		}

		if newConnectionPooler == nil {
			newConnectionPooler = &acidv1.ConnectionPooler{}
		}

		var specSync, updateDeployment bool
		var specReason []string

		if !reflect.DeepEqual(deployment.ObjectMeta.OwnerReferences, c.ownerReferences()) {
			c.logger.Info("new connection pooler owner references do not match the current ones")
			updateDeployment = true
		}

		if oldSpec != nil {
			specSync, specReason = needSyncConnectionPoolerSpecs(oldConnectionPooler, newConnectionPooler, c.logger)
			syncReason = append(syncReason, specReason...)
		}

		newPodAnnotations := c.annotationsSet(c.generatePodAnnotations(&c.Spec))
		deletedPodAnnotations := []string{}
		if changed, reason := c.compareAnnotations(deployment.Spec.Template.Annotations, newPodAnnotations, &deletedPodAnnotations); changed {
			specSync = true
			syncReason = append(syncReason, []string{"new connection pooler's pod template annotations do not match the current ones: " + reason}...)

			for _, anno := range deletedPodAnnotations {
				updatedPodAnnotations[anno] = nil
			}
			templateMetadataReq := map[string]map[string]map[string]map[string]map[string]*string{
				"spec": {"template": {"metadata": {"annotations": updatedPodAnnotations}}}}
			patch, err := json.Marshal(templateMetadataReq)
			if err != nil {
				return nil, fmt.Errorf("could not marshal ObjectMeta for %s connection pooler's pod template: %v", role, err)
			}
			deployment, err = c.KubeClient.Deployments(c.Namespace).Patch(context.TODO(),
				deployment.Name, types.StrategicMergePatchType, patch, metav1.PatchOptions{}, "")
			if err != nil {
				c.logger.Errorf("failed to patch %s connection pooler's pod template: %v", role, err)
				return nil, err
			}

			deployment.Spec.Template.Annotations = newPodAnnotations
		}

		defaultsSync, defaultsReason := c.needSyncConnectionPoolerDefaults(&c.Config, newConnectionPooler, deployment)
		syncReason = append(syncReason, defaultsReason...)

		if specSync || defaultsSync || updateDeployment {
			c.logger.Infof("update connection pooler deployment %s, reason: %+v",
				c.connectionPoolerName(role), syncReason)
			newDeployment, err = c.generateConnectionPoolerDeployment(c.ConnectionPooler[role])
			if err != nil {
				return syncReason, fmt.Errorf("could not generate deployment for connection pooler: %v", err)
			}

			deployment, err = updateConnectionPoolerDeployment(c.KubeClient, newDeployment, updateDeployment)

			if err != nil {
				return syncReason, err
			}
			c.ConnectionPooler[role].Deployment = deployment
		}

		newAnnotations := c.AnnotationsToPropagate(c.annotationsSet(nil)) // including the downscaling annotations
		if changed, _ := c.compareAnnotations(deployment.Annotations, newAnnotations, nil); changed {
			deployment, err = patchConnectionPoolerAnnotations(c.KubeClient, deployment, newAnnotations)
			if err != nil {
				return nil, err
			}
			c.ConnectionPooler[role].Deployment = deployment
		}
	}

	// check if pooler pods must be replaced due to secret update
	listOptions := metav1.ListOptions{
		LabelSelector: labels.Set(c.connectionPoolerLabels(role, true).MatchLabels).String(),
	}
	pods, err = c.listPoolerPods(listOptions)
	if err != nil {
		return nil, err
	}
	for i, pod := range pods {
		if c.getRollingUpdateFlagFromPod(&pod) {
			podName := util.NameFromMeta(pods[i].ObjectMeta)
			err := retryutil.Retry(1*time.Second, 5*time.Second,
				func() (bool, error) {
					err2 := c.KubeClient.Pods(podName.Namespace).Delete(
						context.TODO(),
						podName.Name,
						c.deleteOptions)
					if err2 != nil {
						return false, err2
					}
					return true, nil
				})
			if err != nil {
				return nil, fmt.Errorf("could not delete pooler pod: %v", err)
			}
		} else if changed, _ := c.compareAnnotations(pod.Annotations, deployment.Spec.Template.Annotations, nil); changed {
			metadataReq := map[string]map[string]map[string]*string{"metadata": {}}

			for anno, val := range deployment.Spec.Template.Annotations {
				updatedPodAnnotations[anno] = &val
			}
			metadataReq["metadata"]["annotations"] = updatedPodAnnotations
			patch, err := json.Marshal(metadataReq)
			if err != nil {
				return nil, fmt.Errorf("could not marshal ObjectMeta for %s connection pooler's pods: %v", role, err)
			}
			_, err = c.KubeClient.Pods(pod.Namespace).Patch(context.TODO(), pod.Name, types.StrategicMergePatchType, patch, metav1.PatchOptions{})
			if err != nil {
				return nil, fmt.Errorf("could not patch annotations for %s connection pooler's pod %q: %v", role, pod.Name, err)
			}
		}
	}

	if service, err = c.KubeClient.Services(c.Namespace).Get(context.TODO(), c.connectionPoolerName(role), metav1.GetOptions{}); err == nil {
		c.ConnectionPooler[role].Service = service
		desiredSvc := c.generateConnectionPoolerService(c.ConnectionPooler[role])
		newService, err = c.updateService(role, service, desiredSvc)
		if err != nil {
			return syncReason, fmt.Errorf("could not update %s service to match desired state: %v", role, err)
		}
		c.ConnectionPooler[role].Service = newService
		return NoSync, nil
	}

	if !k8sutil.ResourceNotFound(err) {
		return NoSync, fmt.Errorf("could not get connection pooler service to sync: %v", err)
	}

	c.ConnectionPooler[role].Service = nil
	c.logger.Warningf("service %s for connection pooler synchronization is not found, create it", c.connectionPoolerName(role))

	serviceSpec := c.generateConnectionPoolerService(c.ConnectionPooler[role])
	newService, err = c.KubeClient.
		Services(serviceSpec.Namespace).
		Create(context.TODO(), serviceSpec, metav1.CreateOptions{})

	if err != nil {
		return NoSync, err
	}
	c.ConnectionPooler[role].Service = newService

	return NoSync, nil
}


================================================
File: pkg/cluster/connection_pooler_new_test.go
================================================
package cluster

import (
	"testing"

	"context"

	appsv1 "k8s.io/api/apps/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

	"k8s.io/apimachinery/pkg/labels"

	"k8s.io/client-go/kubernetes/fake"
)

func TestFakeClient(t *testing.T) {
	clientSet := fake.NewSimpleClientset()
	namespace := "default"

	l := labels.Set(map[string]string{
		"application": "spilo",
	})

	deployment := &appsv1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "my-deployment1",
			Namespace: namespace,
			Labels:    l,
		},
	}

	clientSet.AppsV1().Deployments(namespace).Create(context.TODO(), deployment, metav1.CreateOptions{})

	deployment2, _ := clientSet.AppsV1().Deployments(namespace).Get(context.TODO(), "my-deployment1", metav1.GetOptions{})

	if deployment.ObjectMeta.Name != deployment2.ObjectMeta.Name {
		t.Errorf("Deployments are not equal")
	}

	deployments, _ := clientSet.AppsV1().Deployments(namespace).List(context.TODO(), metav1.ListOptions{LabelSelector: "application=spilo"})

	if len(deployments.Items) != 1 {
		t.Errorf("Label search does not work")
	}
}


================================================
File: pkg/cluster/connection_pooler_test.go
================================================
package cluster

import (
	"context"
	"fmt"
	"strings"
	"testing"

	"github.com/stretchr/testify/assert"
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	fakeacidv1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/fake"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"

	appsv1 "k8s.io/api/apps/v1"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes/fake"
)

func newFakeK8sPoolerTestClient() (k8sutil.KubernetesClient, *fake.Clientset) {
	acidClientSet := fakeacidv1.NewSimpleClientset()
	clientSet := fake.NewSimpleClientset()

	return k8sutil.KubernetesClient{
		PodsGetter:         clientSet.CoreV1(),
		PostgresqlsGetter:  acidClientSet.AcidV1(),
		StatefulSetsGetter: clientSet.AppsV1(),
		DeploymentsGetter:  clientSet.AppsV1(),
		ServicesGetter:     clientSet.CoreV1(),
	}, clientSet
}

func mockInstallLookupFunction(schema string, user string) error {
	return nil
}

func boolToPointer(value bool) *bool {
	return &value
}

func deploymentUpdated(cluster *Cluster, err error, reason SyncReason) error {
	for _, role := range [2]PostgresRole{Master, Replica} {

		poolerLabels := cluster.labelsSet(false)
		poolerLabels["application"] = "db-connection-pooler"
		poolerLabels["connection-pooler"] = cluster.connectionPoolerName(role)

		if cluster.ConnectionPooler[role] != nil && cluster.ConnectionPooler[role].Deployment != nil &&
			util.MapContains(cluster.ConnectionPooler[role].Deployment.Labels, poolerLabels) &&
			(cluster.ConnectionPooler[role].Deployment.Spec.Replicas == nil ||
				*cluster.ConnectionPooler[role].Deployment.Spec.Replicas != 2) {
			return fmt.Errorf("Wrong number of instances")
		}
	}
	return nil
}

func objectsAreSaved(cluster *Cluster, err error, reason SyncReason) error {
	if cluster.ConnectionPooler == nil {
		return fmt.Errorf("Connection pooler resources are empty")
	}

	for _, role := range []PostgresRole{Master, Replica} {
		poolerLabels := cluster.labelsSet(false)
		poolerLabels["application"] = "db-connection-pooler"
		poolerLabels["connection-pooler"] = cluster.connectionPoolerName(role)

		if cluster.ConnectionPooler[role].Deployment == nil || !util.MapContains(cluster.ConnectionPooler[role].Deployment.Labels, poolerLabels) {
			return fmt.Errorf("Deployment was not saved or labels not attached %s %s", role, cluster.ConnectionPooler[role].Deployment.Labels)
		}

		if cluster.ConnectionPooler[role].Service == nil || !util.MapContains(cluster.ConnectionPooler[role].Service.Labels, poolerLabels) {
			return fmt.Errorf("Service was not saved or labels not attached %s %s", role, cluster.ConnectionPooler[role].Service.Labels)
		}
	}

	return nil
}

func MasterObjectsAreSaved(cluster *Cluster, err error, reason SyncReason) error {
	if cluster.ConnectionPooler == nil {
		return fmt.Errorf("Connection pooler resources are empty")
	}

	poolerLabels := cluster.labelsSet(false)
	poolerLabels["application"] = "db-connection-pooler"
	poolerLabels["connection-pooler"] = cluster.connectionPoolerName(Master)

	if cluster.ConnectionPooler[Master].Deployment == nil || !util.MapContains(cluster.ConnectionPooler[Master].Deployment.Labels, poolerLabels) {
		return fmt.Errorf("Deployment was not saved or labels not attached %s", cluster.ConnectionPooler[Master].Deployment.Labels)
	}

	if cluster.ConnectionPooler[Master].Service == nil || !util.MapContains(cluster.ConnectionPooler[Master].Service.Labels, poolerLabels) {
		return fmt.Errorf("Service was not saved or labels not attached %s", cluster.ConnectionPooler[Master].Service.Labels)
	}

	return nil
}

func ReplicaObjectsAreSaved(cluster *Cluster, err error, reason SyncReason) error {
	if cluster.ConnectionPooler == nil {
		return fmt.Errorf("Connection pooler resources are empty")
	}

	poolerLabels := cluster.labelsSet(false)
	poolerLabels["application"] = "db-connection-pooler"
	poolerLabels["connection-pooler"] = cluster.connectionPoolerName(Replica)

	if cluster.ConnectionPooler[Replica].Deployment == nil || !util.MapContains(cluster.ConnectionPooler[Replica].Deployment.Labels, poolerLabels) {
		return fmt.Errorf("Deployment was not saved or labels not attached %s", cluster.ConnectionPooler[Replica].Deployment.Labels)
	}

	if cluster.ConnectionPooler[Replica].Service == nil || !util.MapContains(cluster.ConnectionPooler[Replica].Service.Labels, poolerLabels) {
		return fmt.Errorf("Service was not saved or labels not attached %s", cluster.ConnectionPooler[Replica].Service.Labels)
	}

	return nil
}

func objectsAreDeleted(cluster *Cluster, err error, reason SyncReason) error {
	for _, role := range [2]PostgresRole{Master, Replica} {
		if cluster.ConnectionPooler[role] != nil &&
			(cluster.ConnectionPooler[role].Deployment != nil || cluster.ConnectionPooler[role].Service != nil) {
			return fmt.Errorf("Connection pooler was not deleted for role %v", role)
		}
	}

	return nil
}

func OnlyMasterDeleted(cluster *Cluster, err error, reason SyncReason) error {

	if cluster.ConnectionPooler[Master] != nil &&
		(cluster.ConnectionPooler[Master].Deployment != nil || cluster.ConnectionPooler[Master].Service != nil) {
		return fmt.Errorf("Connection pooler master was not deleted")
	}
	return nil
}

func OnlyReplicaDeleted(cluster *Cluster, err error, reason SyncReason) error {

	if cluster.ConnectionPooler[Replica] != nil &&
		(cluster.ConnectionPooler[Replica].Deployment != nil || cluster.ConnectionPooler[Replica].Service != nil) {
		return fmt.Errorf("Connection pooler replica was not deleted")
	}
	return nil
}

func noEmptySync(cluster *Cluster, err error, reason SyncReason) error {
	for _, msg := range reason {
		if strings.HasPrefix(msg, "update [] from '<nil>' to '") {
			return fmt.Errorf("There is an empty reason, %s", msg)
		}
	}

	return nil
}

func TestNeedConnectionPooler(t *testing.T) {
	testName := "Test how connection pooler can be enabled"
	var cluster = New(
		Config{
			OpConfig: config.Config{
				ProtectedRoles: []string{"admin"},
				Auth: config.Auth{
					SuperUsername:       superUserName,
					ReplicationUsername: replicationUserName,
				},
				ConnectionPooler: config.ConnectionPooler{
					ConnectionPoolerDefaultCPURequest:    "100m",
					ConnectionPoolerDefaultCPULimit:      "100m",
					ConnectionPoolerDefaultMemoryRequest: "100Mi",
					ConnectionPoolerDefaultMemoryLimit:   "100Mi",
				},
			},
		}, k8sutil.NewMockKubernetesClient(), acidv1.Postgresql{}, logger, eventRecorder)

	cluster.Spec = acidv1.PostgresSpec{
		ConnectionPooler: &acidv1.ConnectionPooler{},
	}

	if !needMasterConnectionPooler(&cluster.Spec) {
		t.Errorf("%s: Connection pooler is not enabled with full definition",
			testName)
	}

	cluster.Spec = acidv1.PostgresSpec{
		EnableConnectionPooler: boolToPointer(true),
	}

	if !needMasterConnectionPooler(&cluster.Spec) {
		t.Errorf("%s: Connection pooler is not enabled with flag",
			testName)
	}

	cluster.Spec = acidv1.PostgresSpec{
		EnableConnectionPooler: boolToPointer(false),
		ConnectionPooler:       &acidv1.ConnectionPooler{},
	}

	if needMasterConnectionPooler(&cluster.Spec) {
		t.Errorf("%s: Connection pooler is still enabled with flag being false",
			testName)
	}

	cluster.Spec = acidv1.PostgresSpec{
		EnableConnectionPooler: boolToPointer(true),
		ConnectionPooler:       &acidv1.ConnectionPooler{},
	}

	if !needMasterConnectionPooler(&cluster.Spec) {
		t.Errorf("%s: Connection pooler is not enabled with flag and full",
			testName)
	}

	cluster.Spec = acidv1.PostgresSpec{
		EnableConnectionPooler:        boolToPointer(false),
		EnableReplicaConnectionPooler: boolToPointer(false),
		ConnectionPooler:              nil,
	}

	if needMasterConnectionPooler(&cluster.Spec) {
		t.Errorf("%s: Connection pooler is enabled with flag false and nil",
			testName)
	}

	// Test for replica connection pooler
	cluster.Spec = acidv1.PostgresSpec{
		ConnectionPooler: &acidv1.ConnectionPooler{},
	}

	if needReplicaConnectionPooler(&cluster.Spec) {
		t.Errorf("%s: Replica Connection pooler is not enabled with full definition",
			testName)
	}

	cluster.Spec = acidv1.PostgresSpec{
		EnableReplicaConnectionPooler: boolToPointer(true),
	}

	if !needReplicaConnectionPooler(&cluster.Spec) {
		t.Errorf("%s: Replica Connection pooler is not enabled with flag",
			testName)
	}

	cluster.Spec = acidv1.PostgresSpec{
		EnableReplicaConnectionPooler: boolToPointer(false),
		ConnectionPooler:              &acidv1.ConnectionPooler{},
	}

	if needReplicaConnectionPooler(&cluster.Spec) {
		t.Errorf("%s: Replica Connection pooler is still enabled with flag being false",
			testName)
	}

	cluster.Spec = acidv1.PostgresSpec{
		EnableReplicaConnectionPooler: boolToPointer(true),
		ConnectionPooler:              &acidv1.ConnectionPooler{},
	}

	if !needReplicaConnectionPooler(&cluster.Spec) {
		t.Errorf("%s: Replica Connection pooler is not enabled with flag and full",
			testName)
	}
}

func TestConnectionPoolerCreateDeletion(t *testing.T) {

	testName := "test connection pooler creation and deletion"
	clientSet := fake.NewSimpleClientset()
	acidClientSet := fakeacidv1.NewSimpleClientset()
	namespace := "default"

	client := k8sutil.KubernetesClient{
		StatefulSetsGetter: clientSet.AppsV1(),
		ServicesGetter:     clientSet.CoreV1(),
		PodsGetter:         clientSet.CoreV1(),
		DeploymentsGetter:  clientSet.AppsV1(),
		PostgresqlsGetter:  acidClientSet.AcidV1(),
		SecretsGetter:      clientSet.CoreV1(),
	}

	pg := acidv1.Postgresql{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "acid-fake-cluster",
			Namespace: namespace,
		},
		Spec: acidv1.PostgresSpec{
			EnableConnectionPooler:        boolToPointer(true),
			EnableReplicaConnectionPooler: boolToPointer(true),
			Volume: acidv1.Volume{
				Size: "1Gi",
			},
		},
	}

	var cluster = New(
		Config{
			OpConfig: config.Config{
				ConnectionPooler: config.ConnectionPooler{
					ConnectionPoolerDefaultCPURequest:    "100m",
					ConnectionPoolerDefaultCPULimit:      "100m",
					ConnectionPoolerDefaultMemoryRequest: "100Mi",
					ConnectionPoolerDefaultMemoryLimit:   "100Mi",
					NumberOfInstances:                    k8sutil.Int32ToPointer(1),
				},
				PodManagementPolicy: "ordered_ready",
				Resources: config.Resources{
					ClusterLabels:        map[string]string{"application": "spilo"},
					ClusterNameLabel:     "cluster-name",
					DefaultCPURequest:    "300m",
					DefaultCPULimit:      "300m",
					DefaultMemoryRequest: "300Mi",
					DefaultMemoryLimit:   "300Mi",
					PodRoleLabel:         "spilo-role",
				},
			},
		}, client, pg, logger, eventRecorder)

	cluster.Name = "acid-fake-cluster"
	cluster.Namespace = "default"

	_, err := cluster.createService(Master)
	assert.NoError(t, err)
	_, err = cluster.createStatefulSet()
	assert.NoError(t, err)

	reason, err := cluster.createConnectionPooler(mockInstallLookupFunction)

	if err != nil {
		t.Errorf("%s: Cannot create connection pooler, %s, %+v",
			testName, err, reason)
	}
	for _, role := range [2]PostgresRole{Master, Replica} {
		poolerLabels := cluster.labelsSet(false)
		poolerLabels["application"] = "db-connection-pooler"
		poolerLabels["connection-pooler"] = cluster.connectionPoolerName(role)

		if cluster.ConnectionPooler[role] != nil {
			if cluster.ConnectionPooler[role].Deployment == nil && util.MapContains(cluster.ConnectionPooler[role].Deployment.Labels, poolerLabels) {
				t.Errorf("%s: Connection pooler deployment is empty for role %s", testName, role)
			}

			if cluster.ConnectionPooler[role].Service == nil && util.MapContains(cluster.ConnectionPooler[role].Service.Labels, poolerLabels) {
				t.Errorf("%s: Connection pooler service is empty for role %s", testName, role)
			}
		}
	}

	oldSpec := &acidv1.Postgresql{
		Spec: acidv1.PostgresSpec{
			EnableConnectionPooler:        boolToPointer(true),
			EnableReplicaConnectionPooler: boolToPointer(true),
		},
	}
	newSpec := &acidv1.Postgresql{
		Spec: acidv1.PostgresSpec{
			EnableConnectionPooler:        boolToPointer(false),
			EnableReplicaConnectionPooler: boolToPointer(false),
		},
	}

	// Delete connection pooler via sync
	_, err = cluster.syncConnectionPooler(oldSpec, newSpec, mockInstallLookupFunction)
	if err != nil {
		t.Errorf("%s: Cannot sync connection pooler, %s", testName, err)
	}

	for _, role := range [2]PostgresRole{Master, Replica} {
		err = cluster.deleteConnectionPooler(role)
		if err != nil {
			t.Errorf("%s: Cannot delete connection pooler, %s", testName, err)
		}
	}
}

func TestConnectionPoolerSync(t *testing.T) {

	testName := "test connection pooler synchronization"
	clientSet := fake.NewSimpleClientset()
	acidClientSet := fakeacidv1.NewSimpleClientset()
	namespace := "default"

	client := k8sutil.KubernetesClient{
		StatefulSetsGetter: clientSet.AppsV1(),
		ServicesGetter:     clientSet.CoreV1(),
		PodsGetter:         clientSet.CoreV1(),
		DeploymentsGetter:  clientSet.AppsV1(),
		PostgresqlsGetter:  acidClientSet.AcidV1(),
		SecretsGetter:      clientSet.CoreV1(),
	}

	pg := acidv1.Postgresql{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "acid-fake-cluster",
			Namespace: namespace,
		},
		Spec: acidv1.PostgresSpec{
			Volume: acidv1.Volume{
				Size: "1Gi",
			},
		},
	}

	var cluster = New(
		Config{
			OpConfig: config.Config{
				ConnectionPooler: config.ConnectionPooler{
					ConnectionPoolerDefaultCPURequest:    "100m",
					ConnectionPoolerDefaultCPULimit:      "100m",
					ConnectionPoolerDefaultMemoryRequest: "100Mi",
					ConnectionPoolerDefaultMemoryLimit:   "100Mi",
					NumberOfInstances:                    k8sutil.Int32ToPointer(1),
				},
				PodManagementPolicy: "ordered_ready",
				Resources: config.Resources{
					ClusterLabels:        map[string]string{"application": "spilo"},
					ClusterNameLabel:     "cluster-name",
					DefaultCPURequest:    "300m",
					DefaultCPULimit:      "300m",
					DefaultMemoryRequest: "300Mi",
					DefaultMemoryLimit:   "300Mi",
					PodRoleLabel:         "spilo-role",
				},
			},
		}, client, pg, logger, eventRecorder)

	cluster.Name = "acid-fake-cluster"
	cluster.Namespace = "default"

	_, err := cluster.createService(Master)
	assert.NoError(t, err)
	_, err = cluster.createStatefulSet()
	assert.NoError(t, err)

	reason, err := cluster.createConnectionPooler(mockInstallLookupFunction)

	if err != nil {
		t.Errorf("%s: Cannot create connection pooler, %s, %+v",
			testName, err, reason)
	}

	tests := []struct {
		subTest          string
		oldSpec          *acidv1.Postgresql
		newSpec          *acidv1.Postgresql
		cluster          *Cluster
		defaultImage     string
		defaultInstances int32
		check            func(cluster *Cluster, err error, reason SyncReason) error
	}{
		{
			subTest: "create from scratch",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler: &acidv1.ConnectionPooler{},
				},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            MasterObjectsAreSaved,
		},
		{
			subTest: "create if doesn't exist",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler: &acidv1.ConnectionPooler{},
				},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler: &acidv1.ConnectionPooler{},
				},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            MasterObjectsAreSaved,
		},
		{
			subTest: "create if doesn't exist with a flag",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					EnableConnectionPooler: boolToPointer(true),
				},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            MasterObjectsAreSaved,
		},
		{
			subTest: "create no replica with flag",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					EnableReplicaConnectionPooler: boolToPointer(false),
				},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            objectsAreDeleted,
		},
		{
			subTest: "create replica if doesn't exist with a flag",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler:              &acidv1.ConnectionPooler{},
					EnableReplicaConnectionPooler: boolToPointer(true),
				},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            ReplicaObjectsAreSaved,
		},
		{
			subTest: "create both master and replica",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler:              &acidv1.ConnectionPooler{},
					EnableReplicaConnectionPooler: boolToPointer(true),
					EnableConnectionPooler:        boolToPointer(true),
				},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            objectsAreSaved,
		},
		{
			subTest: "delete only replica if not needed",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler:              &acidv1.ConnectionPooler{},
					EnableReplicaConnectionPooler: boolToPointer(true),
				},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler: &acidv1.ConnectionPooler{},
				},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            OnlyReplicaDeleted,
		},
		{
			subTest: "delete only master if not needed",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler:       &acidv1.ConnectionPooler{},
					EnableConnectionPooler: boolToPointer(true),
				},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					EnableReplicaConnectionPooler: boolToPointer(true),
				},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            OnlyMasterDeleted,
		},
		{
			subTest: "delete if not needed",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler: &acidv1.ConnectionPooler{},
				},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            objectsAreDeleted,
		},
		{
			subTest: "cleanup if still there",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            objectsAreDeleted,
		},
		{
			subTest: "update image from changed defaults",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler: &acidv1.ConnectionPooler{},
				},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					ConnectionPooler: &acidv1.ConnectionPooler{},
				},
			},
			cluster:          cluster,
			defaultImage:     "pooler:2.0",
			defaultInstances: 2,
			check:            deploymentUpdated,
		},
		{
			subTest: "there is no sync from nil to an empty spec",
			oldSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					EnableConnectionPooler: boolToPointer(true),
					ConnectionPooler:       nil,
				},
			},
			newSpec: &acidv1.Postgresql{
				Spec: acidv1.PostgresSpec{
					EnableConnectionPooler: boolToPointer(true),
					ConnectionPooler:       &acidv1.ConnectionPooler{},
				},
			},
			cluster:          cluster,
			defaultImage:     "pooler:1.0",
			defaultInstances: 1,
			check:            noEmptySync,
		},
	}
	for _, tt := range tests {
		tt.cluster.OpConfig.ConnectionPooler.Image = tt.defaultImage
		tt.cluster.OpConfig.ConnectionPooler.NumberOfInstances =
			k8sutil.Int32ToPointer(tt.defaultInstances)

		t.Logf("running test for %s [%s]", testName, tt.subTest)

		reason, err := tt.cluster.syncConnectionPooler(tt.oldSpec,
			tt.newSpec, mockInstallLookupFunction)

		if err := tt.check(tt.cluster, err, reason); err != nil {
			t.Errorf("%s [%s]: Could not synchronize, %+v",
				testName, tt.subTest, err)
		}
	}
}

func TestConnectionPoolerPodSpec(t *testing.T) {
	testName := "Test connection pooler pod template generation"
	var cluster = New(
		Config{
			OpConfig: config.Config{
				ProtectedRoles: []string{"admin"},
				Auth: config.Auth{
					SuperUsername:       superUserName,
					ReplicationUsername: replicationUserName,
				},
				PodServiceAccountName: "postgres-pod",
				ConnectionPooler: config.ConnectionPooler{
					MaxDBConnections:                     k8sutil.Int32ToPointer(60),
					ConnectionPoolerDefaultCPURequest:    "100m",
					ConnectionPoolerDefaultCPULimit:      "100m",
					ConnectionPoolerDefaultMemoryRequest: "100Mi",
					ConnectionPoolerDefaultMemoryLimit:   "100Mi",
				},
			},
		}, k8sutil.KubernetesClient{}, acidv1.Postgresql{}, logger, eventRecorder)

	cluster.Spec = acidv1.PostgresSpec{
		ConnectionPooler:              &acidv1.ConnectionPooler{},
		EnableReplicaConnectionPooler: boolToPointer(true),
	}
	var clusterNoDefaultRes = New(
		Config{
			OpConfig: config.Config{
				ProtectedRoles: []string{"admin"},
				Auth: config.Auth{
					SuperUsername:       superUserName,
					ReplicationUsername: replicationUserName,
				},
				ConnectionPooler: config.ConnectionPooler{},
			},
		}, k8sutil.KubernetesClient{}, acidv1.Postgresql{}, logger, eventRecorder)

	clusterNoDefaultRes.Spec = acidv1.PostgresSpec{
		ConnectionPooler:              &acidv1.ConnectionPooler{},
		EnableReplicaConnectionPooler: boolToPointer(true),
	}

	noCheck := func(cluster *Cluster, podSpec *v1.PodTemplateSpec, role PostgresRole) error { return nil }

	tests := []struct {
		subTest string
		spec    *acidv1.PostgresSpec
		cluster *Cluster
		check   func(cluster *Cluster, podSpec *v1.PodTemplateSpec, role PostgresRole) error
	}{
		{
			subTest: "default configuration",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler: &acidv1.ConnectionPooler{},
			},
			cluster: cluster,
			check:   noCheck,
		},
		{
			subTest: "pooler uses pod service account",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler: &acidv1.ConnectionPooler{},
			},
			cluster: cluster,
			check:   testServiceAccount,
		},
		{
			subTest: "no default resources",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler: &acidv1.ConnectionPooler{},
			},
			cluster: clusterNoDefaultRes,
			check:   noCheck,
		},
		{
			subTest: "default resources are set",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler: &acidv1.ConnectionPooler{},
			},
			cluster: cluster,
			check:   testResources,
		},
		{
			subTest: "labels for service",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler:              &acidv1.ConnectionPooler{},
				EnableReplicaConnectionPooler: boolToPointer(true),
			},
			cluster: cluster,
			check:   testLabels,
		},
		{
			subTest: "required envs",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler: &acidv1.ConnectionPooler{},
			},
			cluster: cluster,
			check:   testEnvs,
		},
	}
	for _, role := range [2]PostgresRole{Master, Replica} {
		for _, tt := range tests {
			podSpec, _ := tt.cluster.generateConnectionPoolerPodTemplate(role)

			err := tt.check(cluster, podSpec, role)
			if err != nil {
				t.Errorf("%s [%s]: Pod spec is incorrect, %+v",
					testName, tt.subTest, err)
			}
		}
	}
}

func TestConnectionPoolerDeploymentSpec(t *testing.T) {
	testName := "Test connection pooler deployment spec generation"
	var cluster = New(
		Config{
			OpConfig: config.Config{
				ProtectedRoles: []string{"admin"},
				Auth: config.Auth{
					SuperUsername:       superUserName,
					ReplicationUsername: replicationUserName,
				},
				ConnectionPooler: config.ConnectionPooler{
					ConnectionPoolerDefaultCPURequest:    "100m",
					ConnectionPoolerDefaultCPULimit:      "100m",
					ConnectionPoolerDefaultMemoryRequest: "100Mi",
					ConnectionPoolerDefaultMemoryLimit:   "100Mi",
				},
			},
		}, k8sutil.KubernetesClient{}, acidv1.Postgresql{}, logger, eventRecorder)
	cluster.Statefulset = &appsv1.StatefulSet{
		ObjectMeta: metav1.ObjectMeta{
			Name: "test-sts",
		},
	}
	cluster.ConnectionPooler = map[PostgresRole]*ConnectionPoolerObjects{
		Master: {
			Deployment:     nil,
			Service:        nil,
			LookupFunction: true,
			Name:           "",
			Role:           Master,
		},
	}

	noCheck := func(cluster *Cluster, deployment *appsv1.Deployment) error {
		return nil
	}

	tests := []struct {
		subTest  string
		spec     *acidv1.PostgresSpec
		expected error
		cluster  *Cluster
		check    func(cluster *Cluster, deployment *appsv1.Deployment) error
	}{
		{
			subTest: "default configuration",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler:              &acidv1.ConnectionPooler{},
				EnableReplicaConnectionPooler: boolToPointer(true),
			},
			expected: nil,
			cluster:  cluster,
			check:    noCheck,
		},
		{
			subTest: "owner reference",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler:              &acidv1.ConnectionPooler{},
				EnableReplicaConnectionPooler: boolToPointer(true),
			},
			expected: nil,
			cluster:  cluster,
			check:    testDeploymentOwnerReference,
		},
		{
			subTest: "selector",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler:              &acidv1.ConnectionPooler{},
				EnableReplicaConnectionPooler: boolToPointer(true),
			},
			expected: nil,
			cluster:  cluster,
			check:    testSelector,
		},
	}
	for _, tt := range tests {
		deployment, err := tt.cluster.generateConnectionPoolerDeployment(cluster.ConnectionPooler[Master])

		if err != tt.expected && err.Error() != tt.expected.Error() {
			t.Errorf("%s [%s]: Could not generate deployment spec,\n %+v, expected\n %+v",
				testName, tt.subTest, err, tt.expected)
		}

		err = tt.check(cluster, deployment)
		if err != nil {
			t.Errorf("%s [%s]: Deployment spec is incorrect, %+v",
				testName, tt.subTest, err)
		}
	}
}

func testServiceAccount(cluster *Cluster, podSpec *v1.PodTemplateSpec, role PostgresRole) error {
	poolerServiceAccount := podSpec.Spec.ServiceAccountName

	if poolerServiceAccount != cluster.OpConfig.PodServiceAccountName {
		return fmt.Errorf("Pooler service account does not match, got %+v, expected %+v",
			poolerServiceAccount, cluster.OpConfig.PodServiceAccountName)
	}

	return nil
}

func testResources(cluster *Cluster, podSpec *v1.PodTemplateSpec, role PostgresRole) error {
	cpuReq := podSpec.Spec.Containers[0].Resources.Requests["cpu"]
	if cpuReq.String() != cluster.OpConfig.ConnectionPooler.ConnectionPoolerDefaultCPURequest {
		return fmt.Errorf("CPU request does not match, got %s, expected %s",
			cpuReq.String(), cluster.OpConfig.ConnectionPooler.ConnectionPoolerDefaultCPURequest)
	}

	memReq := podSpec.Spec.Containers[0].Resources.Requests["memory"]
	if memReq.String() != cluster.OpConfig.ConnectionPooler.ConnectionPoolerDefaultMemoryRequest {
		return fmt.Errorf("Memory request does not match, got %s, expected %s",
			memReq.String(), cluster.OpConfig.ConnectionPooler.ConnectionPoolerDefaultMemoryRequest)
	}

	cpuLim := podSpec.Spec.Containers[0].Resources.Limits["cpu"]
	if cpuLim.String() != cluster.OpConfig.ConnectionPooler.ConnectionPoolerDefaultCPULimit {
		return fmt.Errorf("CPU limit does not match, got %s, expected %s",
			cpuLim.String(), cluster.OpConfig.ConnectionPooler.ConnectionPoolerDefaultCPULimit)
	}

	memLim := podSpec.Spec.Containers[0].Resources.Limits["memory"]
	if memLim.String() != cluster.OpConfig.ConnectionPooler.ConnectionPoolerDefaultMemoryLimit {
		return fmt.Errorf("Memory limit does not match, got %s, expected %s",
			memLim.String(), cluster.OpConfig.ConnectionPooler.ConnectionPoolerDefaultMemoryLimit)
	}

	return nil
}

func testLabels(cluster *Cluster, podSpec *v1.PodTemplateSpec, role PostgresRole) error {
	poolerLabels := podSpec.ObjectMeta.Labels["connection-pooler"]

	if poolerLabels != cluster.connectionPoolerLabels(role, true).MatchLabels["connection-pooler"] {
		return fmt.Errorf("Pod labels do not match, got %+v, expected %+v",
			podSpec.ObjectMeta.Labels, cluster.connectionPoolerLabels(role, true).MatchLabels)
	}

	return nil
}

func testSelector(cluster *Cluster, deployment *appsv1.Deployment) error {
	labels := deployment.Spec.Selector.MatchLabels
	expected := cluster.connectionPoolerLabels(Master, true).MatchLabels

	if labels["connection-pooler"] != expected["connection-pooler"] {
		return fmt.Errorf("Labels are incorrect, got %+v, expected %+v",
			labels, expected)
	}

	return nil
}

func testServiceSelector(cluster *Cluster, service *v1.Service, role PostgresRole) error {
	selector := service.Spec.Selector

	if selector["connection-pooler"] != cluster.connectionPoolerName(role) {
		return fmt.Errorf("Selector is incorrect, got %s, expected %s",
			selector["connection-pooler"], cluster.connectionPoolerName(role))
	}

	return nil
}

func TestPoolerTLS(t *testing.T) {
	client, _ := newFakeK8sPoolerTestClient()
	clusterName := "acid-test-cluster"
	namespace := "default"
	tlsSecretName := "my-secret"
	spiloFSGroup := int64(103)
	defaultMode := int32(0640)
	mountPath := "/tls"

	pg := acidv1.Postgresql{
		ObjectMeta: metav1.ObjectMeta{
			Name:      clusterName,
			Namespace: namespace,
		},
		Spec: acidv1.PostgresSpec{
			TeamID: "myapp", NumberOfInstances: 1,
			EnableConnectionPooler: util.True(),
			Resources: &acidv1.Resources{
				ResourceRequests: acidv1.ResourceDescription{CPU: k8sutil.StringToPointer("1"), Memory: k8sutil.StringToPointer("10")},
				ResourceLimits:   acidv1.ResourceDescription{CPU: k8sutil.StringToPointer("1"), Memory: k8sutil.StringToPointer("10")},
			},
			Volume: acidv1.Volume{
				Size: "1G",
			},
			TLS: &acidv1.TLSDescription{
				SecretName: tlsSecretName, CAFile: "ca.crt"},
			AdditionalVolumes: []acidv1.AdditionalVolume{
				{
					Name:      tlsSecretName,
					MountPath: mountPath,
					VolumeSource: v1.VolumeSource{
						Secret: &v1.SecretVolumeSource{
							SecretName:  tlsSecretName,
							DefaultMode: &defaultMode,
						},
					},
				},
			},
		},
	}

	var cluster = New(
		Config{
			OpConfig: config.Config{
				PodManagementPolicy: "ordered_ready",
				ProtectedRoles:      []string{"admin"},
				Auth: config.Auth{
					SuperUsername:       superUserName,
					ReplicationUsername: replicationUserName,
				},
				Resources: config.Resources{
					ClusterLabels:        map[string]string{"application": "spilo"},
					ClusterNameLabel:     "cluster-name",
					DefaultCPURequest:    "300m",
					DefaultCPULimit:      "300m",
					DefaultMemoryRequest: "300Mi",
					DefaultMemoryLimit:   "300Mi",
					PodRoleLabel:         "spilo-role",
					SpiloFSGroup:         &spiloFSGroup,
				},
				ConnectionPooler: config.ConnectionPooler{
					ConnectionPoolerDefaultCPURequest:    "100m",
					ConnectionPoolerDefaultCPULimit:      "100m",
					ConnectionPoolerDefaultMemoryRequest: "100Mi",
					ConnectionPoolerDefaultMemoryLimit:   "100Mi",
				},
				PodServiceAccountName: "postgres-pod",
			},
		}, client, pg, logger, eventRecorder)

	// create a statefulset
	_, err := cluster.createStatefulSet()
	assert.NoError(t, err)

	// create pooler resources
	cluster.ConnectionPooler = map[PostgresRole]*ConnectionPoolerObjects{}
	cluster.ConnectionPooler[Master] = &ConnectionPoolerObjects{
		Deployment:     nil,
		Service:        nil,
		Name:           cluster.connectionPoolerName(Master),
		ClusterName:    clusterName,
		Namespace:      namespace,
		LookupFunction: false,
		Role:           Master,
	}

	_, err = cluster.syncConnectionPoolerWorker(nil, &pg, Master)
	assert.NoError(t, err)

	deploy, err := client.Deployments(namespace).Get(context.TODO(), cluster.connectionPoolerName(Master), metav1.GetOptions{})
	assert.NoError(t, err)

	fsGroup := int64(103)
	assert.Equal(t, &fsGroup, deploy.Spec.Template.Spec.SecurityContext.FSGroup, "has a default FSGroup assigned")

	assert.Equal(t, "postgres-pod", deploy.Spec.Template.Spec.ServiceAccountName, "need to add a service account name")

	volume := v1.Volume{
		Name: "my-secret",
		VolumeSource: v1.VolumeSource{
			Secret: &v1.SecretVolumeSource{
				SecretName:  "my-secret",
				DefaultMode: &defaultMode,
			},
		},
	}
	assert.Contains(t, deploy.Spec.Template.Spec.Volumes, volume, "the pod gets a secret volume")

	poolerContainer := deploy.Spec.Template.Spec.Containers[constants.ConnectionPoolerContainer]
	assert.Contains(t, poolerContainer.VolumeMounts, v1.VolumeMount{
		MountPath: "/tls",
		Name:      "my-secret",
	}, "the volume gets mounted in /tls")

	assert.Contains(t, poolerContainer.Env, v1.EnvVar{Name: "CONNECTION_POOLER_CLIENT_TLS_CRT", Value: "/tls/tls.crt"})
	assert.Contains(t, poolerContainer.Env, v1.EnvVar{Name: "CONNECTION_POOLER_CLIENT_TLS_KEY", Value: "/tls/tls.key"})
	assert.Contains(t, poolerContainer.Env, v1.EnvVar{Name: "CONNECTION_POOLER_CLIENT_CA_FILE", Value: "/tls/ca.crt"})
}

func TestConnectionPoolerServiceSpec(t *testing.T) {
	testName := "Test connection pooler service spec generation"
	var cluster = New(
		Config{
			OpConfig: config.Config{
				ProtectedRoles: []string{"admin"},
				Auth: config.Auth{
					SuperUsername:       superUserName,
					ReplicationUsername: replicationUserName,
				},
				ConnectionPooler: config.ConnectionPooler{
					ConnectionPoolerDefaultCPURequest:    "100m",
					ConnectionPoolerDefaultCPULimit:      "100m",
					ConnectionPoolerDefaultMemoryRequest: "100Mi",
					ConnectionPoolerDefaultMemoryLimit:   "100Mi",
				},
				Resources: config.Resources{
					EnableOwnerReferences: util.True(),
				},
			},
		}, k8sutil.KubernetesClient{}, acidv1.Postgresql{}, logger, eventRecorder)
	cluster.Statefulset = &appsv1.StatefulSet{
		ObjectMeta: metav1.ObjectMeta{
			Name: "test-sts",
		},
	}
	cluster.ConnectionPooler = map[PostgresRole]*ConnectionPoolerObjects{
		Master: {
			Deployment:     nil,
			Service:        nil,
			LookupFunction: false,
			Role:           Master,
		},
		Replica: {
			Deployment:     nil,
			Service:        nil,
			LookupFunction: false,
			Role:           Replica,
		},
	}

	noCheck := func(cluster *Cluster, deployment *v1.Service, role PostgresRole) error {
		return nil
	}

	tests := []struct {
		subTest string
		spec    *acidv1.PostgresSpec
		cluster *Cluster
		check   func(cluster *Cluster, deployment *v1.Service, role PostgresRole) error
	}{
		{
			subTest: "default configuration",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler: &acidv1.ConnectionPooler{},
			},
			cluster: cluster,
			check:   noCheck,
		},
		{
			subTest: "owner reference",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler: &acidv1.ConnectionPooler{},
			},
			cluster: cluster,
			check:   testServiceOwnerReference,
		},
		{
			subTest: "selector",
			spec: &acidv1.PostgresSpec{
				ConnectionPooler:              &acidv1.ConnectionPooler{},
				EnableReplicaConnectionPooler: boolToPointer(true),
			},
			cluster: cluster,
			check:   testServiceSelector,
		},
	}
	for _, role := range [2]PostgresRole{Master, Replica} {
		for _, tt := range tests {
			service := tt.cluster.generateConnectionPoolerService(tt.cluster.ConnectionPooler[role])

			if err := tt.check(cluster, service, role); err != nil {
				t.Errorf("%s [%s]: Service spec is incorrect, %+v",
					testName, tt.subTest, err)
			}
		}
	}
}


================================================
File: pkg/cluster/database.go
================================================
package cluster

import (
	"bytes"
	"database/sql"
	"fmt"
	"net"
	"strings"
	"text/template"
	"time"

	"github.com/lib/pq"

	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/retryutil"
	"github.com/zalando/postgres-operator/pkg/util/users"
)

const (
	getUserSQL = `SELECT a.rolname, COALESCE(a.rolpassword, ''), a.rolsuper, a.rolinherit,
	       a.rolcreaterole, a.rolcreatedb, a.rolcanlogin, s.setconfig,
	       ARRAY(SELECT b.rolname
	             FROM pg_catalog.pg_auth_members m
	             JOIN pg_catalog.pg_authid b ON (m.roleid = b.oid)
	            WHERE m.member = a.oid) as memberof
	FROM pg_catalog.pg_authid a LEFT JOIN pg_db_role_setting s ON (a.oid = s.setrole AND s.setdatabase = 0::oid)
	WHERE a.rolname = ANY($1)
	ORDER BY 1;`

	getUsersForRetention = `SELECT r.rolname, right(r.rolname, 6) AS roldatesuffix
	        FROM pg_roles r
	        JOIN unnest($1::text[]) AS u(name) ON r.rolname LIKE u.name || '%'
			AND right(r.rolname, 6) ~ '^[0-9\.]+$'
			ORDER BY 1;`

	getDatabasesSQL = `SELECT datname, pg_get_userbyid(datdba) AS owner FROM pg_database;`
	getSchemasSQL   = `SELECT n.nspname AS dbschema FROM pg_catalog.pg_namespace n
			WHERE n.nspname !~ '^pg_' AND n.nspname <> 'information_schema' ORDER BY 1`
	getExtensionsSQL = `SELECT e.extname, n.nspname FROM pg_catalog.pg_extension e
	        LEFT JOIN pg_catalog.pg_namespace n ON n.oid = e.extnamespace ORDER BY 1;`

	createDatabaseSQL       = `CREATE DATABASE "%s" OWNER "%s";`
	createDatabaseSchemaSQL = `SET ROLE TO "%s"; CREATE SCHEMA IF NOT EXISTS "%s" AUTHORIZATION "%s"`
	alterDatabaseOwnerSQL   = `ALTER DATABASE "%s" OWNER TO "%s";`
	createExtensionSQL      = `CREATE EXTENSION IF NOT EXISTS "%s" SCHEMA "%s"`
	alterExtensionSQL       = `ALTER EXTENSION "%s" SET SCHEMA "%s"`

	getPublicationsSQL = `SELECT p.pubname, COALESCE(string_agg(pt.schemaname || '.' || pt.tablename, ', ' ORDER BY pt.schemaname, pt.tablename), '') AS pubtables
	        FROM pg_publication p
			LEFT JOIN pg_publication_tables pt ON pt.pubname = p.pubname
			WHERE p.pubowner = 'postgres'::regrole
			AND p.pubname LIKE 'fes_%'
			GROUP BY p.pubname;`
	createPublicationSQL = `CREATE PUBLICATION "%s" FOR TABLE %s WITH (publish = 'insert, update');`
	alterPublicationSQL  = `ALTER PUBLICATION "%s" SET TABLE %s;`
	dropPublicationSQL   = `DROP PUBLICATION "%s";`

	globalDefaultPrivilegesSQL = `SET ROLE TO "%s";
			ALTER DEFAULT PRIVILEGES GRANT USAGE ON SCHEMAS TO "%s","%s";
			ALTER DEFAULT PRIVILEGES GRANT SELECT ON TABLES TO "%s";
			ALTER DEFAULT PRIVILEGES GRANT SELECT ON SEQUENCES TO "%s";
			ALTER DEFAULT PRIVILEGES GRANT INSERT, UPDATE, DELETE ON TABLES TO "%s";
			ALTER DEFAULT PRIVILEGES GRANT USAGE, UPDATE ON SEQUENCES TO "%s";
			ALTER DEFAULT PRIVILEGES GRANT EXECUTE ON FUNCTIONS TO "%s","%s";
			ALTER DEFAULT PRIVILEGES GRANT USAGE ON TYPES TO "%s","%s";`
	schemaDefaultPrivilegesSQL = `SET ROLE TO "%s";
			GRANT USAGE ON SCHEMA "%s" TO "%s","%s";
			ALTER DEFAULT PRIVILEGES IN SCHEMA "%s" GRANT SELECT ON TABLES TO "%s";
			ALTER DEFAULT PRIVILEGES IN SCHEMA "%s" GRANT SELECT ON SEQUENCES TO "%s";
			ALTER DEFAULT PRIVILEGES IN SCHEMA "%s" GRANT INSERT, UPDATE, DELETE ON TABLES TO "%s";
			ALTER DEFAULT PRIVILEGES IN SCHEMA "%s" GRANT USAGE, UPDATE ON SEQUENCES TO "%s";
			ALTER DEFAULT PRIVILEGES IN SCHEMA "%s" GRANT EXECUTE ON FUNCTIONS TO "%s","%s";
			ALTER DEFAULT PRIVILEGES IN SCHEMA "%s" GRANT USAGE ON TYPES TO "%s","%s";`

	connectionPoolerLookup = `
		CREATE SCHEMA IF NOT EXISTS {{.pooler_schema}};

		CREATE OR REPLACE FUNCTION {{.pooler_schema}}.user_lookup(
			in i_username text, out uname text, out phash text)
		RETURNS record AS $$
		BEGIN
			SELECT usename, passwd FROM pg_catalog.pg_shadow
			WHERE usename = i_username INTO uname, phash;
			RETURN;
		END;
		$$ LANGUAGE plpgsql SECURITY DEFINER;

		REVOKE ALL ON FUNCTION {{.pooler_schema}}.user_lookup(text)
			FROM public, {{.pooler_user}};
		GRANT EXECUTE ON FUNCTION {{.pooler_schema}}.user_lookup(text)
			TO {{.pooler_user}};
		GRANT USAGE ON SCHEMA {{.pooler_schema}} TO {{.pooler_user}};
	`
)

func (c *Cluster) pgConnectionString(dbname string) string {
	password := c.systemUsers[constants.SuperuserKeyName].Password

	if dbname == "" {
		dbname = "postgres"
	}

	return fmt.Sprintf("host='%s' dbname='%s' sslmode=require user='%s' password='%s' connect_timeout='%d'",
		fmt.Sprintf("%s.%s.svc.%s", c.Name, c.Namespace, c.OpConfig.ClusterDomain),
		dbname,
		c.systemUsers[constants.SuperuserKeyName].Name,
		strings.Replace(password, "$", "\\$", -1),
		constants.PostgresConnectTimeout/time.Second)
}

func (c *Cluster) databaseAccessDisabled() bool {
	if !c.OpConfig.EnableDBAccess {
		c.logger.Debug("database access is disabled")
	}

	return !c.OpConfig.EnableDBAccess
}

func (c *Cluster) initDbConn() error {
	if c.pgDb != nil {
		return nil
	}

	return c.initDbConnWithName("")
}

// Worker function for connection initialization. This function does not check
// if the connection is already open, if it is then it will be overwritten.
// Callers need to make sure no connection is open, otherwise we could leak
// connections
func (c *Cluster) initDbConnWithName(dbname string) error {
	c.setProcessName("initializing db connection")

	var conn *sql.DB
	connstring := c.pgConnectionString(dbname)

	finalerr := retryutil.Retry(constants.PostgresConnectTimeout, constants.PostgresConnectRetryTimeout,
		func() (bool, error) {
			var err error
			conn, err = sql.Open("postgres", connstring)
			if err == nil {
				err = conn.Ping()
			}

			if err == nil {
				return true, nil
			}

			if _, ok := err.(*net.OpError); ok {
				c.logger.Warningf("could not connect to Postgres database: %v", err)
				return false, nil
			}

			if err2 := conn.Close(); err2 != nil {
				c.logger.Errorf("error when closing Postgres connection after another error: %v", err)
				return false, err2
			}

			// Retry open connection until succeeded.
			c.logger.Warningf("could not connect to Postgres database: %v", err)
			return false, nil
		})

	if finalerr != nil {
		return fmt.Errorf("could not init db connection: %v", finalerr)
	}
	// Limit ourselves to a single connection and allow no idle connections.
	conn.SetMaxOpenConns(1)
	conn.SetMaxIdleConns(-1)

	if c.pgDb != nil {
		msg := "closing an existing connection before opening a new one to %s"
		c.logger.Warningf(msg, dbname)
		c.closeDbConn()
	}

	c.pgDb = conn

	return nil
}

func (c *Cluster) connectionIsClosed() bool {
	return c.pgDb == nil
}

func (c *Cluster) closeDbConn() (err error) {
	c.setProcessName("closing database connection")
	if c.pgDb != nil {
		c.logger.Debug("closing database connection")
		if err = c.pgDb.Close(); err != nil {
			c.logger.Errorf("could not close database connection: %v", err)
		}
		c.pgDb = nil

		return nil
	}
	c.logger.Warning("attempted to close an empty db connection object")
	return nil
}

func (c *Cluster) readPgUsersFromDatabase(userNames []string) (users spec.PgUserMap, err error) {
	c.setProcessName("reading users from the database")
	var rows *sql.Rows
	users = make(spec.PgUserMap)
	if rows, err = c.pgDb.Query(getUserSQL, pq.Array(userNames)); err != nil {
		return nil, fmt.Errorf("error when querying users: %v", err)
	}
	defer func() {
		if err2 := rows.Close(); err2 != nil {
			if err != nil {
				err = fmt.Errorf("error when closing query cursor: %v, previous error: %v", err2, err)
			} else {
				err = fmt.Errorf("error when closing query cursor: %v", err2)
			}
		}
	}()

	for rows.Next() {
		var (
			rolname, rolpassword                                          string
			rolsuper, rolinherit, rolcreaterole, rolcreatedb, rolcanlogin bool
			roloptions, memberof                                          []string
			roldeleted                                                    bool
		)
		err := rows.Scan(&rolname, &rolpassword, &rolsuper, &rolinherit,
			&rolcreaterole, &rolcreatedb, &rolcanlogin, pq.Array(&roloptions), pq.Array(&memberof))
		if err != nil {
			return nil, fmt.Errorf("error when processing user rows: %v", err)
		}
		flags := makeUserFlags(rolsuper, rolinherit, rolcreaterole, rolcreatedb, rolcanlogin)
		// XXX: the code assumes the password we get from pg_authid is always MD5
		parameters := make(map[string]string)
		for _, option := range roloptions {
			fields := strings.Split(option, "=")
			if len(fields) != 2 {
				c.logger.Warningf("skipping malformed option: %q", option)
				continue
			}
			parameters[fields[0]] = fields[1]
		}

		// consider NOLOGIN roles with deleted suffix as deprecated users
		if strings.HasSuffix(rolname, c.OpConfig.RoleDeletionSuffix) && !rolcanlogin {
			roldeleted = true
		}

		users[rolname] = spec.PgUser{Name: rolname, Password: rolpassword, Flags: flags, MemberOf: memberof, Parameters: parameters, Deleted: roldeleted}
	}

	return users, nil
}

func findUsersFromRotation(rotatedUsers []string, db *sql.DB) (map[string]string, error) {
	extraUsers := make(map[string]string, 0)
	rows, err := db.Query(getUsersForRetention, pq.Array(rotatedUsers))
	if err != nil {
		return nil, fmt.Errorf("query failed: %v", err)
	}
	defer func() {
		if err2 := rows.Close(); err2 != nil {
			if err != nil {
				err = fmt.Errorf("error when closing query cursor: %v, previous error: %v", err2, err)
			} else {
				err = fmt.Errorf("error when closing query cursor: %v", err2)
			}
		}
	}()

	for rows.Next() {
		var (
			rolname, roldatesuffix string
		)
		err := rows.Scan(&rolname, &roldatesuffix)
		if err != nil {
			return nil, fmt.Errorf("error when processing rows of deprecated users: %v", err)
		}
		extraUsers[rolname] = roldatesuffix
	}

	return extraUsers, nil
}

func (c *Cluster) cleanupRotatedUsers(rotatedUsers []string, db *sql.DB) error {
	c.setProcessName("checking for rotated users to remove from the database due to configured retention")
	extraUsers, err := findUsersFromRotation(rotatedUsers, db)
	if err != nil {
		return fmt.Errorf("error when querying for deprecated users from password rotation: %v", err)
	}

	// make sure user retention policy aligns with rotation interval
	retenionDays := c.OpConfig.PasswordRotationUserRetention
	if retenionDays < 2*c.OpConfig.PasswordRotationInterval {
		retenionDays = 2 * c.OpConfig.PasswordRotationInterval
		c.logger.Warnf("user retention days too few compared to rotation interval %d - setting it to %d", c.OpConfig.PasswordRotationInterval, retenionDays)
	}
	retentionDate := time.Now().AddDate(0, 0, int(retenionDays)*-1)

	for rotatedUser, dateSuffix := range extraUsers {
		userCreationDate, err := time.Parse(constants.RotationUserDateFormat, dateSuffix)
		if err != nil {
			c.logger.Errorf("could not parse creation date suffix of user %q: %v", rotatedUser, err)
			continue
		}
		if retentionDate.After(userCreationDate) {
			c.logger.Infof("dropping user %q due to configured days in password_rotation_user_retention", rotatedUser)
			if err = users.DropPgUser(rotatedUser, db); err != nil {
				c.logger.Errorf("could not drop role %q: %v", rotatedUser, err)
				continue
			}
		}
	}

	return nil
}

// getDatabases returns the map of current databases with owners
// The caller is responsible for opening and closing the database connection
func (c *Cluster) getDatabases() (dbs map[string]string, err error) {
	var (
		rows *sql.Rows
	)

	if rows, err = c.pgDb.Query(getDatabasesSQL); err != nil {
		return nil, fmt.Errorf("could not query database: %v", err)
	}

	defer func() {
		if err2 := rows.Close(); err2 != nil {
			if err != nil {
				err = fmt.Errorf("error when closing query cursor: %v, previous error: %v", err2, err)
			} else {
				err = fmt.Errorf("error when closing query cursor: %v", err2)
			}
		}
	}()

	dbs = make(map[string]string)

	for rows.Next() {
		var datname, owner string

		if err = rows.Scan(&datname, &owner); err != nil {
			return nil, fmt.Errorf("error when processing row: %v", err)
		}
		dbs[datname] = owner
	}

	return dbs, err
}

// executeCreateDatabase creates new database with the given owner.
// The caller is responsible for opening and closing the database connection.
func (c *Cluster) executeCreateDatabase(databaseName, owner string) error {
	return c.execCreateOrAlterDatabase(databaseName, owner, createDatabaseSQL,
		"creating database", "create database")
}

// executeAlterDatabaseOwner changes the owner of the given database.
// The caller is responsible for opening and closing the database connection.
func (c *Cluster) executeAlterDatabaseOwner(databaseName string, owner string) error {
	return c.execCreateOrAlterDatabase(databaseName, owner, alterDatabaseOwnerSQL,
		"changing owner for database", "alter database owner")
}

func (c *Cluster) execCreateOrAlterDatabase(databaseName, owner, statement, doing, operation string) error {
	if !c.databaseNameOwnerValid(databaseName, owner) {
		return nil
	}
	c.logger.Infof("%s %q owner %q", doing, databaseName, owner)
	if _, err := c.pgDb.Exec(fmt.Sprintf(statement, databaseName, owner)); err != nil {
		return fmt.Errorf("could not execute %s: %v", operation, err)
	}
	return nil
}

func (c *Cluster) databaseNameOwnerValid(databaseName, owner string) bool {
	if _, ok := c.pgUsers[owner]; !ok {
		c.logger.Infof("skipping creation of the %q database, user %q does not exist", databaseName, owner)
		return false
	}

	if !databaseNameRegexp.MatchString(databaseName) {
		c.logger.Infof("database %q has invalid name", databaseName)
		return false
	}
	return true
}

// getSchemas returns the list of current database schemas
// The caller is responsible for opening and closing the database connection
func (c *Cluster) getSchemas() (schemas []string, err error) {
	var (
		rows      *sql.Rows
		dbschemas []string
	)

	if rows, err = c.pgDb.Query(getSchemasSQL); err != nil {
		return nil, fmt.Errorf("could not query database schemas: %v", err)
	}

	defer func() {
		if err2 := rows.Close(); err2 != nil {
			if err != nil {
				err = fmt.Errorf("error when closing query cursor: %v, previous error: %v", err2, err)
			} else {
				err = fmt.Errorf("error when closing query cursor: %v", err2)
			}
		}
	}()

	for rows.Next() {
		var dbschema string

		if err = rows.Scan(&dbschema); err != nil {
			return nil, fmt.Errorf("error when processing row: %v", err)
		}
		dbschemas = append(dbschemas, dbschema)
	}

	return dbschemas, err
}

// executeCreateDatabaseSchema creates new database schema with the given owner.
// The caller is responsible for opening and closing the database connection.
func (c *Cluster) executeCreateDatabaseSchema(databaseName, schemaName, dbOwner string, schemaOwner string) error {
	return c.execCreateDatabaseSchema(databaseName, schemaName, dbOwner, schemaOwner, createDatabaseSchemaSQL,
		"creating database schema", "create database schema")
}

func (c *Cluster) execCreateDatabaseSchema(databaseName, schemaName, dbOwner, schemaOwner, statement, doing, operation string) error {
	if !c.databaseSchemaNameValid(schemaName) {
		return nil
	}
	c.logger.Infof("%s %q owner %q", doing, schemaName, schemaOwner)
	if _, err := c.pgDb.Exec(fmt.Sprintf(statement, dbOwner, schemaName, schemaOwner)); err != nil {
		return fmt.Errorf("could not execute %s: %v", operation, err)
	}

	// set default privileges for schema
	// the schemaOwner defines them for global database roles
	c.execAlterSchemaDefaultPrivileges(schemaName, schemaOwner, databaseName)

	// if schemaOwner and dbOwner differ we know that <databaseName>_<schemaName> default roles were created
	if schemaOwner != dbOwner {
		defaultUsers := c.Spec.PreparedDatabases[databaseName].PreparedSchemas[schemaName].DefaultUsers

		// define schema privileges of <databaseName>_<schemaName>_owner_user for global roles, too
		if defaultUsers {
			c.execAlterSchemaDefaultPrivileges(schemaName, schemaOwner+constants.UserRoleNameSuffix, databaseName)
		}

		// collect all possible owner roles and define default schema privileges
		// for <databaseName>_<schemaName>_reader/writer roles
		owners := c.getOwnerRoles(databaseName, c.Spec.PreparedDatabases[databaseName].DefaultUsers)
		owners = append(owners, c.getOwnerRoles(databaseName+"_"+schemaName, defaultUsers)...)
		for _, owner := range owners {
			c.execAlterSchemaDefaultPrivileges(schemaName, owner, databaseName+"_"+schemaName)
		}
	} else {
		// define schema privileges of <databaseName>_owner_user for global roles, too
		if c.Spec.PreparedDatabases[databaseName].DefaultUsers {
			c.execAlterSchemaDefaultPrivileges(schemaName, schemaOwner+constants.UserRoleNameSuffix, databaseName)
		}
	}

	return nil
}

func (c *Cluster) databaseSchemaNameValid(schemaName string) bool {
	if !databaseNameRegexp.MatchString(schemaName) {
		c.logger.Infof("database schema %q has invalid name", schemaName)
		return false
	}
	return true
}

func (c *Cluster) execAlterSchemaDefaultPrivileges(schemaName, owner, rolePrefix string) error {
	if _, err := c.pgDb.Exec(fmt.Sprintf(schemaDefaultPrivilegesSQL, owner,
		schemaName, rolePrefix+constants.ReaderRoleNameSuffix, rolePrefix+constants.WriterRoleNameSuffix, // schema
		schemaName, rolePrefix+constants.ReaderRoleNameSuffix, // tables
		schemaName, rolePrefix+constants.ReaderRoleNameSuffix, // sequences
		schemaName, rolePrefix+constants.WriterRoleNameSuffix, // tables
		schemaName, rolePrefix+constants.WriterRoleNameSuffix, // sequences
		schemaName, rolePrefix+constants.ReaderRoleNameSuffix, rolePrefix+constants.WriterRoleNameSuffix, // types
		schemaName, rolePrefix+constants.ReaderRoleNameSuffix, rolePrefix+constants.WriterRoleNameSuffix)); err != nil { // functions
		return fmt.Errorf("could not alter default privileges for database schema %s: %v", schemaName, err)
	}

	return nil
}

func (c *Cluster) execAlterGlobalDefaultPrivileges(owner, rolePrefix string) error {
	if _, err := c.pgDb.Exec(fmt.Sprintf(globalDefaultPrivilegesSQL, owner,
		rolePrefix+constants.WriterRoleNameSuffix, rolePrefix+constants.ReaderRoleNameSuffix, // schemas
		rolePrefix+constants.ReaderRoleNameSuffix,                                            // tables
		rolePrefix+constants.ReaderRoleNameSuffix,                                            // sequences
		rolePrefix+constants.WriterRoleNameSuffix,                                            // tables
		rolePrefix+constants.WriterRoleNameSuffix,                                            // sequences
		rolePrefix+constants.ReaderRoleNameSuffix, rolePrefix+constants.WriterRoleNameSuffix, // types
		rolePrefix+constants.ReaderRoleNameSuffix, rolePrefix+constants.WriterRoleNameSuffix)); err != nil { // functions
		return fmt.Errorf("could not alter default privileges for database %s: %v", rolePrefix, err)
	}

	return nil
}

func makeUserFlags(rolsuper, rolinherit, rolcreaterole, rolcreatedb, rolcanlogin bool) (result []string) {
	if rolsuper {
		result = append(result, constants.RoleFlagSuperuser)
	}
	if rolinherit {
		result = append(result, constants.RoleFlagInherit)
	}
	if rolcreaterole {
		result = append(result, constants.RoleFlagCreateRole)
	}
	if rolcreatedb {
		result = append(result, constants.RoleFlagCreateDB)
	}
	if rolcanlogin {
		result = append(result, constants.RoleFlagLogin)
	}

	return result
}

func (c *Cluster) getOwnerRoles(dbObjPath string, withUser bool) (owners []string) {
	owners = append(owners, dbObjPath+constants.OwnerRoleNameSuffix)
	if withUser {
		owners = append(owners, dbObjPath+constants.OwnerRoleNameSuffix+constants.UserRoleNameSuffix)
	}

	return owners
}

// getExtension returns the list of current database extensions
// The caller is responsible for opening and closing the database connection
func (c *Cluster) getExtensions() (dbExtensions map[string]string, err error) {
	var (
		rows *sql.Rows
	)

	if rows, err = c.pgDb.Query(getExtensionsSQL); err != nil {
		return nil, fmt.Errorf("could not query database extensions: %v", err)
	}

	defer func() {
		if err2 := rows.Close(); err2 != nil {
			if err != nil {
				err = fmt.Errorf("error when closing query cursor: %v, previous error: %v", err2, err)
			} else {
				err = fmt.Errorf("error when closing query cursor: %v", err2)
			}
		}
	}()

	dbExtensions = make(map[string]string)

	for rows.Next() {
		var extension, schema string

		if err = rows.Scan(&extension, &schema); err != nil {
			return nil, fmt.Errorf("error when processing row: %v", err)
		}
		dbExtensions[extension] = schema
	}

	return dbExtensions, err
}

// executeCreateExtension creates new extension in the given schema.
// The caller is responsible for opening and closing the database connection.
func (c *Cluster) executeCreateExtension(extName, schemaName string) error {
	return c.execCreateOrAlterExtension(extName, schemaName, createExtensionSQL,
		"creating extension", "create extension")
}

// executeAlterExtension changes the schema of the given extension.
// The caller is responsible for opening and closing the database connection.
func (c *Cluster) executeAlterExtension(extName, schemaName string) error {
	return c.execCreateOrAlterExtension(extName, schemaName, alterExtensionSQL,
		"changing schema for extension", "alter extension schema")
}

func (c *Cluster) execCreateOrAlterExtension(extName, schemaName, statement, doing, operation string) error {

	c.logger.Infof("%s %q schema %q", doing, extName, schemaName)
	if _, err := c.pgDb.Exec(fmt.Sprintf(statement, extName, schemaName)); err != nil {
		return fmt.Errorf("could not execute %s: %v", operation, err)
	}

	return nil
}

// getPublications returns the list of current database publications with tables
// The caller is responsible for opening and closing the database connection
func (c *Cluster) getPublications() (publications map[string]string, err error) {
	var (
		rows *sql.Rows
	)

	if rows, err = c.pgDb.Query(getPublicationsSQL); err != nil {
		return nil, fmt.Errorf("could not query database publications: %v", err)
	}

	defer func() {
		if err2 := rows.Close(); err2 != nil {
			if err != nil {
				err = fmt.Errorf("error when closing query cursor: %v, previous error: %v", err2, err)
			} else {
				err = fmt.Errorf("error when closing query cursor: %v", err2)
			}
		}
	}()

	dbPublications := make(map[string]string)

	for rows.Next() {
		var (
			dbPublication       string
			dbPublicationTables string
		)

		if err = rows.Scan(&dbPublication, &dbPublicationTables); err != nil {
			return nil, fmt.Errorf("error when processing row: %v", err)
		}
		dbPublications[dbPublication] = dbPublicationTables
	}

	return dbPublications, err
}

func (c *Cluster) executeDropPublication(pubName string) error {
	c.logger.Infof("dropping publication %q", pubName)
	if _, err := c.pgDb.Exec(fmt.Sprintf(dropPublicationSQL, pubName)); err != nil {
		return fmt.Errorf("could not execute drop publication: %v", err)
	}
	return nil
}

// executeCreatePublication creates new publication for given tables
// The caller is responsible for opening and closing the database connection.
func (c *Cluster) executeCreatePublication(pubName, tableList string) error {
	return c.execCreateOrAlterPublication(pubName, tableList, createPublicationSQL,
		"creating publication", "create publication")
}

// executeAlterExtension changes the table list of the given publication.
// The caller is responsible for opening and closing the database connection.
func (c *Cluster) executeAlterPublication(pubName, tableList string) error {
	return c.execCreateOrAlterPublication(pubName, tableList, alterPublicationSQL,
		"changing publication", "alter publication tables")
}

func (c *Cluster) execCreateOrAlterPublication(pubName, tableList, statement, doing, operation string) error {

	c.logger.Debugf("%s %q with table list %q", doing, pubName, tableList)
	if _, err := c.pgDb.Exec(fmt.Sprintf(statement, pubName, tableList)); err != nil {
		return fmt.Errorf("could not execute %s: %v", operation, err)
	}

	return nil
}

// Creates a connection pool credentials lookup function in every database to
// perform remote authentication.
func (c *Cluster) installLookupFunction(poolerSchema, poolerUser string) error {
	var stmtBytes bytes.Buffer

	c.logger.Info("Installing lookup function")

	// Open a new connection if not yet done. This connection will be used only
	// to get the list of databases, not for the actuall installation.
	if err := c.initDbConn(); err != nil {
		return fmt.Errorf("could not init database connection")
	}
	defer func() {
		if c.connectionIsClosed() {
			return
		}

		if err := c.closeDbConn(); err != nil {
			c.logger.Errorf("could not close database connection: %v", err)
		}
	}()

	// List of databases we failed to process. At the moment it function just
	// like a flag to retry on the next sync, but in the future we may want to
	// retry only necessary parts, so let's keep the list.
	failedDatabases := []string{}
	currentDatabases, err := c.getDatabases()
	if err != nil {
		msg := "could not get databases to install pooler lookup function: %v"
		return fmt.Errorf(msg, err)
	}

	// We've got the list of target databases, now close this connection to
	// open a new one to every each of them.
	if err := c.closeDbConn(); err != nil {
		c.logger.Errorf("could not close database connection: %v", err)
	}

	templater := template.Must(template.New("sql").Parse(connectionPoolerLookup))
	params := TemplateParams{
		"pooler_schema": poolerSchema,
		"pooler_user":   poolerUser,
	}

	if err := templater.Execute(&stmtBytes, params); err != nil {
		msg := "could not prepare sql statement %+v: %v"
		return fmt.Errorf(msg, params, err)
	}

	for dbname := range currentDatabases {

		if dbname == "template0" || dbname == "template1" {
			continue
		}

		c.logger.Infof("install pooler lookup function into database '%s'", dbname)

		// golang sql will do retries couple of times if pq driver reports
		// connections issues (driver.ErrBadConn), but since our query is
		// idempotent, we can retry in a view of other errors (e.g. due to
		// failover a db is temporary in a read-only mode or so) to make sure
		// it was applied.
		execErr := retryutil.Retry(
			constants.PostgresConnectTimeout,
			constants.PostgresConnectRetryTimeout,
			func() (bool, error) {

				// At this moment we are not connected to any database
				if err := c.initDbConnWithName(dbname); err != nil {
					msg := "could not init database connection to %s"
					return false, fmt.Errorf(msg, dbname)
				}
				defer func() {
					if err := c.closeDbConn(); err != nil {
						msg := "could not close database connection: %v"
						c.logger.Errorf(msg, err)
					}
				}()

				if _, err = c.pgDb.Exec(stmtBytes.String()); err != nil {
					msg := fmt.Errorf("could not execute sql statement %s: %v",
						stmtBytes.String(), err)
					return false, msg
				}

				return true, nil
			})

		if execErr != nil {
			c.logger.Errorf("could not execute after retries %s: %v",
				stmtBytes.String(), err)
			// process other databases
			failedDatabases = append(failedDatabases, dbname)
			continue
		}
		c.logger.Infof("pooler lookup function installed into %s", dbname)
	}

	if len(failedDatabases) > 0 {
		return fmt.Errorf("could not install pooler lookup function in every specified databases")
	}

	return nil
}


================================================
File: pkg/cluster/exec.go
================================================
package cluster

import (
	"bytes"
	"context"
	"fmt"
	"strings"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes/scheme"
	"k8s.io/client-go/tools/remotecommand"

	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util/constants"
)

// ExecCommand executes arbitrary command inside the pod
func (c *Cluster) ExecCommand(podName *spec.NamespacedName, command ...string) (string, error) {
	c.setProcessName("executing command %q", strings.Join(command, " "))

	var (
		execOut bytes.Buffer
		execErr bytes.Buffer
	)

	pod, err := c.KubeClient.Pods(podName.Namespace).Get(context.TODO(), podName.Name, metav1.GetOptions{})
	if err != nil {
		return "", fmt.Errorf("could not get pod info: %v", err)
	}

	// iterate through all containers looking for the one running PostgreSQL.
	targetContainer := -1
	for i, cr := range pod.Spec.Containers {
		if cr.Name == constants.PostgresContainerName {
			targetContainer = i
			break
		}
	}

	if targetContainer < 0 {
		return "", fmt.Errorf("could not find %s container to exec to", constants.PostgresContainerName)
	}

	req := c.KubeClient.RESTClient.Post().
		Resource("pods").
		Name(podName.Name).
		Namespace(podName.Namespace).
		SubResource("exec")
	req.VersionedParams(&v1.PodExecOptions{
		Container: pod.Spec.Containers[targetContainer].Name,
		Command:   command,
		Stdout:    true,
		Stderr:    true,
	}, scheme.ParameterCodec)

	exec, err := remotecommand.NewSPDYExecutor(c.RestConfig, "POST", req.URL())
	if err != nil {
		return "", fmt.Errorf("failed to init executor: %v", err)
	}

	err = exec.StreamWithContext(context.TODO(), remotecommand.StreamOptions{
		Stdout: &execOut,
		Stderr: &execErr,
		Tty:    false,
	})

	if err != nil {
		return "", fmt.Errorf("could not execute: %v", err)
	}

	if execErr.Len() > 0 {
		return "", fmt.Errorf("stderr: %v", execErr.String())
	}

	return execOut.String(), nil
}


================================================
File: pkg/cluster/filesystems.go
================================================
package cluster

import (
	"fmt"
	"strings"

	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/filesystems"
)

func (c *Cluster) getPostgresFilesystemInfo(podName *spec.NamespacedName) (device, fstype string, err error) {
	out, err := c.ExecCommand(podName, "bash", "-c", fmt.Sprintf("df -T %s|tail -1", constants.PostgresDataMount))
	if err != nil {
		return "", "", err
	}
	fields := strings.Fields(out)
	if len(fields) < 2 {
		return "", "", fmt.Errorf("too few fields in the df output")
	}

	return fields[0], fields[1], nil
}

func (c *Cluster) resizePostgresFilesystem(podName *spec.NamespacedName, resizers []filesystems.FilesystemResizer) error {
	// resize2fs always writes to stderr, and ExecCommand considers a non-empty stderr an error
	// first, determine the device and the filesystem
	deviceName, fsType, err := c.getPostgresFilesystemInfo(podName)
	if err != nil {
		return fmt.Errorf("could not get device and type for the postgres filesystem: %v", err)
	}
	for _, resizer := range resizers {
		if !resizer.CanResizeFilesystem(fsType) {
			continue
		}
		err := resizer.ResizeFilesystem(deviceName, func(cmd string) (out string, err error) {
			return c.ExecCommand(podName, "bash", "-c", cmd)
		})

		return err
	}
	return fmt.Errorf("could not resize filesystem: no compatible resizers for the filesystem of type %q", fsType)
}


================================================
File: pkg/cluster/majorversionupgrade.go
================================================
package cluster

import (
	"context"
	"encoding/json"
	"fmt"
	"strings"

	"github.com/Masterminds/semver"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
)

// VersionMap Map of version numbers
var VersionMap = map[string]int{
	"12": 120000,
	"13": 130000,
	"14": 140000,
	"15": 150000,
	"16": 160000,
	"17": 170000,
}

const (
	majorVersionUpgradeSuccessAnnotation = "last-major-upgrade-success"
	majorVersionUpgradeFailureAnnotation = "last-major-upgrade-failure"
)

// IsBiggerPostgresVersion Compare two Postgres version numbers
func IsBiggerPostgresVersion(old string, new string) bool {
	oldN := VersionMap[old]
	newN := VersionMap[new]
	return newN > oldN
}

// GetDesiredMajorVersionAsInt Convert string to comparable integer of PG version
func (c *Cluster) GetDesiredMajorVersionAsInt() int {
	return VersionMap[c.GetDesiredMajorVersion()]
}

// GetDesiredMajorVersion returns major version to use, incl. potential auto upgrade
func (c *Cluster) GetDesiredMajorVersion() string {

	if c.Config.OpConfig.MajorVersionUpgradeMode == "full" {
		// e.g. current is 13, minimal is 13 allowing 13 to 17 clusters, everything below is upgraded
		if IsBiggerPostgresVersion(c.Spec.PgVersion, c.Config.OpConfig.MinimalMajorVersion) {
			c.logger.Infof("overwriting configured major version %s to %s", c.Spec.PgVersion, c.Config.OpConfig.TargetMajorVersion)
			return c.Config.OpConfig.TargetMajorVersion
		}
	}

	return c.Spec.PgVersion
}

func (c *Cluster) isUpgradeAllowedForTeam(owningTeam string) bool {
	allowedTeams := c.OpConfig.MajorVersionUpgradeTeamAllowList

	if len(allowedTeams) == 0 {
		return false
	}

	return util.SliceContains(allowedTeams, owningTeam)
}

func (c *Cluster) annotatePostgresResource(isSuccess bool) error {
	annotations := make(map[string]string)
	currentTime := metav1.Now().Format("2006-01-02T15:04:05Z")
	if isSuccess {
		annotations[majorVersionUpgradeSuccessAnnotation] = currentTime
	} else {
		annotations[majorVersionUpgradeFailureAnnotation] = currentTime
	}
	patchData, err := metaAnnotationsPatch(annotations)
	if err != nil {
		c.logger.Errorf("could not form patch for %s postgresql resource: %v", c.Name, err)
		return err
	}
	_, err = c.KubeClient.Postgresqls(c.Namespace).Patch(context.Background(), c.Name, types.MergePatchType, patchData, metav1.PatchOptions{})
	if err != nil {
		c.logger.Errorf("failed to patch annotations to postgresql resource: %v", err)
		return err
	}
	return nil
}

func (c *Cluster) removeFailuresAnnotation() error {
	annotationToRemove := []map[string]string{
		{
			"op":   "remove",
			"path": fmt.Sprintf("/metadata/annotations/%s", majorVersionUpgradeFailureAnnotation),
		},
	}
	removePatch, err := json.Marshal(annotationToRemove)
	if err != nil {
		c.logger.Errorf("could not form removal patch for %s postgresql resource: %v", c.Name, err)
		return err
	}
	_, err = c.KubeClient.Postgresqls(c.Namespace).Patch(context.Background(), c.Name, types.JSONPatchType, removePatch, metav1.PatchOptions{})
	if err != nil {
		c.logger.Errorf("failed to remove annotations from postgresql resource: %v", err)
		return err
	}
	return nil
}

func (c *Cluster) criticalOperationLabel(pods []v1.Pod, value *string) error {
	metadataReq := map[string]map[string]map[string]*string{"metadata": {"labels": {"critical-operation": value}}}

	patchReq, err := json.Marshal(metadataReq)
	if err != nil {
		return fmt.Errorf("could not marshal ObjectMeta: %v", err)
	}
	for _, pod := range pods {
		_, err = c.KubeClient.Pods(c.Namespace).Patch(context.TODO(), pod.Name, types.StrategicMergePatchType, patchReq, metav1.PatchOptions{})
		if err != nil {
			return err
		}
	}
	return nil
}

/*
Execute upgrade when mode is set to manual or full or when the owning team is allowed for upgrade (and mode is "off").

Manual upgrade means, it is triggered by the user via manifest version change
Full upgrade means, operator also determines the minimal version used accross all clusters and upgrades violators.
*/
func (c *Cluster) majorVersionUpgrade() error {

	if c.OpConfig.MajorVersionUpgradeMode == "off" && !c.isUpgradeAllowedForTeam(c.Spec.TeamID) {
		return nil
	}

	desiredVersion := c.GetDesiredMajorVersionAsInt()

	if c.currentMajorVersion >= desiredVersion {
		if _, exists := c.ObjectMeta.Annotations[majorVersionUpgradeFailureAnnotation]; exists { // if failure annotation exists, remove it
			c.removeFailuresAnnotation()
			c.logger.Infof("removing failure annotation as the cluster is already up to date")
		}
		c.logger.Infof("cluster version up to date. current: %d, min desired: %d", c.currentMajorVersion, desiredVersion)
		return nil
	}

	pods, err := c.listPods()
	if err != nil {
		return err
	}

	allRunning := true
	isStandbyCluster := false

	var masterPod *v1.Pod

	for i, pod := range pods {
		ps, _ := c.patroni.GetMemberData(&pod)

		if ps.Role == "standby_leader" {
			isStandbyCluster = true
			c.currentMajorVersion = ps.ServerVersion
			break
		}

		if ps.State != "running" {
			allRunning = false
			c.logger.Infof("identified non running pod, potentially skipping major version upgrade")
		}

		if ps.Role == "master" || ps.Role == "primary" {
			masterPod = &pods[i]
			c.currentMajorVersion = ps.ServerVersion
		}
	}

	if masterPod == nil {
		c.logger.Infof("no master in the cluster, skipping major version upgrade")
		return nil
	}

	// Recheck version with newest data from Patroni
	if c.currentMajorVersion >= desiredVersion {
		if _, exists := c.ObjectMeta.Annotations[majorVersionUpgradeFailureAnnotation]; exists { // if failure annotation exists, remove it
			c.removeFailuresAnnotation()
			c.logger.Infof("removing failure annotation as the cluster is already up to date")
		}
		c.logger.Infof("recheck cluster version is already up to date. current: %d, min desired: %d", c.currentMajorVersion, desiredVersion)
		return nil
	} else if isStandbyCluster {
		c.logger.Warnf("skipping major version upgrade for %s/%s standby cluster. Re-deploy standby cluster with the required Postgres version specified", c.Namespace, c.Name)
		return nil
	}

	if _, exists := c.ObjectMeta.Annotations[majorVersionUpgradeFailureAnnotation]; exists {
		c.logger.Infof("last major upgrade failed, skipping upgrade")
		return nil
	}

	if !isInMaintenanceWindow(c.Spec.MaintenanceWindows) {
		c.logger.Infof("skipping major version upgrade, not in maintenance window")
		return nil
	}

	members, err := c.patroni.GetClusterMembers(masterPod)
	if err != nil {
		c.logger.Error("could not get cluster members data from Patroni API, skipping major version upgrade")
		return err
	}
	patroniData, err := c.patroni.GetMemberData(masterPod)
	if err != nil {
		c.logger.Error("could not get members data from Patroni API, skipping major version upgrade")
		return err
	}
	patroniVer, err := semver.NewVersion(patroniData.Patroni.Version)
	if err != nil {
		c.logger.Error("error parsing Patroni version")
		patroniVer, _ = semver.NewVersion("3.0.4")
	}
	verConstraint, _ := semver.NewConstraint(">= 3.0.4")
	checkStreaming, _ := verConstraint.Validate(patroniVer)

	for _, member := range members {
		if PostgresRole(member.Role) == Leader {
			continue
		}
		if checkStreaming && member.State != "streaming" {
			c.logger.Infof("skipping major version upgrade, replica %s is not streaming from primary", member.Name)
			return nil
		}
		if member.Lag > 16*1024*1024 {
			c.logger.Infof("skipping major version upgrade, replication lag on member %s is too high", member.Name)
			return nil
		}
	}

	isUpgradeSuccess := true
	numberOfPods := len(pods)
	if allRunning && masterPod != nil {
		c.logger.Infof("healthy cluster ready to upgrade, current: %d desired: %d", c.currentMajorVersion, desiredVersion)
		if c.currentMajorVersion < desiredVersion {
			defer func() error {
				if err = c.criticalOperationLabel(pods, nil); err != nil {
					return fmt.Errorf("failed to remove critical-operation label: %s", err)
				}
				return nil
			}()
			val := "true"
			if err = c.criticalOperationLabel(pods, &val); err != nil {
				return fmt.Errorf("failed to assign critical-operation label: %s", err)
			}

			podName := &spec.NamespacedName{Namespace: masterPod.Namespace, Name: masterPod.Name}
			c.logger.Infof("triggering major version upgrade on pod %s of %d pods", masterPod.Name, numberOfPods)
			c.eventRecorder.Eventf(c.GetReference(), v1.EventTypeNormal, "Major Version Upgrade", "starting major version upgrade on pod %s of %d pods", masterPod.Name, numberOfPods)
			upgradeCommand := fmt.Sprintf("set -o pipefail && /usr/bin/python3 /scripts/inplace_upgrade.py %d 2>&1 | tee last_upgrade.log", numberOfPods)

			c.logger.Debug("checking if the spilo image runs with root or non-root (check for user id=0)")
			resultIdCheck, errIdCheck := c.ExecCommand(podName, "/bin/bash", "-c", "/usr/bin/id -u")
			if errIdCheck != nil {
				c.eventRecorder.Eventf(c.GetReference(), v1.EventTypeWarning, "Major Version Upgrade", "checking user id to run upgrade from %d to %d FAILED: %v", c.currentMajorVersion, desiredVersion, errIdCheck)
			}

			resultIdCheck = strings.TrimSuffix(resultIdCheck, "\n")
			var result, scriptErrMsg string
			if resultIdCheck != "0" {
				c.logger.Infof("user id was identified as: %s, hence default user is non-root already", resultIdCheck)
				result, err = c.ExecCommand(podName, "/bin/bash", "-c", upgradeCommand)
				scriptErrMsg, _ = c.ExecCommand(podName, "/bin/bash", "-c", "tail -n 1 last_upgrade.log")
			} else {
				c.logger.Infof("user id was identified as: %s, using su to reach the postgres user", resultIdCheck)
				result, err = c.ExecCommand(podName, "/bin/su", "postgres", "-c", upgradeCommand)
				scriptErrMsg, _ = c.ExecCommand(podName, "/bin/bash", "-c", "tail -n 1 last_upgrade.log")
			}
			if err != nil {
				isUpgradeSuccess = false
				c.annotatePostgresResource(isUpgradeSuccess)
				c.eventRecorder.Eventf(c.GetReference(), v1.EventTypeWarning, "Major Version Upgrade", "upgrade from %d to %d FAILED: %v", c.currentMajorVersion, desiredVersion, scriptErrMsg)
				return fmt.Errorf(scriptErrMsg)
			}

			c.annotatePostgresResource(isUpgradeSuccess)
			c.logger.Infof("upgrade action triggered and command completed: %s", result[:100])
			c.eventRecorder.Eventf(c.GetReference(), v1.EventTypeNormal, "Major Version Upgrade", "upgrade from %d to %d finished", c.currentMajorVersion, desiredVersion)
		}
	}

	return nil
}


================================================
File: pkg/cluster/pod.go
================================================
package cluster

import (
	"context"
	"fmt"
	"sort"
	"strconv"
	"time"

	"golang.org/x/exp/slices"

	appsv1 "k8s.io/api/apps/v1"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"

	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/patroni"
	"github.com/zalando/postgres-operator/pkg/util/retryutil"
)

func (c *Cluster) listPods() ([]v1.Pod, error) {
	listOptions := metav1.ListOptions{
		LabelSelector: c.labelsSet(false).String(),
	}

	pods, err := c.KubeClient.Pods(c.Namespace).List(context.TODO(), listOptions)
	if err != nil {
		return nil, fmt.Errorf("could not get list of pods: %v", err)
	}

	return pods.Items, nil
}

func (c *Cluster) getRolePods(role PostgresRole) ([]v1.Pod, error) {
	listOptions := metav1.ListOptions{
		LabelSelector: c.roleLabelsSet(false, role).String(),
	}

	pods, err := c.KubeClient.Pods(c.Namespace).List(context.TODO(), listOptions)
	if err != nil {
		return nil, fmt.Errorf("could not get list of pods: %v", err)
	}

	if role == Master && len(pods.Items) > 1 {
		return nil, fmt.Errorf("too many masters")
	}

	return pods.Items, nil
}

// markRollingUpdateFlagForPod sets the indicator for the rolling update requirement
// in the Pod annotation.
func (c *Cluster) markRollingUpdateFlagForPod(pod *v1.Pod, msg string) error {
	// no need to patch pod if annotation is already there
	if c.getRollingUpdateFlagFromPod(pod) {
		return nil
	}

	c.logger.Infof("mark rolling update annotation for %s: reason %s", pod.Name, msg)
	flag := make(map[string]string)
	flag[rollingUpdatePodAnnotationKey] = strconv.FormatBool(true)

	patchData, err := metaAnnotationsPatch(flag)
	if err != nil {
		return fmt.Errorf("could not form patch for pod's rolling update flag: %v", err)
	}

	err = retryutil.Retry(1*time.Second, 5*time.Second,
		func() (bool, error) {
			_, err2 := c.KubeClient.Pods(pod.Namespace).Patch(
				context.TODO(),
				pod.Name,
				types.MergePatchType,
				[]byte(patchData),
				metav1.PatchOptions{},
				"")
			if err2 != nil {
				return false, err2
			}
			return true, nil
		})
	if err != nil {
		return fmt.Errorf("could not patch pod rolling update flag %q: %v", patchData, err)
	}

	return nil
}

// getRollingUpdateFlagFromPod returns the value of the rollingUpdate flag from the given pod
func (c *Cluster) getRollingUpdateFlagFromPod(pod *v1.Pod) (flag bool) {
	anno := pod.GetAnnotations()
	flag = false

	stringFlag, exists := anno[rollingUpdatePodAnnotationKey]
	if exists {
		var err error
		c.logger.Debugf("found rolling update flag on pod %q", pod.Name)
		if flag, err = strconv.ParseBool(stringFlag); err != nil {
			c.logger.Warnf("error when parsing %q annotation for the pod %q: expected boolean value, got %q\n",
				rollingUpdatePodAnnotationKey,
				types.NamespacedName{Namespace: pod.Namespace, Name: pod.Name},
				stringFlag)
		}
	}

	return flag
}

func (c *Cluster) deletePods() error {
	c.logger.Debug("deleting pods")
	pods, err := c.listPods()
	if err != nil {
		return err
	}

	for _, obj := range pods {
		podName := util.NameFromMeta(obj.ObjectMeta)

		c.logger.Debugf("deleting pod %q", podName)
		if err := c.deletePod(podName); err != nil {
			c.logger.Errorf("could not delete pod %q: %v", podName, err)
		} else {
			c.logger.Infof("pod %q has been deleted", podName)
		}
	}
	if len(pods) > 0 {
		c.logger.Debug("pods have been deleted")
	} else {
		c.logger.Debug("no pods to delete")
	}

	return nil
}

func (c *Cluster) deletePod(podName spec.NamespacedName) error {
	c.setProcessName("deleting pod %q", podName)
	ch := c.registerPodSubscriber(podName)
	defer c.unregisterPodSubscriber(podName)

	if err := c.KubeClient.Pods(podName.Namespace).Delete(context.TODO(), podName.Name, c.deleteOptions); err != nil {
		return err
	}

	return c.waitForPodDeletion(ch)
}

func (c *Cluster) unregisterPodSubscriber(podName spec.NamespacedName) {
	c.logger.Debugf("unsubscribing from pod %q events", podName)
	c.podSubscribersMu.Lock()
	defer c.podSubscribersMu.Unlock()

	ch, ok := c.podSubscribers[podName]
	if !ok {
		panic("subscriber for pod '" + podName.String() + "' is not found")
	}

	delete(c.podSubscribers, podName)
	close(ch)
}

func (c *Cluster) registerPodSubscriber(podName spec.NamespacedName) chan PodEvent {
	c.logger.Debugf("subscribing to pod %q", podName)
	c.podSubscribersMu.Lock()
	defer c.podSubscribersMu.Unlock()

	ch := make(chan PodEvent)
	if _, ok := c.podSubscribers[podName]; ok {
		panic("pod '" + podName.String() + "' is already subscribed")
	}
	c.podSubscribers[podName] = ch

	return ch
}

func (c *Cluster) movePodFromEndOfLifeNode(pod *v1.Pod) (*v1.Pod, error) {
	var (
		eol    bool
		err    error
		newPod *v1.Pod
	)
	podName := util.NameFromMeta(pod.ObjectMeta)

	if eol, err = c.podIsEndOfLife(pod); err != nil {
		return nil, fmt.Errorf("could not get node %q: %v", pod.Spec.NodeName, err)
	} else if !eol {
		c.logger.Infof("check failed: pod %q is already on a live node", podName)
		return pod, nil
	}

	c.setProcessName("moving pod %q out of end-of-life node %q", podName, pod.Spec.NodeName)
	c.logger.Infof("moving pod %q out of the end-of-life node %q", podName, pod.Spec.NodeName)

	if newPod, err = c.recreatePod(podName); err != nil {
		return nil, fmt.Errorf("could not move pod: %v", err)
	}

	if newPod.Spec.NodeName == pod.Spec.NodeName {
		return nil, fmt.Errorf("pod %q remained on the same node", podName)
	}

	if eol, err = c.podIsEndOfLife(newPod); err != nil {
		return nil, fmt.Errorf("could not get node %q: %v", pod.Spec.NodeName, err)
	} else if eol {
		c.logger.Warningf("pod %q moved to end-of-life node %q", podName, newPod.Spec.NodeName)
		return newPod, nil
	}

	c.logger.Infof("pod %q moved from node %q to node %q", podName, pod.Spec.NodeName, newPod.Spec.NodeName)

	return newPod, nil
}

// MigrateMasterPod migrates master pod via failover to a replica
func (c *Cluster) MigrateMasterPod(podName spec.NamespacedName) error {
	var (
		err error
		eol bool
	)

	oldMaster, err := c.KubeClient.Pods(podName.Namespace).Get(context.TODO(), podName.Name, metav1.GetOptions{})
	if err != nil {
		return fmt.Errorf("could not get master pod: %v", err)
	}

	c.logger.Infof("starting process to migrate master pod %q", podName)
	if eol, err = c.podIsEndOfLife(oldMaster); err != nil {
		return fmt.Errorf("could not get node %q: %v", oldMaster.Spec.NodeName, err)
	}
	if !eol {
		c.logger.Debug("no action needed: master pod is already on a live node")
		return nil
	}

	if role := PostgresRole(oldMaster.Labels[c.OpConfig.PodRoleLabel]); role != Master {
		c.logger.Warningf("no action needed: pod %q is not the master (anymore)", podName)
		return nil
	}
	// we must have a statefulset in the cluster for the migration to work
	if c.Statefulset == nil {
		var sset *appsv1.StatefulSet
		if sset, err = c.KubeClient.StatefulSets(c.Namespace).Get(
			context.TODO(),
			c.statefulSetName(),
			metav1.GetOptions{}); err != nil {
			return fmt.Errorf("could not retrieve cluster statefulset: %v", err)
		}
		c.Statefulset = sset
	}
	// we may not have a cached statefulset if the initial cluster sync has aborted, revert to the spec in that case
	masterCandidateName := podName
	masterCandidatePod := oldMaster
	if *c.Statefulset.Spec.Replicas > 1 {
		if masterCandidateName, err = c.getSwitchoverCandidate(oldMaster); err != nil {
			return fmt.Errorf("could not find suitable replica pod as candidate for failover: %v", err)
		}
		masterCandidatePod, err = c.KubeClient.Pods(masterCandidateName.Namespace).Get(context.TODO(), masterCandidateName.Name, metav1.GetOptions{})
		if err != nil {
			return fmt.Errorf("could not get master candidate pod: %v", err)
		}
	} else {
		c.logger.Warningf("migrating single pod cluster %q, this will cause downtime of the Postgres cluster until pod is back", c.clusterName())
	}

	// there are two cases for each postgres cluster that has its master pod on the node to migrate from:
	// - the cluster has some replicas - migrate one of those if necessary and failover to it
	// - there are no replicas - just terminate the master and wait until it respawns
	// in both cases the result is the new master up and running on a new node.

	if masterCandidatePod == nil {
		if _, err = c.movePodFromEndOfLifeNode(oldMaster); err != nil {
			return fmt.Errorf("could not move pod: %v", err)
		}
		return nil
	}

	if _, err = c.movePodFromEndOfLifeNode(masterCandidatePod); err != nil {
		return fmt.Errorf("could not move pod: %v", err)
	}

	scheduleSwitchover := false
	if !isInMaintenanceWindow(c.Spec.MaintenanceWindows) {
		c.logger.Infof("postponing switchover, not in maintenance window")
		scheduleSwitchover = true
	}
	err = retryutil.Retry(1*time.Minute, 5*time.Minute,
		func() (bool, error) {
			err := c.Switchover(oldMaster, masterCandidateName, scheduleSwitchover)
			if err != nil {
				c.logger.Errorf("could not switchover to pod %q: %v", masterCandidateName, err)
				return false, nil
			}
			return true, nil
		},
	)

	if err != nil {
		return fmt.Errorf("could not migrate master pod: %v", err)
	}

	return nil
}

// MigrateReplicaPod recreates pod on a new node
func (c *Cluster) MigrateReplicaPod(podName spec.NamespacedName, fromNodeName string) error {
	replicaPod, err := c.KubeClient.Pods(podName.Namespace).Get(context.TODO(), podName.Name, metav1.GetOptions{})
	if err != nil {
		return fmt.Errorf("could not get pod: %v", err)
	}

	c.logger.Infof("migrating replica pod %q to live node", podName)

	if replicaPod.Spec.NodeName != fromNodeName {
		c.logger.Infof("check failed: pod %q has already migrated to node %q", podName, replicaPod.Spec.NodeName)
		return nil
	}

	if role := PostgresRole(replicaPod.Labels[c.OpConfig.PodRoleLabel]); role != Replica {
		return fmt.Errorf("check failed: pod %q is not a replica", podName)
	}

	_, err = c.movePodFromEndOfLifeNode(replicaPod)
	if err != nil {
		return fmt.Errorf("could not move pod: %v", err)
	}

	return nil
}

func (c *Cluster) getPatroniConfig(pod *v1.Pod) (acidv1.Patroni, map[string]string, error) {
	var (
		patroniConfig acidv1.Patroni
		pgParameters  map[string]string
	)
	podName := util.NameFromMeta(pod.ObjectMeta)
	err := retryutil.Retry(c.OpConfig.PatroniAPICheckInterval, c.OpConfig.PatroniAPICheckTimeout,
		func() (bool, error) {
			var err error
			patroniConfig, pgParameters, err = c.patroni.GetConfig(pod)

			if err != nil {
				return false, err
			}
			return true, nil
		},
	)

	if err != nil {
		return acidv1.Patroni{}, nil, fmt.Errorf("could not get Postgres config from pod %s: %v", podName, err)
	}

	return patroniConfig, pgParameters, nil
}

func (c *Cluster) getPatroniMemberData(pod *v1.Pod) (patroni.MemberData, error) {
	var memberData patroni.MemberData
	err := retryutil.Retry(c.OpConfig.PatroniAPICheckInterval, c.OpConfig.PatroniAPICheckTimeout,
		func() (bool, error) {
			var err error
			memberData, err = c.patroni.GetMemberData(pod)

			if err != nil {
				return false, err
			}
			return true, nil
		},
	)
	if err != nil {
		return patroni.MemberData{}, fmt.Errorf("could not get member data: %v", err)
	}
	if memberData.State == "creating replica" {
		return patroni.MemberData{}, fmt.Errorf("replica currently being initialized")
	}

	return memberData, nil
}

func (c *Cluster) recreatePod(podName spec.NamespacedName) (*v1.Pod, error) {
	stopCh := make(chan struct{})
	ch := c.registerPodSubscriber(podName)
	defer c.unregisterPodSubscriber(podName)
	defer close(stopCh)

	err := retryutil.Retry(1*time.Second, 5*time.Second,
		func() (bool, error) {
			err2 := c.KubeClient.Pods(podName.Namespace).Delete(
				context.TODO(),
				podName.Name,
				c.deleteOptions)
			if err2 != nil {
				return false, err2
			}
			return true, nil
		})
	if err != nil {
		return nil, fmt.Errorf("could not delete pod: %v", err)
	}

	if err := c.waitForPodDeletion(ch); err != nil {
		return nil, err
	}
	pod, err := c.waitForPodLabel(ch, stopCh, nil)
	if err != nil {
		return nil, err
	}
	c.logger.Infof("pod %q has been recreated", podName)
	return pod, nil
}

func (c *Cluster) recreatePods(pods []v1.Pod, switchoverCandidates []spec.NamespacedName) error {
	c.setProcessName("starting to recreate pods")
	c.logger.Infof("there are %d pods in the cluster to recreate", len(pods))

	var (
		masterPod, newMasterPod *v1.Pod
	)
	replicas := switchoverCandidates

	for i, pod := range pods {
		role := PostgresRole(pod.Labels[c.OpConfig.PodRoleLabel])

		if role == Master {
			masterPod = &pods[i]
			continue
		}

		podName := util.NameFromMeta(pods[i].ObjectMeta)
		newPod, err := c.recreatePod(podName)
		if err != nil {
			return fmt.Errorf("could not recreate replica pod %q: %v", util.NameFromMeta(pod.ObjectMeta), err)
		}

		newRole := PostgresRole(newPod.Labels[c.OpConfig.PodRoleLabel])
		if newRole == Replica {
			replicas = append(replicas, util.NameFromMeta(pod.ObjectMeta))
		} else if newRole == Master {
			newMasterPod = newPod
		}
	}

	if masterPod != nil {
		// switchover if
		// 1. we have not observed a new master pod when re-creating former replicas
		// 2. we know possible switchover targets even when no replicas were recreated
		if newMasterPod == nil && len(replicas) > 0 {
			masterCandidate, err := c.getSwitchoverCandidate(masterPod)
			if err != nil {
				// do not recreate master now so it will keep the update flag and switchover will be retried on next sync
				return fmt.Errorf("skipping switchover: %v", err)
			}
			if err := c.Switchover(masterPod, masterCandidate, false); err != nil {
				return fmt.Errorf("could not perform switch over: %v", err)
			}
		} else if newMasterPod == nil && len(replicas) == 0 {
			c.logger.Warningf("cannot perform switch over before re-creating the pod: no replicas")
		}
		c.logger.Infof("recreating old master pod %q", util.NameFromMeta(masterPod.ObjectMeta))

		if _, err := c.recreatePod(util.NameFromMeta(masterPod.ObjectMeta)); err != nil {
			return fmt.Errorf("could not recreate old master pod %q: %v", util.NameFromMeta(masterPod.ObjectMeta), err)
		}
	}

	return nil
}

func (c *Cluster) getSwitchoverCandidate(master *v1.Pod) (spec.NamespacedName, error) {

	var members []patroni.ClusterMember
	candidates := make([]patroni.ClusterMember, 0)
	syncCandidates := make([]patroni.ClusterMember, 0)

	err := retryutil.Retry(c.OpConfig.PatroniAPICheckInterval, c.OpConfig.PatroniAPICheckTimeout,
		func() (bool, error) {
			var err error
			members, err = c.patroni.GetClusterMembers(master)
			if err != nil {
				return false, err
			}

			// look for SyncStandby candidates (which also implies pod is in running state)
			for _, member := range members {
				if PostgresRole(member.Role) == SyncStandby {
					syncCandidates = append(syncCandidates, member)
				}
				if PostgresRole(member.Role) != Leader && PostgresRole(member.Role) != StandbyLeader && slices.Contains([]string{"running", "streaming", "in archive recovery"}, member.State) {
					candidates = append(candidates, member)
				}
			}

			// if synchronous mode is enabled and no SyncStandy was found
			// return false for retry - cannot failover with no sync candidate
			if c.Spec.Patroni.SynchronousMode && len(syncCandidates) == 0 {
				c.logger.Warnf("no sync standby found - retrying fetching cluster members")
				return false, nil
			}

			// retry also in asynchronous mode when no replica candidate was found
			if !c.Spec.Patroni.SynchronousMode && len(candidates) == 0 {
				c.logger.Warnf("no replica candidate found - retrying fetching cluster members")
				return false, nil
			}

			return true, nil
		},
	)
	if err != nil {
		return spec.NamespacedName{}, fmt.Errorf("failed to get Patroni cluster members: %s", err)
	}

	// pick candidate with lowest lag
	if len(syncCandidates) > 0 {
		sort.Slice(syncCandidates, func(i, j int) bool {
			return syncCandidates[i].Lag < syncCandidates[j].Lag
		})
		return spec.NamespacedName{Namespace: master.Namespace, Name: syncCandidates[0].Name}, nil
	}
	if len(candidates) > 0 {
		sort.Slice(candidates, func(i, j int) bool {
			return candidates[i].Lag < candidates[j].Lag
		})
		return spec.NamespacedName{Namespace: master.Namespace, Name: candidates[0].Name}, nil
	}

	return spec.NamespacedName{}, fmt.Errorf("no switchover candidate found")
}

func (c *Cluster) podIsEndOfLife(pod *v1.Pod) (bool, error) {
	node, err := c.KubeClient.Nodes().Get(context.TODO(), pod.Spec.NodeName, metav1.GetOptions{})
	if err != nil {
		return false, err
	}
	return node.Spec.Unschedulable || !util.MapContains(node.Labels, c.OpConfig.NodeReadinessLabel), nil

}


================================================
File: pkg/cluster/pod_test.go
================================================
package cluster

import (
	"bytes"
	"fmt"
	"io"
	"net/http"
	"testing"
	"time"

	"github.com/golang/mock/gomock"
	"github.com/zalando/postgres-operator/mocks"
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"github.com/zalando/postgres-operator/pkg/util/patroni"
)

func TestGetSwitchoverCandidate(t *testing.T) {
	testName := "test getting right switchover candidate"
	namespace := "default"

	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	var cluster = New(
		Config{
			OpConfig: config.Config{
				PatroniAPICheckInterval: time.Duration(1),
				PatroniAPICheckTimeout:  time.Duration(5),
			},
		}, k8sutil.KubernetesClient{}, acidv1.Postgresql{}, logger, eventRecorder)

	// simulate different member scenarios
	tests := []struct {
		subtest           string
		clusterJson       string
		syncModeEnabled   bool
		expectedCandidate spec.NamespacedName
		expectedError     error
	}{
		{
			subtest:           "choose sync_standby over replica",
			clusterJson:       `{"members": [{"name": "acid-test-cluster-0", "role": "leader", "state": "running", "api_url": "http://192.168.100.1:8008/patroni", "host": "192.168.100.1", "port": 5432, "timeline": 1}, {"name": "acid-test-cluster-1", "role": "sync_standby", "state": "streaming", "api_url": "http://192.168.100.2:8008/patroni", "host": "192.168.100.2", "port": 5432, "timeline": 1, "lag": 0}, {"name": "acid-test-cluster-2", "role": "replica", "state": "streaming", "api_url": "http://192.168.100.3:8008/patroni", "host": "192.168.100.3", "port": 5432, "timeline": 1, "lag": 0}]}`,
			syncModeEnabled:   true,
			expectedCandidate: spec.NamespacedName{Namespace: namespace, Name: "acid-test-cluster-1"},
			expectedError:     nil,
		},
		{
			subtest:           "no running sync_standby available",
			clusterJson:       `{"members": [{"name": "acid-test-cluster-0", "role": "leader", "state": "running", "api_url": "http://192.168.100.1:8008/patroni", "host": "192.168.100.1", "port": 5432, "timeline": 1}, {"name": "acid-test-cluster-1", "role": "replica", "state": "streaming", "api_url": "http://192.168.100.2:8008/patroni", "host": "192.168.100.2", "port": 5432, "timeline": 1, "lag": 0}]}`,
			syncModeEnabled:   true,
			expectedCandidate: spec.NamespacedName{},
			expectedError:     fmt.Errorf("failed to get Patroni cluster members: unexpected end of JSON input"),
		},
		{
			subtest:           "choose replica with lowest lag",
			clusterJson:       `{"members": [{"name": "acid-test-cluster-0", "role": "leader", "state": "running", "api_url": "http://192.168.100.1:8008/patroni", "host": "192.168.100.1", "port": 5432, "timeline": 1}, {"name": "acid-test-cluster-1", "role": "replica", "state": "streaming", "api_url": "http://192.168.100.2:8008/patroni", "host": "192.168.100.2", "port": 5432, "timeline": 1, "lag": 5}, {"name": "acid-test-cluster-2", "role": "replica", "state": "streaming", "api_url": "http://192.168.100.3:8008/patroni", "host": "192.168.100.3", "port": 5432, "timeline": 1, "lag": 2}]}`,
			syncModeEnabled:   false,
			expectedCandidate: spec.NamespacedName{Namespace: namespace, Name: "acid-test-cluster-2"},
			expectedError:     nil,
		},
		{
			subtest:           "choose first replica when lag is equal everywhere",
			clusterJson:       `{"members": [{"name": "acid-test-cluster-0", "role": "leader", "state": "running", "api_url": "http://192.168.100.1:8008/patroni", "host": "192.168.100.1", "port": 5432, "timeline": 1}, {"name": "acid-test-cluster-1", "role": "replica", "state": "streaming", "api_url": "http://192.168.100.2:8008/patroni", "host": "192.168.100.2", "port": 5432, "timeline": 1, "lag": 5}, {"name": "acid-test-cluster-2", "role": "replica", "state": "running", "api_url": "http://192.168.100.3:8008/patroni", "host": "192.168.100.3", "port": 5432, "timeline": 1, "lag": 5}]}`,
			syncModeEnabled:   false,
			expectedCandidate: spec.NamespacedName{Namespace: namespace, Name: "acid-test-cluster-1"},
			expectedError:     nil,
		},
		{
			subtest:           "no running replica available",
			clusterJson:       `{"members": [{"name": "acid-test-cluster-0", "role": "leader", "state": "running", "api_url": "http://192.168.100.1:8008/patroni", "host": "192.168.100.1", "port": 5432, "timeline": 2}, {"name": "acid-test-cluster-1", "role": "replica", "state": "starting", "api_url": "http://192.168.100.2:8008/patroni", "host": "192.168.100.2", "port": 5432, "timeline": 2}]}`,
			syncModeEnabled:   false,
			expectedCandidate: spec.NamespacedName{},
			expectedError:     fmt.Errorf("failed to get Patroni cluster members: unexpected end of JSON input"),
		},
		{
			subtest:           "replicas with different status",
			clusterJson:       `{"members": [{"name": "acid-test-cluster-0", "role": "leader", "state": "running", "api_url": "http://192.168.100.1:8008/patroni", "host": "192.168.100.1", "port": 5432, "timeline": 1}, {"name": "acid-test-cluster-1", "role": "replica", "state": "streaming", "api_url": "http://192.168.100.2:8008/patroni", "host": "192.168.100.2", "port": 5432, "timeline": 1, "lag": 5}, {"name": "acid-test-cluster-2", "role": "replica", "state": "in archive recovery", "api_url": "http://192.168.100.3:8008/patroni", "host": "192.168.100.3", "port": 5432, "timeline": 1, "lag": 2}]}`,
			syncModeEnabled:   false,
			expectedCandidate: spec.NamespacedName{Namespace: namespace, Name: "acid-test-cluster-2"},
			expectedError:     nil,
		},
	}

	for _, tt := range tests {
		// mocking cluster members
		r := io.NopCloser(bytes.NewReader([]byte(tt.clusterJson)))

		response := http.Response{
			StatusCode: 200,
			Body:       r,
		}

		mockClient := mocks.NewMockHTTPClient(ctrl)
		mockClient.EXPECT().Get(gomock.Any()).Return(&response, nil).AnyTimes()

		p := patroni.New(patroniLogger, mockClient)
		cluster.patroni = p
		mockMasterPod := newMockPod("192.168.100.1")
		mockMasterPod.Namespace = namespace
		cluster.Spec.Patroni.SynchronousMode = tt.syncModeEnabled

		candidate, err := cluster.getSwitchoverCandidate(mockMasterPod)
		if err != nil && err.Error() != tt.expectedError.Error() {
			t.Errorf("%s - %s: unexpected error, %v", testName, tt.subtest, err)
		}

		if candidate != tt.expectedCandidate {
			t.Errorf("%s - %s: unexpect switchover candidate, got %s, expected %s", testName, tt.subtest, candidate, tt.expectedCandidate)
		}
	}
}


================================================
File: pkg/cluster/resources.go
================================================
package cluster

import (
	"context"
	"fmt"
	"strconv"
	"strings"

	appsv1 "k8s.io/api/apps/v1"
	batchv1 "k8s.io/api/batch/v1"
	v1 "k8s.io/api/core/v1"
	policyv1 "k8s.io/api/policy/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"

	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"github.com/zalando/postgres-operator/pkg/util/retryutil"
)

const (
	rollingUpdatePodAnnotationKey = "zalando-postgres-operator-rolling-update-required"
)

func (c *Cluster) listResources() error {
	if c.PrimaryPodDisruptionBudget != nil {
		c.logger.Infof("found primary pod disruption budget: %q (uid: %q)", util.NameFromMeta(c.PrimaryPodDisruptionBudget.ObjectMeta), c.PrimaryPodDisruptionBudget.UID)
	}

	if c.CriticalOpPodDisruptionBudget != nil {
		c.logger.Infof("found pod disruption budget for critical operations: %q (uid: %q)", util.NameFromMeta(c.CriticalOpPodDisruptionBudget.ObjectMeta), c.CriticalOpPodDisruptionBudget.UID)

	}

	if c.Statefulset != nil {
		c.logger.Infof("found statefulset: %q (uid: %q)", util.NameFromMeta(c.Statefulset.ObjectMeta), c.Statefulset.UID)
	}

	for appId, stream := range c.Streams {
		c.logger.Infof("found stream: %q with application id %q (uid: %q)", util.NameFromMeta(stream.ObjectMeta), appId, stream.UID)
	}

	if c.LogicalBackupJob != nil {
		c.logger.Infof("found logical backup job: %q (uid: %q)", util.NameFromMeta(c.LogicalBackupJob.ObjectMeta), c.LogicalBackupJob.UID)
	}

	for uid, secret := range c.Secrets {
		c.logger.Infof("found secret: %q (uid: %q) namespace: %s", util.NameFromMeta(secret.ObjectMeta), uid, secret.ObjectMeta.Namespace)
	}

	for role, service := range c.Services {
		c.logger.Infof("found %s service: %q (uid: %q)", role, util.NameFromMeta(service.ObjectMeta), service.UID)
	}

	for role, endpoint := range c.Endpoints {
		c.logger.Infof("found %s endpoint: %q (uid: %q)", role, util.NameFromMeta(endpoint.ObjectMeta), endpoint.UID)
	}

	if c.patroniKubernetesUseConfigMaps() {
		for suffix, configmap := range c.PatroniConfigMaps {
			c.logger.Infof("found %s Patroni config map: %q (uid: %q)", suffix, util.NameFromMeta(configmap.ObjectMeta), configmap.UID)
		}
	} else {
		for suffix, endpoint := range c.PatroniEndpoints {
			c.logger.Infof("found %s Patroni endpoint: %q (uid: %q)", suffix, util.NameFromMeta(endpoint.ObjectMeta), endpoint.UID)
		}
	}

	pods, err := c.listPods()
	if err != nil {
		return fmt.Errorf("could not get the list of pods: %v", err)
	}

	for _, obj := range pods {
		c.logger.Infof("found pod: %q (uid: %q)", util.NameFromMeta(obj.ObjectMeta), obj.UID)
	}

	for uid, pvc := range c.VolumeClaims {
		c.logger.Infof("found persistent volume claim: %q (uid: %q)", util.NameFromMeta(pvc.ObjectMeta), uid)
	}

	for role, poolerObjs := range c.ConnectionPooler {
		if poolerObjs.Deployment != nil {
			c.logger.Infof("found %s pooler deployment: %q (uid: %q) ", role, util.NameFromMeta(poolerObjs.Deployment.ObjectMeta), poolerObjs.Deployment.UID)
		}
		if poolerObjs.Service != nil {
			c.logger.Infof("found %s pooler service: %q (uid: %q) ", role, util.NameFromMeta(poolerObjs.Service.ObjectMeta), poolerObjs.Service.UID)
		}
	}

	return nil
}

func (c *Cluster) createStatefulSet() (*appsv1.StatefulSet, error) {
	c.setProcessName("creating statefulset")
	// check if it's allowed that spec contains initContainers
	if c.Spec.InitContainers != nil && len(c.Spec.InitContainers) > 0 &&
		c.OpConfig.EnableInitContainers != nil && !(*c.OpConfig.EnableInitContainers) {
		return nil, fmt.Errorf("initContainers specified but disabled in configuration")
	}
	// check if it's allowed that spec contains sidecars
	if c.Spec.Sidecars != nil && len(c.Spec.Sidecars) > 0 &&
		c.OpConfig.EnableSidecars != nil && !(*c.OpConfig.EnableSidecars) {
		return nil, fmt.Errorf("sidecar containers specified but disabled in configuration")
	}

	statefulSetSpec, err := c.generateStatefulSet(&c.Spec)
	if err != nil {
		return nil, fmt.Errorf("could not generate statefulset: %v", err)
	}
	statefulSet, err := c.KubeClient.StatefulSets(statefulSetSpec.Namespace).Create(
		context.TODO(),
		statefulSetSpec,
		metav1.CreateOptions{})
	if err != nil {
		return nil, err
	}
	c.Statefulset = statefulSet
	c.logger.Debugf("created new statefulset %q, uid: %q", util.NameFromMeta(statefulSet.ObjectMeta), statefulSet.UID)

	return statefulSet, nil
}

func getPodIndex(podName string) (int32, error) {
	parts := strings.Split(podName, "-")
	if len(parts) == 0 {
		return 0, fmt.Errorf("pod has no index part")
	}

	postfix := parts[len(parts)-1]
	res, err := strconv.ParseInt(postfix, 10, 32)
	if err != nil {
		return 0, fmt.Errorf("could not parse pod index: %v", err)
	}

	return int32(res), nil
}

func (c *Cluster) preScaleDown(newStatefulSet *appsv1.StatefulSet) error {
	masterPod, err := c.getRolePods(Master)
	if err != nil {
		return fmt.Errorf("could not get master pod: %v", err)
	}
	if len(masterPod) == 0 {
		return fmt.Errorf("no master pod is running in the cluster")
	}

	podNum, err := getPodIndex(masterPod[0].Name)
	if err != nil {
		return fmt.Errorf("could not get pod number: %v", err)
	}

	//Check if scale down affects current master pod
	if *newStatefulSet.Spec.Replicas >= podNum+1 {
		return nil
	}

	podName := fmt.Sprintf("%s-0", c.Statefulset.Name)
	masterCandidatePod, err := c.KubeClient.Pods(c.clusterNamespace()).Get(context.TODO(), podName, metav1.GetOptions{})
	if err != nil {
		return fmt.Errorf("could not get master candidate pod: %v", err)
	}

	// some sanity check
	if !util.MapContains(masterCandidatePod.Labels, c.OpConfig.ClusterLabels) ||
		!util.MapContains(masterCandidatePod.Labels, map[string]string{c.OpConfig.ClusterNameLabel: c.Name}) {
		return fmt.Errorf("pod %q does not belong to cluster", podName)
	}

	if err := c.patroni.Switchover(&masterPod[0], masterCandidatePod.Name, ""); err != nil {
		return fmt.Errorf("could not switchover: %v", err)
	}

	return nil
}

func (c *Cluster) updateStatefulSet(newStatefulSet *appsv1.StatefulSet) error {
	c.setProcessName("updating statefulset")
	if c.Statefulset == nil {
		return fmt.Errorf("there is no statefulset in the cluster")
	}
	statefulSetName := util.NameFromMeta(c.Statefulset.ObjectMeta)

	//scale down
	if *c.Statefulset.Spec.Replicas > *newStatefulSet.Spec.Replicas {
		if err := c.preScaleDown(newStatefulSet); err != nil {
			c.logger.Warningf("could not scale down: %v", err)
		}
	}
	c.logger.Debug("updating statefulset")

	patchData, err := specPatch(newStatefulSet.Spec)
	if err != nil {
		return fmt.Errorf("could not form patch for the statefulset %q: %v", statefulSetName, err)
	}

	statefulSet, err := c.KubeClient.StatefulSets(c.Statefulset.Namespace).Patch(
		context.TODO(),
		c.Statefulset.Name,
		types.MergePatchType,
		patchData,
		metav1.PatchOptions{},
		"")
	if err != nil {
		return fmt.Errorf("could not patch statefulset spec %q: %v", statefulSetName, err)
	}

	c.Statefulset = statefulSet

	return nil
}

// replaceStatefulSet deletes an old StatefulSet and creates the new using spec in the PostgreSQL CRD.
func (c *Cluster) replaceStatefulSet(newStatefulSet *appsv1.StatefulSet) error {
	c.setProcessName("replacing statefulset")
	if c.Statefulset == nil {
		return fmt.Errorf("there is no statefulset in the cluster")
	}

	statefulSetName := util.NameFromMeta(c.Statefulset.ObjectMeta)
	c.logger.Debug("replacing statefulset")

	// Delete the current statefulset without deleting the pods
	deletePropagationPolicy := metav1.DeletePropagationOrphan
	oldStatefulset := c.Statefulset

	options := metav1.DeleteOptions{PropagationPolicy: &deletePropagationPolicy}
	err := c.KubeClient.StatefulSets(oldStatefulset.Namespace).Delete(context.TODO(), oldStatefulset.Name, options)
	if err != nil {
		return fmt.Errorf("could not delete statefulset %q: %v", statefulSetName, err)
	}
	// make sure we clear the stored statefulset status if the subsequent create fails.
	c.Statefulset = nil
	// wait until the statefulset is truly deleted
	c.logger.Debug("waiting for the statefulset to be deleted")

	err = retryutil.Retry(c.OpConfig.ResourceCheckInterval, c.OpConfig.ResourceCheckTimeout,
		func() (bool, error) {
			_, err2 := c.KubeClient.StatefulSets(oldStatefulset.Namespace).Get(context.TODO(), oldStatefulset.Name, metav1.GetOptions{})
			if err2 == nil {
				return false, nil
			}
			if k8sutil.ResourceNotFound(err2) {
				return true, nil
			}
			return false, err2
		})
	if err != nil {
		return fmt.Errorf("could not delete statefulset: %v", err)
	}

	// create the new statefulset with the desired spec. It would take over the remaining pods.
	createdStatefulset, err := c.KubeClient.StatefulSets(newStatefulSet.Namespace).Create(context.TODO(), newStatefulSet, metav1.CreateOptions{})
	if err != nil {
		return fmt.Errorf("could not create statefulset %q: %v", statefulSetName, err)
	}
	// check that all the previous replicas were picked up.
	if newStatefulSet.Spec.Replicas == oldStatefulset.Spec.Replicas &&
		createdStatefulset.Status.Replicas != oldStatefulset.Status.Replicas {
		c.logger.Warningf("number of pods for the old and updated Statefulsets is not identical")
	}

	c.Statefulset = createdStatefulset
	return nil
}

func (c *Cluster) deleteStatefulSet() error {
	c.setProcessName("deleting statefulset")
	c.logger.Debug("deleting statefulset")
	if c.Statefulset == nil {
		c.logger.Debug("there is no statefulset in the cluster")
		return nil
	}

	err := c.KubeClient.StatefulSets(c.Statefulset.Namespace).Delete(context.TODO(), c.Statefulset.Name, c.deleteOptions)
	if k8sutil.ResourceNotFound(err) {
		c.logger.Debugf("statefulset %q has already been deleted", util.NameFromMeta(c.Statefulset.ObjectMeta))
	} else if err != nil {
		return err
	}

	c.logger.Infof("statefulset %q has been deleted", util.NameFromMeta(c.Statefulset.ObjectMeta))
	c.Statefulset = nil

	if err := c.deletePods(); err != nil {
		return fmt.Errorf("could not delete pods: %v", err)
	}

	if c.OpConfig.EnablePersistentVolumeClaimDeletion != nil && *c.OpConfig.EnablePersistentVolumeClaimDeletion {
		if err := c.deletePersistentVolumeClaims(); err != nil {
			return fmt.Errorf("could not delete persistent volume claims: %v", err)
		}
	} else {
		c.logger.Info("not deleting persistent volume claims because disabled in configuration")
	}

	return nil
}

func (c *Cluster) createService(role PostgresRole) (*v1.Service, error) {
	c.setProcessName("creating %v service", role)

	serviceSpec := c.generateService(role, &c.Spec)
	service, err := c.KubeClient.Services(serviceSpec.Namespace).Create(context.TODO(), serviceSpec, metav1.CreateOptions{})
	if err != nil {
		return nil, err
	}

	c.Services[role] = service
	return service, nil
}

func (c *Cluster) updateService(role PostgresRole, oldService *v1.Service, newService *v1.Service) (*v1.Service, error) {
	var err error
	svc := oldService

	serviceName := util.NameFromMeta(oldService.ObjectMeta)
	match, reason := c.compareServices(oldService, newService)
	if !match {
		c.logServiceChanges(role, oldService, newService, false, reason)
		c.setProcessName("updating %v service", role)

		// now, patch the service spec, but when disabling LoadBalancers do update instead
		// patch does not work because of LoadBalancerSourceRanges field (even if set to nil)
		oldServiceType := oldService.Spec.Type
		newServiceType := newService.Spec.Type
		if newServiceType == "ClusterIP" && newServiceType != oldServiceType {
			newService.ResourceVersion = oldService.ResourceVersion
			newService.Spec.ClusterIP = oldService.Spec.ClusterIP
		}
		svc, err = c.KubeClient.Services(serviceName.Namespace).Update(context.TODO(), newService, metav1.UpdateOptions{})
		if err != nil {
			return nil, fmt.Errorf("could not update service %q: %v", serviceName, err)
		}
	}

	if changed, _ := c.compareAnnotations(oldService.Annotations, newService.Annotations, nil); changed {
		patchData, err := metaAnnotationsPatch(newService.Annotations)
		if err != nil {
			return nil, fmt.Errorf("could not form patch for service %q annotations: %v", oldService.Name, err)
		}
		svc, err = c.KubeClient.Services(serviceName.Namespace).Patch(context.TODO(), newService.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{})
		if err != nil {
			return nil, fmt.Errorf("could not patch annotations for service %q: %v", oldService.Name, err)
		}
	}

	return svc, nil
}

func (c *Cluster) deleteService(role PostgresRole) error {
	c.setProcessName("deleting service")
	c.logger.Debugf("deleting %s service", role)

	if c.Services[role] == nil {
		c.logger.Debugf("No service for %s role was found, nothing to delete", role)
		return nil
	}

	if err := c.KubeClient.Services(c.Services[role].Namespace).Delete(context.TODO(), c.Services[role].Name, c.deleteOptions); err != nil {
		if !k8sutil.ResourceNotFound(err) {
			return fmt.Errorf("could not delete %s service: %v", role, err)
		}
		c.logger.Debugf("%s service has already been deleted", role)
	}

	c.logger.Infof("%s service %q has been deleted", role, util.NameFromMeta(c.Services[role].ObjectMeta))
	delete(c.Services, role)

	return nil
}

func (c *Cluster) createEndpoint(role PostgresRole) (*v1.Endpoints, error) {
	var (
		subsets []v1.EndpointSubset
	)
	c.setProcessName("creating endpoint")
	if !c.isNewCluster() {
		subsets = c.generateEndpointSubsets(role)
	} else {
		// Patroni will populate the master endpoint for the new cluster
		// The replica endpoint will be filled-in by the service selector.
		subsets = make([]v1.EndpointSubset, 0)
	}
	endpointsSpec := c.generateEndpoint(role, subsets)

	endpoints, err := c.KubeClient.Endpoints(endpointsSpec.Namespace).Create(context.TODO(), endpointsSpec, metav1.CreateOptions{})
	if err != nil {
		return nil, fmt.Errorf("could not create %s endpoint: %v", role, err)
	}

	c.Endpoints[role] = endpoints

	return endpoints, nil
}

func (c *Cluster) generateEndpointSubsets(role PostgresRole) []v1.EndpointSubset {
	result := make([]v1.EndpointSubset, 0)
	pods, err := c.getRolePods(role)
	if err != nil {
		if role == Master {
			c.logger.Warningf("could not obtain the address for %s pod: %v", role, err)
		} else {
			c.logger.Warningf("could not obtain the addresses for %s pods: %v", role, err)
		}
		return result
	}

	endPointAddresses := make([]v1.EndpointAddress, 0)
	for _, pod := range pods {
		endPointAddresses = append(endPointAddresses, v1.EndpointAddress{IP: pod.Status.PodIP})
	}
	if len(endPointAddresses) > 0 {
		result = append(result, v1.EndpointSubset{
			Addresses: endPointAddresses,
			Ports:     []v1.EndpointPort{{Name: "postgresql", Port: 5432, Protocol: "TCP"}},
		})
	} else if role == Master {
		c.logger.Warningf("master is not running, generated master endpoint does not contain any addresses")
	}

	return result
}

func (c *Cluster) createPrimaryPodDisruptionBudget() error {
	c.logger.Debug("creating primary pod disruption budget")
	if c.PrimaryPodDisruptionBudget != nil {
		c.logger.Warning("primary pod disruption budget already exists in the cluster")
		return nil
	}

	podDisruptionBudgetSpec := c.generatePrimaryPodDisruptionBudget()
	podDisruptionBudget, err := c.KubeClient.
		PodDisruptionBudgets(podDisruptionBudgetSpec.Namespace).
		Create(context.TODO(), podDisruptionBudgetSpec, metav1.CreateOptions{})

	if err != nil {
		return err
	}
	c.logger.Infof("primary pod disruption budget %q has been successfully created", util.NameFromMeta(podDisruptionBudget.ObjectMeta))
	c.PrimaryPodDisruptionBudget = podDisruptionBudget

	return nil
}

func (c *Cluster) createCriticalOpPodDisruptionBudget() error {
	c.logger.Debug("creating pod disruption budget for critical operations")
	if c.CriticalOpPodDisruptionBudget != nil {
		c.logger.Warning("pod disruption budget for critical operations already exists in the cluster")
		return nil
	}

	podDisruptionBudgetSpec := c.generateCriticalOpPodDisruptionBudget()
	podDisruptionBudget, err := c.KubeClient.
		PodDisruptionBudgets(podDisruptionBudgetSpec.Namespace).
		Create(context.TODO(), podDisruptionBudgetSpec, metav1.CreateOptions{})

	if err != nil {
		return err
	}
	c.logger.Infof("pod disruption budget for critical operations %q has been successfully created", util.NameFromMeta(podDisruptionBudget.ObjectMeta))
	c.CriticalOpPodDisruptionBudget = podDisruptionBudget

	return nil
}

func (c *Cluster) createPodDisruptionBudgets() error {
	errors := make([]string, 0)

	err := c.createPrimaryPodDisruptionBudget()
	if err != nil {
		errors = append(errors, fmt.Sprintf("could not create primary pod disruption budget: %v", err))
	}

	err = c.createCriticalOpPodDisruptionBudget()
	if err != nil {
		errors = append(errors, fmt.Sprintf("could not create pod disruption budget for critical operations: %v", err))
	}

	if len(errors) > 0 {
		return fmt.Errorf("%v", strings.Join(errors, `', '`))
	}
	return nil
}

func (c *Cluster) updatePrimaryPodDisruptionBudget(pdb *policyv1.PodDisruptionBudget) error {
	c.logger.Debug("updating primary pod disruption budget")
	if c.PrimaryPodDisruptionBudget == nil {
		return fmt.Errorf("there is no primary pod disruption budget in the cluster")
	}

	if err := c.deletePrimaryPodDisruptionBudget(); err != nil {
		return fmt.Errorf("could not delete primary pod disruption budget: %v", err)
	}

	newPdb, err := c.KubeClient.
		PodDisruptionBudgets(pdb.Namespace).
		Create(context.TODO(), pdb, metav1.CreateOptions{})
	if err != nil {
		return fmt.Errorf("could not create primary pod disruption budget: %v", err)
	}
	c.PrimaryPodDisruptionBudget = newPdb

	return nil
}

func (c *Cluster) updateCriticalOpPodDisruptionBudget(pdb *policyv1.PodDisruptionBudget) error {
	c.logger.Debug("updating pod disruption budget for critical operations")
	if c.CriticalOpPodDisruptionBudget == nil {
		return fmt.Errorf("there is no pod disruption budget for critical operations in the cluster")
	}

	if err := c.deleteCriticalOpPodDisruptionBudget(); err != nil {
		return fmt.Errorf("could not delete pod disruption budget for critical operations: %v", err)
	}

	newPdb, err := c.KubeClient.
		PodDisruptionBudgets(pdb.Namespace).
		Create(context.TODO(), pdb, metav1.CreateOptions{})
	if err != nil {
		return fmt.Errorf("could not create pod disruption budget for critical operations: %v", err)
	}
	c.CriticalOpPodDisruptionBudget = newPdb

	return nil
}

func (c *Cluster) deletePrimaryPodDisruptionBudget() error {
	c.logger.Debug("deleting primary pod disruption budget")
	if c.PrimaryPodDisruptionBudget == nil {
		c.logger.Debug("there is no primary pod disruption budget in the cluster")
		return nil
	}

	pdbName := util.NameFromMeta(c.PrimaryPodDisruptionBudget.ObjectMeta)
	err := c.KubeClient.
		PodDisruptionBudgets(c.PrimaryPodDisruptionBudget.Namespace).
		Delete(context.TODO(), c.PrimaryPodDisruptionBudget.Name, c.deleteOptions)
	if k8sutil.ResourceNotFound(err) {
		c.logger.Debugf("PodDisruptionBudget %q has already been deleted", util.NameFromMeta(c.PrimaryPodDisruptionBudget.ObjectMeta))
	} else if err != nil {
		return fmt.Errorf("could not delete primary pod disruption budget: %v", err)
	}

	c.logger.Infof("pod disruption budget %q has been deleted", util.NameFromMeta(c.PrimaryPodDisruptionBudget.ObjectMeta))
	c.PrimaryPodDisruptionBudget = nil

	err = retryutil.Retry(c.OpConfig.ResourceCheckInterval, c.OpConfig.ResourceCheckTimeout,
		func() (bool, error) {
			_, err2 := c.KubeClient.PodDisruptionBudgets(pdbName.Namespace).Get(context.TODO(), pdbName.Name, metav1.GetOptions{})
			if err2 == nil {
				return false, nil
			}
			if k8sutil.ResourceNotFound(err2) {
				return true, nil
			}
			return false, err2
		})
	if err != nil {
		return fmt.Errorf("could not delete primary pod disruption budget: %v", err)
	}

	return nil
}

func (c *Cluster) deleteCriticalOpPodDisruptionBudget() error {
	c.logger.Debug("deleting pod disruption budget for critical operations")
	if c.CriticalOpPodDisruptionBudget == nil {
		c.logger.Debug("there is no pod disruption budget for critical operations in the cluster")
		return nil
	}

	pdbName := util.NameFromMeta(c.CriticalOpPodDisruptionBudget.ObjectMeta)
	err := c.KubeClient.
		PodDisruptionBudgets(c.CriticalOpPodDisruptionBudget.Namespace).
		Delete(context.TODO(), c.CriticalOpPodDisruptionBudget.Name, c.deleteOptions)
	if k8sutil.ResourceNotFound(err) {
		c.logger.Debugf("PodDisruptionBudget %q has already been deleted", util.NameFromMeta(c.CriticalOpPodDisruptionBudget.ObjectMeta))
	} else if err != nil {
		return fmt.Errorf("could not delete pod disruption budget for critical operations: %v", err)
	}

	c.logger.Infof("pod disruption budget %q has been deleted", util.NameFromMeta(c.CriticalOpPodDisruptionBudget.ObjectMeta))
	c.CriticalOpPodDisruptionBudget = nil

	err = retryutil.Retry(c.OpConfig.ResourceCheckInterval, c.OpConfig.ResourceCheckTimeout,
		func() (bool, error) {
			_, err2 := c.KubeClient.PodDisruptionBudgets(pdbName.Namespace).Get(context.TODO(), pdbName.Name, metav1.GetOptions{})
			if err2 == nil {
				return false, nil
			}
			if k8sutil.ResourceNotFound(err2) {
				return true, nil
			}
			return false, err2
		})
	if err != nil {
		return fmt.Errorf("could not delete pod disruption budget for critical operations: %v", err)
	}

	return nil
}

func (c *Cluster) deletePodDisruptionBudgets() error {
	errors := make([]string, 0)

	if err := c.deletePrimaryPodDisruptionBudget(); err != nil {
		errors = append(errors, fmt.Sprintf("%v", err))
	}

	if err := c.deleteCriticalOpPodDisruptionBudget(); err != nil {
		errors = append(errors, fmt.Sprintf("%v", err))
	}

	if len(errors) > 0 {
		return fmt.Errorf("%v", strings.Join(errors, `', '`))
	}
	return nil
}

func (c *Cluster) deleteEndpoint(role PostgresRole) error {
	c.setProcessName("deleting endpoint")
	c.logger.Debugf("deleting %s endpoint", role)
	if c.Endpoints[role] == nil {
		c.logger.Debugf("there is no %s endpoint in the cluster", role)
		return nil
	}

	if err := c.KubeClient.Endpoints(c.Endpoints[role].Namespace).Delete(context.TODO(), c.Endpoints[role].Name, c.deleteOptions); err != nil {
		if !k8sutil.ResourceNotFound(err) {
			return fmt.Errorf("could not delete %s endpoint: %v", role, err)
		}
		c.logger.Debugf("%s endpoint has already been deleted", role)
	}

	c.logger.Infof("%s endpoint %q has been deleted", role, util.NameFromMeta(c.Endpoints[role].ObjectMeta))
	delete(c.Endpoints, role)

	return nil
}

func (c *Cluster) deletePatroniResources() error {
	c.setProcessName("deleting Patroni resources")
	errors := make([]string, 0)

	if err := c.deleteService(Patroni); err != nil {
		errors = append(errors, fmt.Sprintf("%v", err))
	}

	for _, suffix := range patroniObjectSuffixes {
		if c.patroniKubernetesUseConfigMaps() {
			if err := c.deletePatroniConfigMap(suffix); err != nil {
				errors = append(errors, fmt.Sprintf("%v", err))
			}
		} else {
			if err := c.deletePatroniEndpoint(suffix); err != nil {
				errors = append(errors, fmt.Sprintf("%v", err))
			}
		}
	}

	if len(errors) > 0 {
		return fmt.Errorf("%v", strings.Join(errors, `', '`))
	}

	return nil
}

func (c *Cluster) deletePatroniConfigMap(suffix string) error {
	c.setProcessName("deleting Patroni config map")
	c.logger.Debugf("deleting %s Patroni config map", suffix)
	cm := c.PatroniConfigMaps[suffix]
	if cm == nil {
		c.logger.Debugf("there is no %s Patroni config map in the cluster", suffix)
		return nil
	}

	if err := c.KubeClient.ConfigMaps(cm.Namespace).Delete(context.TODO(), cm.Name, c.deleteOptions); err != nil {
		if !k8sutil.ResourceNotFound(err) {
			return fmt.Errorf("could not delete %s Patroni config map %q: %v", suffix, cm.Name, err)
		}
		c.logger.Debugf("%s Patroni config map has already been deleted", suffix)
	}

	c.logger.Infof("%s Patroni config map %q has been deleted", suffix, util.NameFromMeta(cm.ObjectMeta))
	delete(c.PatroniConfigMaps, suffix)

	return nil
}

func (c *Cluster) deletePatroniEndpoint(suffix string) error {
	c.setProcessName("deleting Patroni endpoint")
	c.logger.Debugf("deleting %s Patroni endpoint", suffix)
	ep := c.PatroniEndpoints[suffix]
	if ep == nil {
		c.logger.Debugf("there is no %s Patroni endpoint in the cluster", suffix)
		return nil
	}

	if err := c.KubeClient.Endpoints(ep.Namespace).Delete(context.TODO(), ep.Name, c.deleteOptions); err != nil {
		if !k8sutil.ResourceNotFound(err) {
			return fmt.Errorf("could not delete %s Patroni endpoint %q: %v", suffix, ep.Name, err)
		}
		c.logger.Debugf("%s Patroni endpoint has already been deleted", suffix)
	}

	c.logger.Infof("%s Patroni endpoint %q has been deleted", suffix, util.NameFromMeta(ep.ObjectMeta))
	delete(c.PatroniEndpoints, suffix)

	return nil
}

func (c *Cluster) deleteSecrets() error {
	c.setProcessName("deleting secrets")
	errors := make([]string, 0)

	for uid := range c.Secrets {
		err := c.deleteSecret(uid)
		if err != nil {
			errors = append(errors, fmt.Sprintf("%v", err))
		}
	}

	if len(errors) > 0 {
		return fmt.Errorf("could not delete all secrets: %v", strings.Join(errors, `', '`))
	}

	return nil
}

func (c *Cluster) deleteSecret(uid types.UID) error {
	c.setProcessName("deleting secret")
	secret := c.Secrets[uid]
	secretName := util.NameFromMeta(secret.ObjectMeta)
	c.logger.Debugf("deleting secret %q", secretName)
	err := c.KubeClient.Secrets(secret.Namespace).Delete(context.TODO(), secret.Name, c.deleteOptions)
	if k8sutil.ResourceNotFound(err) {
		c.logger.Debugf("secret %q has already been deleted", secretName)
	} else if err != nil {
		return fmt.Errorf("could not delete secret %q: %v", secretName, err)
	}
	c.logger.Infof("secret %q has been deleted", secretName)
	delete(c.Secrets, uid)

	return nil
}

func (c *Cluster) createRoles() (err error) {
	// TODO: figure out what to do with duplicate names (humans and robots) among pgUsers
	return c.syncRoles()
}

func (c *Cluster) createLogicalBackupJob() (err error) {

	c.setProcessName("creating a k8s cron job for logical backups")

	logicalBackupJobSpec, err := c.generateLogicalBackupJob()
	if err != nil {
		return fmt.Errorf("could not generate k8s cron job spec: %v", err)
	}

	cronJob, err := c.KubeClient.CronJobsGetter.CronJobs(c.Namespace).Create(context.TODO(), logicalBackupJobSpec, metav1.CreateOptions{})
	if err != nil {
		return fmt.Errorf("could not create k8s cron job: %v", err)
	}
	c.LogicalBackupJob = cronJob

	return nil
}

func (c *Cluster) patchLogicalBackupJob(newJob *batchv1.CronJob) error {
	c.setProcessName("patching logical backup job")

	patchData, err := specPatch(newJob.Spec)
	if err != nil {
		return fmt.Errorf("could not form patch for the logical backup job: %v", err)
	}

	// update the backup job spec
	cronJob, err := c.KubeClient.CronJobsGetter.CronJobs(c.Namespace).Patch(
		context.TODO(),
		c.getLogicalBackupJobName(),
		types.MergePatchType,
		patchData,
		metav1.PatchOptions{},
		"")
	if err != nil {
		return fmt.Errorf("could not patch logical backup job: %v", err)
	}
	c.LogicalBackupJob = cronJob

	return nil
}

func (c *Cluster) deleteLogicalBackupJob() error {
	if c.LogicalBackupJob == nil {
		return nil
	}
	c.logger.Info("removing the logical backup job")

	err := c.KubeClient.CronJobsGetter.CronJobs(c.LogicalBackupJob.Namespace).Delete(context.TODO(), c.getLogicalBackupJobName(), c.deleteOptions)
	if k8sutil.ResourceNotFound(err) {
		c.logger.Debugf("logical backup cron job %q has already been deleted", c.getLogicalBackupJobName())
	} else if err != nil {
		return err
	}
	c.LogicalBackupJob = nil

	return nil
}

// GetServiceMaster returns cluster's kubernetes master Service
func (c *Cluster) GetServiceMaster() *v1.Service {
	return c.Services[Master]
}

// GetServiceReplica returns cluster's kubernetes replica Service
func (c *Cluster) GetServiceReplica() *v1.Service {
	return c.Services[Replica]
}

// GetEndpointMaster returns cluster's kubernetes master Endpoint
func (c *Cluster) GetEndpointMaster() *v1.Endpoints {
	return c.Endpoints[Master]
}

// GetEndpointReplica returns cluster's kubernetes replica Endpoint
func (c *Cluster) GetEndpointReplica() *v1.Endpoints {
	return c.Endpoints[Replica]
}

// GetStatefulSet returns cluster's kubernetes StatefulSet
func (c *Cluster) GetStatefulSet() *appsv1.StatefulSet {
	return c.Statefulset
}

// GetPrimaryPodDisruptionBudget returns cluster's primary kubernetes PodDisruptionBudget
func (c *Cluster) GetPrimaryPodDisruptionBudget() *policyv1.PodDisruptionBudget {
	return c.PrimaryPodDisruptionBudget
}

// GetCriticalOpPodDisruptionBudget returns cluster's kubernetes PodDisruptionBudget for critical operations
func (c *Cluster) GetCriticalOpPodDisruptionBudget() *policyv1.PodDisruptionBudget {
	return c.CriticalOpPodDisruptionBudget
}


================================================
File: pkg/cluster/streams.go
================================================
package cluster

import (
	"context"
	"encoding/json"
	"fmt"
	"reflect"
	"sort"
	"strings"

	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	zalandov1 "github.com/zalando/postgres-operator/pkg/apis/zalando.org/v1"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
)

func (c *Cluster) createStreams(appId string) (*zalandov1.FabricEventStream, error) {
	c.setProcessName("creating streams")

	fes := c.generateFabricEventStream(appId)
	streamCRD, err := c.KubeClient.FabricEventStreams(c.Namespace).Create(context.TODO(), fes, metav1.CreateOptions{})
	if err != nil {
		return nil, err
	}

	return streamCRD, nil
}

func (c *Cluster) updateStreams(newEventStreams *zalandov1.FabricEventStream) (patchedStream *zalandov1.FabricEventStream, err error) {
	c.setProcessName("updating event streams")

	patch, err := json.Marshal(newEventStreams)
	if err != nil {
		return nil, fmt.Errorf("could not marshal new event stream CRD %q: %v", newEventStreams.Name, err)
	}
	if patchedStream, err = c.KubeClient.FabricEventStreams(newEventStreams.Namespace).Patch(
		context.TODO(), newEventStreams.Name, types.MergePatchType, patch, metav1.PatchOptions{}); err != nil {
		return nil, err
	}

	return patchedStream, nil
}

func (c *Cluster) deleteStream(appId string) error {
	c.setProcessName("deleting event stream")
	c.logger.Debugf("deleting event stream with applicationId %s", appId)

	err := c.KubeClient.FabricEventStreams(c.Streams[appId].Namespace).Delete(context.TODO(), c.Streams[appId].Name, metav1.DeleteOptions{})
	if err != nil {
		return fmt.Errorf("could not delete event stream %q with applicationId %s: %v", c.Streams[appId].Name, appId, err)
	}
	c.logger.Infof("event stream %q with applicationId %s has been successfully deleted", c.Streams[appId].Name, appId)
	delete(c.Streams, appId)

	return nil
}

func (c *Cluster) deleteStreams() error {
	// check if stream CRD is installed before trying a delete
	_, err := c.KubeClient.CustomResourceDefinitions().Get(context.TODO(), constants.EventStreamCRDName, metav1.GetOptions{})
	if k8sutil.ResourceNotFound(err) {
		return nil
	}
	c.setProcessName("deleting event streams")
	errors := make([]string, 0)

	for appId := range c.Streams {
		err := c.deleteStream(appId)
		if err != nil {
			errors = append(errors, fmt.Sprintf("%v", err))
		}
	}

	if len(errors) > 0 {
		return fmt.Errorf("could not delete all event stream custom resources: %v", strings.Join(errors, `', '`))
	}

	return nil
}

func getDistinctApplicationIds(streams []acidv1.Stream) []string {
	appIds := make([]string, 0)
	for _, stream := range streams {
		if !util.SliceContains(appIds, stream.ApplicationId) {
			appIds = append(appIds, stream.ApplicationId)
		}
	}

	return appIds
}

func (c *Cluster) syncPublication(dbName string, databaseSlotsList map[string]zalandov1.Slot, slotsToSync *map[string]map[string]string) error {
	createPublications := make(map[string]string)
	alterPublications := make(map[string]string)
	deletePublications := []string{}

	defer func() {
		if err := c.closeDbConn(); err != nil {
			c.logger.Errorf("could not close database connection: %v", err)
		}
	}()

	// check for existing publications
	if err := c.initDbConnWithName(dbName); err != nil {
		return fmt.Errorf("could not init database connection: %v", err)
	}

	currentPublications, err := c.getPublications()
	if err != nil {
		return fmt.Errorf("could not get current publications: %v", err)
	}

	for slotName, slotAndPublication := range databaseSlotsList {
		tables := slotAndPublication.Publication
		tableNames := make([]string, len(tables))
		i := 0
		for t := range tables {
			tableName, schemaName := getTableSchema(t)
			tableNames[i] = fmt.Sprintf("%s.%s", schemaName, tableName)
			i++
		}
		sort.Strings(tableNames)
		tableList := strings.Join(tableNames, ", ")

		currentTables, exists := currentPublications[slotName]
		if !exists {
			createPublications[slotName] = tableList
		} else if currentTables != tableList {
			alterPublications[slotName] = tableList
		} else {
			(*slotsToSync)[slotName] = slotAndPublication.Slot
		}
	}

	// check if there is any deletion
	for slotName := range currentPublications {
		if _, exists := databaseSlotsList[slotName]; !exists {
			deletePublications = append(deletePublications, slotName)
		}
	}

	if len(createPublications)+len(alterPublications)+len(deletePublications) == 0 {
		return nil
	}

	errors := make([]string, 0)
	for publicationName, tables := range createPublications {
		if err = c.executeCreatePublication(publicationName, tables); err != nil {
			errors = append(errors, fmt.Sprintf("creation of publication %q failed: %v", publicationName, err))
			continue
		}
		(*slotsToSync)[publicationName] = databaseSlotsList[publicationName].Slot
	}
	for publicationName, tables := range alterPublications {
		if err = c.executeAlterPublication(publicationName, tables); err != nil {
			errors = append(errors, fmt.Sprintf("update of publication %q failed: %v", publicationName, err))
			continue
		}
		(*slotsToSync)[publicationName] = databaseSlotsList[publicationName].Slot
	}
	for _, publicationName := range deletePublications {
		if err = c.executeDropPublication(publicationName); err != nil {
			errors = append(errors, fmt.Sprintf("deletion of publication %q failed: %v", publicationName, err))
			continue
		}
		(*slotsToSync)[publicationName] = nil
	}

	if len(errors) > 0 {
		return fmt.Errorf("%v", strings.Join(errors, `', '`))
	}

	return nil
}

func (c *Cluster) generateFabricEventStream(appId string) *zalandov1.FabricEventStream {
	eventStreams := make([]zalandov1.EventStream, 0)
	resourceAnnotations := map[string]string{}
	var err, err2 error

	for _, stream := range c.Spec.Streams {
		if stream.ApplicationId != appId {
			continue
		}

		err = setResourceAnnotation(&resourceAnnotations, stream.CPU, constants.EventStreamCpuAnnotationKey)
		err2 = setResourceAnnotation(&resourceAnnotations, stream.Memory, constants.EventStreamMemoryAnnotationKey)
		if err != nil || err2 != nil {
			c.logger.Warningf("could not set resource annotation for event stream: %v", err)
		}

		for tableName, table := range stream.Tables {
			streamSource := c.getEventStreamSource(stream, tableName, table.IdColumn)
			streamFlow := getEventStreamFlow(table.PayloadColumn)
			streamSink := getEventStreamSink(stream, table.EventType)
			streamRecovery := getEventStreamRecovery(stream, table.RecoveryEventType, table.EventType, table.IgnoreRecovery)

			eventStreams = append(eventStreams, zalandov1.EventStream{
				EventStreamFlow:     streamFlow,
				EventStreamRecovery: streamRecovery,
				EventStreamSink:     streamSink,
				EventStreamSource:   streamSource})
		}
	}

	return &zalandov1.FabricEventStream{
		TypeMeta: metav1.TypeMeta{
			APIVersion: constants.EventStreamCRDApiVersion,
			Kind:       constants.EventStreamCRDKind,
		},
		ObjectMeta: metav1.ObjectMeta{
			// max length for cluster name is 58 so we can only add 5 more characters / numbers
			Name:            fmt.Sprintf("%s-%s", c.Name, strings.ToLower(util.RandomPassword(5))),
			Namespace:       c.Namespace,
			Labels:          c.labelsSet(true),
			Annotations:     c.AnnotationsToPropagate(c.annotationsSet(resourceAnnotations)),
			OwnerReferences: c.ownerReferences(),
		},
		Spec: zalandov1.FabricEventStreamSpec{
			ApplicationId: appId,
			EventStreams:  eventStreams,
		},
	}
}

func setResourceAnnotation(annotations *map[string]string, resource *string, key string) error {
	var (
		isSmaller bool
		err       error
	)
	if resource != nil {
		currentValue, exists := (*annotations)[key]
		if exists {
			isSmaller, err = util.IsSmallerQuantity(currentValue, *resource)
			if err != nil {
				return fmt.Errorf("could not compare resource in %q annotation: %v", key, err)
			}
		}
		if isSmaller || !exists {
			(*annotations)[key] = *resource
		}
	}

	return nil
}

func (c *Cluster) getEventStreamSource(stream acidv1.Stream, tableName string, idColumn *string) zalandov1.EventStreamSource {
	table, schema := getTableSchema(tableName)
	streamFilter := stream.Filter[tableName]
	return zalandov1.EventStreamSource{
		Type:             constants.EventStreamSourcePGType,
		Schema:           schema,
		EventStreamTable: getOutboxTable(table, idColumn),
		Filter:           streamFilter,
		Connection: c.getStreamConnection(
			stream.Database,
			constants.EventStreamSourceSlotPrefix+constants.UserRoleNameSuffix,
			stream.ApplicationId),
	}
}

func getEventStreamFlow(payloadColumn *string) zalandov1.EventStreamFlow {
	return zalandov1.EventStreamFlow{
		Type:          constants.EventStreamFlowPgGenericType,
		PayloadColumn: payloadColumn,
	}
}

func getEventStreamSink(stream acidv1.Stream, eventType string) zalandov1.EventStreamSink {
	return zalandov1.EventStreamSink{
		Type:         constants.EventStreamSinkNakadiType,
		EventType:    eventType,
		MaxBatchSize: stream.BatchSize,
	}
}

func getEventStreamRecovery(stream acidv1.Stream, recoveryEventType, eventType string, ignoreRecovery *bool) zalandov1.EventStreamRecovery {
	if (stream.EnableRecovery != nil && !*stream.EnableRecovery) ||
		(stream.EnableRecovery == nil && recoveryEventType == "") {
		return zalandov1.EventStreamRecovery{
			Type: constants.EventStreamRecoveryNoneType,
		}
	}

	if ignoreRecovery != nil && *ignoreRecovery {
		return zalandov1.EventStreamRecovery{
			Type: constants.EventStreamRecoveryIgnoreType,
		}
	}

	if stream.EnableRecovery != nil && *stream.EnableRecovery && recoveryEventType == "" {
		recoveryEventType = fmt.Sprintf("%s-%s", eventType, constants.EventStreamRecoverySuffix)
	}

	return zalandov1.EventStreamRecovery{
		Type: constants.EventStreamRecoveryDLQType,
		Sink: &zalandov1.EventStreamSink{
			Type:         constants.EventStreamSinkNakadiType,
			EventType:    recoveryEventType,
			MaxBatchSize: stream.BatchSize,
		},
	}
}

func getTableSchema(fullTableName string) (tableName, schemaName string) {
	schemaName = "public"
	tableName = fullTableName
	if strings.Contains(fullTableName, ".") {
		schemaName = strings.Split(fullTableName, ".")[0]
		tableName = strings.Split(fullTableName, ".")[1]
	}

	return tableName, schemaName
}

func getOutboxTable(tableName string, idColumn *string) zalandov1.EventStreamTable {
	return zalandov1.EventStreamTable{
		Name:     tableName,
		IDColumn: idColumn,
	}
}

func getSlotName(dbName, appId string) string {
	return fmt.Sprintf("%s_%s_%s", constants.EventStreamSourceSlotPrefix, dbName, strings.Replace(appId, "-", "_", -1))
}

func (c *Cluster) getStreamConnection(database, user, appId string) zalandov1.Connection {
	return zalandov1.Connection{
		Url:        fmt.Sprintf("jdbc:postgresql://%s.%s/%s?user=%s&ssl=true&sslmode=require", c.Name, c.Namespace, database, user),
		SlotName:   getSlotName(database, appId),
		PluginType: constants.EventStreamSourcePluginType,
		DBAuth: zalandov1.DBAuth{
			Type:        constants.EventStreamSourceAuthType,
			Name:        c.credentialSecretNameForCluster(user, c.Name),
			UserKey:     "username",
			PasswordKey: "password",
		},
	}
}

func (c *Cluster) syncStreams() error {
	c.setProcessName("syncing streams")

	_, err := c.KubeClient.CustomResourceDefinitions().Get(context.TODO(), constants.EventStreamCRDName, metav1.GetOptions{})
	if k8sutil.ResourceNotFound(err) {
		c.logger.Debug("event stream CRD not installed, skipping")
		return nil
	}

	databaseSlots := make(map[string]map[string]zalandov1.Slot)
	slotsToSync := make(map[string]map[string]string)
	requiredPatroniConfig := c.Spec.Patroni

	if len(requiredPatroniConfig.Slots) > 0 {
		for slotName, slotConfig := range requiredPatroniConfig.Slots {
			slotsToSync[slotName] = slotConfig
		}
	}

	if err := c.initDbConn(); err != nil {
		return fmt.Errorf("could not init database connection")
	}
	defer func() {
		if err := c.closeDbConn(); err != nil {
			c.logger.Errorf("could not close database connection: %v", err)
		}
	}()
	listDatabases, err := c.getDatabases()
	if err != nil {
		return fmt.Errorf("could not get list of databases: %v", err)
	}
	// get database name with empty list of slot, except template0 and template1
	for dbName := range listDatabases {
		if dbName != "template0" && dbName != "template1" {
			databaseSlots[dbName] = map[string]zalandov1.Slot{}
		}
	}

	// get list of required slots and publications, group by database
	for _, stream := range c.Spec.Streams {
		if _, exists := databaseSlots[stream.Database]; !exists {
			c.logger.Warningf("database %q does not exist in the cluster", stream.Database)
			continue
		}
		slot := map[string]string{
			"database": stream.Database,
			"plugin":   constants.EventStreamSourcePluginType,
			"type":     "logical",
		}
		slotName := getSlotName(stream.Database, stream.ApplicationId)
		if _, exists := databaseSlots[stream.Database][slotName]; !exists {
			databaseSlots[stream.Database][slotName] = zalandov1.Slot{
				Slot:        slot,
				Publication: stream.Tables,
			}
		} else {
			slotAndPublication := databaseSlots[stream.Database][slotName]
			streamTables := slotAndPublication.Publication
			for tableName, table := range stream.Tables {
				if _, exists := streamTables[tableName]; !exists {
					streamTables[tableName] = table
				}
			}
			slotAndPublication.Publication = streamTables
			databaseSlots[stream.Database][slotName] = slotAndPublication
		}
	}

	// sync publication in a database
	c.logger.Debug("syncing database publications")
	for dbName, databaseSlotsList := range databaseSlots {
		err := c.syncPublication(dbName, databaseSlotsList, &slotsToSync)
		if err != nil {
			c.logger.Warningf("could not sync all publications in database %q: %v", dbName, err)
			continue
		}
	}

	c.logger.Debug("syncing logical replication slots")
	pods, err := c.listPods()
	if err != nil {
		return fmt.Errorf("could not get list of pods to sync logical replication slots via Patroni API: %v", err)
	}

	// sync logical replication slots in Patroni config
	requiredPatroniConfig.Slots = slotsToSync
	configPatched, _, _, err := c.syncPatroniConfig(pods, requiredPatroniConfig, nil)
	if err != nil {
		c.logger.Warningf("Patroni config updated? %v - errors during config sync: %v", configPatched, err)
	}

	// finally sync stream CRDs
	// get distinct application IDs from streams section
	// there will be a separate event stream resource for each ID
	appIds := getDistinctApplicationIds(c.Spec.Streams)
	for _, appId := range appIds {
		if hasSlotsInSync(appId, databaseSlots, slotsToSync) {
			if err = c.syncStream(appId); err != nil {
				c.logger.Warningf("could not sync event streams with applicationId %s: %v", appId, err)
			}
		} else {
			c.logger.Warningf("database replication slots %#v for streams with applicationId %s not in sync, skipping event stream sync", slotsToSync, appId)
		}
	}

	// check if there is any deletion
	if err = c.cleanupRemovedStreams(appIds); err != nil {
		return fmt.Errorf("%v", err)
	}

	return nil
}

func hasSlotsInSync(appId string, databaseSlots map[string]map[string]zalandov1.Slot, slotsToSync map[string]map[string]string) bool {
	allSlotsInSync := true
	for dbName, slots := range databaseSlots {
		for slotName := range slots {
			if slotName == getSlotName(dbName, appId) {
				if slot, exists := slotsToSync[slotName]; !exists || slot == nil {
					allSlotsInSync = false
					continue
				}
			}
		}
	}

	return allSlotsInSync
}

func (c *Cluster) syncStream(appId string) error {
	var (
		streams *zalandov1.FabricEventStreamList
		err     error
	)
	c.setProcessName("syncing stream with applicationId %s", appId)
	c.logger.Debugf("syncing stream with applicationId %s", appId)

	listOptions := metav1.ListOptions{
		LabelSelector: c.labelsSet(false).String(),
	}
	streams, err = c.KubeClient.FabricEventStreams(c.Namespace).List(context.TODO(), listOptions)
	if err != nil {
		return fmt.Errorf("could not list of FabricEventStreams for applicationId %s: %v", appId, err)
	}

	streamExists := false
	for _, stream := range streams.Items {
		if stream.Spec.ApplicationId != appId {
			continue
		}
		streamExists = true
		desiredStreams := c.generateFabricEventStream(appId)
		if !reflect.DeepEqual(stream.ObjectMeta.OwnerReferences, desiredStreams.ObjectMeta.OwnerReferences) {
			c.logger.Infof("owner references of event streams with applicationId %s do not match the current ones", appId)
			stream.ObjectMeta.OwnerReferences = desiredStreams.ObjectMeta.OwnerReferences
			c.setProcessName("updating event streams with applicationId %s", appId)
			stream, err := c.KubeClient.FabricEventStreams(stream.Namespace).Update(context.TODO(), &stream, metav1.UpdateOptions{})
			if err != nil {
				return fmt.Errorf("could not update event streams with applicationId %s: %v", appId, err)
			}
			c.Streams[appId] = stream
		}
		if match, reason := c.compareStreams(&stream, desiredStreams); !match {
			c.logger.Infof("updating event streams with applicationId %s: %s", appId, reason)
			// make sure to keep the old name with randomly generated suffix
			desiredStreams.ObjectMeta.Name = stream.ObjectMeta.Name
			updatedStream, err := c.updateStreams(desiredStreams)
			if err != nil {
				return fmt.Errorf("failed updating event streams %s with applicationId %s: %v", stream.Name, appId, err)
			}
			c.Streams[appId] = updatedStream
			c.logger.Infof("event streams %q with applicationId %s have been successfully updated", updatedStream.Name, appId)
		}
		break
	}

	if !streamExists {
		c.logger.Infof("event streams with applicationId %s do not exist, create it", appId)
		createdStream, err := c.createStreams(appId)
		if err != nil {
			return fmt.Errorf("failed creating event streams with applicationId %s: %v", appId, err)
		}
		c.logger.Infof("event streams %q have been successfully created", createdStream.Name)
		c.Streams[appId] = createdStream
	}

	return nil
}

func (c *Cluster) compareStreams(curEventStreams, newEventStreams *zalandov1.FabricEventStream) (match bool, reason string) {
	reasons := make([]string, 0)
	desiredAnnotations := make(map[string]string)
	match = true

	// stream operator can add extra annotations so incl. current annotations in desired annotations
	for curKey, curValue := range curEventStreams.Annotations {
		if _, exists := desiredAnnotations[curKey]; !exists {
			desiredAnnotations[curKey] = curValue
		}
	}
	// add/or override annotations if cpu and memory values were changed
	for newKey, newValue := range newEventStreams.Annotations {
		desiredAnnotations[newKey] = newValue
	}
	if changed, reason := c.compareAnnotations(curEventStreams.ObjectMeta.Annotations, desiredAnnotations, nil); changed {
		match = false
		reasons = append(reasons, fmt.Sprintf("new streams annotations do not match: %s", reason))
	}

	if !reflect.DeepEqual(curEventStreams.ObjectMeta.Labels, newEventStreams.ObjectMeta.Labels) {
		match = false
		reasons = append(reasons, "new streams labels do not match the current ones")
	}

	if changed, reason := sameEventStreams(curEventStreams.Spec.EventStreams, newEventStreams.Spec.EventStreams); !changed {
		match = false
		reasons = append(reasons, fmt.Sprintf("new streams EventStreams array does not match : %s", reason))
	}

	return match, strings.Join(reasons, ", ")
}

func sameEventStreams(curEventStreams, newEventStreams []zalandov1.EventStream) (match bool, reason string) {
	if len(newEventStreams) != len(curEventStreams) {
		return false, "number of defined streams is different"
	}

	for _, newStream := range newEventStreams {
		match = false
		reason = "event stream specs differ"
		for _, curStream := range curEventStreams {
			if reflect.DeepEqual(newStream.EventStreamSource, curStream.EventStreamSource) &&
				reflect.DeepEqual(newStream.EventStreamFlow, curStream.EventStreamFlow) &&
				reflect.DeepEqual(newStream.EventStreamSink, curStream.EventStreamSink) &&
				reflect.DeepEqual(newStream.EventStreamRecovery, curStream.EventStreamRecovery) {
				match = true
				break
			}
		}
		if !match {
			return false, reason
		}
	}

	return true, ""
}

func (c *Cluster) cleanupRemovedStreams(appIds []string) error {
	errors := make([]string, 0)
	for appId := range c.Streams {
		if !util.SliceContains(appIds, appId) {
			c.logger.Infof("event streams with applicationId %s do not exist in the manifest, delete it", appId)
			err := c.deleteStream(appId)
			if err != nil {
				errors = append(errors, fmt.Sprintf("failed deleting event streams with applicationId %s: %v", appId, err))
			}
		}
	}

	if len(errors) > 0 {
		return fmt.Errorf("could not delete all removed event streams: %v", strings.Join(errors, `', '`))
	}

	return nil
}


================================================
File: pkg/cluster/streams_test.go
================================================
package cluster

import (
	"fmt"
	"reflect"
	"strings"

	"context"
	"testing"

	"github.com/stretchr/testify/assert"
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	zalandov1 "github.com/zalando/postgres-operator/pkg/apis/zalando.org/v1"
	fakezalandov1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/fake"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
)

var (
	clusterName string = "acid-stream-cluster"
	namespace   string = "default"
	appId       string = "test-app"
	dbName      string = "foo"
	fesUser     string = fmt.Sprintf("%s%s", constants.EventStreamSourceSlotPrefix, constants.UserRoleNameSuffix)
	slotName    string = fmt.Sprintf("%s_%s_%s", constants.EventStreamSourceSlotPrefix, dbName, strings.Replace(appId, "-", "_", -1))

	zalandoClientSet = fakezalandov1.NewSimpleClientset()

	client = k8sutil.KubernetesClient{
		FabricEventStreamsGetter: zalandoClientSet.ZalandoV1(),
		PostgresqlsGetter:        zalandoClientSet.AcidV1(),
		PodsGetter:               clientSet.CoreV1(),
		StatefulSetsGetter:       clientSet.AppsV1(),
	}

	pg = acidv1.Postgresql{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Postgresql",
			APIVersion: "acid.zalan.do/v1",
		},
		ObjectMeta: metav1.ObjectMeta{
			Name:      clusterName,
			Namespace: namespace,
		},
		Spec: acidv1.PostgresSpec{
			Databases: map[string]string{
				dbName: fmt.Sprintf("%s%s", dbName, constants.UserRoleNameSuffix),
			},
			Streams: []acidv1.Stream{
				{
					ApplicationId: appId,
					Database:      "foo",
					Tables: map[string]acidv1.StreamTable{
						"data.bar": {
							EventType:     "stream-type-a",
							IdColumn:      k8sutil.StringToPointer("b_id"),
							PayloadColumn: k8sutil.StringToPointer("b_payload"),
						},
						"data.foobar": {
							EventType:         "stream-type-b",
							RecoveryEventType: "stream-type-b-dlq",
						},
						"data.foofoobar": {
							EventType:      "stream-type-c",
							IgnoreRecovery: util.True(),
						},
					},
					EnableRecovery: util.True(),
					Filter: map[string]*string{
						"data.bar": k8sutil.StringToPointer("[?(@.source.txId > 500 && @.source.lsn > 123456)]"),
					},
					BatchSize: k8sutil.UInt32ToPointer(uint32(100)),
					CPU:       k8sutil.StringToPointer("250m"),
				},
			},
			TeamID: "acid",
			Volume: acidv1.Volume{
				Size: "1Gi",
			},
		},
	}

	fes = &zalandov1.FabricEventStream{
		TypeMeta: metav1.TypeMeta{
			APIVersion: constants.EventStreamCRDApiVersion,
			Kind:       constants.EventStreamCRDKind,
		},
		ObjectMeta: metav1.ObjectMeta{
			Name:      fmt.Sprintf("%s-12345", clusterName),
			Namespace: namespace,
			Annotations: map[string]string{
				constants.EventStreamCpuAnnotationKey: "250m",
			},
			Labels: map[string]string{
				"application":  "spilo",
				"cluster-name": clusterName,
				"team":         "acid",
			},
			OwnerReferences: []metav1.OwnerReference{
				{
					APIVersion: "apps/v1",
					Kind:       "StatefulSet",
					Name:       "acid-test-cluster",
					Controller: util.True(),
				},
			},
		},
		Spec: zalandov1.FabricEventStreamSpec{
			ApplicationId: appId,
			EventStreams: []zalandov1.EventStream{
				{
					EventStreamFlow: zalandov1.EventStreamFlow{
						PayloadColumn: k8sutil.StringToPointer("b_payload"),
						Type:          constants.EventStreamFlowPgGenericType,
					},
					EventStreamRecovery: zalandov1.EventStreamRecovery{
						Type: constants.EventStreamRecoveryDLQType,
						Sink: &zalandov1.EventStreamSink{
							EventType:    fmt.Sprintf("%s-%s", "stream-type-a", constants.EventStreamRecoverySuffix),
							MaxBatchSize: k8sutil.UInt32ToPointer(uint32(100)),
							Type:         constants.EventStreamSinkNakadiType,
						},
					},
					EventStreamSink: zalandov1.EventStreamSink{
						EventType:    "stream-type-a",
						MaxBatchSize: k8sutil.UInt32ToPointer(uint32(100)),
						Type:         constants.EventStreamSinkNakadiType,
					},
					EventStreamSource: zalandov1.EventStreamSource{
						Filter: k8sutil.StringToPointer("[?(@.source.txId > 500 && @.source.lsn > 123456)]"),
						Connection: zalandov1.Connection{
							DBAuth: zalandov1.DBAuth{
								Name:        fmt.Sprintf("fes-user.%s.credentials.postgresql.acid.zalan.do", clusterName),
								PasswordKey: "password",
								Type:        constants.EventStreamSourceAuthType,
								UserKey:     "username",
							},
							Url:        fmt.Sprintf("jdbc:postgresql://%s.%s/foo?user=%s&ssl=true&sslmode=require", clusterName, namespace, fesUser),
							SlotName:   slotName,
							PluginType: constants.EventStreamSourcePluginType,
						},
						Schema: "data",
						EventStreamTable: zalandov1.EventStreamTable{
							IDColumn: k8sutil.StringToPointer("b_id"),
							Name:     "bar",
						},
						Type: constants.EventStreamSourcePGType,
					},
				},
				{
					EventStreamFlow: zalandov1.EventStreamFlow{
						Type: constants.EventStreamFlowPgGenericType,
					},
					EventStreamRecovery: zalandov1.EventStreamRecovery{
						Type: constants.EventStreamRecoveryDLQType,
						Sink: &zalandov1.EventStreamSink{
							EventType:    "stream-type-b-dlq",
							MaxBatchSize: k8sutil.UInt32ToPointer(uint32(100)),
							Type:         constants.EventStreamSinkNakadiType,
						},
					},
					EventStreamSink: zalandov1.EventStreamSink{
						EventType:    "stream-type-b",
						MaxBatchSize: k8sutil.UInt32ToPointer(uint32(100)),
						Type:         constants.EventStreamSinkNakadiType,
					},
					EventStreamSource: zalandov1.EventStreamSource{
						Connection: zalandov1.Connection{
							DBAuth: zalandov1.DBAuth{
								Name:        fmt.Sprintf("fes-user.%s.credentials.postgresql.acid.zalan.do", clusterName),
								PasswordKey: "password",
								Type:        constants.EventStreamSourceAuthType,
								UserKey:     "username",
							},
							Url:        fmt.Sprintf("jdbc:postgresql://%s.%s/foo?user=%s&ssl=true&sslmode=require", clusterName, namespace, fesUser),
							SlotName:   slotName,
							PluginType: constants.EventStreamSourcePluginType,
						},
						Schema: "data",
						EventStreamTable: zalandov1.EventStreamTable{
							Name: "foobar",
						},
						Type: constants.EventStreamSourcePGType,
					},
				},
				{
					EventStreamFlow: zalandov1.EventStreamFlow{
						Type: constants.EventStreamFlowPgGenericType,
					},
					EventStreamRecovery: zalandov1.EventStreamRecovery{
						Type: constants.EventStreamRecoveryIgnoreType,
					},
					EventStreamSink: zalandov1.EventStreamSink{
						EventType:    "stream-type-c",
						MaxBatchSize: k8sutil.UInt32ToPointer(uint32(100)),
						Type:         constants.EventStreamSinkNakadiType,
					},
					EventStreamSource: zalandov1.EventStreamSource{
						Connection: zalandov1.Connection{
							DBAuth: zalandov1.DBAuth{
								Name:        fmt.Sprintf("fes-user.%s.credentials.postgresql.acid.zalan.do", clusterName),
								PasswordKey: "password",
								Type:        constants.EventStreamSourceAuthType,
								UserKey:     "username",
							},
							Url:        fmt.Sprintf("jdbc:postgresql://%s.%s/foo?user=%s&ssl=true&sslmode=require", clusterName, namespace, fesUser),
							SlotName:   slotName,
							PluginType: constants.EventStreamSourcePluginType,
						},
						Schema: "data",
						EventStreamTable: zalandov1.EventStreamTable{
							Name: "foofoobar",
						},
						Type: constants.EventStreamSourcePGType,
					},
				},
			},
		},
	}

	cluster = New(
		Config{
			OpConfig: config.Config{
				Auth: config.Auth{
					SecretNameTemplate: "{username}.{cluster}.credentials.{tprkind}.{tprgroup}",
				},
				PodManagementPolicy: "ordered_ready",
				Resources: config.Resources{
					ClusterLabels:        map[string]string{"application": "spilo"},
					ClusterNameLabel:     "cluster-name",
					DefaultCPURequest:    "300m",
					DefaultCPULimit:      "300m",
					DefaultMemoryRequest: "300Mi",
					DefaultMemoryLimit:   "300Mi",
					PodRoleLabel:         "spilo-role",
				},
			},
		}, client, pg, logger, eventRecorder)
)

func TestGatherApplicationIds(t *testing.T) {
	testAppIds := []string{appId}
	appIds := getDistinctApplicationIds(pg.Spec.Streams)

	if !util.IsEqualIgnoreOrder(testAppIds, appIds) {
		t.Errorf("list of applicationIds does not match, expected %#v, got %#v", testAppIds, appIds)
	}
}

func TestHasSlotsInSync(t *testing.T) {
	cluster.Name = clusterName
	cluster.Namespace = namespace

	appId2 := fmt.Sprintf("%s-2", appId)
	dbNotExists := "dbnotexists"
	slotNotExists := fmt.Sprintf("%s_%s_%s", constants.EventStreamSourceSlotPrefix, dbNotExists, strings.Replace(appId, "-", "_", -1))
	slotNotExistsAppId2 := fmt.Sprintf("%s_%s_%s", constants.EventStreamSourceSlotPrefix, dbNotExists, strings.Replace(appId2, "-", "_", -1))

	tests := []struct {
		subTest       string
		applicationId string
		expectedSlots map[string]map[string]zalandov1.Slot
		actualSlots   map[string]map[string]string
		slotsInSync   bool
	}{
		{
			subTest:       fmt.Sprintf("slots in sync for applicationId %s", appId),
			applicationId: appId,
			expectedSlots: map[string]map[string]zalandov1.Slot{
				dbName: {
					slotName: zalandov1.Slot{
						Slot: map[string]string{
							"databases": dbName,
							"plugin":    constants.EventStreamSourcePluginType,
							"type":      "logical",
						},
						Publication: map[string]acidv1.StreamTable{
							"test1": {
								EventType: "stream-type-a",
							},
						},
					},
				},
			},
			actualSlots: map[string]map[string]string{
				slotName: {
					"databases": dbName,
					"plugin":    constants.EventStreamSourcePluginType,
					"type":      "logical",
				},
			},
			slotsInSync: true,
		}, {
			subTest:       fmt.Sprintf("slots empty for applicationId %s after create or update of publication failed", appId),
			applicationId: appId,
			expectedSlots: map[string]map[string]zalandov1.Slot{
				dbNotExists: {
					slotNotExists: zalandov1.Slot{
						Slot: map[string]string{
							"databases": dbName,
							"plugin":    constants.EventStreamSourcePluginType,
							"type":      "logical",
						},
						Publication: map[string]acidv1.StreamTable{
							"test1": {
								EventType: "stream-type-a",
							},
						},
					},
				},
			},
			actualSlots: map[string]map[string]string{},
			slotsInSync: false,
		}, {
			subTest:       fmt.Sprintf("slot with empty definition for applicationId %s after publication git deleted", appId),
			applicationId: appId,
			expectedSlots: map[string]map[string]zalandov1.Slot{
				dbNotExists: {
					slotNotExists: zalandov1.Slot{
						Slot: map[string]string{
							"databases": dbName,
							"plugin":    constants.EventStreamSourcePluginType,
							"type":      "logical",
						},
						Publication: map[string]acidv1.StreamTable{
							"test1": {
								EventType: "stream-type-a",
							},
						},
					},
				},
			},
			actualSlots: map[string]map[string]string{
				slotName: nil,
			},
			slotsInSync: false,
		}, {
			subTest:       fmt.Sprintf("one slot not in sync for applicationId %s because database does not exist", appId),
			applicationId: appId,
			expectedSlots: map[string]map[string]zalandov1.Slot{
				dbName: {
					slotName: zalandov1.Slot{
						Slot: map[string]string{
							"databases": dbName,
							"plugin":    constants.EventStreamSourcePluginType,
							"type":      "logical",
						},
						Publication: map[string]acidv1.StreamTable{
							"test1": {
								EventType: "stream-type-a",
							},
						},
					},
				},
				dbNotExists: {
					slotNotExists: zalandov1.Slot{
						Slot: map[string]string{
							"databases": "dbnotexists",
							"plugin":    constants.EventStreamSourcePluginType,
							"type":      "logical",
						},
						Publication: map[string]acidv1.StreamTable{
							"test2": {
								EventType: "stream-type-b",
							},
						},
					},
				},
			},
			actualSlots: map[string]map[string]string{
				slotName: {
					"databases": dbName,
					"plugin":    constants.EventStreamSourcePluginType,
					"type":      "logical",
				},
			},
			slotsInSync: false,
		}, {
			subTest:       fmt.Sprintf("slots in sync for applicationId %s, but not for %s - checking %s should return true", appId, appId2, appId),
			applicationId: appId,
			expectedSlots: map[string]map[string]zalandov1.Slot{
				dbName: {
					slotName: zalandov1.Slot{
						Slot: map[string]string{
							"databases": dbName,
							"plugin":    constants.EventStreamSourcePluginType,
							"type":      "logical",
						},
						Publication: map[string]acidv1.StreamTable{
							"test1": {
								EventType: "stream-type-a",
							},
						},
					},
				},
				dbNotExists: {
					slotNotExistsAppId2: zalandov1.Slot{
						Slot: map[string]string{
							"databases": "dbnotexists",
							"plugin":    constants.EventStreamSourcePluginType,
							"type":      "logical",
						},
						Publication: map[string]acidv1.StreamTable{
							"test2": {
								EventType: "stream-type-b",
							},
						},
					},
				},
			},
			actualSlots: map[string]map[string]string{
				slotName: {
					"databases": dbName,
					"plugin":    constants.EventStreamSourcePluginType,
					"type":      "logical",
				},
			},
			slotsInSync: true,
		}, {
			subTest:       fmt.Sprintf("slots in sync for applicationId %s, but not for %s - checking %s should return false", appId, appId2, appId2),
			applicationId: appId2,
			expectedSlots: map[string]map[string]zalandov1.Slot{
				dbName: {
					slotName: zalandov1.Slot{
						Slot: map[string]string{
							"databases": dbName,
							"plugin":    constants.EventStreamSourcePluginType,
							"type":      "logical",
						},
						Publication: map[string]acidv1.StreamTable{
							"test1": {
								EventType: "stream-type-a",
							},
						},
					},
				},
				dbNotExists: {
					slotNotExistsAppId2: zalandov1.Slot{
						Slot: map[string]string{
							"databases": "dbnotexists",
							"plugin":    constants.EventStreamSourcePluginType,
							"type":      "logical",
						},
						Publication: map[string]acidv1.StreamTable{
							"test2": {
								EventType: "stream-type-b",
							},
						},
					},
				},
			},
			actualSlots: map[string]map[string]string{
				slotName: {
					"databases": dbName,
					"plugin":    constants.EventStreamSourcePluginType,
					"type":      "logical",
				},
			},
			slotsInSync: false,
		},
	}

	for _, tt := range tests {
		result := hasSlotsInSync(tt.applicationId, tt.expectedSlots, tt.actualSlots)
		if result != tt.slotsInSync {
			t.Errorf("%s: unexpected result for slot test of applicationId: %v, expected slots %#v, actual slots %#v", tt.subTest, tt.applicationId, tt.expectedSlots, tt.actualSlots)
		}
	}
}

func TestGenerateFabricEventStream(t *testing.T) {
	cluster.Name = clusterName
	cluster.Namespace = namespace

	// create the streams
	err := cluster.syncStream(appId)
	assert.NoError(t, err)

	// compare generated stream with expected stream
	result := cluster.generateFabricEventStream(appId)
	if match, _ := cluster.compareStreams(result, fes); !match {
		t.Errorf("malformed FabricEventStream, expected %#v, got %#v", fes, result)
	}

	listOptions := metav1.ListOptions{
		LabelSelector: cluster.labelsSet(false).String(),
	}
	streams, err := cluster.KubeClient.FabricEventStreams(namespace).List(context.TODO(), listOptions)
	assert.NoError(t, err)
	assert.Equalf(t, 1, len(streams.Items), "unexpected number of streams found: got %d, but expected only one", len(streams.Items))

	// compare stream returned from API with expected stream
	if match, _ := cluster.compareStreams(&streams.Items[0], fes); !match {
		t.Errorf("malformed FabricEventStream returned from API, expected %#v, got %#v", fes, streams.Items[0])
	}

	// sync streams once again
	err = cluster.syncStream(appId)
	assert.NoError(t, err)

	streams, err = cluster.KubeClient.FabricEventStreams(namespace).List(context.TODO(), listOptions)
	assert.NoError(t, err)
	assert.Equalf(t, 1, len(streams.Items), "unexpected number of streams found: got %d, but expected only one", len(streams.Items))

	// compare stream resturned from API with generated stream
	if match, _ := cluster.compareStreams(&streams.Items[0], result); !match {
		t.Errorf("returned FabricEventStream differs from generated one, expected %#v, got %#v", result, streams.Items[0])
	}
}

func newFabricEventStream(streams []zalandov1.EventStream, annotations map[string]string) *zalandov1.FabricEventStream {
	return &zalandov1.FabricEventStream{
		ObjectMeta: metav1.ObjectMeta{
			Name:        fmt.Sprintf("%s-12345", clusterName),
			Annotations: annotations,
		},
		Spec: zalandov1.FabricEventStreamSpec{
			ApplicationId: appId,
			EventStreams:  streams,
		},
	}
}

func TestSyncStreams(t *testing.T) {
	newClusterName := fmt.Sprintf("%s-2", pg.Name)
	pg.Name = newClusterName
	var cluster = New(
		Config{
			OpConfig: config.Config{
				PodManagementPolicy: "ordered_ready",
				Resources: config.Resources{
					ClusterLabels:        map[string]string{"application": "spilo"},
					ClusterNameLabel:     "cluster-name",
					DefaultCPURequest:    "300m",
					DefaultCPULimit:      "300m",
					DefaultMemoryRequest: "300Mi",
					DefaultMemoryLimit:   "300Mi",
					PodRoleLabel:         "spilo-role",
				},
			},
		}, client, pg, logger, eventRecorder)

	_, err := cluster.KubeClient.Postgresqls(namespace).Create(
		context.TODO(), &pg, metav1.CreateOptions{})
	assert.NoError(t, err)

	// create the stream
	err = cluster.syncStream(appId)
	assert.NoError(t, err)

	// sync the stream again
	err = cluster.syncStream(appId)
	assert.NoError(t, err)

	// check that only one stream remains after sync
	listOptions := metav1.ListOptions{
		LabelSelector: cluster.labelsSet(false).String(),
	}
	streams, err := cluster.KubeClient.FabricEventStreams(namespace).List(context.TODO(), listOptions)
	assert.NoError(t, err)
	assert.Equalf(t, 1, len(streams.Items), "unexpected number of streams found: got %d, but expected only 1", len(streams.Items))
}

func TestSameStreams(t *testing.T) {
	testName := "TestSameStreams"
	annotationsA := map[string]string{constants.EventStreamMemoryAnnotationKey: "500Mi"}
	annotationsB := map[string]string{constants.EventStreamMemoryAnnotationKey: "1Gi"}

	stream1 := zalandov1.EventStream{
		EventStreamFlow:     zalandov1.EventStreamFlow{},
		EventStreamRecovery: zalandov1.EventStreamRecovery{},
		EventStreamSink: zalandov1.EventStreamSink{
			EventType: "stream-type-a",
		},
		EventStreamSource: zalandov1.EventStreamSource{
			EventStreamTable: zalandov1.EventStreamTable{
				Name: "foo",
			},
		},
	}

	stream2 := zalandov1.EventStream{
		EventStreamFlow:     zalandov1.EventStreamFlow{},
		EventStreamRecovery: zalandov1.EventStreamRecovery{},
		EventStreamSink: zalandov1.EventStreamSink{
			EventType: "stream-type-b",
		},
		EventStreamSource: zalandov1.EventStreamSource{
			EventStreamTable: zalandov1.EventStreamTable{
				Name: "bar",
			},
		},
	}

	stream3 := zalandov1.EventStream{
		EventStreamFlow: zalandov1.EventStreamFlow{},
		EventStreamRecovery: zalandov1.EventStreamRecovery{
			Type: constants.EventStreamRecoveryNoneType,
		},
		EventStreamSink: zalandov1.EventStreamSink{
			EventType: "stream-type-b",
		},
		EventStreamSource: zalandov1.EventStreamSource{
			EventStreamTable: zalandov1.EventStreamTable{
				Name: "bar",
			},
		},
	}

	tests := []struct {
		subTest  string
		streamsA *zalandov1.FabricEventStream
		streamsB *zalandov1.FabricEventStream
		match    bool
		reason   string
	}{
		{
			subTest:  "identical streams",
			streamsA: newFabricEventStream([]zalandov1.EventStream{stream1, stream2}, annotationsA),
			streamsB: newFabricEventStream([]zalandov1.EventStream{stream1, stream2}, annotationsA),
			match:    true,
			reason:   "",
		},
		{
			subTest:  "same streams different order",
			streamsA: newFabricEventStream([]zalandov1.EventStream{stream1, stream2}, nil),
			streamsB: newFabricEventStream([]zalandov1.EventStream{stream2, stream1}, nil),
			match:    true,
			reason:   "",
		},
		{
			subTest:  "same streams different order",
			streamsA: newFabricEventStream([]zalandov1.EventStream{stream1}, nil),
			streamsB: newFabricEventStream([]zalandov1.EventStream{stream1, stream2}, nil),
			match:    false,
			reason:   "new streams EventStreams array does not match : number of defined streams is different",
		},
		{
			subTest:  "different number of streams",
			streamsA: newFabricEventStream([]zalandov1.EventStream{stream1}, nil),
			streamsB: newFabricEventStream([]zalandov1.EventStream{stream1, stream2}, nil),
			match:    false,
			reason:   "new streams EventStreams array does not match : number of defined streams is different",
		},
		{
			subTest:  "event stream specs differ",
			streamsA: newFabricEventStream([]zalandov1.EventStream{stream1, stream2}, nil),
			streamsB: fes,
			match:    false,
			reason:   "new streams annotations do not match:  Added \"fes.zalando.org/FES_CPU\" with value \"250m\"., new streams labels do not match the current ones, new streams EventStreams array does not match : number of defined streams is different",
		},
		{
			subTest:  "event stream recovery specs differ",
			streamsA: newFabricEventStream([]zalandov1.EventStream{stream2}, nil),
			streamsB: newFabricEventStream([]zalandov1.EventStream{stream3}, nil),
			match:    false,
			reason:   "new streams EventStreams array does not match : event stream specs differ",
		},
		{
			subTest:  "event stream with new annotations",
			streamsA: newFabricEventStream([]zalandov1.EventStream{stream2}, nil),
			streamsB: newFabricEventStream([]zalandov1.EventStream{stream2}, annotationsA),
			match:    false,
			reason:   "new streams annotations do not match:  Added \"fes.zalando.org/FES_MEMORY\" with value \"500Mi\".",
		},
		{
			subTest:  "event stream annotations differ",
			streamsA: newFabricEventStream([]zalandov1.EventStream{stream3}, annotationsA),
			streamsB: newFabricEventStream([]zalandov1.EventStream{stream3}, annotationsB),
			match:    false,
			reason:   "new streams annotations do not match:  \"fes.zalando.org/FES_MEMORY\" changed from \"500Mi\" to \"1Gi\".",
		},
	}

	for _, tt := range tests {
		streamsMatch, matchReason := cluster.compareStreams(tt.streamsA, tt.streamsB)
		if streamsMatch != tt.match || matchReason != tt.reason {
			t.Errorf("%s %s: unexpected match result when comparing streams: got %s, expected %s",
				testName, tt.subTest, matchReason, tt.reason)
		}
	}
}

func TestUpdateStreams(t *testing.T) {
	pg.Name = fmt.Sprintf("%s-3", pg.Name)
	var cluster = New(
		Config{
			OpConfig: config.Config{
				PodManagementPolicy: "ordered_ready",
				Resources: config.Resources{
					ClusterLabels:         map[string]string{"application": "spilo"},
					ClusterNameLabel:      "cluster-name",
					DefaultCPURequest:     "300m",
					DefaultCPULimit:       "300m",
					DefaultMemoryRequest:  "300Mi",
					DefaultMemoryLimit:    "300Mi",
					EnableOwnerReferences: util.True(),
					PodRoleLabel:          "spilo-role",
				},
			},
		}, client, pg, logger, eventRecorder)

	_, err := cluster.KubeClient.Postgresqls(namespace).Create(
		context.TODO(), &pg, metav1.CreateOptions{})
	assert.NoError(t, err)

	// create stream with different owner reference
	fes.ObjectMeta.Name = fmt.Sprintf("%s-12345", pg.Name)
	fes.ObjectMeta.Labels["cluster-name"] = pg.Name
	createdStream, err := cluster.KubeClient.FabricEventStreams(namespace).Create(
		context.TODO(), fes, metav1.CreateOptions{})
	assert.NoError(t, err)
	assert.Equal(t, createdStream.Spec.ApplicationId, appId)

	// sync the stream which should update the owner reference
	err = cluster.syncStream(appId)
	assert.NoError(t, err)

	// check that only one stream exists after sync
	listOptions := metav1.ListOptions{
		LabelSelector: cluster.labelsSet(true).String(),
	}
	streams, err := cluster.KubeClient.FabricEventStreams(namespace).List(context.TODO(), listOptions)
	assert.NoError(t, err)
	assert.Equalf(t, 1, len(streams.Items), "unexpected number of streams found: got %d, but expected only 1", len(streams.Items))

	// compare owner references
	if !reflect.DeepEqual(streams.Items[0].OwnerReferences, cluster.ownerReferences()) {
		t.Errorf("unexpected owner references, expected %#v, got %#v", cluster.ownerReferences(), streams.Items[0].OwnerReferences)
	}

	// change specs of streams and patch CRD
	for i, stream := range pg.Spec.Streams {
		if stream.ApplicationId == appId {
			streamTable := stream.Tables["data.bar"]
			streamTable.EventType = "stream-type-c"
			stream.Tables["data.bar"] = streamTable
			stream.BatchSize = k8sutil.UInt32ToPointer(uint32(250))
			pg.Spec.Streams[i] = stream
		}
	}

	// compare stream returned from API with expected stream
	streams = patchPostgresqlStreams(t, cluster, &pg.Spec, listOptions)
	result := cluster.generateFabricEventStream(appId)
	if match, _ := cluster.compareStreams(&streams.Items[0], result); !match {
		t.Errorf("Malformed FabricEventStream after updating manifest, expected %#v, got %#v", streams.Items[0], result)
	}

	// disable recovery
	for idx, stream := range pg.Spec.Streams {
		if stream.ApplicationId == appId {
			stream.EnableRecovery = util.False()
			pg.Spec.Streams[idx] = stream
		}
	}

	streams = patchPostgresqlStreams(t, cluster, &pg.Spec, listOptions)
	result = cluster.generateFabricEventStream(appId)
	if match, _ := cluster.compareStreams(&streams.Items[0], result); !match {
		t.Errorf("Malformed FabricEventStream after disabling event recovery, expected %#v, got %#v", streams.Items[0], result)
	}
}

func patchPostgresqlStreams(t *testing.T, cluster *Cluster, pgSpec *acidv1.PostgresSpec, listOptions metav1.ListOptions) (streams *zalandov1.FabricEventStreamList) {
	patchData, err := specPatch(pgSpec)
	assert.NoError(t, err)

	pgPatched, err := cluster.KubeClient.Postgresqls(namespace).Patch(
		context.TODO(), cluster.Name, types.MergePatchType, patchData, metav1.PatchOptions{}, "spec")
	assert.NoError(t, err)

	cluster.Postgresql.Spec = pgPatched.Spec
	err = cluster.syncStream(appId)
	assert.NoError(t, err)

	streams, err = cluster.KubeClient.FabricEventStreams(namespace).List(context.TODO(), listOptions)
	assert.NoError(t, err)

	return streams
}

func TestDeleteStreams(t *testing.T) {
	pg.Name = fmt.Sprintf("%s-4", pg.Name)
	var cluster = New(
		Config{
			OpConfig: config.Config{
				PodManagementPolicy: "ordered_ready",
				Resources: config.Resources{
					ClusterLabels:        map[string]string{"application": "spilo"},
					ClusterNameLabel:     "cluster-name",
					DefaultCPURequest:    "300m",
					DefaultCPULimit:      "300m",
					DefaultMemoryRequest: "300Mi",
					DefaultMemoryLimit:   "300Mi",
					PodRoleLabel:         "spilo-role",
				},
			},
		}, client, pg, logger, eventRecorder)

	_, err := cluster.KubeClient.Postgresqls(namespace).Create(
		context.TODO(), &pg, metav1.CreateOptions{})
	assert.NoError(t, err)

	// create the stream
	err = cluster.syncStream(appId)
	assert.NoError(t, err)

	// change specs of streams and patch CRD
	for i, stream := range pg.Spec.Streams {
		if stream.ApplicationId == appId {
			streamTable := stream.Tables["data.bar"]
			streamTable.EventType = "stream-type-c"
			stream.Tables["data.bar"] = streamTable
			stream.BatchSize = k8sutil.UInt32ToPointer(uint32(250))
			pg.Spec.Streams[i] = stream
		}
	}

	// compare stream returned from API with expected stream
	listOptions := metav1.ListOptions{
		LabelSelector: cluster.labelsSet(false).String(),
	}
	streams := patchPostgresqlStreams(t, cluster, &pg.Spec, listOptions)
	result := cluster.generateFabricEventStream(appId)
	if match, _ := cluster.compareStreams(&streams.Items[0], result); !match {
		t.Errorf("Malformed FabricEventStream after updating manifest, expected %#v, got %#v", streams.Items[0], result)
	}

	// change teamId and check that stream is updated
	pg.Spec.TeamID = "new-team"
	streams = patchPostgresqlStreams(t, cluster, &pg.Spec, listOptions)
	result = cluster.generateFabricEventStream(appId)
	if match, _ := cluster.compareStreams(&streams.Items[0], result); !match {
		t.Errorf("Malformed FabricEventStream after updating teamId, expected %#v, got %#v", streams.Items[0].ObjectMeta.Labels, result.ObjectMeta.Labels)
	}

	// disable recovery
	for idx, stream := range pg.Spec.Streams {
		if stream.ApplicationId == appId {
			stream.EnableRecovery = util.False()
			pg.Spec.Streams[idx] = stream
		}
	}

	streams = patchPostgresqlStreams(t, cluster, &pg.Spec, listOptions)
	result = cluster.generateFabricEventStream(appId)
	if match, _ := cluster.compareStreams(&streams.Items[0], result); !match {
		t.Errorf("Malformed FabricEventStream after disabling event recovery, expected %#v, got %#v", streams.Items[0], result)
	}

	// remove streams from manifest
	pg.Spec.Streams = nil
	pgUpdated, err := cluster.KubeClient.Postgresqls(namespace).Update(
		context.TODO(), &pg, metav1.UpdateOptions{})
	assert.NoError(t, err)

	appIds := getDistinctApplicationIds(pgUpdated.Spec.Streams)
	cluster.cleanupRemovedStreams(appIds)

	// check that streams have been deleted
	streams, err = cluster.KubeClient.FabricEventStreams(namespace).List(context.TODO(), listOptions)
	assert.NoError(t, err)
	assert.Equalf(t, 0, len(streams.Items), "unexpected number of streams found: got %d, but expected none", len(streams.Items))

	// create stream to test deleteStreams code
	fes.ObjectMeta.Name = fmt.Sprintf("%s-12345", pg.Name)
	fes.ObjectMeta.Labels["cluster-name"] = pg.Name
	_, err = cluster.KubeClient.FabricEventStreams(namespace).Create(
		context.TODO(), fes, metav1.CreateOptions{})
	assert.NoError(t, err)

	// sync it once to cluster struct
	err = cluster.syncStream(appId)
	assert.NoError(t, err)

	// we need a mock client because deleteStreams checks for CRD existance
	mockClient := k8sutil.NewMockKubernetesClient()
	cluster.KubeClient.CustomResourceDefinitionsGetter = mockClient.CustomResourceDefinitionsGetter
	cluster.deleteStreams()

	// check that streams have been deleted
	streams, err = cluster.KubeClient.FabricEventStreams(namespace).List(context.TODO(), listOptions)
	assert.NoError(t, err)
	assert.Equalf(t, 0, len(streams.Items), "unexpected number of streams found: got %d, but expected none", len(streams.Items))
}


================================================
File: pkg/cluster/sync_test.go
================================================
package cluster

import (
	"bytes"
	"fmt"
	"io"
	"net/http"
	"testing"
	"time"

	"context"

	"golang.org/x/exp/slices"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"

	"github.com/golang/mock/gomock"
	"github.com/sirupsen/logrus"
	"github.com/stretchr/testify/assert"
	"github.com/zalando/postgres-operator/mocks"
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	fakeacidv1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/fake"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"github.com/zalando/postgres-operator/pkg/util/patroni"
	"k8s.io/client-go/kubernetes/fake"
)

var patroniLogger = logrus.New().WithField("test", "patroni")
var acidClientSet = fakeacidv1.NewSimpleClientset()
var clientSet = fake.NewSimpleClientset()

func newMockPod(ip string) *v1.Pod {
	return &v1.Pod{
		Status: v1.PodStatus{
			PodIP: ip,
		},
	}
}

func newFakeK8sSyncClient() (k8sutil.KubernetesClient, *fake.Clientset) {
	return k8sutil.KubernetesClient{
		PodsGetter:         clientSet.CoreV1(),
		PostgresqlsGetter:  acidClientSet.AcidV1(),
		StatefulSetsGetter: clientSet.AppsV1(),
	}, clientSet
}

func newFakeK8sSyncSecretsClient() (k8sutil.KubernetesClient, *fake.Clientset) {
	return k8sutil.KubernetesClient{
		SecretsGetter: clientSet.CoreV1(),
	}, clientSet
}

func TestSyncStatefulSetsAnnotations(t *testing.T) {
	testName := "test syncing statefulsets annotations"
	client, _ := newFakeK8sSyncClient()
	clusterName := "acid-test-cluster"
	namespace := "default"
	inheritedAnnotation := "environment"

	pg := acidv1.Postgresql{
		ObjectMeta: metav1.ObjectMeta{
			Name:        clusterName,
			Namespace:   namespace,
			Annotations: map[string]string{inheritedAnnotation: "test"},
		},
		Spec: acidv1.PostgresSpec{
			Volume: acidv1.Volume{
				Size: "1Gi",
			},
		},
	}

	var cluster = New(
		Config{
			OpConfig: config.Config{
				PodManagementPolicy: "ordered_ready",
				Resources: config.Resources{
					ClusterLabels:         map[string]string{"application": "spilo"},
					ClusterNameLabel:      "cluster-name",
					DefaultCPURequest:     "300m",
					DefaultCPULimit:       "300m",
					DefaultMemoryRequest:  "300Mi",
					DefaultMemoryLimit:    "300Mi",
					InheritedAnnotations:  []string{inheritedAnnotation},
					PodRoleLabel:          "spilo-role",
					ResourceCheckInterval: time.Duration(3),
					ResourceCheckTimeout:  time.Duration(10),
				},
			},
		}, client, pg, logger, eventRecorder)

	cluster.Name = clusterName
	cluster.Namespace = namespace

	// create a statefulset
	_, err := cluster.createStatefulSet()
	assert.NoError(t, err)

	// patch statefulset and add annotation
	patchData, err := metaAnnotationsPatch(map[string]string{"test-anno": "true"})
	assert.NoError(t, err)

	newSts, err := cluster.KubeClient.StatefulSets(namespace).Patch(
		context.TODO(),
		clusterName,
		types.MergePatchType,
		[]byte(patchData),
		metav1.PatchOptions{},
		"")
	assert.NoError(t, err)

	cluster.Statefulset = newSts

	// first compare running with desired statefulset - they should not match
	// because no inherited annotations or downscaler annotations are configured
	desiredSts, err := cluster.generateStatefulSet(&cluster.Postgresql.Spec)
	assert.NoError(t, err)

	cmp := cluster.compareStatefulSetWith(desiredSts)
	if cmp.match {
		t.Errorf("%s: match between current and desired statefulsets albeit differences: %#v", testName, cmp)
	}

	// now sync statefulset - the diff will trigger a replacement of the statefulset
	cluster.syncStatefulSet()

	// compare again after the SYNC - must be identical to the desired state
	cmp = cluster.compareStatefulSetWith(desiredSts)
	if !cmp.match {
		t.Errorf("%s: current and desired statefulsets are not matching %#v", testName, cmp)
	}

	// check if inherited annotation exists
	if _, exists := desiredSts.Annotations[inheritedAnnotation]; !exists {
		t.Errorf("%s: inherited annotation not found in desired statefulset: %#v", testName, desiredSts.Annotations)
	}
}

func TestPodAnnotationsSync(t *testing.T) {
	clusterName := "acid-test-cluster-2"
	namespace := "default"
	podAnnotation := "no-scale-down"
	podAnnotations := map[string]string{podAnnotation: "true"}
	customPodAnnotation := "foo"
	customPodAnnotations := map[string]string{customPodAnnotation: "true"}

	ctrl := gomock.NewController(t)
	defer ctrl.Finish()
	mockClient := mocks.NewMockHTTPClient(ctrl)
	client, _ := newFakeK8sAnnotationsClient()

	pg := acidv1.Postgresql{
		ObjectMeta: metav1.ObjectMeta{
			Name:      clusterName,
			Namespace: namespace,
		},
		Spec: acidv1.PostgresSpec{
			Volume: acidv1.Volume{
				Size: "1Gi",
			},
			EnableConnectionPooler:        boolToPointer(true),
			EnableLogicalBackup:           true,
			EnableReplicaConnectionPooler: boolToPointer(true),
			PodAnnotations:                podAnnotations,
			NumberOfInstances:             2,
		},
	}

	var cluster = New(
		Config{
			OpConfig: config.Config{
				PatroniAPICheckInterval: time.Duration(1),
				PatroniAPICheckTimeout:  time.Duration(5),
				PodManagementPolicy:     "ordered_ready",
				CustomPodAnnotations:    customPodAnnotations,
				ConnectionPooler: config.ConnectionPooler{
					ConnectionPoolerDefaultCPURequest:    "100m",
					ConnectionPoolerDefaultCPULimit:      "100m",
					ConnectionPoolerDefaultMemoryRequest: "100Mi",
					ConnectionPoolerDefaultMemoryLimit:   "100Mi",
					NumberOfInstances:                    k8sutil.Int32ToPointer(1),
				},
				Resources: config.Resources{
					ClusterLabels:         map[string]string{"application": "spilo"},
					ClusterNameLabel:      "cluster-name",
					DefaultCPURequest:     "300m",
					DefaultCPULimit:       "300m",
					DefaultMemoryRequest:  "300Mi",
					DefaultMemoryLimit:    "300Mi",
					MaxInstances:          -1,
					PodRoleLabel:          "spilo-role",
					ResourceCheckInterval: time.Duration(3),
					ResourceCheckTimeout:  time.Duration(10),
				},
			},
		}, client, pg, logger, eventRecorder)

	configJson := `{"postgresql": {"parameters": {"log_min_duration_statement": 200, "max_connections": 50}}}, "ttl": 20}`
	response := http.Response{
		StatusCode: 200,
		Body:       io.NopCloser(bytes.NewReader([]byte(configJson))),
	}

	mockClient.EXPECT().Do(gomock.Any()).Return(&response, nil).AnyTimes()
	cluster.patroni = patroni.New(patroniLogger, mockClient)
	cluster.Name = clusterName
	cluster.Namespace = namespace
	clusterOptions := clusterLabelsOptions(cluster)

	// create a statefulset
	_, err := cluster.createStatefulSet()
	assert.NoError(t, err)
	// create a pods
	podsList := createPods(cluster)
	for _, pod := range podsList {
		_, err = cluster.KubeClient.Pods(namespace).Create(context.TODO(), &pod, metav1.CreateOptions{})
		assert.NoError(t, err)
	}
	// create connection pooler
	_, err = cluster.createConnectionPooler(mockInstallLookupFunction)
	assert.NoError(t, err)

	// create cron job
	err = cluster.createLogicalBackupJob()
	assert.NoError(t, err)

	annotateResources(cluster)
	err = cluster.Sync(&cluster.Postgresql)
	assert.NoError(t, err)

	// 1. PodAnnotations set
	stsList, err := cluster.KubeClient.StatefulSets(namespace).List(context.TODO(), clusterOptions)
	assert.NoError(t, err)
	for _, sts := range stsList.Items {
		for _, annotation := range []string{podAnnotation, customPodAnnotation} {
			assert.Contains(t, sts.Spec.Template.Annotations, annotation)
		}
	}

	for _, role := range []PostgresRole{Master, Replica} {
		deploy, err := cluster.KubeClient.Deployments(namespace).Get(context.TODO(), cluster.connectionPoolerName(role), metav1.GetOptions{})
		assert.NoError(t, err)
		for _, annotation := range []string{podAnnotation, customPodAnnotation} {
			assert.Contains(t, deploy.Spec.Template.Annotations, annotation,
				fmt.Sprintf("pooler deployment pod template %s should contain annotation %s, found %#v",
					deploy.Name, annotation, deploy.Spec.Template.Annotations))
		}
	}

	podList, err := cluster.KubeClient.Pods(namespace).List(context.TODO(), clusterOptions)
	assert.NoError(t, err)
	for _, pod := range podList.Items {
		for _, annotation := range []string{podAnnotation, customPodAnnotation} {
			assert.Contains(t, pod.Annotations, annotation,
				fmt.Sprintf("pod %s should contain annotation %s, found %#v", pod.Name, annotation, pod.Annotations))
		}
	}

	cronJobList, err := cluster.KubeClient.CronJobs(namespace).List(context.TODO(), clusterOptions)
	assert.NoError(t, err)
	for _, cronJob := range cronJobList.Items {
		for _, annotation := range []string{podAnnotation, customPodAnnotation} {
			assert.Contains(t, cronJob.Spec.JobTemplate.Spec.Template.Annotations, annotation,
				fmt.Sprintf("logical backup cron job's pod template should contain annotation %s, found %#v",
					annotation, cronJob.Spec.JobTemplate.Spec.Template.Annotations))
		}
	}

	// 2 PodAnnotations removed
	newSpec := cluster.Postgresql.DeepCopy()
	newSpec.Spec.PodAnnotations = nil
	cluster.OpConfig.CustomPodAnnotations = nil
	err = cluster.Sync(newSpec)
	assert.NoError(t, err)

	stsList, err = cluster.KubeClient.StatefulSets(namespace).List(context.TODO(), clusterOptions)
	assert.NoError(t, err)
	for _, sts := range stsList.Items {
		for _, annotation := range []string{podAnnotation, customPodAnnotation} {
			assert.NotContains(t, sts.Spec.Template.Annotations, annotation)
		}
	}

	for _, role := range []PostgresRole{Master, Replica} {
		deploy, err := cluster.KubeClient.Deployments(namespace).Get(context.TODO(), cluster.connectionPoolerName(role), metav1.GetOptions{})
		assert.NoError(t, err)
		for _, annotation := range []string{podAnnotation, customPodAnnotation} {
			assert.NotContains(t, deploy.Spec.Template.Annotations, annotation,
				fmt.Sprintf("pooler deployment pod template %s should not contain annotation %s, found %#v",
					deploy.Name, annotation, deploy.Spec.Template.Annotations))
		}
	}

	podList, err = cluster.KubeClient.Pods(namespace).List(context.TODO(), clusterOptions)
	assert.NoError(t, err)
	for _, pod := range podList.Items {
		for _, annotation := range []string{podAnnotation, customPodAnnotation} {
			assert.NotContains(t, pod.Annotations, annotation,
				fmt.Sprintf("pod %s should not contain annotation %s, found %#v", pod.Name, annotation, pod.Annotations))
		}
	}

	cronJobList, err = cluster.KubeClient.CronJobs(namespace).List(context.TODO(), clusterOptions)
	assert.NoError(t, err)
	for _, cronJob := range cronJobList.Items {
		for _, annotation := range []string{podAnnotation, customPodAnnotation} {
			assert.NotContains(t, cronJob.Spec.JobTemplate.Spec.Template.Annotations, annotation,
				fmt.Sprintf("logical backup cron job's pod template should not contain annotation %s, found %#v",
					annotation, cronJob.Spec.JobTemplate.Spec.Template.Annotations))
		}
	}
}

func TestCheckAndSetGlobalPostgreSQLConfiguration(t *testing.T) {
	testName := "test config comparison"
	client, _ := newFakeK8sSyncClient()
	clusterName := "acid-test-cluster"
	namespace := "default"
	testSlots := map[string]map[string]string{
		"slot1": {
			"type":     "logical",
			"plugin":   "wal2json",
			"database": "foo",
		},
	}

	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	defaultPgParameters := map[string]string{
		"log_min_duration_statement": "200",
		"max_connections":            "50",
	}
	defaultPatroniParameters := acidv1.Patroni{
		TTL: 20,
	}

	pg := acidv1.Postgresql{
		ObjectMeta: metav1.ObjectMeta{
			Name:      clusterName,
			Namespace: namespace,
		},
		Spec: acidv1.PostgresSpec{
			Patroni: defaultPatroniParameters,
			PostgresqlParam: acidv1.PostgresqlParam{
				Parameters: defaultPgParameters,
			},
			Volume: acidv1.Volume{
				Size: "1Gi",
			},
		},
	}

	var cluster = New(
		Config{
			OpConfig: config.Config{
				PodManagementPolicy: "ordered_ready",
				Resources: config.Resources{
					ClusterLabels:         map[string]string{"application": "spilo"},
					ClusterNameLabel:      "cluster-name",
					DefaultCPURequest:     "300m",
					DefaultCPULimit:       "300m",
					DefaultMemoryRequest:  "300Mi",
					DefaultMemoryLimit:    "300Mi",
					PodRoleLabel:          "spilo-role",
					ResourceCheckInterval: time.Duration(3),
					ResourceCheckTimeout:  time.Duration(10),
				},
			},
		}, client, pg, logger, eventRecorder)

	// mocking a config after setConfig is called
	configJson := `{"postgresql": {"parameters": {"log_min_duration_statement": 200, "max_connections": 50}}}, "ttl": 20}`
	r := io.NopCloser(bytes.NewReader([]byte(configJson)))

	response := http.Response{
		StatusCode: 200,
		Body:       r,
	}

	mockClient := mocks.NewMockHTTPClient(ctrl)
	mockClient.EXPECT().Do(gomock.Any()).Return(&response, nil).AnyTimes()

	p := patroni.New(patroniLogger, mockClient)
	cluster.patroni = p
	mockPod := newMockPod("192.168.100.1")

	// simulate existing config that differs from cluster.Spec
	tests := []struct {
		subtest         string
		patroni         acidv1.Patroni
		desiredSlots    map[string]map[string]string
		removedSlots    map[string]map[string]string
		pgParams        map[string]string
		shouldBePatched bool
		restartPrimary  bool
	}{
		{
			subtest: "Patroni and Postgresql.Parameters do not differ",
			patroni: acidv1.Patroni{
				TTL: 20,
			},
			pgParams: map[string]string{
				"log_min_duration_statement": "200",
				"max_connections":            "50",
			},
			shouldBePatched: false,
			restartPrimary:  false,
		},
		{
			subtest: "Patroni and Postgresql.Parameters differ - restart replica first",
			patroni: acidv1.Patroni{
				TTL: 30, // desired 20
			},
			pgParams: map[string]string{
				"log_min_duration_statement": "500", // desired 200
				"max_connections":            "100", // desired 50
			},
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest: "multiple Postgresql.Parameters differ - restart replica first",
			patroni: defaultPatroniParameters,
			pgParams: map[string]string{
				"log_min_duration_statement": "500", // desired 200
				"max_connections":            "100", // desired 50
			},
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest: "desired max_connections bigger - restart replica first",
			patroni: defaultPatroniParameters,
			pgParams: map[string]string{
				"log_min_duration_statement": "200",
				"max_connections":            "30", // desired 50
			},
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest: "desired max_connections smaller - restart master first",
			patroni: defaultPatroniParameters,
			pgParams: map[string]string{
				"log_min_duration_statement": "200",
				"max_connections":            "100", // desired 50
			},
			shouldBePatched: true,
			restartPrimary:  true,
		},
		{
			subtest: "slot does not exist but is desired",
			patroni: acidv1.Patroni{
				TTL: 20,
			},
			desiredSlots: testSlots,
			pgParams: map[string]string{
				"log_min_duration_statement": "200",
				"max_connections":            "50",
			},
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest: "slot exist, nothing specified in manifest",
			patroni: acidv1.Patroni{
				TTL: 20,
				Slots: map[string]map[string]string{
					"slot1": {
						"type":     "logical",
						"plugin":   "pgoutput",
						"database": "foo",
					},
				},
			},
			pgParams: map[string]string{
				"log_min_duration_statement": "200",
				"max_connections":            "50",
			},
			shouldBePatched: false,
			restartPrimary:  false,
		},
		{
			subtest: "slot is removed from manifest",
			patroni: acidv1.Patroni{
				TTL: 20,
				Slots: map[string]map[string]string{
					"slot1": {
						"type":     "logical",
						"plugin":   "pgoutput",
						"database": "foo",
					},
				},
			},
			removedSlots: testSlots,
			pgParams: map[string]string{
				"log_min_duration_statement": "200",
				"max_connections":            "50",
			},
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest: "slot plugin differs",
			patroni: acidv1.Patroni{
				TTL: 20,
				Slots: map[string]map[string]string{
					"slot1": {
						"type":     "logical",
						"plugin":   "pgoutput",
						"database": "foo",
					},
				},
			},
			desiredSlots: testSlots,
			pgParams: map[string]string{
				"log_min_duration_statement": "200",
				"max_connections":            "50",
			},
			shouldBePatched: true,
			restartPrimary:  false,
		},
	}

	for _, tt := range tests {
		if len(tt.desiredSlots) > 0 {
			cluster.Spec.Patroni.Slots = tt.desiredSlots
		}
		if len(tt.removedSlots) > 0 {
			for slotName, removedSlot := range tt.removedSlots {
				cluster.replicationSlots[slotName] = removedSlot
			}
		}

		configPatched, requirePrimaryRestart, err := cluster.checkAndSetGlobalPostgreSQLConfiguration(mockPod, tt.patroni, cluster.Spec.Patroni, tt.pgParams, cluster.Spec.Parameters)
		assert.NoError(t, err)
		if configPatched != tt.shouldBePatched {
			t.Errorf("%s - %s: expected config update did not happen", testName, tt.subtest)
		}
		if requirePrimaryRestart != tt.restartPrimary {
			t.Errorf("%s - %s: wrong master restart strategy, got restart %v, expected restart %v", testName, tt.subtest, requirePrimaryRestart, tt.restartPrimary)
		}

		// reset slots for next tests
		cluster.Spec.Patroni.Slots = nil
		cluster.replicationSlots = make(map[string]interface{})
	}

	testsFailsafe := []struct {
		subtest         string
		operatorVal     *bool
		effectiveVal    *bool
		desiredVal      bool
		shouldBePatched bool
		restartPrimary  bool
	}{
		{
			subtest:         "Not set in operator config, not set for pg cluster. Set to true in the pg config.",
			operatorVal:     nil,
			effectiveVal:    nil,
			desiredVal:      true,
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest:         "Not set in operator config, disabled for pg cluster. Set to true in the pg config.",
			operatorVal:     nil,
			effectiveVal:    util.False(),
			desiredVal:      true,
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest:         "Not set in operator config, not set for pg cluster. Set to false in the pg config.",
			operatorVal:     nil,
			effectiveVal:    nil,
			desiredVal:      false,
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest:         "Not set in operator config, enabled for pg cluster. Set to false in the pg config.",
			operatorVal:     nil,
			effectiveVal:    util.True(),
			desiredVal:      false,
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest:         "Enabled in operator config, not set for pg cluster. Set to false in the pg config.",
			operatorVal:     util.True(),
			effectiveVal:    nil,
			desiredVal:      false,
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest:         "Enabled in operator config, disabled for pg cluster. Set to true in the pg config.",
			operatorVal:     util.True(),
			effectiveVal:    util.False(),
			desiredVal:      true,
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest:         "Disabled in operator config, not set for pg cluster. Set to true in the pg config.",
			operatorVal:     util.False(),
			effectiveVal:    nil,
			desiredVal:      true,
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest:         "Disabled in operator config, enabled for pg cluster. Set to false in the pg config.",
			operatorVal:     util.False(),
			effectiveVal:    util.True(),
			desiredVal:      false,
			shouldBePatched: true,
			restartPrimary:  false,
		},
		{
			subtest:         "Disabled in operator config, enabled for pg cluster. Set to true in the pg config.",
			operatorVal:     util.False(),
			effectiveVal:    util.True(),
			desiredVal:      true,
			shouldBePatched: false, // should not require patching
			restartPrimary:  false,
		},
	}

	for _, tt := range testsFailsafe {
		patroniConf := defaultPatroniParameters

		if tt.operatorVal != nil {
			cluster.OpConfig.EnablePatroniFailsafeMode = tt.operatorVal
		}
		if tt.effectiveVal != nil {
			patroniConf.FailsafeMode = tt.effectiveVal
		}
		cluster.Spec.Patroni.FailsafeMode = &tt.desiredVal

		configPatched, requirePrimaryRestart, err := cluster.checkAndSetGlobalPostgreSQLConfiguration(mockPod, patroniConf, cluster.Spec.Patroni, defaultPgParameters, cluster.Spec.Parameters)
		assert.NoError(t, err)
		if configPatched != tt.shouldBePatched {
			t.Errorf("%s - %s: expected update went wrong", testName, tt.subtest)
		}
		if requirePrimaryRestart != tt.restartPrimary {
			t.Errorf("%s - %s: wrong master restart strategy, got restart %v, expected restart %v", testName, tt.subtest, requirePrimaryRestart, tt.restartPrimary)
		}
	}
}

func TestSyncStandbyClusterConfiguration(t *testing.T) {
	client, _ := newFakeK8sSyncClient()
	clusterName := "acid-standby-cluster"
	applicationLabel := "spilo"
	namespace := "default"

	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	pg := acidv1.Postgresql{
		ObjectMeta: metav1.ObjectMeta{
			Name:      clusterName,
			Namespace: namespace,
		},
		Spec: acidv1.PostgresSpec{
			NumberOfInstances: int32(1),
			Volume: acidv1.Volume{
				Size: "1Gi",
			},
		},
	}

	var cluster = New(
		Config{
			OpConfig: config.Config{
				PatroniAPICheckInterval: time.Duration(1),
				PatroniAPICheckTimeout:  time.Duration(5),
				PodManagementPolicy:     "ordered_ready",
				Resources: config.Resources{
					ClusterLabels:         map[string]string{"application": applicationLabel},
					ClusterNameLabel:      "cluster-name",
					DefaultCPURequest:     "300m",
					DefaultCPULimit:       "300m",
					DefaultMemoryRequest:  "300Mi",
					DefaultMemoryLimit:    "300Mi",
					MinInstances:          int32(-1),
					MaxInstances:          int32(-1),
					PodRoleLabel:          "spilo-role",
					ResourceCheckInterval: time.Duration(3),
					ResourceCheckTimeout:  time.Duration(10),
				},
			},
		}, client, pg, logger, eventRecorder)

	cluster.Name = clusterName
	cluster.Namespace = namespace

	// mocking a config after getConfig is called
	mockClient := mocks.NewMockHTTPClient(ctrl)
	configJson := `{"ttl": 20}`
	r := io.NopCloser(bytes.NewReader([]byte(configJson)))
	response := http.Response{
		StatusCode: 200,
		Body:       r,
	}
	mockClient.EXPECT().Get(gomock.Any()).Return(&response, nil).AnyTimes()

	// mocking a config after setConfig is called
	standbyJson := `{"standby_cluster":{"create_replica_methods":["bootstrap_standby_with_wale","basebackup_fast_xlog"],"restore_command":"envdir \"/run/etc/wal-e.d/env-standby\" /scripts/restore_command.sh \"%f\" \"%p\""}}`
	r = io.NopCloser(bytes.NewReader([]byte(standbyJson)))
	response = http.Response{
		StatusCode: 200,
		Body:       r,
	}
	mockClient.EXPECT().Do(gomock.Any()).Return(&response, nil).AnyTimes()
	p := patroni.New(patroniLogger, mockClient)
	cluster.patroni = p

	mockPod := newMockPod("192.168.100.1")
	mockPod.Name = fmt.Sprintf("%s-0", clusterName)
	mockPod.Namespace = namespace
	podLabels := map[string]string{
		"cluster-name": clusterName,
		"application":  applicationLabel,
		"spilo-role":   "master",
	}
	mockPod.Labels = podLabels
	client.PodsGetter.Pods(namespace).Create(context.TODO(), mockPod, metav1.CreateOptions{})

	// create a statefulset
	sts, err := cluster.createStatefulSet()
	assert.NoError(t, err)

	// check that pods do not have a STANDBY_* environment variable
	assert.NotContains(t, sts.Spec.Template.Spec.Containers[0].Env, v1.EnvVar{Name: "STANDBY_METHOD", Value: "STANDBY_WITH_WALE"})

	// add standby section
	cluster.Spec.StandbyCluster = &acidv1.StandbyDescription{
		S3WalPath: "s3://custom/path/to/bucket/",
	}
	cluster.syncStatefulSet()
	updatedSts := cluster.Statefulset

	// check that pods do not have a STANDBY_* environment variable
	assert.Contains(t, updatedSts.Spec.Template.Spec.Containers[0].Env, v1.EnvVar{Name: "STANDBY_METHOD", Value: "STANDBY_WITH_WALE"})

	// this should update the Patroni config
	err = cluster.syncStandbyClusterConfiguration()
	assert.NoError(t, err)

	configJson = `{"standby_cluster":{"create_replica_methods":["bootstrap_standby_with_wale","basebackup_fast_xlog"],"restore_command":"envdir \"/run/etc/wal-e.d/env-standby\" /scripts/restore_command.sh \"%f\" \"%p\""}, "ttl": 20}`
	r = io.NopCloser(bytes.NewReader([]byte(configJson)))
	response = http.Response{
		StatusCode: 200,
		Body:       r,
	}
	mockClient.EXPECT().Get(gomock.Any()).Return(&response, nil).AnyTimes()

	pods, err := cluster.listPods()
	assert.NoError(t, err)

	_, _, err = cluster.patroni.GetConfig(&pods[0])
	assert.NoError(t, err)
	// ToDo extend GetConfig to return standy_cluster setting to compare
	/*
		defaultStandbyParameters := map[string]interface{}{
			"create_replica_methods": []string{"bootstrap_standby_with_wale", "basebackup_fast_xlog"},
			"restore_command":        "envdir \"/run/etc/wal-e.d/env-standby\" /scripts/restore_command.sh \"%f\" \"%p\"",
		}
		assert.True(t, reflect.DeepEqual(defaultStandbyParameters, standbyCluster))
	*/
	// remove standby section
	cluster.Spec.StandbyCluster = &acidv1.StandbyDescription{}
	cluster.syncStatefulSet()
	updatedSts2 := cluster.Statefulset

	// check that pods do not have a STANDBY_* environment variable
	assert.NotContains(t, updatedSts2.Spec.Template.Spec.Containers[0].Env, v1.EnvVar{Name: "STANDBY_METHOD", Value: "STANDBY_WITH_WALE"})

	// this should update the Patroni config again
	err = cluster.syncStandbyClusterConfiguration()
	assert.NoError(t, err)
}

func TestUpdateSecret(t *testing.T) {
	testName := "test syncing secrets"
	client, _ := newFakeK8sSyncSecretsClient()

	clusterName := "acid-test-cluster"
	namespace := "default"
	dbname := "app"
	dbowner := "appowner"
	appUser := "foo"
	secretTemplate := config.StringTemplate("{username}.{cluster}.credentials")
	retentionUsers := make([]string, 0)

	// define manifest users and enable rotation for dbowner
	pg := acidv1.Postgresql{
		ObjectMeta: metav1.ObjectMeta{
			Name:      clusterName,
			Namespace: namespace,
		},
		Spec: acidv1.PostgresSpec{
			Databases:                      map[string]string{dbname: dbowner},
			Users:                          map[string]acidv1.UserFlags{appUser: {}, "bar": {}, dbowner: {}},
			UsersIgnoringSecretRotation:    []string{"bar"},
			UsersWithInPlaceSecretRotation: []string{dbowner},
			Streams: []acidv1.Stream{
				{
					ApplicationId: appId,
					Database:      dbname,
					Tables: map[string]acidv1.StreamTable{
						"data.foo": {
							EventType: "stream-type-b",
						},
					},
				},
			},
			Volume: acidv1.Volume{
				Size: "1Gi",
			},
		},
	}

	// new cluster with enabled password rotation
	var cluster = New(
		Config{
			OpConfig: config.Config{
				Auth: config.Auth{
					SuperUsername:                 "postgres",
					ReplicationUsername:           "standby",
					SecretNameTemplate:            secretTemplate,
					EnablePasswordRotation:        true,
					PasswordRotationInterval:      1,
					PasswordRotationUserRetention: 3,
				},
				Resources: config.Resources{
					ClusterLabels:    map[string]string{"application": "spilo"},
					ClusterNameLabel: "cluster-name",
				},
			},
		}, client, pg, logger, eventRecorder)

	cluster.Name = clusterName
	cluster.Namespace = namespace
	cluster.pgUsers = map[string]spec.PgUser{}

	// init all users
	cluster.initUsers()
	// create secrets
	cluster.syncSecrets()
	// initialize rotation with current time
	cluster.syncSecrets()

	dayAfterTomorrow := time.Now().AddDate(0, 0, 2)

	allUsers := make(map[string]spec.PgUser)
	for _, pgUser := range cluster.pgUsers {
		allUsers[pgUser.Name] = pgUser
	}
	for _, systemUser := range cluster.systemUsers {
		allUsers[systemUser.Name] = systemUser
	}

	for username, pgUser := range allUsers {
		// first, get the secret
		secretName := cluster.credentialSecretName(username)
		secret, err := cluster.KubeClient.Secrets(namespace).Get(context.TODO(), secretName, metav1.GetOptions{})
		assert.NoError(t, err)
		secretPassword := string(secret.Data["password"])

		// now update the secret setting a next rotation date (tomorrow + interval)
		cluster.updateSecret(username, secret, &retentionUsers, dayAfterTomorrow)
		updatedSecret, err := cluster.KubeClient.Secrets(namespace).Get(context.TODO(), secretName, metav1.GetOptions{})
		assert.NoError(t, err)

		// check that passwords are different
		rotatedPassword := string(updatedSecret.Data["password"])
		if secretPassword == rotatedPassword {
			// passwords for system users should not have been rotated
			if pgUser.Origin != spec.RoleOriginManifest {
				continue
			}
			if slices.Contains(pg.Spec.UsersIgnoringSecretRotation, username) {
				continue
			}
			t.Errorf("%s: password unchanged in updated secret for %s", testName, username)
		}

		// check that next rotation date is tomorrow + interval, not date in secret + interval
		nextRotation := string(updatedSecret.Data["nextRotation"])
		_, nextRotationDate := cluster.getNextRotationDate(dayAfterTomorrow)
		if nextRotation != nextRotationDate {
			t.Errorf("%s: updated secret of %s does not contain correct rotation date: expected %s, got %s", testName, username, nextRotationDate, nextRotation)
		}

		// compare username, when it's dbowner they should be equal because of UsersWithInPlaceSecretRotation
		secretUsername := string(updatedSecret.Data["username"])
		if pgUser.IsDbOwner {
			if secretUsername != username {
				t.Errorf("%s: username differs in updated secret: expected %s, got %s", testName, username, secretUsername)
			}
		} else {
			rotatedUsername := username + dayAfterTomorrow.Format(constants.RotationUserDateFormat)
			if secretUsername != rotatedUsername {
				t.Errorf("%s: updated secret does not contain correct username: expected %s, got %s", testName, rotatedUsername, secretUsername)
			}
			// whenever there's a rotation the retentionUsers list is extended or updated
			if len(retentionUsers) != 1 {
				t.Errorf("%s: unexpected number of users to drop - expected only %s, found %d", testName, username, len(retentionUsers))
			}
		}
	}

	// switch rotation for foo to in-place
	inPlaceRotationUsers := []string{dbowner, appUser}
	cluster.Spec.UsersWithInPlaceSecretRotation = inPlaceRotationUsers
	cluster.initUsers()
	cluster.syncSecrets()
	updatedSecret, err := cluster.KubeClient.Secrets(namespace).Get(context.TODO(), cluster.credentialSecretName(appUser), metav1.GetOptions{})
	assert.NoError(t, err)

	// username in secret should be switched to original user
	currentUsername := string(updatedSecret.Data["username"])
	if currentUsername != appUser {
		t.Errorf("%s: updated secret does not contain correct username: expected %s, got %s", testName, appUser, currentUsername)
	}

	// switch rotation back to rotation user
	inPlaceRotationUsers = []string{dbowner}
	cluster.Spec.UsersWithInPlaceSecretRotation = inPlaceRotationUsers
	cluster.initUsers()
	cluster.syncSecrets()
	updatedSecret, err = cluster.KubeClient.Secrets(namespace).Get(context.TODO(), cluster.credentialSecretName(appUser), metav1.GetOptions{})
	assert.NoError(t, err)

	// username in secret will only be switched after next rotation date is passed
	currentUsername = string(updatedSecret.Data["username"])
	if currentUsername != appUser {
		t.Errorf("%s: updated secret does not contain expected username: expected %s, got %s", testName, appUser, currentUsername)
	}
}


================================================
File: pkg/cluster/types.go
================================================
package cluster

import (
	"time"

	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	appsv1 "k8s.io/api/apps/v1"
	v1 "k8s.io/api/core/v1"
	policyv1 "k8s.io/api/policy/v1"
	"k8s.io/apimachinery/pkg/types"
)

// PostgresRole describes role of the node
type PostgresRole string

const (
	// spilo roles
	Master  PostgresRole = "master"
	Replica PostgresRole = "replica"
	Patroni PostgresRole = "config"

	// roles returned by Patroni cluster endpoint
	Leader        PostgresRole = "leader"
	StandbyLeader PostgresRole = "standby_leader"
	SyncStandby   PostgresRole = "sync_standby"
)

// PodEventType represents the type of a pod-related event
type PodEventType string

// Possible values for the EventType
const (
	PodEventAdd    PodEventType = "ADD"
	PodEventUpdate PodEventType = "UPDATE"
	PodEventDelete PodEventType = "DELETE"
)

// PodEvent describes the event for a single Pod
type PodEvent struct {
	ResourceVersion string
	PodName         types.NamespacedName
	PrevPod         *v1.Pod
	CurPod          *v1.Pod
	EventType       PodEventType
}

// Process describes process of the cluster
type Process struct {
	Name      string
	StartTime time.Time
}

// WorkerStatus describes status of the worker
type WorkerStatus struct {
	CurrentCluster types.NamespacedName
	CurrentProcess Process
}

// ClusterStatus describes status of the cluster
type ClusterStatus struct {
	Team                          string
	Cluster                       string
	Namespace                     string
	MasterService                 *v1.Service
	ReplicaService                *v1.Service
	MasterEndpoint                *v1.Endpoints
	ReplicaEndpoint               *v1.Endpoints
	StatefulSet                   *appsv1.StatefulSet
	PrimaryPodDisruptionBudget    *policyv1.PodDisruptionBudget
	CriticalOpPodDisruptionBudget *policyv1.PodDisruptionBudget

	CurrentProcess Process
	Worker         uint32
	Status         acidv1.PostgresStatus
	Spec           acidv1.PostgresSpec
	Error          error
}

type TemplateParams map[string]interface{}

type InstallFunction func(schema string, user string) error

type SyncReason []string

// no sync happened, empty value
var NoSync SyncReason = []string{}


================================================
File: pkg/cluster/util.go
================================================
package cluster

import (
	"bytes"
	"context"
	"encoding/gob"
	"encoding/json"
	"fmt"
	"net/http"
	"reflect"
	"sort"
	"strings"
	"time"

	appsv1 "k8s.io/api/apps/v1"
	v1 "k8s.io/api/core/v1"
	policyv1 "k8s.io/api/policy/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"

	"github.com/sirupsen/logrus"
	acidzalando "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do"
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"github.com/zalando/postgres-operator/pkg/util/nicediff"
	"github.com/zalando/postgres-operator/pkg/util/retryutil"
)

// OAuthTokenGetter provides the method for fetching OAuth tokens
type OAuthTokenGetter interface {
	getOAuthToken() (string, error)
}

// SecretOauthTokenGetter enables fetching OAuth tokens by reading Kubernetes secrets
type SecretOauthTokenGetter struct {
	kubeClient           *k8sutil.KubernetesClient
	OAuthTokenSecretName spec.NamespacedName
}

func newSecretOauthTokenGetter(kubeClient *k8sutil.KubernetesClient,
	OAuthTokenSecretName spec.NamespacedName) *SecretOauthTokenGetter {
	return &SecretOauthTokenGetter{kubeClient, OAuthTokenSecretName}
}

func (g *SecretOauthTokenGetter) getOAuthToken() (string, error) {
	//TODO: we can move this function to the Controller in case it will be needed there. As for now we use it only in the Cluster
	// Temporary getting postgresql-operator secret from the NamespaceDefault
	credentialsSecret, err := g.kubeClient.
		Secrets(g.OAuthTokenSecretName.Namespace).
		Get(context.TODO(), g.OAuthTokenSecretName.Name, metav1.GetOptions{})

	if err != nil {
		return "", fmt.Errorf("could not get credentials secret: %v", err)
	}
	data := credentialsSecret.Data

	if string(data["read-only-token-type"]) != "Bearer" {
		return "", fmt.Errorf("wrong token type: %v", data["read-only-token-type"])
	}

	return string(data["read-only-token-secret"]), nil
}

func isValidUsername(username string) bool {
	return userRegexp.MatchString(username)
}

func (c *Cluster) isProtectedUsername(username string) bool {
	for _, protected := range c.OpConfig.ProtectedRoles {
		if username == protected {
			return true
		}
	}
	return false
}

func (c *Cluster) isSystemUsername(username string) bool {
	// is there a pooler system user defined
	for _, systemUser := range c.systemUsers {
		if username == systemUser.Name {
			return true
		}
	}

	return false
}

func isValidFlag(flag string) bool {
	for _, validFlag := range []string{constants.RoleFlagSuperuser, constants.RoleFlagLogin, constants.RoleFlagCreateDB,
		constants.RoleFlagInherit, constants.RoleFlagReplication, constants.RoleFlagByPassRLS,
		constants.RoleFlagCreateRole} {
		if flag == validFlag || flag == "NO"+validFlag {
			return true
		}
	}
	return false
}

func invertFlag(flag string) string {
	if flag[:2] == "NO" {
		return flag[2:]
	}
	return "NO" + flag
}

func normalizeUserFlags(userFlags []string) ([]string, error) {
	uniqueFlags := make(map[string]bool)
	addLogin := true

	for _, flag := range userFlags {
		if !alphaNumericRegexp.MatchString(flag) {
			return nil, fmt.Errorf("user flag %q is not alphanumeric", flag)
		}

		flag = strings.ToUpper(flag)
		if _, ok := uniqueFlags[flag]; !ok {
			if !isValidFlag(flag) {
				return nil, fmt.Errorf("user flag %q is not valid", flag)
			}
			invFlag := invertFlag(flag)
			if uniqueFlags[invFlag] {
				return nil, fmt.Errorf("conflicting user flags: %q and %q", flag, invFlag)
			}
			uniqueFlags[flag] = true
		}
	}

	flags := []string{}
	for k := range uniqueFlags {
		if k == constants.RoleFlagNoLogin || k == constants.RoleFlagLogin {
			addLogin = false
			if k == constants.RoleFlagNoLogin {
				// we don't add NOLOGIN to the list of flags to be consistent with what we get
				// from the readPgUsersFromDatabase in SyncUsers
				continue
			}
		}
		flags = append(flags, k)
	}
	if addLogin {
		flags = append(flags, constants.RoleFlagLogin)
	}
	sort.Strings(flags)
	return flags, nil
}

// specPatch produces a JSON of the Kubernetes object specification passed (typically service or
// statefulset) to use it in a MergePatch.
func specPatch(spec interface{}) ([]byte, error) {
	return json.Marshal(struct {
		Spec interface{} `json:"spec"`
	}{spec})
}

// metaAnnotationsPatch produces a JSON of the object metadata that has only the annotation
// field in order to use it in a MergePatch. Note that we don't patch the complete metadata, since
// it contains the current revision of the object that could be outdated at the time we patch.
func metaAnnotationsPatch(annotations map[string]string) ([]byte, error) {
	var meta metav1.ObjectMeta
	meta.Annotations = annotations
	return json.Marshal(struct {
		ObjMeta interface{} `json:"metadata"`
	}{&meta})
}

func (c *Cluster) logPDBChanges(old, new *policyv1.PodDisruptionBudget, isUpdate bool, reason string) {
	if isUpdate {
		c.logger.Infof("pod disruption budget %q has been changed", util.NameFromMeta(old.ObjectMeta))
	} else {
		c.logger.Infof("pod disruption budget %q is not in the desired state and needs to be updated",
			util.NameFromMeta(old.ObjectMeta),
		)
	}

	logNiceDiff(c.logger, old.Spec, new.Spec)

	if reason != "" {
		c.logger.Infof("reason: %s", reason)
	}
}

func logNiceDiff(log *logrus.Entry, old, new interface{}) {
	o, erro := json.MarshalIndent(old, "", "  ")
	n, errn := json.MarshalIndent(new, "", "  ")

	if erro != nil || errn != nil {
		panic("could not marshal API objects, should not happen")
	}

	nice := nicediff.Diff(string(o), string(n), true)
	for _, s := range strings.Split(nice, "\n") {
		// " is not needed in the value to understand
		log.Debug(strings.ReplaceAll(s, "\"", ""))
	}
}

func (c *Cluster) logStatefulSetChanges(old, new *appsv1.StatefulSet, isUpdate bool, reasons []string) {
	if isUpdate {
		c.logger.Infof("statefulset %s has been changed", util.NameFromMeta(old.ObjectMeta))
	} else {
		c.logger.Infof("statefulset %s is not in the desired state and needs to be updated",
			util.NameFromMeta(old.ObjectMeta),
		)
	}

	logNiceDiff(c.logger, old.Spec, new.Spec)

	if !reflect.DeepEqual(old.Annotations, new.Annotations) {
		c.logger.Debug("metadata.annotation are different")
		logNiceDiff(c.logger, old.Annotations, new.Annotations)
	}

	if len(reasons) > 0 {
		for _, reason := range reasons {
			c.logger.Infof("reason: %s", reason)
		}
	}
}

func (c *Cluster) logServiceChanges(role PostgresRole, old, new *v1.Service, isUpdate bool, reason string) {
	if isUpdate {
		c.logger.Infof("%s service %s has been changed",
			role, util.NameFromMeta(old.ObjectMeta),
		)
	} else {
		c.logger.Infof("%s service %s is not in the desired state and needs to be updated",
			role, util.NameFromMeta(old.ObjectMeta),
		)
	}

	logNiceDiff(c.logger, old.Spec, new.Spec)

	if reason != "" {
		c.logger.Infof("reason: %s", reason)
	}
}

func getPostgresContainer(podSpec *v1.PodSpec) (pgContainer v1.Container) {
	for _, container := range podSpec.Containers {
		if container.Name == constants.PostgresContainerName {
			pgContainer = container
		}
	}

	// if no postgres container was found, take the first one in the podSpec
	if reflect.DeepEqual(pgContainer, v1.Container{}) && len(podSpec.Containers) > 0 {
		pgContainer = podSpec.Containers[0]
	}
	return pgContainer
}

func (c *Cluster) getTeamMembers(teamID string) ([]string, error) {

	if teamID == "" {
		msg := "no teamId specified"
		if c.OpConfig.EnableTeamIdClusternamePrefix {
			return nil, fmt.Errorf(msg)
		}
		c.logger.Warnf(msg)
		return nil, nil
	}

	members := []string{}

	if c.OpConfig.EnablePostgresTeamCRD && c.Config.PgTeamMap != nil {
		c.logger.Debugf("fetching possible additional team members for team %q", teamID)
		additionalMembers := []string{}

		for team, membership := range *c.Config.PgTeamMap {
			if team == teamID {
				additionalMembers = membership.AdditionalMembers
				c.logger.Debugf("found %d additional members for team %q", len(additionalMembers), teamID)
			}
		}

		members = append(members, additionalMembers...)
	}

	if !c.OpConfig.EnableTeamsAPI {
		c.logger.Debug("team API is disabled")
		return members, nil
	}

	token, err := c.oauthTokenGetter.getOAuthToken()
	if err != nil {
		return nil, fmt.Errorf("could not get oauth token to authenticate to team service API: %v", err)
	}

	teamInfo, statusCode, err := c.teamsAPIClient.TeamInfo(teamID, token)

	if err != nil {
		if statusCode == http.StatusNotFound {
			c.logger.Warningf("could not get team info for team %q: %v", teamID, err)
		} else {
			return nil, fmt.Errorf("could not get team info for team %q: %v", teamID, err)
		}
	} else {
		for _, member := range teamInfo.Members {
			if !(util.SliceContains(members, member)) {
				members = append(members, member)
			}
		}
	}
	return members, nil
}

// Returns annotations to be passed to child objects
func (c *Cluster) annotationsSet(annotations map[string]string) map[string]string {

	if annotations == nil {
		annotations = make(map[string]string)
	}

	pgCRDAnnotations := c.ObjectMeta.Annotations

	// allow to inherit certain labels from the 'postgres' object
	for k, v := range pgCRDAnnotations {
		for _, match := range c.OpConfig.InheritedAnnotations {
			if k == match {
				annotations[k] = v
			}
		}
	}

	if len(annotations) > 0 {
		return annotations
	}

	return nil
}

func (c *Cluster) waitForPodLabel(podEvents chan PodEvent, stopCh chan struct{}, role *PostgresRole) (*v1.Pod, error) {
	timeout := time.After(c.OpConfig.PodLabelWaitTimeout)
	for {
		select {
		case podEvent := <-podEvents:
			podRole := PostgresRole(podEvent.CurPod.Labels[c.OpConfig.PodRoleLabel])

			if role == nil {
				if podRole == Master || podRole == Replica {
					return podEvent.CurPod, nil
				}
			} else if *role == podRole {
				return podEvent.CurPod, nil
			}
		case <-timeout:
			return nil, fmt.Errorf("pod label wait timeout")
		case <-stopCh:
			return nil, fmt.Errorf("pod label wait cancelled")
		}
	}
}

func (c *Cluster) waitForPodDeletion(podEvents chan PodEvent) error {
	timeout := time.After(c.OpConfig.PodDeletionWaitTimeout)
	for {
		select {
		case podEvent := <-podEvents:
			if podEvent.EventType == PodEventDelete {
				return nil
			}
		case <-timeout:
			return fmt.Errorf("pod deletion wait timeout")
		}
	}
}

func (c *Cluster) waitStatefulsetReady() error {
	return retryutil.Retry(c.OpConfig.ResourceCheckInterval, c.OpConfig.ResourceCheckTimeout,
		func() (bool, error) {
			listOptions := metav1.ListOptions{
				LabelSelector: c.labelsSet(false).String(),
			}
			ss, err := c.KubeClient.StatefulSets(c.Namespace).List(context.TODO(), listOptions)
			if err != nil {
				return false, err
			}

			if len(ss.Items) != 1 {
				return false, fmt.Errorf("statefulset is not found")
			}

			return *ss.Items[0].Spec.Replicas == ss.Items[0].Status.Replicas, nil
		})
}

func (c *Cluster) _waitPodLabelsReady(anyReplica bool) error {
	var (
		podsNumber int
	)
	ls := c.labelsSet(false)
	namespace := c.Namespace

	listOptions := metav1.ListOptions{
		LabelSelector: ls.String(),
	}
	masterListOption := metav1.ListOptions{
		LabelSelector: labels.Merge(ls, labels.Set{
			c.OpConfig.PodRoleLabel: string(Master),
		}).String(),
	}
	replicaListOption := metav1.ListOptions{
		LabelSelector: labels.Merge(ls, labels.Set{
			c.OpConfig.PodRoleLabel: string(Replica),
		}).String(),
	}
	podsNumber = 1
	if !anyReplica {
		pods, err := c.KubeClient.Pods(namespace).List(context.TODO(), listOptions)
		if err != nil {
			return err
		}
		podsNumber = len(pods.Items)
		c.logger.Debugf("Waiting for %d pods to become ready", podsNumber)
	} else {
		c.logger.Debug("Waiting for any replica pod to become ready")
	}

	err := retryutil.Retry(c.OpConfig.ResourceCheckInterval, c.OpConfig.ResourceCheckTimeout,
		func() (bool, error) {
			masterCount := 0
			if !anyReplica {
				masterPods, err2 := c.KubeClient.Pods(namespace).List(context.TODO(), masterListOption)
				if err2 != nil {
					return false, err2
				}
				if len(masterPods.Items) > 1 {
					return false, fmt.Errorf("too many masters (%d pods with the master label found)",
						len(masterPods.Items))
				}
				masterCount = len(masterPods.Items)
			}
			replicaPods, err2 := c.KubeClient.Pods(namespace).List(context.TODO(), replicaListOption)
			if err2 != nil {
				return false, err2
			}
			replicaCount := len(replicaPods.Items)
			if anyReplica && replicaCount > 0 {
				c.logger.Debugf("Found %d running replica pods", replicaCount)
				return true, nil
			}

			return masterCount+replicaCount >= podsNumber, nil
		})

	return err
}

func (c *Cluster) waitForAllPodsLabelReady() error {
	return c._waitPodLabelsReady(false)
}

func (c *Cluster) waitStatefulsetPodsReady() error {
	c.setProcessName("waiting for the pods of the statefulset")
	// TODO: wait for the first Pod only
	if err := c.waitStatefulsetReady(); err != nil {
		return fmt.Errorf("stateful set error: %v", err)
	}

	// TODO: wait only for master
	if err := c.waitForAllPodsLabelReady(); err != nil {
		return fmt.Errorf("pod labels error: %v", err)
	}

	return nil
}

// Returns labels used to create or list k8s objects such as pods
// For backward compatibility, shouldAddExtraLabels must be false
// when listing k8s objects. See operator PR #252
func (c *Cluster) labelsSet(shouldAddExtraLabels bool) labels.Set {
	lbls := make(map[string]string)
	for k, v := range c.OpConfig.ClusterLabels {
		lbls[k] = v
	}
	lbls[c.OpConfig.ClusterNameLabel] = c.Name

	if shouldAddExtraLabels {
		// enables filtering resources owned by a team
		lbls["team"] = c.Postgresql.Spec.TeamID

		// allow to inherit certain labels from the 'postgres' object
		if spec, err := c.GetSpec(); err == nil {
			for k, v := range spec.ObjectMeta.Labels {
				for _, match := range c.OpConfig.InheritedLabels {
					if k == match {
						lbls[k] = v
					}
				}
			}
		} else {
			c.logger.Warningf("could not get the list of InheritedLabels for cluster %q: %v", c.Name, err)
		}
	}

	return labels.Set(lbls)
}

func (c *Cluster) labelsSelector() *metav1.LabelSelector {
	return &metav1.LabelSelector{
		MatchLabels:      c.labelsSet(false),
		MatchExpressions: nil,
	}
}

func (c *Cluster) roleLabelsSet(shouldAddExtraLabels bool, role PostgresRole) labels.Set {
	lbls := c.labelsSet(shouldAddExtraLabels)
	lbls[c.OpConfig.PodRoleLabel] = string(role)
	return lbls
}

func (c *Cluster) dnsName(role PostgresRole) string {
	var dnsString, oldDnsString string

	if role == Master {
		dnsString = c.masterDNSName(c.Name)
	} else {
		dnsString = c.replicaDNSName(c.Name)
	}

	// if cluster name starts with teamID we might need to provide backwards compatibility
	clusterNameWithoutTeamPrefix, _ := acidv1.ExtractClusterName(c.Name, c.Spec.TeamID)
	if clusterNameWithoutTeamPrefix != "" {
		if role == Master {
			oldDnsString = c.oldMasterDNSName(clusterNameWithoutTeamPrefix)
		} else {
			oldDnsString = c.oldReplicaDNSName(clusterNameWithoutTeamPrefix)
		}
		dnsString = fmt.Sprintf("%s,%s", dnsString, oldDnsString)
	}

	return dnsString
}

func (c *Cluster) masterDNSName(clusterName string) string {
	return strings.ToLower(c.OpConfig.MasterDNSNameFormat.Format(
		"cluster", clusterName,
		"namespace", c.Namespace,
		"team", c.teamName(),
		"hostedzone", c.OpConfig.DbHostedZone))
}

func (c *Cluster) replicaDNSName(clusterName string) string {
	return strings.ToLower(c.OpConfig.ReplicaDNSNameFormat.Format(
		"cluster", clusterName,
		"namespace", c.Namespace,
		"team", c.teamName(),
		"hostedzone", c.OpConfig.DbHostedZone))
}

func (c *Cluster) oldMasterDNSName(clusterName string) string {
	return strings.ToLower(c.OpConfig.MasterLegacyDNSNameFormat.Format(
		"cluster", clusterName,
		"team", c.teamName(),
		"hostedzone", c.OpConfig.DbHostedZone))
}

func (c *Cluster) oldReplicaDNSName(clusterName string) string {
	return strings.ToLower(c.OpConfig.ReplicaLegacyDNSNameFormat.Format(
		"cluster", clusterName,
		"team", c.teamName(),
		"hostedzone", c.OpConfig.DbHostedZone))
}

func (c *Cluster) credentialSecretName(username string) string {
	return c.credentialSecretNameForCluster(username, c.Name)
}

func (c *Cluster) credentialSecretNameForCluster(username string, clusterName string) string {
	// secret  must consist of lower case alphanumeric characters, '-' or '.',
	// and must start and end with an alphanumeric character

	return c.OpConfig.SecretNameTemplate.Format(
		"username", strings.Replace(username, "_", "-", -1),
		"cluster", clusterName,
		"tprkind", acidv1.PostgresCRDResourceKind,
		"tprgroup", acidzalando.GroupName)
}

func cloneSpec(from *acidv1.Postgresql) (*acidv1.Postgresql, error) {
	var (
		buf    bytes.Buffer
		result *acidv1.Postgresql
		err    error
	)
	enc := gob.NewEncoder(&buf)
	if err = enc.Encode(*from); err != nil {
		return nil, fmt.Errorf("could not encode the spec: %v", err)
	}
	dec := gob.NewDecoder(&buf)
	if err = dec.Decode(&result); err != nil {
		return nil, fmt.Errorf("could not decode the spec: %v", err)
	}
	return result, nil
}

func (c *Cluster) setSpec(newSpec *acidv1.Postgresql) {
	c.specMu.Lock()
	c.Postgresql = *newSpec
	c.specMu.Unlock()
}

// GetSpec returns a copy of the operator-side spec of a Postgres cluster in a thread-safe manner
func (c *Cluster) GetSpec() (*acidv1.Postgresql, error) {
	c.specMu.RLock()
	defer c.specMu.RUnlock()
	return cloneSpec(&c.Postgresql)
}

func (c *Cluster) patroniUsesKubernetes() bool {
	return c.OpConfig.EtcdHost == ""
}

func (c *Cluster) patroniKubernetesUseConfigMaps() bool {
	if !c.patroniUsesKubernetes() {
		return false
	}

	// otherwise, follow the operator configuration
	return c.OpConfig.KubernetesUseConfigMaps
}

// Earlier arguments take priority
func mergeContainers(containers ...[]v1.Container) ([]v1.Container, []string) {
	containerNameTaken := map[string]bool{}
	result := make([]v1.Container, 0)
	conflicts := make([]string, 0)

	for _, containerArray := range containers {
		for _, container := range containerArray {
			if _, taken := containerNameTaken[container.Name]; taken {
				conflicts = append(conflicts, container.Name)
			} else {
				containerNameTaken[container.Name] = true
				result = append(result, container)
			}
		}
	}
	return result, conflicts
}

func trimCronjobName(name string) string {
	maxLength := 52
	if len(name) > maxLength {
		name = name[0:maxLength]
		name = strings.TrimRight(name, "-")
	}
	return name
}

func parseResourceRequirements(resourcesRequirement v1.ResourceRequirements) (acidv1.Resources, error) {
	var resources acidv1.Resources
	resourcesJSON, err := json.Marshal(resourcesRequirement)
	if err != nil {
		return acidv1.Resources{}, fmt.Errorf("could not marshal K8s resources requirements")
	}
	if err = json.Unmarshal(resourcesJSON, &resources); err != nil {
		return acidv1.Resources{}, fmt.Errorf("could not unmarshal K8s resources requirements into acidv1.Resources struct")
	}
	return resources, nil
}

func isInMaintenanceWindow(specMaintenanceWindows []acidv1.MaintenanceWindow) bool {
	if len(specMaintenanceWindows) == 0 {
		return true
	}
	now := time.Now()
	currentDay := now.Weekday()
	currentTime := now.Format("15:04")

	for _, window := range specMaintenanceWindows {
		startTime := window.StartTime.Format("15:04")
		endTime := window.EndTime.Format("15:04")

		if window.Everyday || window.Weekday == currentDay {
			if currentTime >= startTime && currentTime <= endTime {
				return true
			}
		}
	}
	return false
}


================================================
File: pkg/cluster/util_test.go
================================================
package cluster

import (
	"bytes"
	"context"
	"fmt"
	"io"
	"maps"
	"net/http"
	"reflect"
	"testing"
	"time"

	"github.com/golang/mock/gomock"
	"github.com/stretchr/testify/assert"
	"github.com/zalando/postgres-operator/mocks"
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	fakeacidv1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/fake"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"github.com/zalando/postgres-operator/pkg/util/patroni"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/types"
	k8sFake "k8s.io/client-go/kubernetes/fake"
)

var externalAnnotations = map[string]string{"existing": "annotation"}

func mustParseTime(s string) metav1.Time {
	v, err := time.Parse("15:04", s)
	if err != nil {
		panic(err)
	}

	return metav1.Time{Time: v.UTC()}
}

func newFakeK8sAnnotationsClient() (k8sutil.KubernetesClient, *k8sFake.Clientset) {
	clientSet := k8sFake.NewSimpleClientset()
	acidClientSet := fakeacidv1.NewSimpleClientset()

	return k8sutil.KubernetesClient{
		PodDisruptionBudgetsGetter:   clientSet.PolicyV1(),
		SecretsGetter:                clientSet.CoreV1(),
		ServicesGetter:               clientSet.CoreV1(),
		StatefulSetsGetter:           clientSet.AppsV1(),
		PostgresqlsGetter:            acidClientSet.AcidV1(),
		PersistentVolumeClaimsGetter: clientSet.CoreV1(),
		PersistentVolumesGetter:      clientSet.CoreV1(),
		EndpointsGetter:              clientSet.CoreV1(),
		ConfigMapsGetter:             clientSet.CoreV1(),
		PodsGetter:                   clientSet.CoreV1(),
		DeploymentsGetter:            clientSet.AppsV1(),
		CronJobsGetter:               clientSet.BatchV1(),
	}, clientSet
}

func clusterLabelsOptions(cluster *Cluster) metav1.ListOptions {
	clusterLabel := labels.Set(map[string]string{cluster.OpConfig.ClusterNameLabel: cluster.Name})
	return metav1.ListOptions{
		LabelSelector: clusterLabel.String(),
	}
}

func checkResourcesInheritedAnnotations(cluster *Cluster, resultAnnotations map[string]string) error {
	clusterOptions := clusterLabelsOptions(cluster)
	// helper functions
	containsAnnotations := func(expected map[string]string, actual map[string]string, objName string, objType string) error {
		if !util.MapContains(actual, expected) {
			return fmt.Errorf("%s %v expected annotations %#v to be contained in %#v", objType, objName, expected, actual)
		}
		return nil
	}

	updateAnnotations := func(annotations map[string]string) map[string]string {
		result := make(map[string]string, 0)
		for anno := range annotations {
			if _, ok := externalAnnotations[anno]; !ok {
				result[anno] = annotations[anno]
			}
		}
		return result
	}

	checkSts := func(annotations map[string]string) error {
		stsList, err := cluster.KubeClient.StatefulSets(namespace).List(context.TODO(), clusterOptions)
		if err != nil {
			return err
		}
		stsAnnotations := updateAnnotations(annotations)

		for _, sts := range stsList.Items {
			if err := containsAnnotations(stsAnnotations, sts.Annotations, sts.ObjectMeta.Name, "StatefulSet"); err != nil {
				return err
			}
			// pod template
			if err := containsAnnotations(stsAnnotations, sts.Spec.Template.Annotations, sts.ObjectMeta.Name, "StatefulSet pod template"); err != nil {
				return err
			}
			// pvc template
			if err := containsAnnotations(stsAnnotations, sts.Spec.VolumeClaimTemplates[0].Annotations, sts.ObjectMeta.Name, "StatefulSet pvc template"); err != nil {
				return err
			}
		}
		return nil
	}

	checkPods := func(annotations map[string]string) error {
		podList, err := cluster.KubeClient.Pods(namespace).List(context.TODO(), clusterOptions)
		if err != nil {
			return err
		}
		for _, pod := range podList.Items {
			if err := containsAnnotations(annotations, pod.Annotations, pod.ObjectMeta.Name, "Pod"); err != nil {
				return err
			}
		}
		return nil
	}

	checkSvc := func(annotations map[string]string) error {
		svcList, err := cluster.KubeClient.Services(namespace).List(context.TODO(), clusterOptions)
		if err != nil {
			return err
		}
		for _, svc := range svcList.Items {
			if err := containsAnnotations(annotations, svc.Annotations, svc.ObjectMeta.Name, "Service"); err != nil {
				return err
			}
		}
		return nil
	}

	checkPdb := func(annotations map[string]string) error {
		pdbList, err := cluster.KubeClient.PodDisruptionBudgets(namespace).List(context.TODO(), clusterOptions)
		if err != nil {
			return err
		}
		for _, pdb := range pdbList.Items {
			if err := containsAnnotations(updateAnnotations(annotations), pdb.Annotations, pdb.ObjectMeta.Name, "Pod Disruption Budget"); err != nil {
				return err
			}
		}
		return nil
	}

	checkPvc := func(annotations map[string]string) error {
		pvcList, err := cluster.KubeClient.PersistentVolumeClaims(namespace).List(context.TODO(), clusterOptions)
		if err != nil {
			return err
		}
		for _, pvc := range pvcList.Items {
			if err := containsAnnotations(annotations, pvc.Annotations, pvc.ObjectMeta.Name, "Volume claim"); err != nil {
				return err
			}
		}
		return nil
	}

	checkPooler := func(annotations map[string]string) error {
		for _, role := range []PostgresRole{Master, Replica} {
			deploy, err := cluster.KubeClient.Deployments(namespace).Get(context.TODO(), cluster.connectionPoolerName(role), metav1.GetOptions{})
			if err != nil {
				return err
			}
			if err := containsAnnotations(annotations, deploy.Annotations, deploy.Name, "Deployment"); err != nil {
				return err
			}
			if err := containsAnnotations(updateAnnotations(annotations), deploy.Spec.Template.Annotations, deploy.Name, "Pooler pod template"); err != nil {
				return err
			}
		}
		return nil
	}

	checkCronJob := func(annotations map[string]string) error {
		cronJobList, err := cluster.KubeClient.CronJobs(namespace).List(context.TODO(), clusterOptions)
		if err != nil {
			return err
		}
		for _, cronJob := range cronJobList.Items {
			if err := containsAnnotations(annotations, cronJob.Annotations, cronJob.ObjectMeta.Name, "Logical backup cron job"); err != nil {
				return err
			}
			if err := containsAnnotations(updateAnnotations(annotations), cronJob.Spec.JobTemplate.Spec.Template.Annotations, cronJob.Name, "Logical backup cron job pod template"); err != nil {
				return err
			}
		}
		return nil
	}

	checkSecrets := func(annotations map[string]string) error {
		secretList, err := cluster.KubeClient.Secrets(namespace).List(context.TODO(), clusterOptions)
		if err != nil {
			return err
		}
		for _, secret := range secretList.Items {
			if err := containsAnnotations(annotations, secret.Annotations, secret.Name, "Secret"); err != nil {
				return err
			}
		}
		return nil
	}

	checkEndpoints := func(annotations map[string]string) error {
		endpointsList, err := cluster.KubeClient.Endpoints(namespace).List(context.TODO(), clusterOptions)
		if err != nil {
			return err
		}
		for _, ep := range endpointsList.Items {
			if err := containsAnnotations(annotations, ep.Annotations, ep.Name, "Endpoints"); err != nil {
				return err
			}
		}
		return nil
	}

	checkConfigMaps := func(annotations map[string]string) error {
		cmList, err := cluster.KubeClient.ConfigMaps(namespace).List(context.TODO(), clusterOptions)
		if err != nil {
			return err
		}
		for _, cm := range cmList.Items {
			if err := containsAnnotations(annotations, cm.Annotations, cm.ObjectMeta.Name, "ConfigMap"); err != nil {
				return err
			}
		}
		return nil
	}

	checkFuncs := []func(map[string]string) error{
		checkSts, checkPods, checkSvc, checkPdb, checkPooler, checkCronJob, checkPvc, checkSecrets, checkEndpoints, checkConfigMaps,
	}
	for _, f := range checkFuncs {
		if err := f(resultAnnotations); err != nil {
			return err
		}
	}
	return nil
}

func createPods(cluster *Cluster) []v1.Pod {
	podsList := make([]v1.Pod, 0)
	for i, role := range []PostgresRole{Master, Replica} {
		podsList = append(podsList, v1.Pod{
			ObjectMeta: metav1.ObjectMeta{
				Name:      fmt.Sprintf("%s-%d", cluster.Name, i),
				Namespace: namespace,
				Labels: map[string]string{
					"application":  "spilo",
					"cluster-name": cluster.Name,
					"spilo-role":   string(role),
				},
			},
		})
		podsList = append(podsList, v1.Pod{
			ObjectMeta: metav1.ObjectMeta{
				Name:      fmt.Sprintf("%s-pooler-%s", cluster.Name, role),
				Namespace: namespace,
				Labels:    cluster.connectionPoolerLabels(role, true).MatchLabels,
			},
		})
	}

	return podsList
}

func newInheritedAnnotationsCluster(client k8sutil.KubernetesClient) (*Cluster, error) {
	pg := acidv1.Postgresql{
		ObjectMeta: metav1.ObjectMeta{
			Name: clusterName,
			Annotations: map[string]string{
				"owned-by": "acid",
				"foo":      "bar", // should not be inherited
			},
		},
		Spec: acidv1.PostgresSpec{
			EnableConnectionPooler:        boolToPointer(true),
			EnableReplicaConnectionPooler: boolToPointer(true),
			EnableLogicalBackup:           true,
			Volume: acidv1.Volume{
				Size: "1Gi",
			},
			NumberOfInstances: 2,
		},
	}

	cluster := New(
		Config{
			OpConfig: config.Config{
				PatroniAPICheckInterval: time.Duration(1),
				PatroniAPICheckTimeout:  time.Duration(5),
				KubernetesUseConfigMaps: true,
				ConnectionPooler: config.ConnectionPooler{
					ConnectionPoolerDefaultCPURequest:    "100m",
					ConnectionPoolerDefaultCPULimit:      "100m",
					ConnectionPoolerDefaultMemoryRequest: "100Mi",
					ConnectionPoolerDefaultMemoryLimit:   "100Mi",
					NumberOfInstances:                    k8sutil.Int32ToPointer(1),
				},
				PDBNameFormat:       "postgres-{cluster}-pdb",
				PodManagementPolicy: "ordered_ready",
				Resources: config.Resources{
					ClusterLabels:         map[string]string{"application": "spilo"},
					ClusterNameLabel:      "cluster-name",
					DefaultCPURequest:     "300m",
					DefaultCPULimit:       "300m",
					DefaultMemoryRequest:  "300Mi",
					DefaultMemoryLimit:    "300Mi",
					InheritedAnnotations:  []string{"owned-by"},
					PodRoleLabel:          "spilo-role",
					ResourceCheckInterval: time.Duration(testResourceCheckInterval),
					ResourceCheckTimeout:  time.Duration(testResourceCheckTimeout),
					MinInstances:          -1,
					MaxInstances:          -1,
				},
			},
		}, client, pg, logger, eventRecorder)
	cluster.Name = clusterName
	cluster.Namespace = namespace
	_, err := cluster.createStatefulSet()
	if err != nil {
		return nil, err
	}
	_, err = cluster.createService(Master)
	if err != nil {
		return nil, err
	}
	err = cluster.createPodDisruptionBudgets()
	if err != nil {
		return nil, err
	}
	_, err = cluster.createConnectionPooler(mockInstallLookupFunction)
	if err != nil {
		return nil, err
	}
	err = cluster.createLogicalBackupJob()
	if err != nil {
		return nil, err
	}
	pvcList := CreatePVCs(namespace, clusterName, cluster.labelsSet(false), 2, "1Gi")
	for _, pvc := range pvcList.Items {
		_, err = cluster.KubeClient.PersistentVolumeClaims(namespace).Create(context.TODO(), &pvc, metav1.CreateOptions{})
		if err != nil {
			return nil, err
		}
	}
	podsList := createPods(cluster)
	for _, pod := range podsList {
		_, err = cluster.KubeClient.Pods(namespace).Create(context.TODO(), &pod, metav1.CreateOptions{})
		if err != nil {
			return nil, err
		}
	}

	// resources which Patroni creates
	if err = createPatroniResources(cluster); err != nil {
		return nil, err
	}

	return cluster, nil
}

func createPatroniResources(cluster *Cluster) error {
	patroniService := cluster.generateService(Replica, &pg.Spec)
	patroniService.ObjectMeta.Name = cluster.serviceName(Patroni)
	_, err := cluster.KubeClient.Services(namespace).Create(context.TODO(), patroniService, metav1.CreateOptions{})
	if err != nil {
		return err
	}

	for _, suffix := range patroniObjectSuffixes {
		metadata := metav1.ObjectMeta{
			Name:      fmt.Sprintf("%s-%s", clusterName, suffix),
			Namespace: namespace,
			Annotations: map[string]string{
				"initialize": "123456789",
			},
			Labels: cluster.labelsSet(false),
		}

		if cluster.OpConfig.KubernetesUseConfigMaps {
			configMap := v1.ConfigMap{
				ObjectMeta: metadata,
			}
			_, err := cluster.KubeClient.ConfigMaps(namespace).Create(context.TODO(), &configMap, metav1.CreateOptions{})
			if err != nil {
				return err
			}
		} else {
			endpoints := v1.Endpoints{
				ObjectMeta: metadata,
			}
			_, err := cluster.KubeClient.Endpoints(namespace).Create(context.TODO(), &endpoints, metav1.CreateOptions{})
			if err != nil {
				return err
			}
		}
	}

	return nil
}

func annotateResources(cluster *Cluster) error {
	clusterOptions := clusterLabelsOptions(cluster)
	patchData, err := metaAnnotationsPatch(externalAnnotations)
	if err != nil {
		return err
	}

	stsList, err := cluster.KubeClient.StatefulSets(namespace).List(context.TODO(), clusterOptions)
	if err != nil {
		return err
	}
	for _, sts := range stsList.Items {
		sts.Annotations = externalAnnotations
		if _, err = cluster.KubeClient.StatefulSets(namespace).Patch(context.TODO(), sts.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{}); err != nil {
			return err
		}
	}

	podList, err := cluster.KubeClient.Pods(namespace).List(context.TODO(), clusterOptions)
	if err != nil {
		return err
	}
	for _, pod := range podList.Items {
		pod.Annotations = externalAnnotations
		if _, err = cluster.KubeClient.Pods(namespace).Patch(context.TODO(), pod.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{}); err != nil {
			return err
		}
	}

	svcList, err := cluster.KubeClient.Services(namespace).List(context.TODO(), clusterOptions)
	if err != nil {
		return err
	}
	for _, svc := range svcList.Items {
		svc.Annotations = externalAnnotations
		if _, err = cluster.KubeClient.Services(namespace).Patch(context.TODO(), svc.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{}); err != nil {
			return err
		}
	}

	pdbList, err := cluster.KubeClient.PodDisruptionBudgets(namespace).List(context.TODO(), clusterOptions)
	if err != nil {
		return err
	}
	for _, pdb := range pdbList.Items {
		pdb.Annotations = externalAnnotations
		_, err = cluster.KubeClient.PodDisruptionBudgets(namespace).Patch(context.TODO(), pdb.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{})
		if err != nil {
			return err
		}
	}

	cronJobList, err := cluster.KubeClient.CronJobs(namespace).List(context.TODO(), clusterOptions)
	if err != nil {
		return err
	}
	for _, cronJob := range cronJobList.Items {
		cronJob.Annotations = externalAnnotations
		_, err = cluster.KubeClient.CronJobs(namespace).Patch(context.TODO(), cronJob.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{})
		if err != nil {
			return err
		}
	}

	pvcList, err := cluster.KubeClient.PersistentVolumeClaims(namespace).List(context.TODO(), clusterOptions)
	if err != nil {
		return err
	}
	for _, pvc := range pvcList.Items {
		pvc.Annotations = externalAnnotations
		if _, err = cluster.KubeClient.PersistentVolumeClaims(namespace).Patch(context.TODO(), pvc.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{}); err != nil {
			return err
		}
	}

	for _, role := range []PostgresRole{Master, Replica} {
		deploy, err := cluster.KubeClient.Deployments(namespace).Get(context.TODO(), cluster.connectionPoolerName(role), metav1.GetOptions{})
		if err != nil {
			return err
		}
		deploy.Annotations = externalAnnotations
		if _, err = cluster.KubeClient.Deployments(namespace).Patch(context.TODO(), deploy.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{}); err != nil {
			return err
		}
	}

	secrets, err := cluster.KubeClient.Secrets(namespace).List(context.TODO(), clusterOptions)
	if err != nil {
		return err
	}
	for _, secret := range secrets.Items {
		secret.Annotations = externalAnnotations
		if _, err = cluster.KubeClient.Secrets(namespace).Patch(context.TODO(), secret.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{}); err != nil {
			return err
		}
	}

	endpoints, err := cluster.KubeClient.Endpoints(namespace).List(context.TODO(), clusterOptions)
	if err != nil {
		return err
	}
	for _, ep := range endpoints.Items {
		ep.Annotations = externalAnnotations
		if _, err = cluster.KubeClient.Endpoints(namespace).Patch(context.TODO(), ep.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{}); err != nil {
			return err
		}
	}

	configMaps, err := cluster.KubeClient.ConfigMaps(namespace).List(context.TODO(), clusterOptions)
	if err != nil {
		return err
	}
	for _, cm := range configMaps.Items {
		cm.Annotations = externalAnnotations
		if _, err = cluster.KubeClient.ConfigMaps(namespace).Patch(context.TODO(), cm.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{}); err != nil {
			return err
		}
	}

	return nil
}

func TestInheritedAnnotations(t *testing.T) {
	// mocks
	ctrl := gomock.NewController(t)
	defer ctrl.Finish()
	client, _ := newFakeK8sAnnotationsClient()
	mockClient := mocks.NewMockHTTPClient(ctrl)

	cluster, err := newInheritedAnnotationsCluster(client)
	assert.NoError(t, err)

	configJson := `{"postgresql": {"parameters": {"log_min_duration_statement": 200, "max_connections": 50}}}, "ttl": 20}`
	response := http.Response{
		StatusCode: 200,
		Body:       io.NopCloser(bytes.NewReader([]byte(configJson))),
	}
	mockClient.EXPECT().Do(gomock.Any()).Return(&response, nil).AnyTimes()
	cluster.patroni = patroni.New(patroniLogger, mockClient)

	err = cluster.Sync(&cluster.Postgresql)
	assert.NoError(t, err)

	filterLabels := cluster.labelsSet(false)

	// Finally, tests!
	result := map[string]string{"owned-by": "acid"}
	assert.True(t, reflect.DeepEqual(result, cluster.annotationsSet(nil)))

	// 1. Check initial state
	err = checkResourcesInheritedAnnotations(cluster, result)
	assert.NoError(t, err)

	// 2. Check annotation value change

	// 2.1 Sync event
	newSpec := cluster.Postgresql.DeepCopy()
	newSpec.Annotations["owned-by"] = "fooSync"
	result["owned-by"] = "fooSync"

	err = cluster.Sync(newSpec)
	assert.NoError(t, err)
	err = checkResourcesInheritedAnnotations(cluster, result)
	assert.NoError(t, err)

	// + existing PVC without annotations
	cluster.KubeClient.PersistentVolumeClaims(namespace).Create(context.TODO(), &CreatePVCs(namespace, clusterName, filterLabels, 3, "1Gi").Items[2], metav1.CreateOptions{})
	err = cluster.Sync(newSpec)
	assert.NoError(t, err)
	err = checkResourcesInheritedAnnotations(cluster, result)
	assert.NoError(t, err)

	// 2.2 Update event
	newSpec = cluster.Postgresql.DeepCopy()
	newSpec.Annotations["owned-by"] = "fooUpdate"
	result["owned-by"] = "fooUpdate"
	// + new PVC
	cluster.KubeClient.PersistentVolumeClaims(namespace).Create(context.TODO(), &CreatePVCs(namespace, clusterName, filterLabels, 4, "1Gi").Items[3], metav1.CreateOptions{})

	err = cluster.Update(cluster.Postgresql.DeepCopy(), newSpec)
	assert.NoError(t, err)

	err = checkResourcesInheritedAnnotations(cluster, result)
	assert.NoError(t, err)

	// 3. Change from ConfigMaps to Endpoints
	err = cluster.deletePatroniResources()
	assert.NoError(t, err)
	cluster.OpConfig.KubernetesUseConfigMaps = false
	err = createPatroniResources(cluster)
	assert.NoError(t, err)
	err = cluster.Sync(newSpec.DeepCopy())
	assert.NoError(t, err)
	err = checkResourcesInheritedAnnotations(cluster, result)
	assert.NoError(t, err)

	// 4. Existing annotations (should not be removed)
	err = annotateResources(cluster)
	assert.NoError(t, err)
	maps.Copy(result, externalAnnotations)
	err = cluster.Sync(newSpec.DeepCopy())
	assert.NoError(t, err)
	err = checkResourcesInheritedAnnotations(cluster, result)
	assert.NoError(t, err)
}

func Test_trimCronjobName(t *testing.T) {
	type args struct {
		name string
	}
	tests := []struct {
		name string
		args args
		want string
	}{
		{
			name: "short name",
			args: args{
				name: "short-name",
			},
			want: "short-name",
		},
		{
			name: "long name",
			args: args{
				name: "very-very-very-very-very-very-very-very-very-long-db-name",
			},
			want: "very-very-very-very-very-very-very-very-very-long-db",
		},
		{
			name: "long name should not end with dash",
			args: args{
				name: "very-very-very-very-very-very-very-very-very-----------long-db-name",
			},
			want: "very-very-very-very-very-very-very-very-very",
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := trimCronjobName(tt.args.name); got != tt.want {
				t.Errorf("trimCronjobName() = %v, want %v", got, tt.want)
			}
		})
	}
}

func TestisInMaintenanceWindow(t *testing.T) {
	now := time.Now()
	futureTimeStart := now.Add(1 * time.Hour)
	futureTimeStartFormatted := futureTimeStart.Format("15:04")
	futureTimeEnd := now.Add(2 * time.Hour)
	futureTimeEndFormatted := futureTimeEnd.Format("15:04")

	tests := []struct {
		name     string
		windows  []acidv1.MaintenanceWindow
		expected bool
	}{
		{
			name:     "no maintenance windows",
			windows:  nil,
			expected: true,
		},
		{
			name: "maintenance windows with everyday",
			windows: []acidv1.MaintenanceWindow{
				{
					Everyday:  true,
					StartTime: mustParseTime("00:00"),
					EndTime:   mustParseTime("23:59"),
				},
			},
			expected: true,
		},
		{
			name: "maintenance windows with weekday",
			windows: []acidv1.MaintenanceWindow{
				{
					Weekday:   now.Weekday(),
					StartTime: mustParseTime("00:00"),
					EndTime:   mustParseTime("23:59"),
				},
			},
			expected: true,
		},
		{
			name: "maintenance windows with future interval time",
			windows: []acidv1.MaintenanceWindow{
				{
					Weekday:   now.Weekday(),
					StartTime: mustParseTime(futureTimeStartFormatted),
					EndTime:   mustParseTime(futureTimeEndFormatted),
				},
			},
			expected: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			cluster.Spec.MaintenanceWindows = tt.windows
			if isInMaintenanceWindow(cluster.Spec.MaintenanceWindows) != tt.expected {
				t.Errorf("Expected isInMaintenanceWindow to return %t", tt.expected)
			}
		})
	}
}


================================================
File: pkg/cluster/volumes.go
================================================
package cluster

import (
	"context"
	"fmt"
	"strconv"
	"strings"

	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"

	"github.com/aws/aws-sdk-go/aws"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/filesystems"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"github.com/zalando/postgres-operator/pkg/util/volumes"
)

func (c *Cluster) syncVolumes() error {
	c.logger.Debugf("syncing volumes using %q storage resize mode", c.OpConfig.StorageResizeMode)
	var err error

	// check quantity string once, and do not bother with it anymore anywhere else
	_, err = resource.ParseQuantity(c.Spec.Volume.Size)
	if err != nil {
		return fmt.Errorf("could not parse volume size from the manifest: %v", err)
	}

	if c.OpConfig.StorageResizeMode == "mixed" {
		// mixed op uses AWS API to adjust size, throughput, iops, and calls pvc change for file system resize
		// in case of errors we proceed to let K8s do its work, favoring disk space increase of other adjustments

		err = c.populateVolumeMetaData()
		if err != nil {
			c.logger.Errorf("populating EBS meta data failed, skipping potential adjustments: %v", err)
		} else {
			err = c.syncUnderlyingEBSVolume()
			if err != nil {
				c.logger.Errorf("errors occured during EBS volume adjustments: %v", err)
			}
		}
	}

	if err = c.syncVolumeClaims(); err != nil {
		err = fmt.Errorf("could not sync persistent volume claims: %v", err)
		return err
	}

	if c.OpConfig.StorageResizeMode == "ebs" {
		// potentially enlarge volumes before changing the statefulset. By doing that
		// in this order we make sure the operator is not stuck waiting for a pod that
		// cannot start because it ran out of disk space.
		// TODO: handle the case of the cluster that is downsized and enlarged again
		// (there will be a volume from the old pod for which we can't act before the
		//  the statefulset modification is concluded)
		if err = c.syncEbsVolumes(); err != nil {
			err = fmt.Errorf("could not sync persistent volumes: %v", err)
			return err
		}
	}

	return nil
}

func (c *Cluster) syncUnderlyingEBSVolume() error {
	c.logger.Debug("starting to sync EBS volumes: type, iops, throughput, and size")

	var (
		err     error
		newSize resource.Quantity
	)

	targetValue := c.Spec.Volume
	if newSize, err = resource.ParseQuantity(targetValue.Size); err != nil {
		return fmt.Errorf("could not parse volume size: %v", err)
	}
	targetSize := quantityToGigabyte(newSize)

	awsGp3 := aws.String("gp3")
	awsIo2 := aws.String("io2")

	errors := make([]string, 0)

	for _, volume := range c.EBSVolumes {
		var modifyIops *int64
		var modifyThroughput *int64
		var modifySize *int64
		var modifyType *string

		if targetValue.Iops != nil && *targetValue.Iops >= int64(3000) {
			if volume.Iops != *targetValue.Iops {
				modifyIops = targetValue.Iops
			}
		}

		if targetValue.Throughput != nil && *targetValue.Throughput >= int64(125) {
			if volume.Throughput != *targetValue.Throughput {
				modifyThroughput = targetValue.Throughput
			}
		}

		if targetSize > volume.Size {
			modifySize = &targetSize
		}

		if modifyIops != nil || modifyThroughput != nil || modifySize != nil {
			if modifyIops != nil || modifyThroughput != nil {
				// we default to gp3 if iops and throughput are configured
				modifyType = awsGp3
				if targetValue.VolumeType == "io2" {
					modifyType = awsIo2
				}
			} else if targetValue.VolumeType == "gp3" && volume.VolumeType != "gp3" {
				modifyType = awsGp3
			} else {
				// do not touch type
				modifyType = nil
			}

			err = c.VolumeResizer.ModifyVolume(volume.VolumeID, modifyType, modifySize, modifyIops, modifyThroughput)
			if err != nil {
				errors = append(errors, fmt.Sprintf("modify failed: %v, showing current EBS values: volume-id=%s size=%d iops=%d throughput=%d", err, volume.VolumeID, volume.Size, volume.Iops, volume.Throughput))
			}
		}
	}

	if len(errors) > 0 {
		for _, s := range errors {
			c.logger.Warningf(s)
		}
	}
	return nil
}

func (c *Cluster) populateVolumeMetaData() error {
	c.logger.Debug("starting reading ebs meta data")

	pvs, err := c.listPersistentVolumes()
	if err != nil {
		return fmt.Errorf("could not list persistent volumes: %v", err)
	}
	if len(pvs) == 0 {
		c.EBSVolumes = make(map[string]volumes.VolumeProperties)
		return fmt.Errorf("no persistent volumes found")
	}
	c.logger.Debugf("found %d persistent volumes, size of known volumes %d", len(pvs), len(c.EBSVolumes))

	volumeIds := []string{}
	var volumeID string
	for _, pv := range pvs {
		volumeID, err = c.VolumeResizer.GetProviderVolumeID(pv)
		if err != nil {
			continue
		}

		volumeIds = append(volumeIds, volumeID)
	}

	currentVolumes, err := c.VolumeResizer.DescribeVolumes(volumeIds)
	if nil != err {
		return err
	}

	if len(currentVolumes) != len(c.EBSVolumes) && len(c.EBSVolumes) > 0 {
		c.logger.Infof("number of ebs volumes (%d) discovered differs from already known volumes (%d)", len(currentVolumes), len(c.EBSVolumes))
	}

	// reset map, operator is not responsible for dangling ebs volumes
	c.EBSVolumes = make(map[string]volumes.VolumeProperties)
	for _, volume := range currentVolumes {
		c.EBSVolumes[volume.VolumeID] = volume
	}

	return nil
}

// syncVolumeClaims reads all persistent volume claims and checks that their size matches the one declared in the statefulset.
func (c *Cluster) syncVolumeClaims() error {
	c.setProcessName("syncing volume claims")

	ignoreResize := false

	if c.OpConfig.StorageResizeMode == "off" || c.OpConfig.StorageResizeMode == "ebs" {
		ignoreResize = true
		c.logger.Debugf("Storage resize mode is set to %q. Skipping volume size sync of persistent volume claims.", c.OpConfig.StorageResizeMode)
	}

	newSize, err := resource.ParseQuantity(c.Spec.Volume.Size)
	if err != nil {
		return fmt.Errorf("could not parse volume size from the manifest: %v", err)
	}
	manifestSize := quantityToGigabyte(newSize)

	pvcs, err := c.listPersistentVolumeClaims()
	if err != nil {
		return fmt.Errorf("could not list persistent volume claims: %v", err)
	}
	for _, pvc := range pvcs {
		c.VolumeClaims[pvc.UID] = &pvc
		needsUpdate := false
		currentSize := quantityToGigabyte(pvc.Spec.Resources.Requests[v1.ResourceStorage])
		if !ignoreResize && currentSize != manifestSize {
			if currentSize < manifestSize {
				pvc.Spec.Resources.Requests[v1.ResourceStorage] = newSize
				needsUpdate = true
				c.logger.Infof("persistent volume claim for volume %q needs to be resized", pvc.Name)
			} else {
				c.logger.Warningf("cannot shrink persistent volume")
			}
		}

		if needsUpdate {
			c.logger.Infof("updating persistent volume claim definition for volume %q", pvc.Name)
			updatedPvc, err := c.KubeClient.PersistentVolumeClaims(pvc.Namespace).Update(context.TODO(), &pvc, metav1.UpdateOptions{})
			if err != nil {
				return fmt.Errorf("could not update persistent volume claim: %q", err)
			}
			c.VolumeClaims[pvc.UID] = updatedPvc
			c.logger.Infof("successfully updated persistent volume claim %q", pvc.Name)
		} else {
			c.logger.Debugf("volume claim for volume %q do not require updates", pvc.Name)
		}

		newAnnotations := c.annotationsSet(nil)
		if changed, _ := c.compareAnnotations(pvc.Annotations, newAnnotations, nil); changed {
			patchData, err := metaAnnotationsPatch(newAnnotations)
			if err != nil {
				return fmt.Errorf("could not form patch for the persistent volume claim for volume %q: %v", pvc.Name, err)
			}
			patchedPvc, err := c.KubeClient.PersistentVolumeClaims(pvc.Namespace).Patch(context.TODO(), pvc.Name, types.MergePatchType, []byte(patchData), metav1.PatchOptions{})
			if err != nil {
				return fmt.Errorf("could not patch annotations of the persistent volume claim for volume %q: %v", pvc.Name, err)
			}
			c.VolumeClaims[pvc.UID] = patchedPvc
		}
	}

	c.logger.Debug("volume claims have been synced successfully")

	return nil
}

// syncVolumes reads all persistent volumes and checks that their size matches the one declared in the statefulset.
func (c *Cluster) syncEbsVolumes() error {
	c.setProcessName("syncing EBS volumes")

	act, err := c.volumesNeedResizing()
	if err != nil {
		return fmt.Errorf("could not compare size of the volumes: %v", err)
	}
	if !act {
		return nil
	}

	if err := c.resizeVolumes(); err != nil {
		return fmt.Errorf("could not sync volumes: %v", err)
	}

	c.logger.Debug("volumes have been synced successfully")

	return nil
}

func (c *Cluster) listPersistentVolumeClaims() ([]v1.PersistentVolumeClaim, error) {
	ns := c.Namespace
	listOptions := metav1.ListOptions{
		LabelSelector: c.labelsSet(false).String(),
	}

	pvcs, err := c.KubeClient.PersistentVolumeClaims(ns).List(context.TODO(), listOptions)
	if err != nil {
		return nil, fmt.Errorf("could not list of persistent volume claims: %v", err)
	}
	return pvcs.Items, nil
}

func (c *Cluster) deletePersistentVolumeClaims() error {
	c.setProcessName("deleting persistent volume claims")
	errors := make([]string, 0)
	for uid := range c.VolumeClaims {
		err := c.deletePersistentVolumeClaim(uid)
		if err != nil {
			errors = append(errors, fmt.Sprintf("%v", err))
		}
	}

	if len(errors) > 0 {
		c.logger.Warningf("could not delete all persistent volume claims: %v", strings.Join(errors, `', '`))
	}

	return nil
}

func (c *Cluster) deletePersistentVolumeClaim(uid types.UID) error {
	c.setProcessName("deleting persistent volume claim")
	pvc := c.VolumeClaims[uid]
	c.logger.Debugf("deleting persistent volume claim %q", pvc.Name)
	err := c.KubeClient.PersistentVolumeClaims(pvc.Namespace).Delete(context.TODO(), pvc.Name, c.deleteOptions)
	if k8sutil.ResourceNotFound(err) {
		c.logger.Debugf("persistent volume claim %q has already been deleted", pvc.Name)
	} else if err != nil {
		return fmt.Errorf("could not delete persistent volume claim %q: %v", pvc.Name, err)
	}
	c.logger.Infof("persistent volume claim %q has been deleted", pvc.Name)
	delete(c.VolumeClaims, uid)

	return nil
}

func (c *Cluster) listPersistentVolumes() ([]*v1.PersistentVolume, error) {
	result := make([]*v1.PersistentVolume, 0)

	pvcs, err := c.listPersistentVolumeClaims()
	if err != nil {
		return nil, fmt.Errorf("could not list cluster's persistent volume claims: %v", err)
	}

	pods, err := c.listPods()
	if err != nil {
		return nil, fmt.Errorf("could not get list of running pods for resizing persistent volumes: %v", err)
	}

	lastPodIndex := len(pods) - 1

	for _, pvc := range pvcs {
		lastDash := strings.LastIndex(pvc.Name, "-")
		if lastDash > 0 && lastDash < len(pvc.Name)-1 {
			pvcNumber, err := strconv.Atoi(pvc.Name[lastDash+1:])
			if err != nil {
				return nil, fmt.Errorf("could not convert last part of the persistent volume claim name %q to a number", pvc.Name)
			}
			if pvcNumber > lastPodIndex {
				c.logger.Debugf("skipping persistent volume %q corresponding to a non-running pods", pvc.Name)
				continue
			}
		}
		pv, err := c.KubeClient.PersistentVolumes().Get(context.TODO(), pvc.Spec.VolumeName, metav1.GetOptions{})
		if err != nil {
			return nil, fmt.Errorf("could not get PersistentVolume: %v", err)
		}
		result = append(result, pv)
	}

	return result, nil
}

// resizeVolumes resize persistent volumes compatible with the given resizer interface
func (c *Cluster) resizeVolumes() error {
	if c.VolumeResizer == nil {
		return fmt.Errorf("no volume resizer set for EBS volume handling")
	}

	c.setProcessName("resizing EBS volumes")

	newQuantity, err := resource.ParseQuantity(c.Spec.Volume.Size)
	if err != nil {
		return fmt.Errorf("could not parse volume size: %v", err)
	}

	newSize := quantityToGigabyte(newQuantity)
	resizer := c.VolumeResizer
	var totalIncompatible int

	pvs, err := c.listPersistentVolumes()
	if err != nil {
		return fmt.Errorf("could not list persistent volumes: %v", err)
	}

	for _, pv := range pvs {
		volumeSize := quantityToGigabyte(pv.Spec.Capacity[v1.ResourceStorage])
		if volumeSize >= newSize {
			if volumeSize > newSize {
				c.logger.Warningf("cannot shrink persistent volume")
			}
			continue
		}
		compatible := false

		if !resizer.VolumeBelongsToProvider(pv) {
			continue
		}
		compatible = true
		if !resizer.IsConnectedToProvider() {
			err := resizer.ConnectToProvider()
			if err != nil {
				return fmt.Errorf("could not connect to the volume provider: %v", err)
			}
			defer func() {
				if err := resizer.DisconnectFromProvider(); err != nil {
					c.logger.Errorf("%v", err)
				}
			}()
		}
		awsVolumeID, err := resizer.GetProviderVolumeID(pv)
		if err != nil {
			return err
		}
		c.logger.Infof("updating persistent volume %q to %d", pv.Name, newSize)
		if err := resizer.ResizeVolume(awsVolumeID, newSize); err != nil {
			return fmt.Errorf("could not resize EBS volume %q: %v", awsVolumeID, err)
		}
		c.logger.Infof("resizing the filesystem on the volume %q", pv.Name)
		podName := getPodNameFromPersistentVolume(pv)
		if err := c.resizePostgresFilesystem(podName, []filesystems.FilesystemResizer{&filesystems.Ext234Resize{}}); err != nil {
			return fmt.Errorf("could not resize the filesystem on pod %q: %v", podName, err)
		}
		c.logger.Infof("filesystem resize successful on volume %q", pv.Name)
		pv.Spec.Capacity[v1.ResourceStorage] = newQuantity
		c.logger.Infof("updating persistent volume definition for volume %q", pv.Name)
		if _, err := c.KubeClient.PersistentVolumes().Update(context.TODO(), pv, metav1.UpdateOptions{}); err != nil {
			return fmt.Errorf("could not update persistent volume: %q", err)
		}
		c.logger.Infof("successfully updated persistent volume %q", pv.Name)

		if !compatible {
			c.logger.Warningf("volume %q is incompatible with all available resizing providers, consider switching storage_resize_mode to pvc or off", pv.Name)
			totalIncompatible++
		}
	}
	if totalIncompatible > 0 {
		return fmt.Errorf("could not resize EBS volumes: some persistent volumes are not compatible with existing resizing providers")
	}
	return nil
}

func (c *Cluster) volumesNeedResizing() (bool, error) {
	newQuantity, _ := resource.ParseQuantity(c.Spec.Volume.Size)
	newSize := quantityToGigabyte(newQuantity)

	vols, err := c.listPersistentVolumes()
	if err != nil {
		return false, err
	}
	for _, pv := range vols {
		currentSize := quantityToGigabyte(pv.Spec.Capacity[v1.ResourceStorage])
		if currentSize != newSize {
			return true, nil
		}
	}
	return false, nil
}

// getPodNameFromPersistentVolume returns a pod name that it extracts from the volume claim ref.
func getPodNameFromPersistentVolume(pv *v1.PersistentVolume) *spec.NamespacedName {
	namespace := pv.Spec.ClaimRef.Namespace
	name := pv.Spec.ClaimRef.Name[len(constants.DataVolumeName)+1:]
	return &spec.NamespacedName{Namespace: namespace, Name: name}
}

func quantityToGigabyte(q resource.Quantity) int64 {
	return q.ScaledValue(0) / (1 * constants.Gigabyte)
}

func (c *Cluster) executeEBSMigration() error {
	pvs, err := c.listPersistentVolumes()
	if err != nil {
		return fmt.Errorf("could not list persistent volumes: %v", err)
	}
	if len(pvs) == 0 {
		c.logger.Warningf("no persistent volumes found - skipping EBS migration")
		return nil
	}
	c.logger.Debugf("found %d volumes, size of known volumes %d", len(pvs), len(c.EBSVolumes))

	if len(pvs) == len(c.EBSVolumes) {
		hasGp2 := false
		for _, v := range c.EBSVolumes {
			if v.VolumeType == "gp2" {
				hasGp2 = true
			}
		}

		if !hasGp2 {
			c.logger.Debugf("no EBS gp2 volumes left to migrate")
			return nil
		}
	}

	var i3000 int64 = 3000
	var i125 int64 = 125

	for _, volume := range c.EBSVolumes {
		if volume.VolumeType == "gp2" && volume.Size < c.OpConfig.EnableEBSGp3MigrationMaxSize {
			c.logger.Infof("modifying EBS volume %s to type gp3 migration (%d)", volume.VolumeID, volume.Size)
			err = c.VolumeResizer.ModifyVolume(volume.VolumeID, aws.String("gp3"), &volume.Size, &i3000, &i125)
			if nil != err {
				c.logger.Warningf("modifying volume %s failed: %v", volume.VolumeID, err)
			}
		} else {
			c.logger.Debugf("skipping EBS volume %s to type gp3 migration (%d)", volume.VolumeID, volume.Size)
		}
		c.EBSVolumes[volume.VolumeID] = volume
	}

	return nil
}


================================================
File: pkg/cluster/volumes_test.go
================================================
package cluster

import (
	"fmt"
	"testing"

	"context"

	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"

	"github.com/aws/aws-sdk-go/aws"
	"github.com/golang/mock/gomock"

	"github.com/stretchr/testify/assert"
	"github.com/zalando/postgres-operator/mocks"
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"github.com/zalando/postgres-operator/pkg/util/volumes"
	"k8s.io/client-go/kubernetes/fake"
)

type testVolume struct {
	size        int64
	iops        int64
	throughtput int64
	volType     string
}

var testVol = testVolume{
	size:        100,
	iops:        300,
	throughtput: 125,
	volType:     "gp2",
}

func newFakeK8sPVCclient() (k8sutil.KubernetesClient, *fake.Clientset) {
	clientSet := fake.NewSimpleClientset()

	return k8sutil.KubernetesClient{
		PersistentVolumeClaimsGetter: clientSet.CoreV1(),
		PersistentVolumesGetter:      clientSet.CoreV1(),
		PodsGetter:                   clientSet.CoreV1(),
	}, clientSet
}

func TestResizeVolumeClaim(t *testing.T) {
	testName := "test resizing of persistent volume claims"
	client, _ := newFakeK8sPVCclient()
	clusterName := "acid-test-cluster"
	namespace := "default"
	newVolumeSize := "2Gi"

	storage1Gi, err := resource.ParseQuantity("1Gi")
	assert.NoError(t, err)

	// new cluster with pvc storage resize mode and configured labels
	var cluster = New(
		Config{
			OpConfig: config.Config{
				Resources: config.Resources{
					ClusterLabels:    map[string]string{"application": "spilo"},
					ClusterNameLabel: "cluster-name",
				},
				StorageResizeMode: "pvc",
			},
		}, client, acidv1.Postgresql{}, logger, eventRecorder)

	// set metadata, so that labels will get correct values
	cluster.Name = clusterName
	cluster.Namespace = namespace
	filterLabels := cluster.labelsSet(false)
	cluster.Spec.Volume.Size = newVolumeSize

	// define and create PVCs for 1Gi volumes
	pvcList := CreatePVCs(namespace, clusterName, filterLabels, 2, "1Gi")
	// add another PVC with different cluster name
	pvcList.Items = append(pvcList.Items, CreatePVCs(namespace, clusterName+"-2", labels.Set{}, 1, "1Gi").Items[0])

	for _, pvc := range pvcList.Items {
		cluster.KubeClient.PersistentVolumeClaims(namespace).Create(context.TODO(), &pvc, metav1.CreateOptions{})
	}

	// test resizing
	cluster.syncVolumes()

	pvcs, err := cluster.listPersistentVolumeClaims()
	assert.NoError(t, err)

	// check if listPersistentVolumeClaims returns only the PVCs matching the filter
	if len(pvcs) != len(pvcList.Items)-1 {
		t.Errorf("%s: could not find all persistent volume claims, got %v, expected %v", testName, len(pvcs), len(pvcList.Items)-1)
	}

	// check if PVCs were correctly resized
	for _, pvc := range pvcs {
		newStorageSize := quantityToGigabyte(pvc.Spec.Resources.Requests[v1.ResourceStorage])
		expectedQuantity, err := resource.ParseQuantity(newVolumeSize)
		assert.NoError(t, err)
		expectedSize := quantityToGigabyte(expectedQuantity)
		if newStorageSize != expectedSize {
			t.Errorf("%s: resizing failed, got %v, expected %v", testName, newStorageSize, expectedSize)
		}
	}

	// check if other PVC was not resized
	pvc2, err := cluster.KubeClient.PersistentVolumeClaims(namespace).Get(context.TODO(), constants.DataVolumeName+"-"+clusterName+"-2-0", metav1.GetOptions{})
	assert.NoError(t, err)
	unchangedSize := quantityToGigabyte(pvc2.Spec.Resources.Requests[v1.ResourceStorage])
	expectedSize := quantityToGigabyte(storage1Gi)
	if unchangedSize != expectedSize {
		t.Errorf("%s: volume size changed, got %v, expected %v", testName, unchangedSize, expectedSize)
	}
}

func TestQuantityToGigabyte(t *testing.T) {
	tests := []struct {
		name        string
		quantityStr string
		expected    int64
	}{
		{
			"test with 1Gi",
			"1Gi",
			1,
		},
		{
			"test with float",
			"1.5Gi",
			int64(1),
		},
		{
			"test with 1000Mi",
			"1000Mi",
			int64(0),
		},
	}

	for _, tt := range tests {
		quantity, err := resource.ParseQuantity(tt.quantityStr)
		assert.NoError(t, err)
		gigabyte := quantityToGigabyte(quantity)
		if gigabyte != tt.expected {
			t.Errorf("%s: got %v, expected %v", tt.name, gigabyte, tt.expected)
		}
	}
}

func CreatePVCs(namespace string, clusterName string, labels labels.Set, n int, size string) v1.PersistentVolumeClaimList {
	// define and create PVCs for 1Gi volumes
	storage1Gi, _ := resource.ParseQuantity(size)
	pvcList := v1.PersistentVolumeClaimList{
		Items: []v1.PersistentVolumeClaim{},
	}

	for i := 0; i < n; i++ {
		pvc := v1.PersistentVolumeClaim{
			ObjectMeta: metav1.ObjectMeta{
				Name:      fmt.Sprintf("%s-%s-%d", constants.DataVolumeName, clusterName, i),
				Namespace: namespace,
				Labels:    labels,
			},
			Spec: v1.PersistentVolumeClaimSpec{
				Resources: v1.VolumeResourceRequirements{
					Requests: v1.ResourceList{
						v1.ResourceStorage: storage1Gi,
					},
				},
				VolumeName: fmt.Sprintf("persistent-volume-%d", i),
			},
		}
		pvcList.Items = append(pvcList.Items, pvc)
	}

	return pvcList
}

func TestMigrateEBS(t *testing.T) {
	client, _ := newFakeK8sPVCclient()
	clusterName := "acid-test-cluster"
	namespace := "default"

	// new cluster with pvc storage resize mode and configured labels
	var cluster = New(
		Config{
			OpConfig: config.Config{
				Resources: config.Resources{
					ClusterLabels:    map[string]string{"application": "spilo"},
					ClusterNameLabel: "cluster-name",
				},
				StorageResizeMode:            "pvc",
				EnableEBSGp3Migration:        true,
				EnableEBSGp3MigrationMaxSize: 1000,
			},
		}, client, acidv1.Postgresql{}, logger, eventRecorder)
	cluster.Spec.Volume.Size = "1Gi"

	// set metadata, so that labels will get correct values
	cluster.Name = clusterName
	cluster.Namespace = namespace
	filterLabels := cluster.labelsSet(false)

	testVolumes := []testVolume{testVol, testVol}

	initTestVolumesAndPods(cluster.KubeClient, namespace, clusterName, filterLabels, testVolumes)

	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	resizer := mocks.NewMockVolumeResizer(ctrl)

	resizer.EXPECT().ExtractVolumeID(gomock.Eq("aws://eu-central-1b/ebs-volume-1")).Return("ebs-volume-1", nil)
	resizer.EXPECT().ExtractVolumeID(gomock.Eq("aws://eu-central-1b/ebs-volume-2")).Return("ebs-volume-2", nil)

	resizer.EXPECT().GetProviderVolumeID(gomock.Any()).
		DoAndReturn(func(pv *v1.PersistentVolume) (string, error) {
			return resizer.ExtractVolumeID(pv.Spec.AWSElasticBlockStore.VolumeID)
		}).
		Times(2)

	resizer.EXPECT().DescribeVolumes(gomock.Eq([]string{"ebs-volume-1", "ebs-volume-2"})).Return(
		[]volumes.VolumeProperties{
			{VolumeID: "ebs-volume-1", VolumeType: "gp2", Size: 100},
			{VolumeID: "ebs-volume-2", VolumeType: "gp3", Size: 100}}, nil)

	// expect only gp2 volume to be modified
	resizer.EXPECT().ModifyVolume(gomock.Eq("ebs-volume-1"), gomock.Eq(aws.String("gp3")), gomock.Any(), gomock.Any(), gomock.Any()).Return(nil)

	cluster.VolumeResizer = resizer
	err := cluster.populateVolumeMetaData()
	assert.NoError(t, err)
	err = cluster.executeEBSMigration()
	assert.NoError(t, err)
}

func initTestVolumesAndPods(client k8sutil.KubernetesClient, namespace, clustername string, labels labels.Set, volumes []testVolume) {
	i := 0
	for _, v := range volumes {
		storage1Gi, _ := resource.ParseQuantity(fmt.Sprintf("%d", v.size))

		ps := v1.PersistentVolumeSpec{}
		ps.AWSElasticBlockStore = &v1.AWSElasticBlockStoreVolumeSource{}
		ps.AWSElasticBlockStore.VolumeID = fmt.Sprintf("aws://eu-central-1b/ebs-volume-%d", i+1)

		pv := v1.PersistentVolume{
			ObjectMeta: metav1.ObjectMeta{
				Name: fmt.Sprintf("persistent-volume-%d", i),
			},
			Spec: ps,
		}

		client.PersistentVolumes().Create(context.TODO(), &pv, metav1.CreateOptions{})

		pvc := v1.PersistentVolumeClaim{
			ObjectMeta: metav1.ObjectMeta{
				Name:      fmt.Sprintf("%s-%s-%d", constants.DataVolumeName, clustername, i),
				Namespace: namespace,
				Labels:    labels,
			},
			Spec: v1.PersistentVolumeClaimSpec{
				Resources: v1.VolumeResourceRequirements{
					Requests: v1.ResourceList{
						v1.ResourceStorage: storage1Gi,
					},
				},
				VolumeName: fmt.Sprintf("persistent-volume-%d", i),
			},
		}

		client.PersistentVolumeClaims(namespace).Create(context.TODO(), &pvc, metav1.CreateOptions{})

		pod := v1.Pod{
			ObjectMeta: metav1.ObjectMeta{
				Name:   fmt.Sprintf("%s-%d", clustername, i),
				Labels: labels,
			},
			Spec: v1.PodSpec{},
		}

		client.Pods(namespace).Create(context.TODO(), &pod, metav1.CreateOptions{})

		i = i + 1
	}
}

func TestMigrateGp3Support(t *testing.T) {
	client, _ := newFakeK8sPVCclient()
	clusterName := "acid-test-cluster"
	namespace := "default"

	// new cluster with pvc storage resize mode and configured labels
	var cluster = New(
		Config{
			OpConfig: config.Config{
				Resources: config.Resources{
					ClusterLabels:    map[string]string{"application": "spilo"},
					ClusterNameLabel: "cluster-name",
				},
				StorageResizeMode:            "mixed",
				EnableEBSGp3Migration:        false,
				EnableEBSGp3MigrationMaxSize: 1000,
			},
		}, client, acidv1.Postgresql{}, logger, eventRecorder)

	cluster.Spec.Volume.Size = "150Gi"
	cluster.Spec.Volume.Iops = aws.Int64(6000)
	cluster.Spec.Volume.Throughput = aws.Int64(275)

	// set metadata, so that labels will get correct values
	cluster.Name = clusterName
	cluster.Namespace = namespace
	filterLabels := cluster.labelsSet(false)

	testVolumes := []testVolume{testVol, testVol, testVol}

	initTestVolumesAndPods(cluster.KubeClient, namespace, clusterName, filterLabels, testVolumes)

	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	resizer := mocks.NewMockVolumeResizer(ctrl)

	resizer.EXPECT().ExtractVolumeID(gomock.Eq("aws://eu-central-1b/ebs-volume-1")).Return("ebs-volume-1", nil)
	resizer.EXPECT().ExtractVolumeID(gomock.Eq("aws://eu-central-1b/ebs-volume-2")).Return("ebs-volume-2", nil)
	resizer.EXPECT().ExtractVolumeID(gomock.Eq("aws://eu-central-1b/ebs-volume-3")).Return("ebs-volume-3", nil)

	resizer.EXPECT().GetProviderVolumeID(gomock.Any()).
		DoAndReturn(func(pv *v1.PersistentVolume) (string, error) {
			return resizer.ExtractVolumeID(pv.Spec.AWSElasticBlockStore.VolumeID)
		}).
		Times(3)

	resizer.EXPECT().DescribeVolumes(gomock.Eq([]string{"ebs-volume-1", "ebs-volume-2", "ebs-volume-3"})).Return(
		[]volumes.VolumeProperties{
			{VolumeID: "ebs-volume-1", VolumeType: "gp3", Size: 100, Iops: 3000},
			{VolumeID: "ebs-volume-2", VolumeType: "gp3", Size: 105, Iops: 4000},
			{VolumeID: "ebs-volume-3", VolumeType: "gp3", Size: 151, Iops: 6000, Throughput: 275}}, nil)

	// expect only gp2 volume to be modified
	resizer.EXPECT().ModifyVolume(gomock.Eq("ebs-volume-1"), gomock.Eq(aws.String("gp3")), gomock.Eq(aws.Int64(150)), gomock.Eq(aws.Int64(6000)), gomock.Eq(aws.Int64(275))).Return(nil)
	resizer.EXPECT().ModifyVolume(gomock.Eq("ebs-volume-2"), gomock.Eq(aws.String("gp3")), gomock.Eq(aws.Int64(150)), gomock.Eq(aws.Int64(6000)), gomock.Eq(aws.Int64(275))).Return(nil)
	// resizer.EXPECT().ModifyVolume(gomock.Eq("ebs-volume-3"), gomock.Eq(aws.String("gp3")), gomock.Any(), gomock.Any(), gomock.Any()).Return(nil)

	cluster.VolumeResizer = resizer
	cluster.syncVolumes()
}

func TestManualGp2Gp3Support(t *testing.T) {
	client, _ := newFakeK8sPVCclient()
	clusterName := "acid-test-cluster"
	namespace := "default"

	// new cluster with pvc storage resize mode and configured labels
	var cluster = New(
		Config{
			OpConfig: config.Config{
				Resources: config.Resources{
					ClusterLabels:    map[string]string{"application": "spilo"},
					ClusterNameLabel: "cluster-name",
				},
				StorageResizeMode:            "mixed",
				EnableEBSGp3Migration:        false,
				EnableEBSGp3MigrationMaxSize: 1000,
			},
		}, client, acidv1.Postgresql{}, logger, eventRecorder)

	cluster.Spec.Volume.Size = "150Gi"
	cluster.Spec.Volume.Iops = aws.Int64(6000)
	cluster.Spec.Volume.Throughput = aws.Int64(275)

	// set metadata, so that labels will get correct values
	cluster.Name = clusterName
	cluster.Namespace = namespace
	filterLabels := cluster.labelsSet(false)

	testVolumes := []testVolume{testVol, testVol}

	initTestVolumesAndPods(cluster.KubeClient, namespace, clusterName, filterLabels, testVolumes)

	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	resizer := mocks.NewMockVolumeResizer(ctrl)

	resizer.EXPECT().ExtractVolumeID(gomock.Eq("aws://eu-central-1b/ebs-volume-1")).Return("ebs-volume-1", nil)
	resizer.EXPECT().ExtractVolumeID(gomock.Eq("aws://eu-central-1b/ebs-volume-2")).Return("ebs-volume-2", nil)

	resizer.EXPECT().GetProviderVolumeID(gomock.Any()).
		DoAndReturn(func(pv *v1.PersistentVolume) (string, error) {
			return resizer.ExtractVolumeID(pv.Spec.AWSElasticBlockStore.VolumeID)
		}).
		Times(2)

	resizer.EXPECT().DescribeVolumes(gomock.Eq([]string{"ebs-volume-1", "ebs-volume-2"})).Return(
		[]volumes.VolumeProperties{
			{VolumeID: "ebs-volume-1", VolumeType: "gp2", Size: 150, Iops: 3000},
			{VolumeID: "ebs-volume-2", VolumeType: "gp2", Size: 150, Iops: 4000},
		}, nil)

	// expect only gp2 volume to be modified
	resizer.EXPECT().ModifyVolume(gomock.Eq("ebs-volume-1"), gomock.Eq(aws.String("gp3")), gomock.Nil(), gomock.Eq(aws.Int64(6000)), gomock.Eq(aws.Int64(275))).Return(nil)
	resizer.EXPECT().ModifyVolume(gomock.Eq("ebs-volume-2"), gomock.Eq(aws.String("gp3")), gomock.Nil(), gomock.Eq(aws.Int64(6000)), gomock.Eq(aws.Int64(275))).Return(nil)

	cluster.VolumeResizer = resizer
	cluster.syncVolumes()
}

func TestDontTouchType(t *testing.T) {
	client, _ := newFakeK8sPVCclient()
	clusterName := "acid-test-cluster"
	namespace := "default"

	// new cluster with pvc storage resize mode and configured labels
	var cluster = New(
		Config{
			OpConfig: config.Config{
				Resources: config.Resources{
					ClusterLabels:    map[string]string{"application": "spilo"},
					ClusterNameLabel: "cluster-name",
				},
				StorageResizeMode:            "mixed",
				EnableEBSGp3Migration:        false,
				EnableEBSGp3MigrationMaxSize: 1000,
			},
		}, client, acidv1.Postgresql{}, logger, eventRecorder)

	cluster.Spec.Volume.Size = "177Gi"

	// set metadata, so that labels will get correct values
	cluster.Name = clusterName
	cluster.Namespace = namespace
	filterLabels := cluster.labelsSet(false)

	testVolumes := []testVolume{
		{
			size: 150,
		},
		{
			size: 150,
		},
	}

	initTestVolumesAndPods(cluster.KubeClient, namespace, clusterName, filterLabels, testVolumes)

	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	resizer := mocks.NewMockVolumeResizer(ctrl)

	resizer.EXPECT().ExtractVolumeID(gomock.Eq("aws://eu-central-1b/ebs-volume-1")).Return("ebs-volume-1", nil)
	resizer.EXPECT().ExtractVolumeID(gomock.Eq("aws://eu-central-1b/ebs-volume-2")).Return("ebs-volume-2", nil)

	resizer.EXPECT().GetProviderVolumeID(gomock.Any()).
		DoAndReturn(func(pv *v1.PersistentVolume) (string, error) {
			return resizer.ExtractVolumeID(pv.Spec.AWSElasticBlockStore.VolumeID)
		}).
		Times(2)

	resizer.EXPECT().DescribeVolumes(gomock.Eq([]string{"ebs-volume-1", "ebs-volume-2"})).Return(
		[]volumes.VolumeProperties{
			{VolumeID: "ebs-volume-1", VolumeType: "gp2", Size: 150, Iops: 3000},
			{VolumeID: "ebs-volume-2", VolumeType: "gp2", Size: 150, Iops: 4000},
		}, nil)

	// expect only gp2 volume to be modified
	resizer.EXPECT().ModifyVolume(gomock.Eq("ebs-volume-1"), gomock.Nil(), gomock.Eq(aws.Int64(177)), gomock.Nil(), gomock.Nil()).Return(nil)
	resizer.EXPECT().ModifyVolume(gomock.Eq("ebs-volume-2"), gomock.Nil(), gomock.Eq(aws.Int64(177)), gomock.Nil(), gomock.Nil()).Return(nil)

	cluster.VolumeResizer = resizer
	cluster.syncVolumes()
}


================================================
File: pkg/controller/controller.go
================================================
package controller

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"os"
	"strings"
	"sync"
	"time"

	"github.com/sirupsen/logrus"
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/apiserver"
	"github.com/zalando/postgres-operator/pkg/cluster"
	acidv1informer "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/teams"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"github.com/zalando/postgres-operator/pkg/util/ringlog"
	v1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/kubernetes/scheme"
	typedcorev1 "k8s.io/client-go/kubernetes/typed/core/v1"
	"k8s.io/client-go/tools/cache"
	"k8s.io/client-go/tools/record"
	"k8s.io/client-go/tools/reference"
)

// Controller represents operator controller
type Controller struct {
	config    spec.ControllerConfig
	opConfig  *config.Config
	pgTeamMap teams.PostgresTeamMap

	logger     *logrus.Entry
	KubeClient k8sutil.KubernetesClient
	apiserver  *apiserver.Server

	eventRecorder    record.EventRecorder
	eventBroadcaster record.EventBroadcaster

	stopCh chan struct{}

	controllerID     string
	curWorkerID      uint32 //initialized with 0
	curWorkerCluster sync.Map
	clusterWorkers   map[spec.NamespacedName]uint32
	clustersMu       sync.RWMutex
	clusters         map[spec.NamespacedName]*cluster.Cluster
	clusterLogs      map[spec.NamespacedName]ringlog.RingLogger
	clusterHistory   map[spec.NamespacedName]ringlog.RingLogger // history of the cluster changes
	teamClusters     map[string][]spec.NamespacedName

	postgresqlInformer   cache.SharedIndexInformer
	postgresTeamInformer cache.SharedIndexInformer
	podInformer          cache.SharedIndexInformer
	nodesInformer        cache.SharedIndexInformer
	podCh                chan cluster.PodEvent

	clusterEventQueues    []*cache.FIFO // [workerID]Queue
	lastClusterSyncTime   int64
	lastClusterRepairTime int64

	workerLogs map[uint32]ringlog.RingLogger

	PodServiceAccount            *v1.ServiceAccount
	PodServiceAccountRoleBinding *rbacv1.RoleBinding
}

// NewController creates a new controller
func NewController(controllerConfig *spec.ControllerConfig, controllerId string) *Controller {
	logger := logrus.New()
	if controllerConfig.EnableJsonLogging {
		logger.SetFormatter(&logrus.JSONFormatter{})
	} else {
		if os.Getenv("LOG_NOQUOTE") != "" {
			logger.SetFormatter(&logrus.TextFormatter{PadLevelText: true, DisableQuote: true})
		}
	}

	var myComponentName = "postgres-operator"
	if controllerId != "" {
		myComponentName += "/" + controllerId
	}

	eventBroadcaster := record.NewBroadcaster()

	// disabling the sending of events also to the logoutput
	// the operator currently duplicates a lot of log entries with this setup
	// eventBroadcaster.StartLogging(logger.Infof)
	scheme := scheme.Scheme
	acidv1.AddToScheme(scheme)
	recorder := eventBroadcaster.NewRecorder(scheme, v1.EventSource{Component: myComponentName})

	c := &Controller{
		config:           *controllerConfig,
		opConfig:         &config.Config{},
		logger:           logger.WithField("pkg", "controller"),
		eventRecorder:    recorder,
		eventBroadcaster: eventBroadcaster,
		controllerID:     controllerId,
		curWorkerCluster: sync.Map{},
		clusterWorkers:   make(map[spec.NamespacedName]uint32),
		clusters:         make(map[spec.NamespacedName]*cluster.Cluster),
		clusterLogs:      make(map[spec.NamespacedName]ringlog.RingLogger),
		clusterHistory:   make(map[spec.NamespacedName]ringlog.RingLogger),
		teamClusters:     make(map[string][]spec.NamespacedName),
		stopCh:           make(chan struct{}),
		podCh:            make(chan cluster.PodEvent),
	}
	logger.Hooks.Add(c)

	return c
}

func (c *Controller) initClients() {
	var err error

	c.KubeClient, err = k8sutil.NewFromConfig(c.config.RestConfig)
	if err != nil {
		c.logger.Fatalf("could not create kubernetes clients: %v", err)
	}
	c.eventBroadcaster.StartRecordingToSink(&typedcorev1.EventSinkImpl{Interface: c.KubeClient.EventsGetter.Events("")})
	if err != nil {
		c.logger.Fatalf("could not setup kubernetes event sink: %v", err)
	}

}

func (c *Controller) initOperatorConfig() {
	configMapData := make(map[string]string)

	if c.config.ConfigMapName != (spec.NamespacedName{}) {
		configMap, err := c.KubeClient.ConfigMaps(c.config.ConfigMapName.Namespace).
			Get(context.TODO(), c.config.ConfigMapName.Name, metav1.GetOptions{})
		if err != nil {
			panic(err)
		}

		configMapData = configMap.Data
	} else {
		c.logger.Infoln("no ConfigMap specified. Loading default values")
	}

	c.opConfig = config.NewFromMap(configMapData)
	c.warnOnDeprecatedOperatorParameters()

	if c.opConfig.SetMemoryRequestToLimit {

		isSmaller, err := util.IsSmallerQuantity(c.opConfig.DefaultMemoryRequest, c.opConfig.DefaultMemoryLimit)
		if err != nil {
			panic(err)
		}
		if isSmaller {
			c.logger.Warningf("The default memory request of %v for Postgres containers is increased to match the default memory limit of %v.", c.opConfig.DefaultMemoryRequest, c.opConfig.DefaultMemoryLimit)
			c.opConfig.DefaultMemoryRequest = c.opConfig.DefaultMemoryLimit
		}

		isSmaller, err = util.IsSmallerQuantity(c.opConfig.ScalyrMemoryRequest, c.opConfig.ScalyrMemoryLimit)
		if err != nil {
			panic(err)
		}
		if isSmaller {
			c.logger.Warningf("The memory request of %v for the Scalyr sidecar container is increased to match the memory limit of %v.", c.opConfig.ScalyrMemoryRequest, c.opConfig.ScalyrMemoryLimit)
			c.opConfig.ScalyrMemoryRequest = c.opConfig.ScalyrMemoryLimit
		}

		// generateStatefulSet adjusts values for individual Postgres clusters
	}

}

func (c *Controller) modifyConfigFromEnvironment() {
	c.opConfig.WatchedNamespace = c.getEffectiveNamespace(os.Getenv("WATCHED_NAMESPACE"), c.opConfig.WatchedNamespace)

	if c.config.NoDatabaseAccess {
		c.opConfig.EnableDBAccess = false
	}
	if c.config.NoTeamsAPI {
		c.opConfig.EnableTeamsAPI = false
	}
	scalyrAPIKey := os.Getenv("SCALYR_API_KEY")
	if scalyrAPIKey != "" {
		c.opConfig.ScalyrAPIKey = scalyrAPIKey
	}
}

// warningOnDeprecatedParameters emits warnings upon finding deprecated parmaters
func (c *Controller) warnOnDeprecatedOperatorParameters() {
	if c.opConfig.EnableLoadBalancer != nil {
		c.logger.Warningf("Operator configuration parameter 'enable_load_balancer' is deprecated and takes no effect. " +
			"Consider using the 'enable_master_load_balancer' or 'enable_replica_load_balancer' instead.")
	}

	if len(c.opConfig.SidecarImages) > 0 {
		c.logger.Warningf("Operator configuration parameter 'sidecar_docker_images' is deprecated. " +
			"Consider using 'sidecars' instead.")
	}
}

func compactValue(v string) string {
	var compact bytes.Buffer
	if err := json.Compact(&compact, []byte(v)); err != nil {
		panic("Hard coded json strings broken!")
	}
	return compact.String()
}

func (c *Controller) initPodServiceAccount() {

	if c.opConfig.PodServiceAccountDefinition == "" {
		stringValue := `
		{
			"apiVersion": "v1",
			"kind": "ServiceAccount",
			"metadata": {
				"name": "postgres-pod"
			}
		}`

		c.opConfig.PodServiceAccountDefinition = compactValue(stringValue)

	}

	// re-uses k8s internal parsing. See k8s client-go issue #193 for explanation
	decode := scheme.Codecs.UniversalDeserializer().Decode
	obj, groupVersionKind, err := decode([]byte(c.opConfig.PodServiceAccountDefinition), nil, nil)

	switch {
	case err != nil:
		panic(fmt.Errorf("Unable to parse pod service account definition from the operator configuration: %v", err))
	case groupVersionKind.Kind != "ServiceAccount":
		panic(fmt.Errorf("pod service account definition in the operator configuration defines another type of resource: %v", groupVersionKind.Kind))
	default:
		c.PodServiceAccount = obj.(*v1.ServiceAccount)
		if c.PodServiceAccount.Name != c.opConfig.PodServiceAccountName {
			c.logger.Warnf("in the operator configuration, the pod service account name %v does not match the name %v given in the account definition; using the former for consistency", c.opConfig.PodServiceAccountName, c.PodServiceAccount.Name)
			c.PodServiceAccount.Name = c.opConfig.PodServiceAccountName
		}
		c.PodServiceAccount.Namespace = ""
	}

	// actual service accounts are deployed at the time of Postgres/Spilo cluster creation
}

func (c *Controller) initRoleBinding() {

	// service account on its own lacks any rights starting with k8s v1.8
	// operator binds it to the cluster role with sufficient privileges
	// we assume the role is created by the k8s administrator
	if c.opConfig.PodServiceAccountRoleBindingDefinition == "" {
		stringValue := fmt.Sprintf(`
		{
			"apiVersion": "rbac.authorization.k8s.io/v1",
			"kind": "RoleBinding",
			"metadata": {
				   "name": "%s"
			},
			"roleRef": {
				"apiGroup": "rbac.authorization.k8s.io",
				"kind": "ClusterRole",
				"name": "%s"
			},
			"subjects": [
				{
					"kind": "ServiceAccount",
					"name": "%s"
				}
			]
		}`, c.PodServiceAccount.Name, c.PodServiceAccount.Name, c.PodServiceAccount.Name)
		c.opConfig.PodServiceAccountRoleBindingDefinition = compactValue(stringValue)
	}
	c.logger.Info("Parse role bindings")
	// re-uses k8s internal parsing. See k8s client-go issue #193 for explanation
	decode := scheme.Codecs.UniversalDeserializer().Decode
	obj, groupVersionKind, err := decode([]byte(c.opConfig.PodServiceAccountRoleBindingDefinition), nil, nil)

	switch {
	case err != nil:
		panic(fmt.Errorf("unable to parse the role binding definition from the operator configuration: %v", err))
	case groupVersionKind.Kind != "RoleBinding":
		panic(fmt.Errorf("role binding definition in the operator configuration defines another type of resource: %v", groupVersionKind.Kind))
	default:
		c.PodServiceAccountRoleBinding = obj.(*rbacv1.RoleBinding)
		c.PodServiceAccountRoleBinding.Namespace = ""
		c.logger.Info("successfully parsed")

	}

	// actual roles bindings ar*logrus.Entrye deployed at the time of Postgres/Spilo cluster creation
}

func logMultiLineConfig(log *logrus.Entry, config string) {
	lines := strings.Split(config, "\n")
	for _, l := range lines {
		log.Infof("%s", l)
	}
}

func (c *Controller) initController() {
	c.initClients()
	c.controllerID = os.Getenv("CONTROLLER_ID")

	if configObjectName := os.Getenv("POSTGRES_OPERATOR_CONFIGURATION_OBJECT"); configObjectName != "" {
		if c.opConfig.EnableCRDRegistration != nil && *c.opConfig.EnableCRDRegistration {
			if err := c.createConfigurationCRD(); err != nil {
				c.logger.Fatalf("could not register Operator Configuration CustomResourceDefinition: %v", err)
			}
		}
		if cfg, err := c.readOperatorConfigurationFromCRD(spec.GetOperatorNamespace(), configObjectName); err != nil {
			c.logger.Fatalf("unable to read operator configuration: %v", err)
		} else {
			c.opConfig = c.importConfigurationFromCRD(&cfg.Configuration)
		}
	} else {
		c.initOperatorConfig()
	}
	c.initPodServiceAccount()
	c.initRoleBinding()

	c.modifyConfigFromEnvironment()

	if c.opConfig.EnableCRDRegistration != nil && *c.opConfig.EnableCRDRegistration {
		if err := c.createPostgresCRD(); err != nil {
			c.logger.Fatalf("could not register Postgres CustomResourceDefinition: %v", err)
		}
	}

	c.initSharedInformers()

	c.pgTeamMap = teams.PostgresTeamMap{}
	if c.opConfig.EnablePostgresTeamCRD {
		c.loadPostgresTeams()
	}

	if c.opConfig.DebugLogging {
		c.logger.Logger.Level = logrus.DebugLevel
	}

	logMultiLineConfig(c.logger, c.opConfig.MustMarshal())

	roleDefs := c.getInfrastructureRoleDefinitions()
	if infraRoles, err := c.getInfrastructureRoles(roleDefs); err != nil {
		c.logger.Warningf("could not get infrastructure roles: %v", err)
	} else {
		c.config.InfrastructureRoles = infraRoles
	}

	c.clusterEventQueues = make([]*cache.FIFO, c.opConfig.Workers)
	c.workerLogs = make(map[uint32]ringlog.RingLogger, c.opConfig.Workers)
	for i := range c.clusterEventQueues {
		c.clusterEventQueues[i] = cache.NewFIFO(func(obj interface{}) (string, error) {
			e, ok := obj.(ClusterEvent)
			if !ok {
				return "", fmt.Errorf("could not cast to ClusterEvent")
			}

			return queueClusterKey(e.EventType, e.UID), nil
		})
	}

	c.apiserver = apiserver.New(c, c.opConfig.APIPort, c.logger.Logger)
}

func (c *Controller) initSharedInformers() {

	// Postgresqls
	c.postgresqlInformer = acidv1informer.NewPostgresqlInformer(
		c.KubeClient.AcidV1ClientSet,
		c.opConfig.WatchedNamespace,
		constants.QueueResyncPeriodTPR,
		cache.Indexers{})

	c.postgresqlInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    c.postgresqlAdd,
		UpdateFunc: c.postgresqlUpdate,
		DeleteFunc: c.postgresqlDelete,
	})

	// PostgresTeams
	if c.opConfig.EnablePostgresTeamCRD {
		c.postgresTeamInformer = acidv1informer.NewPostgresTeamInformer(
			c.KubeClient.AcidV1ClientSet,
			c.opConfig.WatchedNamespace,
			constants.QueueResyncPeriodTPR*6, // 30 min
			cache.Indexers{})

		c.postgresTeamInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
			AddFunc:    c.postgresTeamAdd,
			UpdateFunc: c.postgresTeamUpdate,
		})
	}

	// Pods
	podLw := &cache.ListWatch{
		ListFunc:  c.podListFunc,
		WatchFunc: c.podWatchFunc,
	}

	c.podInformer = cache.NewSharedIndexInformer(
		podLw,
		&v1.Pod{},
		constants.QueueResyncPeriodPod,
		cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})

	c.podInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    c.podAdd,
		UpdateFunc: c.podUpdate,
		DeleteFunc: c.podDelete,
	})

	// Kubernetes Nodes
	nodeLw := &cache.ListWatch{
		ListFunc:  c.nodeListFunc,
		WatchFunc: c.nodeWatchFunc,
	}

	c.nodesInformer = cache.NewSharedIndexInformer(
		nodeLw,
		&v1.Node{},
		constants.QueueResyncPeriodNode,
		cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})

	c.nodesInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    c.nodeAdd,
		UpdateFunc: c.nodeUpdate,
		DeleteFunc: c.nodeDelete,
	})
}

// Run starts background controller processes
func (c *Controller) Run(stopCh <-chan struct{}, wg *sync.WaitGroup) {
	c.initController()

	// start workers reading from the events queue to prevent the initial sync from blocking on it.
	for i := range c.clusterEventQueues {
		wg.Add(1)
		c.workerLogs[uint32(i)] = ringlog.New(c.opConfig.RingLogLines)
		go c.processClusterEventsQueue(i, stopCh, wg)
	}

	// populate clusters before starting nodeInformer that relies on it and run the initial sync
	if err := c.acquireInitialListOfClusters(); err != nil {
		panic("could not acquire initial list of clusters")
	}

	wg.Add(5 + util.Bool2Int(c.opConfig.EnablePostgresTeamCRD))
	go c.runPodInformer(stopCh, wg)
	go c.runPostgresqlInformer(stopCh, wg)
	go c.clusterResync(stopCh, wg)
	go c.apiserver.Run(stopCh, wg)
	go c.kubeNodesInformer(stopCh, wg)

	if c.opConfig.EnablePostgresTeamCRD {
		go c.runPostgresTeamInformer(stopCh, wg)
	}

	c.logger.Info("started working in background")
}

func (c *Controller) runPodInformer(stopCh <-chan struct{}, wg *sync.WaitGroup) {
	defer wg.Done()

	c.podInformer.Run(stopCh)
}

func (c *Controller) runPostgresqlInformer(stopCh <-chan struct{}, wg *sync.WaitGroup) {
	defer wg.Done()

	c.postgresqlInformer.Run(stopCh)
}

func (c *Controller) runPostgresTeamInformer(stopCh <-chan struct{}, wg *sync.WaitGroup) {
	defer wg.Done()

	c.postgresTeamInformer.Run(stopCh)
}

func queueClusterKey(eventType EventType, uid types.UID) string {
	return fmt.Sprintf("%s-%s", eventType, uid)
}

func (c *Controller) kubeNodesInformer(stopCh <-chan struct{}, wg *sync.WaitGroup) {
	defer wg.Done()

	c.nodesInformer.Run(stopCh)
}

func (c *Controller) getEffectiveNamespace(namespaceFromEnvironment, namespaceFromConfigMap string) string {

	namespace := util.Coalesce(namespaceFromEnvironment, util.Coalesce(namespaceFromConfigMap, spec.GetOperatorNamespace()))

	if namespace == "*" {

		namespace = v1.NamespaceAll
		c.logger.Infof("Listening to all namespaces")

	} else {

		if _, err := c.KubeClient.Namespaces().Get(context.TODO(), namespace, metav1.GetOptions{}); err != nil {
			c.logger.Fatalf("Could not find the watched namespace %q", namespace)
		} else {
			c.logger.Infof("Listenting to the specific namespace %q", namespace)
		}

	}

	return namespace
}

// GetReference of Postgres CR object
// i.e. required to emit events to this resource
func (c *Controller) GetReference(postgresql *acidv1.Postgresql) *v1.ObjectReference {
	ref, err := reference.GetReference(scheme.Scheme, postgresql)
	if err != nil {
		c.logger.Errorf("could not get reference for Postgresql CR %v/%v: %v", postgresql.Namespace, postgresql.Name, err)
	}
	return ref
}

func (c *Controller) meetsClusterDeleteAnnotations(postgresql *acidv1.Postgresql) error {

	deleteAnnotationDateKey := c.opConfig.DeleteAnnotationDateKey
	currentTime := time.Now()
	currentDate := currentTime.Format("2006-01-02") // go's reference date

	if deleteAnnotationDateKey != "" {
		if deleteDate, ok := postgresql.Annotations[deleteAnnotationDateKey]; ok {
			if deleteDate != currentDate {
				return fmt.Errorf("annotation %s not matching the current date: got %s, expected %s", deleteAnnotationDateKey, deleteDate, currentDate)
			}
		} else {
			return fmt.Errorf("annotation %s not set in manifest to allow cluster deletion", deleteAnnotationDateKey)
		}
	}

	deleteAnnotationNameKey := c.opConfig.DeleteAnnotationNameKey

	if deleteAnnotationNameKey != "" {
		if clusterName, ok := postgresql.Annotations[deleteAnnotationNameKey]; ok {
			if clusterName != postgresql.Name {
				return fmt.Errorf("annotation %s not matching the cluster name: got %s, expected %s", deleteAnnotationNameKey, clusterName, postgresql.Name)
			}
		} else {
			return fmt.Errorf("annotation %s not set in manifest to allow cluster deletion", deleteAnnotationNameKey)
		}
	}

	return nil
}

// hasOwnership returns true if the controller is the "owner" of the postgresql.
// Whether it's owner is determined by the value of 'acid.zalan.do/controller'
// annotation. If the value matches the controllerID then it owns it, or if the
// controllerID is "" and there's no annotation set.
func (c *Controller) hasOwnership(postgresql *acidv1.Postgresql) bool {
	if postgresql.Annotations != nil {
		if owner, ok := postgresql.Annotations[constants.PostgresqlControllerAnnotationKey]; ok {
			return owner == c.controllerID
		}
	}
	return c.controllerID == ""
}


================================================
File: pkg/controller/logs_and_api.go
================================================
package controller

import (
	"fmt"
	"sort"
	"sync/atomic"

	"github.com/sirupsen/logrus"

	"github.com/zalando/postgres-operator/pkg/cluster"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"k8s.io/apimachinery/pkg/types"
)

// ClusterStatus provides status of the cluster
func (c *Controller) ClusterStatus(namespace, cluster string) (*cluster.ClusterStatus, error) {

	clusterName := spec.NamespacedName{
		Namespace: namespace,
		Name:      cluster,
	}

	c.clustersMu.RLock()
	cl, ok := c.clusters[clusterName]
	c.clustersMu.RUnlock()
	if !ok {
		return nil, fmt.Errorf("could not find cluster")
	}

	status := cl.GetStatus()
	status.Worker = c.clusterWorkerID(clusterName)

	return status, nil
}

// ClusterDatabasesMap returns for each cluster the list of databases running there
func (c *Controller) ClusterDatabasesMap() map[string][]string {

	m := make(map[string][]string)

	// avoid modifying the cluster list while we are fetching each one of them.
	c.clustersMu.RLock()
	defer c.clustersMu.RUnlock()
	for _, cluster := range c.clusters {
		// GetSpec holds the specMu lock of a cluster
		if spec, err := cluster.GetSpec(); err == nil {
			for database := range spec.Spec.Databases {
				m[cluster.Name] = append(m[cluster.Name], database)
			}
			sort.Strings(m[cluster.Name])
		} else {
			c.logger.Warningf("could not get the list of databases for cluster %q: %v", cluster.Name, err)
		}
	}

	return m
}

// TeamClusterList returns team-clusters map
func (c *Controller) TeamClusterList() map[string][]spec.NamespacedName {
	return c.teamClusters
}

// GetConfig returns controller config
func (c *Controller) GetConfig() *spec.ControllerConfig {
	return &c.config
}

// GetOperatorConfig returns operator config
func (c *Controller) GetOperatorConfig() *config.Config {
	return c.opConfig
}

// GetStatus dumps current config and status of the controller
func (c *Controller) GetStatus() *spec.ControllerStatus {
	c.clustersMu.RLock()
	clustersCnt := len(c.clusters)
	c.clustersMu.RUnlock()

	queueSizes := make(map[int]int, c.opConfig.Workers)
	for workerID, queue := range c.clusterEventQueues {
		queueSizes[workerID] = len(queue.ListKeys())
	}

	return &spec.ControllerStatus{
		LastSyncTime:    atomic.LoadInt64(&c.lastClusterSyncTime),
		Clusters:        clustersCnt,
		WorkerQueueSize: queueSizes,
	}
}

// ClusterLogs dumps cluster ring logs
func (c *Controller) ClusterLogs(namespace, name string) ([]*spec.LogEntry, error) {

	clusterName := spec.NamespacedName{
		Namespace: namespace,
		Name:      name,
	}

	c.clustersMu.RLock()
	cl, ok := c.clusterLogs[clusterName]
	c.clustersMu.RUnlock()
	if !ok {
		return nil, fmt.Errorf("could not find cluster")
	}

	res := make([]*spec.LogEntry, 0)
	for _, e := range cl.Walk() {
		logEntry := e.(*spec.LogEntry)
		logEntry.ClusterName = nil

		res = append(res, logEntry)
	}

	return res, nil
}

// WorkerLogs dumps logs of the worker
func (c *Controller) WorkerLogs(workerID uint32) ([]*spec.LogEntry, error) {
	lg, ok := c.workerLogs[workerID]
	if !ok {
		return nil, fmt.Errorf("could not find worker")
	}

	res := make([]*spec.LogEntry, 0)
	for _, e := range lg.Walk() {
		logEntry := e.(*spec.LogEntry)
		logEntry.Worker = nil

		res = append(res, logEntry)
	}

	return res, nil
}

// Levels returns logrus levels for which hook must fire
func (c *Controller) Levels() []logrus.Level {
	return logrus.AllLevels
}

// Fire is a logrus hook
func (c *Controller) Fire(e *logrus.Entry) error {
	var clusterName spec.NamespacedName

	v, ok := e.Data["cluster-name"]
	if !ok {
		return nil
	}
	clusterName = v.(spec.NamespacedName)
	c.clustersMu.RLock()
	clusterRingLog, ok := c.clusterLogs[clusterName]
	c.clustersMu.RUnlock()
	if !ok {
		return nil
	}

	logEntry := &spec.LogEntry{
		Time:        e.Time,
		Level:       e.Level,
		ClusterName: &clusterName,
		Message:     e.Message,
	}

	if v, hasWorker := e.Data["worker"]; hasWorker {
		id := v.(uint32)

		logEntry.Worker = &id
	}
	clusterRingLog.Insert(logEntry)

	if logEntry.Worker == nil {
		return nil
	}
	c.workerLogs[*logEntry.Worker].Insert(logEntry) // workerLogs map is immutable. No need to lock it

	return nil
}

// ListQueue dumps cluster event queue of the provided worker
func (c *Controller) ListQueue(workerID uint32) (*spec.QueueDump, error) {
	if workerID >= uint32(len(c.clusterEventQueues)) {
		return nil, fmt.Errorf("could not find worker")
	}

	q := c.clusterEventQueues[workerID]
	return &spec.QueueDump{
		Keys: q.ListKeys(),
		List: q.List(),
	}, nil
}

// GetWorkersCnt returns number of the workers
func (c *Controller) GetWorkersCnt() uint32 {
	return c.opConfig.Workers
}

//WorkerStatus provides status of the worker
func (c *Controller) WorkerStatus(workerID uint32) (*cluster.WorkerStatus, error) {
	obj, ok := c.curWorkerCluster.Load(workerID)
	if !ok || obj == nil {
		return nil, nil
	}

	cl, ok := obj.(*cluster.Cluster)
	if !ok {
		return nil, fmt.Errorf("could not cast to Cluster struct")
	}

	return &cluster.WorkerStatus{
		CurrentCluster: types.NamespacedName(util.NameFromMeta(cl.ObjectMeta)),
		CurrentProcess: cl.GetCurrentProcess(),
	}, nil
}

// ClusterHistory dumps history of cluster changes
func (c *Controller) ClusterHistory(namespace, name string) ([]*spec.Diff, error) {

	clusterName := spec.NamespacedName{
		Namespace: namespace,
		Name:      name,
	}

	c.clustersMu.RLock()
	cl, ok := c.clusterHistory[clusterName]
	c.clustersMu.RUnlock()
	if !ok {
		return nil, fmt.Errorf("could not find cluster")
	}

	res := make([]*spec.Diff, 0)
	for _, e := range cl.Walk() {
		res = append(res, e.(*spec.Diff))
	}

	return res, nil
}


================================================
File: pkg/controller/node.go
================================================
package controller

import (
	"context"
	"fmt"
	"time"

	"github.com/zalando/postgres-operator/pkg/util/retryutil"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/watch"

	"github.com/zalando/postgres-operator/pkg/cluster"
	"github.com/zalando/postgres-operator/pkg/util"
)

func (c *Controller) nodeListFunc(options metav1.ListOptions) (runtime.Object, error) {
	opts := metav1.ListOptions{
		Watch:           options.Watch,
		ResourceVersion: options.ResourceVersion,
		TimeoutSeconds:  options.TimeoutSeconds,
	}

	return c.KubeClient.Nodes().List(context.TODO(), opts)
}

func (c *Controller) nodeWatchFunc(options metav1.ListOptions) (watch.Interface, error) {
	opts := metav1.ListOptions{
		Watch:           options.Watch,
		ResourceVersion: options.ResourceVersion,
		TimeoutSeconds:  options.TimeoutSeconds,
	}

	return c.KubeClient.Nodes().Watch(context.TODO(), opts)
}

func (c *Controller) nodeAdd(obj interface{}) {
	node, ok := obj.(*v1.Node)
	if !ok {
		return
	}

	c.logger.Debugf("new node has been added: %s (%s)", util.NameFromMeta(node.ObjectMeta), node.Spec.ProviderID)

	// check if the node became not ready while the operator was down (otherwise we would have caught it in nodeUpdate)
	if !c.nodeIsReady(node) {
		c.moveMasterPodsOffNode(node)
	}
}

func (c *Controller) nodeUpdate(prev, cur interface{}) {
	nodePrev, ok := prev.(*v1.Node)
	if !ok {
		return
	}

	nodeCur, ok := cur.(*v1.Node)
	if !ok {
		return
	}

	if util.MapContains(nodeCur.Labels, map[string]string{"master": "true"}) {
		return
	}

	// do nothing if the node should have already triggered an update or
	// if only one of the label and the unschedulability criteria are met.
	if !c.nodeIsReady(nodePrev) || c.nodeIsReady(nodeCur) {
		return
	}

	c.moveMasterPodsOffNode(nodeCur)

}

func (c *Controller) nodeIsReady(node *v1.Node) bool {
	return (!node.Spec.Unschedulable || (len(c.opConfig.NodeReadinessLabel) > 0 && util.MapContains(node.Labels, c.opConfig.NodeReadinessLabel)) ||
		util.MapContains(node.Labels, map[string]string{"master": "true"}))
}

func (c *Controller) attemptToMoveMasterPodsOffNode(node *v1.Node) error {
	nodeName := util.NameFromMeta(node.ObjectMeta)
	c.logger.Infof("moving pods: node %q became unschedulable and does not have a ready label: %q",
		nodeName, c.opConfig.NodeReadinessLabel)

	opts := metav1.ListOptions{
		LabelSelector: labels.Set(c.opConfig.ClusterLabels).String(),
	}
	podList, err := c.KubeClient.Pods(c.opConfig.WatchedNamespace).List(context.TODO(), opts)
	if err != nil {
		c.logger.Errorf("could not fetch list of the pods: %v", err)
		return err
	}

	nodePods := make([]*v1.Pod, 0)
	for i, pod := range podList.Items {
		if pod.Spec.NodeName == node.Name {
			nodePods = append(nodePods, &podList.Items[i])
		}
	}

	clusters := make(map[*cluster.Cluster]bool)
	masterPods := make(map[*v1.Pod]*cluster.Cluster)
	movedPods := 0
	for _, pod := range nodePods {
		podName := util.NameFromMeta(pod.ObjectMeta)

		role, ok := pod.Labels[c.opConfig.PodRoleLabel]
		if !ok || cluster.PostgresRole(role) != cluster.Master {
			if !ok {
				c.logger.Warningf("could not move pod %q: pod has no role", podName)
			}
			continue
		}

		clusterName := c.podClusterName(pod)

		c.clustersMu.RLock()
		cl, ok := c.clusters[clusterName]
		c.clustersMu.RUnlock()
		if !ok {
			c.logger.Warningf("could not move pod %q: pod does not belong to a known cluster", podName)
			continue
		}

		if !clusters[cl] {
			clusters[cl] = true
		}

		masterPods[pod] = cl
	}

	for cl := range clusters {
		cl.Lock()
	}

	for pod, cl := range masterPods {
		podName := util.NameFromMeta(pod.ObjectMeta)

		if err := cl.MigrateMasterPod(podName); err != nil {
			c.logger.Errorf("could not move master pod %q: %v", podName, err)
		} else {
			movedPods++
		}
	}

	for cl := range clusters {
		cl.Unlock()
	}

	totalPods := len(masterPods)

	c.logger.Infof("%d/%d master pods have been moved out from the %q node",
		movedPods, totalPods, nodeName)

	if leftPods := totalPods - movedPods; leftPods > 0 {
		return fmt.Errorf("could not move master %d/%d pods from the %q node",
			leftPods, totalPods, nodeName)
	}

	return nil
}

func (c *Controller) nodeDelete(obj interface{}) {
	node, ok := obj.(*v1.Node)
	if !ok {
		return
	}

	c.logger.Debugf("node has been deleted: %q (%s)", util.NameFromMeta(node.ObjectMeta), node.Spec.ProviderID)
}

func (c *Controller) moveMasterPodsOffNode(node *v1.Node) {
	// retry to move master until configured timeout is reached
	err := retryutil.Retry(1*time.Minute, c.opConfig.MasterPodMoveTimeout,
		func() (bool, error) {
			err := c.attemptToMoveMasterPodsOffNode(node)
			if err != nil {
				return false, err
			}
			return true, nil
		},
	)

	if err != nil {
		c.logger.Warningf("failed to move master pods from the node %q: %v", node.Name, err)
	}

}


================================================
File: pkg/controller/node_test.go
================================================
package controller

import (
	"testing"

	"github.com/zalando/postgres-operator/pkg/spec"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

const (
	readyLabel = "lifecycle-status"
	readyValue = "ready"
)

func newNodeTestController() *Controller {
	var controller = NewController(&spec.ControllerConfig{}, "node-test")
	return controller
}

func makeNode(labels map[string]string, isSchedulable bool) *v1.Node {
	return &v1.Node{
		ObjectMeta: metav1.ObjectMeta{
			Namespace: v1.NamespaceDefault,
			Labels:    labels,
		},
		Spec: v1.NodeSpec{
			Unschedulable: !isSchedulable,
		},
	}
}

var nodeTestController = newNodeTestController()

func TestNodeIsReady(t *testing.T) {
	testName := "TestNodeIsReady"
	var testTable = []struct {
		in             *v1.Node
		out            bool
		readinessLabel map[string]string
	}{
		{
			in:             makeNode(map[string]string{"foo": "bar"}, true),
			out:            true,
			readinessLabel: map[string]string{readyLabel: readyValue},
		},
		{
			in:             makeNode(map[string]string{"foo": "bar"}, false),
			out:            false,
			readinessLabel: map[string]string{readyLabel: readyValue},
		},
		{
			in:             makeNode(map[string]string{readyLabel: readyValue}, false),
			out:            true,
			readinessLabel: map[string]string{readyLabel: readyValue},
		},
		{
			in:             makeNode(map[string]string{"foo": "bar", "master": "true"}, false),
			out:            true,
			readinessLabel: map[string]string{readyLabel: readyValue},
		},
		{
			in:             makeNode(map[string]string{"foo": "bar", "master": "true"}, false),
			out:            true,
			readinessLabel: map[string]string{readyLabel: readyValue},
		},
		{
			in:             makeNode(map[string]string{"foo": "bar"}, true),
			out:            true,
			readinessLabel: map[string]string{},
		},
		{
			in:             makeNode(map[string]string{"foo": "bar"}, false),
			out:            false,
			readinessLabel: map[string]string{},
		},
		{
			in:             makeNode(map[string]string{readyLabel: readyValue}, false),
			out:            false,
			readinessLabel: map[string]string{},
		},
		{
			in:             makeNode(map[string]string{"foo": "bar", "master": "true"}, false),
			out:            true,
			readinessLabel: map[string]string{},
		},
	}
	for _, tt := range testTable {
		nodeTestController.opConfig.NodeReadinessLabel = tt.readinessLabel
		if isReady := nodeTestController.nodeIsReady(tt.in); isReady != tt.out {
			t.Errorf("%s: expected response %t does not match the actual %t for the node %#v",
				testName, tt.out, isReady, tt.in)
		}
	}
}


================================================
File: pkg/controller/operator_config.go
================================================
package controller

import (
	"context"
	"fmt"

	"time"

	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func (c *Controller) readOperatorConfigurationFromCRD(configObjectNamespace, configObjectName string) (*acidv1.OperatorConfiguration, error) {

	config, err := c.KubeClient.OperatorConfigurationsGetter.OperatorConfigurations(configObjectNamespace).Get(
		context.TODO(), configObjectName, metav1.GetOptions{})
	if err != nil {
		return nil, fmt.Errorf("could not get operator configuration object %q: %v", configObjectName, err)
	}

	return config, nil
}

// importConfigurationFromCRD is a transitional function that converts CRD configuration to the one based on the configmap
func (c *Controller) importConfigurationFromCRD(fromCRD *acidv1.OperatorConfigurationData) *config.Config {
	result := &config.Config{}

	// general config
	result.EnableCRDRegistration = util.CoalesceBool(fromCRD.EnableCRDRegistration, util.True())
	result.EnableCRDValidation = util.CoalesceBool(fromCRD.EnableCRDValidation, util.True())
	result.CRDCategories = util.CoalesceStrArr(fromCRD.CRDCategories, []string{"all"})
	result.EnableLazySpiloUpgrade = fromCRD.EnableLazySpiloUpgrade
	result.EnablePgVersionEnvVar = fromCRD.EnablePgVersionEnvVar
	result.EnableSpiloWalPathCompat = fromCRD.EnableSpiloWalPathCompat
	result.EnableTeamIdClusternamePrefix = fromCRD.EnableTeamIdClusternamePrefix
	result.EtcdHost = fromCRD.EtcdHost
	result.KubernetesUseConfigMaps = fromCRD.KubernetesUseConfigMaps
	result.DockerImage = util.Coalesce(fromCRD.DockerImage, "ghcr.io/zalando/spilo-17:4.0-p2")
	result.Workers = util.CoalesceUInt32(fromCRD.Workers, 8)
	result.MinInstances = fromCRD.MinInstances
	result.MaxInstances = fromCRD.MaxInstances
	result.IgnoreInstanceLimitsAnnotationKey = fromCRD.IgnoreInstanceLimitsAnnotationKey
	result.ResyncPeriod = util.CoalesceDuration(time.Duration(fromCRD.ResyncPeriod), "30m")
	result.RepairPeriod = util.CoalesceDuration(time.Duration(fromCRD.RepairPeriod), "5m")
	result.SetMemoryRequestToLimit = fromCRD.SetMemoryRequestToLimit
	result.ShmVolume = util.CoalesceBool(fromCRD.ShmVolume, util.True())
	result.SidecarImages = fromCRD.SidecarImages
	result.SidecarContainers = fromCRD.SidecarContainers

	// user config
	result.SuperUsername = util.Coalesce(fromCRD.PostgresUsersConfiguration.SuperUsername, "postgres")
	result.ReplicationUsername = util.Coalesce(fromCRD.PostgresUsersConfiguration.ReplicationUsername, "standby")
	result.AdditionalOwnerRoles = fromCRD.PostgresUsersConfiguration.AdditionalOwnerRoles
	result.EnablePasswordRotation = fromCRD.PostgresUsersConfiguration.EnablePasswordRotation
	result.PasswordRotationInterval = util.CoalesceUInt32(fromCRD.PostgresUsersConfiguration.PasswordRotationInterval, 90)
	result.PasswordRotationUserRetention = util.CoalesceUInt32(fromCRD.PostgresUsersConfiguration.DeepCopy().PasswordRotationUserRetention, 180)

	// major version upgrade config
	result.MajorVersionUpgradeMode = util.Coalesce(fromCRD.MajorVersionUpgrade.MajorVersionUpgradeMode, "manual")
	result.MajorVersionUpgradeTeamAllowList = fromCRD.MajorVersionUpgrade.MajorVersionUpgradeTeamAllowList
	result.MinimalMajorVersion = util.Coalesce(fromCRD.MajorVersionUpgrade.MinimalMajorVersion, "13")
	result.TargetMajorVersion = util.Coalesce(fromCRD.MajorVersionUpgrade.TargetMajorVersion, "17")

	// kubernetes config
	result.EnableOwnerReferences = util.CoalesceBool(fromCRD.Kubernetes.EnableOwnerReferences, util.False())
	result.CustomPodAnnotations = fromCRD.Kubernetes.CustomPodAnnotations
	result.PodServiceAccountName = util.Coalesce(fromCRD.Kubernetes.PodServiceAccountName, "postgres-pod")
	result.PodServiceAccountDefinition = fromCRD.Kubernetes.PodServiceAccountDefinition
	result.PodServiceAccountRoleBindingDefinition = fromCRD.Kubernetes.PodServiceAccountRoleBindingDefinition
	result.PodEnvironmentConfigMap = fromCRD.Kubernetes.PodEnvironmentConfigMap
	result.PodEnvironmentSecret = fromCRD.Kubernetes.PodEnvironmentSecret
	result.PodTerminateGracePeriod = util.CoalesceDuration(time.Duration(fromCRD.Kubernetes.PodTerminateGracePeriod), "5m")
	result.SpiloPrivileged = fromCRD.Kubernetes.SpiloPrivileged
	result.SpiloAllowPrivilegeEscalation = util.CoalesceBool(fromCRD.Kubernetes.SpiloAllowPrivilegeEscalation, util.True())
	result.SpiloRunAsUser = fromCRD.Kubernetes.SpiloRunAsUser
	result.SpiloRunAsGroup = fromCRD.Kubernetes.SpiloRunAsGroup
	result.SpiloFSGroup = fromCRD.Kubernetes.SpiloFSGroup
	result.AdditionalPodCapabilities = fromCRD.Kubernetes.AdditionalPodCapabilities
	result.ClusterDomain = util.Coalesce(fromCRD.Kubernetes.ClusterDomain, "cluster.local")
	result.WatchedNamespace = fromCRD.Kubernetes.WatchedNamespace
	result.PDBNameFormat = fromCRD.Kubernetes.PDBNameFormat
	result.PDBMasterLabelSelector = util.CoalesceBool(fromCRD.Kubernetes.PDBMasterLabelSelector, util.True())
	result.EnablePodDisruptionBudget = util.CoalesceBool(fromCRD.Kubernetes.EnablePodDisruptionBudget, util.True())
	result.StorageResizeMode = util.Coalesce(fromCRD.Kubernetes.StorageResizeMode, "pvc")
	result.EnableInitContainers = util.CoalesceBool(fromCRD.Kubernetes.EnableInitContainers, util.True())
	result.EnableSidecars = util.CoalesceBool(fromCRD.Kubernetes.EnableSidecars, util.True())
	result.SharePgSocketWithSidecars = util.CoalesceBool(fromCRD.Kubernetes.SharePgSocketWithSidecars, util.False())
	result.SecretNameTemplate = fromCRD.Kubernetes.SecretNameTemplate
	result.OAuthTokenSecretName = fromCRD.Kubernetes.OAuthTokenSecretName
	result.EnableCrossNamespaceSecret = fromCRD.Kubernetes.EnableCrossNamespaceSecret
	result.EnableFinalizers = util.CoalesceBool(fromCRD.Kubernetes.EnableFinalizers, util.False())

	result.InfrastructureRolesSecretName = fromCRD.Kubernetes.InfrastructureRolesSecretName
	if fromCRD.Kubernetes.InfrastructureRolesDefs != nil {
		result.InfrastructureRoles = []*config.InfrastructureRole{}
		for _, secret := range fromCRD.Kubernetes.InfrastructureRolesDefs {
			result.InfrastructureRoles = append(
				result.InfrastructureRoles,
				&config.InfrastructureRole{
					SecretName:  secret.SecretName,
					UserKey:     secret.UserKey,
					RoleKey:     secret.RoleKey,
					PasswordKey: secret.PasswordKey,
				})
		}
	}

	result.PodRoleLabel = util.Coalesce(fromCRD.Kubernetes.PodRoleLabel, "spilo-role")
	result.ClusterLabels = util.CoalesceStrMap(fromCRD.Kubernetes.ClusterLabels, map[string]string{"application": "spilo"})
	result.InheritedLabels = fromCRD.Kubernetes.InheritedLabels
	result.InheritedAnnotations = fromCRD.Kubernetes.InheritedAnnotations
	result.DownscalerAnnotations = fromCRD.Kubernetes.DownscalerAnnotations
	result.IgnoredAnnotations = fromCRD.Kubernetes.IgnoredAnnotations
	result.ClusterNameLabel = util.Coalesce(fromCRD.Kubernetes.ClusterNameLabel, "cluster-name")
	result.DeleteAnnotationDateKey = fromCRD.Kubernetes.DeleteAnnotationDateKey
	result.DeleteAnnotationNameKey = fromCRD.Kubernetes.DeleteAnnotationNameKey
	result.NodeReadinessLabel = fromCRD.Kubernetes.NodeReadinessLabel
	result.NodeReadinessLabelMerge = fromCRD.Kubernetes.NodeReadinessLabelMerge
	result.PodPriorityClassName = fromCRD.Kubernetes.PodPriorityClassName
	result.PodManagementPolicy = util.Coalesce(fromCRD.Kubernetes.PodManagementPolicy, "ordered_ready")
	result.PersistentVolumeClaimRetentionPolicy = fromCRD.Kubernetes.PersistentVolumeClaimRetentionPolicy
	result.EnableSecretsDeletion = util.CoalesceBool(fromCRD.Kubernetes.EnableSecretsDeletion, util.True())
	result.EnablePersistentVolumeClaimDeletion = util.CoalesceBool(fromCRD.Kubernetes.EnablePersistentVolumeClaimDeletion, util.True())
	result.EnableReadinessProbe = fromCRD.Kubernetes.EnableReadinessProbe
	result.MasterPodMoveTimeout = util.CoalesceDuration(time.Duration(fromCRD.Kubernetes.MasterPodMoveTimeout), "10m")
	result.EnablePodAntiAffinity = fromCRD.Kubernetes.EnablePodAntiAffinity
	result.PodAntiAffinityTopologyKey = util.Coalesce(fromCRD.Kubernetes.PodAntiAffinityTopologyKey, "kubernetes.io/hostname")
	result.PodAntiAffinityPreferredDuringScheduling = fromCRD.Kubernetes.PodAntiAffinityPreferredDuringScheduling
	result.PodToleration = fromCRD.Kubernetes.PodToleration

	// Postgres Pod resources
	result.DefaultCPURequest = fromCRD.PostgresPodResources.DefaultCPURequest
	result.DefaultMemoryRequest = fromCRD.PostgresPodResources.DefaultMemoryRequest
	result.DefaultCPULimit = fromCRD.PostgresPodResources.DefaultCPULimit
	result.DefaultMemoryLimit = fromCRD.PostgresPodResources.DefaultMemoryLimit
	result.MinCPULimit = fromCRD.PostgresPodResources.MinCPULimit
	result.MinMemoryLimit = fromCRD.PostgresPodResources.MinMemoryLimit
	result.MaxCPURequest = fromCRD.PostgresPodResources.MaxCPURequest
	result.MaxMemoryRequest = fromCRD.PostgresPodResources.MaxMemoryRequest

	// timeout config
	result.ResourceCheckInterval = util.CoalesceDuration(time.Duration(fromCRD.Timeouts.ResourceCheckInterval), "3s")
	result.ResourceCheckTimeout = util.CoalesceDuration(time.Duration(fromCRD.Timeouts.ResourceCheckTimeout), "10m")
	result.PodLabelWaitTimeout = util.CoalesceDuration(time.Duration(fromCRD.Timeouts.PodLabelWaitTimeout), "10m")
	result.PodDeletionWaitTimeout = util.CoalesceDuration(time.Duration(fromCRD.Timeouts.PodDeletionWaitTimeout), "10m")
	result.ReadyWaitInterval = util.CoalesceDuration(time.Duration(fromCRD.Timeouts.ReadyWaitInterval), "4s")
	result.ReadyWaitTimeout = util.CoalesceDuration(time.Duration(fromCRD.Timeouts.ReadyWaitTimeout), "30s")
	result.PatroniAPICheckInterval = util.CoalesceDuration(time.Duration(fromCRD.Timeouts.PatroniAPICheckInterval), "1s")
	result.PatroniAPICheckTimeout = util.CoalesceDuration(time.Duration(fromCRD.Timeouts.PatroniAPICheckTimeout), "5s")

	// load balancer config
	result.DbHostedZone = util.Coalesce(fromCRD.LoadBalancer.DbHostedZone, "db.example.com")
	result.EnableMasterLoadBalancer = fromCRD.LoadBalancer.EnableMasterLoadBalancer
	result.EnableMasterPoolerLoadBalancer = fromCRD.LoadBalancer.EnableMasterPoolerLoadBalancer
	result.EnableReplicaLoadBalancer = fromCRD.LoadBalancer.EnableReplicaLoadBalancer
	result.EnableReplicaPoolerLoadBalancer = fromCRD.LoadBalancer.EnableReplicaPoolerLoadBalancer
	result.CustomServiceAnnotations = fromCRD.LoadBalancer.CustomServiceAnnotations
	result.MasterDNSNameFormat = fromCRD.LoadBalancer.MasterDNSNameFormat
	result.MasterLegacyDNSNameFormat = fromCRD.LoadBalancer.MasterLegacyDNSNameFormat
	result.ReplicaDNSNameFormat = fromCRD.LoadBalancer.ReplicaDNSNameFormat
	result.ReplicaLegacyDNSNameFormat = fromCRD.LoadBalancer.ReplicaLegacyDNSNameFormat
	result.ExternalTrafficPolicy = util.Coalesce(fromCRD.LoadBalancer.ExternalTrafficPolicy, "Cluster")

	// AWS or GCP config
	result.WALES3Bucket = fromCRD.AWSGCP.WALES3Bucket
	result.AWSRegion = fromCRD.AWSGCP.AWSRegion
	result.LogS3Bucket = fromCRD.AWSGCP.LogS3Bucket
	result.KubeIAMRole = fromCRD.AWSGCP.KubeIAMRole
	result.WALGSBucket = fromCRD.AWSGCP.WALGSBucket
	result.GCPCredentials = fromCRD.AWSGCP.GCPCredentials
	result.WALAZStorageAccount = fromCRD.AWSGCP.WALAZStorageAccount
	result.AdditionalSecretMount = fromCRD.AWSGCP.AdditionalSecretMount
	result.AdditionalSecretMountPath = fromCRD.AWSGCP.AdditionalSecretMountPath
	result.EnableEBSGp3Migration = fromCRD.AWSGCP.EnableEBSGp3Migration
	result.EnableEBSGp3MigrationMaxSize = util.CoalesceInt64(fromCRD.AWSGCP.EnableEBSGp3MigrationMaxSize, 1000)

	// logical backup config
	result.LogicalBackupSchedule = util.Coalesce(fromCRD.LogicalBackup.Schedule, "30 00 * * *")
	result.LogicalBackupDockerImage = util.Coalesce(fromCRD.LogicalBackup.DockerImage, "ghcr.io/zalando/postgres-operator/logical-backup:v1.14.0")
	result.LogicalBackupProvider = util.Coalesce(fromCRD.LogicalBackup.BackupProvider, "s3")
	result.LogicalBackupAzureStorageAccountName = fromCRD.LogicalBackup.AzureStorageAccountName
	result.LogicalBackupAzureStorageAccountKey = fromCRD.LogicalBackup.AzureStorageAccountKey
	result.LogicalBackupAzureStorageContainer = fromCRD.LogicalBackup.AzureStorageContainer
	result.LogicalBackupS3Bucket = fromCRD.LogicalBackup.S3Bucket
	result.LogicalBackupS3BucketPrefix = util.Coalesce(fromCRD.LogicalBackup.S3BucketPrefix, "spilo")
	result.LogicalBackupS3Region = fromCRD.LogicalBackup.S3Region
	result.LogicalBackupS3Endpoint = fromCRD.LogicalBackup.S3Endpoint
	result.LogicalBackupS3AccessKeyID = fromCRD.LogicalBackup.S3AccessKeyID
	result.LogicalBackupS3SecretAccessKey = fromCRD.LogicalBackup.S3SecretAccessKey
	result.LogicalBackupS3SSE = fromCRD.LogicalBackup.S3SSE
	result.LogicalBackupS3RetentionTime = fromCRD.LogicalBackup.RetentionTime
	result.LogicalBackupGoogleApplicationCredentials = fromCRD.LogicalBackup.GoogleApplicationCredentials
	result.LogicalBackupJobPrefix = util.Coalesce(fromCRD.LogicalBackup.JobPrefix, "logical-backup-")
	result.LogicalBackupCronjobEnvironmentSecret = fromCRD.LogicalBackup.CronjobEnvironmentSecret
	result.LogicalBackupCPURequest = fromCRD.LogicalBackup.CPURequest
	result.LogicalBackupMemoryRequest = fromCRD.LogicalBackup.MemoryRequest
	result.LogicalBackupCPULimit = fromCRD.LogicalBackup.CPULimit
	result.LogicalBackupMemoryLimit = fromCRD.LogicalBackup.MemoryLimit

	// debug config
	result.DebugLogging = fromCRD.OperatorDebug.DebugLogging
	result.EnableDBAccess = fromCRD.OperatorDebug.EnableDBAccess

	// Teams API config
	result.EnableTeamsAPI = fromCRD.TeamsAPI.EnableTeamsAPI
	result.TeamsAPIUrl = util.Coalesce(fromCRD.TeamsAPI.TeamsAPIUrl, "https://teams.example.com/api/")
	result.TeamAPIRoleConfiguration = util.CoalesceStrMap(fromCRD.TeamsAPI.TeamAPIRoleConfiguration, map[string]string{"log_statement": "all"})
	result.EnableTeamSuperuser = fromCRD.TeamsAPI.EnableTeamSuperuser
	result.EnableAdminRoleForUsers = fromCRD.TeamsAPI.EnableAdminRoleForUsers
	result.TeamAdminRole = fromCRD.TeamsAPI.TeamAdminRole
	result.PamRoleName = util.Coalesce(fromCRD.TeamsAPI.PamRoleName, "zalandos")
	result.PamConfiguration = util.Coalesce(fromCRD.TeamsAPI.PamConfiguration, "https://info.example.com/oauth2/tokeninfo?access_token= uid realm=/employees")
	result.ProtectedRoles = util.CoalesceStrArr(fromCRD.TeamsAPI.ProtectedRoles, []string{"admin", "cron_admin"})
	result.PostgresSuperuserTeams = fromCRD.TeamsAPI.PostgresSuperuserTeams
	result.EnablePostgresTeamCRD = fromCRD.TeamsAPI.EnablePostgresTeamCRD
	result.EnablePostgresTeamCRDSuperusers = fromCRD.TeamsAPI.EnablePostgresTeamCRDSuperusers
	result.EnableTeamMemberDeprecation = fromCRD.TeamsAPI.EnableTeamMemberDeprecation
	result.RoleDeletionSuffix = util.Coalesce(fromCRD.TeamsAPI.RoleDeletionSuffix, "_deleted")

	// logging REST API config
	result.APIPort = util.CoalesceInt(fromCRD.LoggingRESTAPI.APIPort, 8080)
	result.RingLogLines = util.CoalesceInt(fromCRD.LoggingRESTAPI.RingLogLines, 100)
	result.ClusterHistoryEntries = util.CoalesceInt(fromCRD.LoggingRESTAPI.ClusterHistoryEntries, 1000)

	// Scalyr config
	result.ScalyrAPIKey = fromCRD.Scalyr.ScalyrAPIKey
	result.ScalyrImage = fromCRD.Scalyr.ScalyrImage
	result.ScalyrServerURL = fromCRD.Scalyr.ScalyrServerURL
	result.ScalyrCPURequest = fromCRD.Scalyr.ScalyrCPURequest
	result.ScalyrMemoryRequest = fromCRD.Scalyr.ScalyrMemoryRequest
	result.ScalyrCPULimit = fromCRD.Scalyr.ScalyrCPULimit
	result.ScalyrMemoryLimit = fromCRD.Scalyr.ScalyrMemoryLimit

	// Patroni config
	result.EnablePatroniFailsafeMode = util.CoalesceBool(fromCRD.Patroni.FailsafeMode, util.False())

	// Connection pooler. Looks like we can't use defaulting in CRD before 1.17,
	// so ensure default values here.
	result.ConnectionPooler.NumberOfInstances = util.CoalesceInt32(
		fromCRD.ConnectionPooler.NumberOfInstances,
		k8sutil.Int32ToPointer(2))

	result.ConnectionPooler.NumberOfInstances = util.MaxInt32(
		result.ConnectionPooler.NumberOfInstances,
		k8sutil.Int32ToPointer(2))

	result.ConnectionPooler.Schema = util.Coalesce(
		fromCRD.ConnectionPooler.Schema,
		constants.ConnectionPoolerSchemaName)

	result.ConnectionPooler.User = util.Coalesce(
		fromCRD.ConnectionPooler.User,
		constants.ConnectionPoolerUserName)

	if result.ConnectionPooler.User == result.SuperUsername {
		msg := "connection pool user is not allowed to be the same as super user, username: %s"
		panic(fmt.Errorf(msg, result.ConnectionPooler.User))
	}

	result.ConnectionPooler.Image = util.Coalesce(
		fromCRD.ConnectionPooler.Image,
		"registry.opensource.zalan.do/acid/pgbouncer")

	result.ConnectionPooler.Mode = util.Coalesce(
		fromCRD.ConnectionPooler.Mode,
		constants.ConnectionPoolerDefaultMode)

	result.ConnectionPooler.ConnectionPoolerDefaultCPURequest = fromCRD.ConnectionPooler.DefaultCPURequest
	result.ConnectionPooler.ConnectionPoolerDefaultMemoryRequest = fromCRD.ConnectionPooler.DefaultMemoryRequest
	result.ConnectionPooler.ConnectionPoolerDefaultCPULimit = fromCRD.ConnectionPooler.DefaultCPULimit
	result.ConnectionPooler.ConnectionPoolerDefaultMemoryLimit = fromCRD.ConnectionPooler.DefaultMemoryLimit

	result.ConnectionPooler.MaxDBConnections = util.CoalesceInt32(
		fromCRD.ConnectionPooler.MaxDBConnections,
		k8sutil.Int32ToPointer(constants.ConnectionPoolerMaxDBConnections))

	return result
}


================================================
File: pkg/controller/pod.go
================================================
package controller

import (
	"context"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/watch"

	"github.com/zalando/postgres-operator/pkg/cluster"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util"
	"k8s.io/apimachinery/pkg/types"
)

func (c *Controller) podListFunc(options metav1.ListOptions) (runtime.Object, error) {
	opts := metav1.ListOptions{
		Watch:           options.Watch,
		ResourceVersion: options.ResourceVersion,
		TimeoutSeconds:  options.TimeoutSeconds,
	}

	return c.KubeClient.Pods(c.opConfig.WatchedNamespace).List(context.TODO(), opts)
}

func (c *Controller) podWatchFunc(options metav1.ListOptions) (watch.Interface, error) {
	opts := metav1.ListOptions{
		Watch:           options.Watch,
		ResourceVersion: options.ResourceVersion,
		TimeoutSeconds:  options.TimeoutSeconds,
	}

	return c.KubeClient.Pods(c.opConfig.WatchedNamespace).Watch(context.TODO(), opts)
}

func (c *Controller) dispatchPodEvent(clusterName spec.NamespacedName, event cluster.PodEvent) {
	c.clustersMu.RLock()
	cluster, ok := c.clusters[clusterName]
	c.clustersMu.RUnlock()
	if ok {
		cluster.ReceivePodEvent(event)
	}
}

func (c *Controller) podAdd(obj interface{}) {
	if pod, ok := obj.(*v1.Pod); ok {
		c.preparePodEventForDispatch(pod, nil, cluster.PodEventAdd)
	}
}

func (c *Controller) podUpdate(prev, cur interface{}) {
	prevPod, ok := prev.(*v1.Pod)
	if !ok {
		return
	}

	curPod, ok := cur.(*v1.Pod)
	if !ok {
		return
	}

	c.preparePodEventForDispatch(curPod, prevPod, cluster.PodEventUpdate)
}

func (c *Controller) podDelete(obj interface{}) {

	if pod, ok := obj.(*v1.Pod); ok {
		c.preparePodEventForDispatch(pod, nil, cluster.PodEventDelete)
	}
}

func (c *Controller) preparePodEventForDispatch(curPod, prevPod *v1.Pod, event cluster.PodEventType) {
	podEvent := cluster.PodEvent{
		PodName:         types.NamespacedName(util.NameFromMeta(curPod.ObjectMeta)),
		CurPod:          curPod,
		PrevPod:         prevPod,
		EventType:       event,
		ResourceVersion: curPod.ResourceVersion,
	}

	c.dispatchPodEvent(c.podClusterName(curPod), podEvent)
}


================================================
File: pkg/controller/postgresql.go
================================================
package controller

import (
	"context"
	"encoding/json"
	"fmt"
	"reflect"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/sirupsen/logrus"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/tools/cache"

	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/cluster"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"github.com/zalando/postgres-operator/pkg/util/ringlog"
)

func (c *Controller) clusterResync(stopCh <-chan struct{}, wg *sync.WaitGroup) {
	defer wg.Done()
	ticker := time.NewTicker(c.opConfig.ResyncPeriod)

	for {
		select {
		case <-ticker.C:
			if err := c.clusterListAndSync(); err != nil {
				c.logger.Errorf("could not list clusters: %v", err)
			}
		case <-stopCh:
			return
		}
	}
}

// clusterListFunc obtains a list of all PostgreSQL clusters
func (c *Controller) listClusters(options metav1.ListOptions) (*acidv1.PostgresqlList, error) {
	var pgList acidv1.PostgresqlList

	// TODO: use the SharedInformer cache instead of quering Kubernetes API directly.
	list, err := c.KubeClient.PostgresqlsGetter.Postgresqls(c.opConfig.WatchedNamespace).List(context.TODO(), options)
	if err != nil {
		c.logger.Errorf("could not list postgresql objects: %v", err)
	}
	if c.controllerID != "" {
		c.logger.Debugf("watch only clusters with controllerID %q", c.controllerID)
	}
	for _, pg := range list.Items {
		if pg.Error == "" && c.hasOwnership(&pg) {
			pgList.Items = append(pgList.Items, pg)
		}
	}

	return &pgList, err
}

// clusterListAndSync lists all manifests and decides whether to run the sync or repair.
func (c *Controller) clusterListAndSync() error {
	var (
		err   error
		event EventType
	)

	currentTime := time.Now().Unix()
	timeFromPreviousSync := currentTime - atomic.LoadInt64(&c.lastClusterSyncTime)
	timeFromPreviousRepair := currentTime - atomic.LoadInt64(&c.lastClusterRepairTime)

	if timeFromPreviousSync >= int64(c.opConfig.ResyncPeriod.Seconds()) {
		event = EventSync
	} else if timeFromPreviousRepair >= int64(c.opConfig.RepairPeriod.Seconds()) {
		event = EventRepair
	}
	if event != "" {
		var list *acidv1.PostgresqlList
		if list, err = c.listClusters(metav1.ListOptions{ResourceVersion: "0"}); err != nil {
			return err
		}
		c.queueEvents(list, event)
	} else {
		c.logger.Infof("not enough time passed since the last sync (%v seconds) or repair (%v seconds)",
			timeFromPreviousSync, timeFromPreviousRepair)
	}
	return nil
}

// queueEvents queues a sync or repair event for every cluster with a valid manifest
func (c *Controller) queueEvents(list *acidv1.PostgresqlList, event EventType) {
	var activeClustersCnt, failedClustersCnt, clustersToRepair int
	for i, pg := range list.Items {
		// XXX: check the cluster status field instead
		if pg.Error != "" {
			failedClustersCnt++
			continue
		}
		activeClustersCnt++
		// check if that cluster needs repair
		if event == EventRepair {
			if pg.Status.Success() {
				continue
			} else {
				clustersToRepair++
			}
		}
		c.queueClusterEvent(nil, &list.Items[i], event)
	}
	if len(list.Items) > 0 {
		if failedClustersCnt > 0 && activeClustersCnt == 0 {
			c.logger.Infof("there are no clusters running. %d are in the failed state", failedClustersCnt)
		} else if failedClustersCnt == 0 && activeClustersCnt > 0 {
			c.logger.Infof("there are %d clusters running", activeClustersCnt)
		} else {
			c.logger.Infof("there are %d clusters running and %d are in the failed state", activeClustersCnt, failedClustersCnt)
		}
		if clustersToRepair > 0 {
			c.logger.Infof("%d clusters are scheduled for a repair scan", clustersToRepair)
		}
	} else {
		c.logger.Infof("no clusters running")
	}
	if event == EventRepair || event == EventSync {
		atomic.StoreInt64(&c.lastClusterRepairTime, time.Now().Unix())
		if event == EventSync {
			atomic.StoreInt64(&c.lastClusterSyncTime, time.Now().Unix())
		}
	}
}

func (c *Controller) acquireInitialListOfClusters() error {
	var (
		list        *acidv1.PostgresqlList
		err         error
		clusterName spec.NamespacedName
	)

	if list, err = c.listClusters(metav1.ListOptions{ResourceVersion: "0"}); err != nil {
		return err
	}
	c.logger.Debug("acquiring initial list of clusters")
	for _, pg := range list.Items {
		// XXX: check the cluster status field instead
		if pg.Error != "" {
			continue
		}
		clusterName = util.NameFromMeta(pg.ObjectMeta)
		c.addCluster(c.logger, clusterName, &pg)
		c.logger.Debugf("added new cluster: %q", clusterName)
	}
	// initiate initial sync of all clusters.
	c.queueEvents(list, EventSync)
	return nil
}

func (c *Controller) addCluster(lg *logrus.Entry, clusterName spec.NamespacedName, pgSpec *acidv1.Postgresql) (*cluster.Cluster, error) {
	if c.opConfig.EnableTeamIdClusternamePrefix {
		if _, err := acidv1.ExtractClusterName(clusterName.Name, pgSpec.Spec.TeamID); err != nil {
			c.KubeClient.SetPostgresCRDStatus(clusterName, acidv1.ClusterStatusInvalid)
			return nil, err
		}
	}

	cl := cluster.New(c.makeClusterConfig(), c.KubeClient, *pgSpec, lg, c.eventRecorder)
	cl.Run(c.stopCh)
	teamName := strings.ToLower(cl.Spec.TeamID)

	defer c.clustersMu.Unlock()
	c.clustersMu.Lock()

	c.teamClusters[teamName] = append(c.teamClusters[teamName], clusterName)
	c.clusters[clusterName] = cl
	c.clusterLogs[clusterName] = ringlog.New(c.opConfig.RingLogLines)
	c.clusterHistory[clusterName] = ringlog.New(c.opConfig.ClusterHistoryEntries)

	return cl, nil
}

func (c *Controller) processEvent(event ClusterEvent) {
	var clusterName spec.NamespacedName
	var clHistory ringlog.RingLogger
	var err error

	lg := c.logger.WithField("worker", event.WorkerID)

	if event.EventType == EventAdd || event.EventType == EventSync || event.EventType == EventRepair {
		clusterName = util.NameFromMeta(event.NewSpec.ObjectMeta)
	} else {
		clusterName = util.NameFromMeta(event.OldSpec.ObjectMeta)
	}
	lg = lg.WithField("cluster-name", clusterName)

	c.clustersMu.RLock()
	cl, clusterFound := c.clusters[clusterName]
	if clusterFound {
		clHistory = c.clusterHistory[clusterName]
	}
	c.clustersMu.RUnlock()

	defer c.curWorkerCluster.Store(event.WorkerID, nil)

	if event.EventType == EventRepair {
		runRepair, lastOperationStatus := cl.NeedsRepair()
		if !runRepair {
			lg.Debugf("observed cluster status %s, repair is not required", lastOperationStatus)
			return
		}
		lg.Debugf("observed cluster status %s, running sync scan to repair the cluster", lastOperationStatus)
		event.EventType = EventSync
	}

	if event.EventType == EventAdd || event.EventType == EventUpdate || event.EventType == EventSync {
		// handle deprecated parameters by possibly assigning their values to the new ones.
		if event.OldSpec != nil {
			c.mergeDeprecatedPostgreSQLSpecParameters(&event.OldSpec.Spec)
		}
		if event.NewSpec != nil {
			c.warnOnDeprecatedPostgreSQLSpecParameters(&event.NewSpec.Spec)
			c.mergeDeprecatedPostgreSQLSpecParameters(&event.NewSpec.Spec)
		}

		if err = c.submitRBACCredentials(event); err != nil {
			c.logger.Warnf("pods and/or Patroni may misfunction due to the lack of permissions: %v", err)
		}

	}

	switch event.EventType {
	case EventAdd:
		if clusterFound {
			lg.Infof("received add event for already existing Postgres cluster")
			return
		}

		lg.Infof("creating a new Postgres cluster")

		cl, err = c.addCluster(lg, clusterName, event.NewSpec)
		if err != nil {
			lg.Errorf("creation of cluster is blocked: %v", err)
			return
		}

		c.curWorkerCluster.Store(event.WorkerID, cl)

		err = cl.Create()
		if err != nil {
			cl.Status = acidv1.PostgresStatus{PostgresClusterStatus: acidv1.ClusterStatusInvalid}
			cl.Error = fmt.Sprintf("could not create cluster: %v", err)
			lg.Error(cl.Error)
			c.eventRecorder.Eventf(cl.GetReference(), v1.EventTypeWarning, "Create", "%v", cl.Error)
			return
		}

		lg.Infoln("cluster has been created")
	case EventUpdate:
		lg.Infoln("update of the cluster started")

		if !clusterFound {
			lg.Warningln("cluster does not exist")
			return
		}
		c.curWorkerCluster.Store(event.WorkerID, cl)
		err = cl.Update(event.OldSpec, event.NewSpec)
		if err != nil {
			cl.Error = fmt.Sprintf("could not update cluster: %v", err)
			lg.Error(cl.Error)

			return
		}
		cl.Error = ""
		lg.Infoln("cluster has been updated")

		clHistory.Insert(&spec.Diff{
			EventTime:   event.EventTime,
			ProcessTime: time.Now(),
			Diff:        util.Diff(event.OldSpec, event.NewSpec),
		})
	case EventDelete:
		if !clusterFound {
			lg.Errorf("unknown cluster: %q", clusterName)
			return
		}

		teamName := strings.ToLower(cl.Spec.TeamID)
		c.curWorkerCluster.Store(event.WorkerID, cl)

		// when using finalizers the deletion already happened
		if c.opConfig.EnableFinalizers == nil || !*c.opConfig.EnableFinalizers {
			lg.Infoln("deletion of the cluster started")
			if err := cl.Delete(); err != nil {
				cl.Error = fmt.Sprintf("could not delete cluster: %v", err)
				c.eventRecorder.Eventf(cl.GetReference(), v1.EventTypeWarning, "Delete", "%v", cl.Error)
			}
		}

		func() {
			defer c.clustersMu.Unlock()
			c.clustersMu.Lock()

			delete(c.clusters, clusterName)
			delete(c.clusterLogs, clusterName)
			delete(c.clusterHistory, clusterName)
			for i, val := range c.teamClusters[teamName] {
				if val == clusterName {
					copy(c.teamClusters[teamName][i:], c.teamClusters[teamName][i+1:])
					c.teamClusters[teamName][len(c.teamClusters[teamName])-1] = spec.NamespacedName{}
					c.teamClusters[teamName] = c.teamClusters[teamName][:len(c.teamClusters[teamName])-1]
					break
				}
			}
		}()

		lg.Infof("cluster has been deleted")
	case EventSync:
		lg.Infof("syncing of the cluster started")

		// no race condition because a cluster is always processed by single worker
		if !clusterFound {
			cl, err = c.addCluster(lg, clusterName, event.NewSpec)
			if err != nil {
				lg.Errorf("syncing of cluster is blocked: %v", err)
				return
			}
		}

		c.curWorkerCluster.Store(event.WorkerID, cl)

		// has this cluster been marked as deleted already, then we shall start cleaning up
		if !cl.ObjectMeta.DeletionTimestamp.IsZero() {
			lg.Infof("cluster has a DeletionTimestamp of %s, starting deletion now.", cl.ObjectMeta.DeletionTimestamp.Format(time.RFC3339))
			if err = cl.Delete(); err != nil {
				cl.Error = fmt.Sprintf("error deleting cluster and its resources: %v", err)
				c.eventRecorder.Eventf(cl.GetReference(), v1.EventTypeWarning, "Delete", "%v", cl.Error)
				lg.Error(cl.Error)
				return
			}
		} else {
			if err = cl.Sync(event.NewSpec); err != nil {
				cl.Error = fmt.Sprintf("could not sync cluster: %v", err)
				c.eventRecorder.Eventf(cl.GetReference(), v1.EventTypeWarning, "Sync", "%v", cl.Error)
				lg.Error(cl.Error)
				return
			}
			lg.Infof("cluster has been synced")
		}
		cl.Error = ""
	}
}

func (c *Controller) processClusterEventsQueue(idx int, stopCh <-chan struct{}, wg *sync.WaitGroup) {
	defer wg.Done()

	go func() {
		<-stopCh
		c.clusterEventQueues[idx].Close()
	}()

	for {
		obj, err := c.clusterEventQueues[idx].Pop(cache.PopProcessFunc(func(interface{}, bool) error { return nil }))
		if err != nil {
			if err == cache.ErrFIFOClosed {
				return
			}
			c.logger.Errorf("error when processing cluster events queue: %v", err)
			continue
		}
		event, ok := obj.(ClusterEvent)
		if !ok {
			c.logger.Errorf("could not cast to ClusterEvent")
		}

		c.processEvent(event)
	}
}

func (c *Controller) warnOnDeprecatedPostgreSQLSpecParameters(spec *acidv1.PostgresSpec) {

	deprecate := func(deprecated, replacement string) {
		c.logger.Warningf("parameter %q is deprecated. Consider setting %q instead", deprecated, replacement)
	}

	if spec.UseLoadBalancer != nil {
		deprecate("useLoadBalancer", "enableMasterLoadBalancer")
	}
	if spec.ReplicaLoadBalancer != nil {
		deprecate("replicaLoadBalancer", "enableReplicaLoadBalancer")
	}

	if (spec.UseLoadBalancer != nil || spec.ReplicaLoadBalancer != nil) &&
		(spec.EnableReplicaLoadBalancer != nil || spec.EnableMasterLoadBalancer != nil) {
		c.logger.Warnf("both old and new load balancer parameters are present in the manifest, ignoring old ones")
	}
}

// mergeDeprecatedPostgreSQLSpecParameters modifies the spec passed to the cluster by setting current parameter
// values from the obsolete ones. Note: while the spec that is modified is a copy made in queueClusterEvent, it is
// still a shallow copy, so be extra careful not to modify values pointer fields point to, but copy them instead.
func (c *Controller) mergeDeprecatedPostgreSQLSpecParameters(spec *acidv1.PostgresSpec) *acidv1.PostgresSpec {
	if (spec.UseLoadBalancer != nil || spec.ReplicaLoadBalancer != nil) &&
		(spec.EnableReplicaLoadBalancer == nil && spec.EnableMasterLoadBalancer == nil) {
		if spec.UseLoadBalancer != nil {
			spec.EnableMasterLoadBalancer = new(bool)
			*spec.EnableMasterLoadBalancer = *spec.UseLoadBalancer
		}
		if spec.ReplicaLoadBalancer != nil {
			spec.EnableReplicaLoadBalancer = new(bool)
			*spec.EnableReplicaLoadBalancer = *spec.ReplicaLoadBalancer
		}
	}
	spec.ReplicaLoadBalancer = nil
	spec.UseLoadBalancer = nil

	return spec
}

func (c *Controller) queueClusterEvent(informerOldSpec, informerNewSpec *acidv1.Postgresql, eventType EventType) {
	var (
		uid          types.UID
		clusterName  spec.NamespacedName
		clusterError string
	)

	if informerOldSpec != nil { //update, delete
		uid = informerOldSpec.GetUID()
		clusterName = util.NameFromMeta(informerOldSpec.ObjectMeta)

		// user is fixing previously incorrect spec
		if eventType == EventUpdate && informerNewSpec.Error == "" && informerOldSpec.Error != "" {
			eventType = EventSync
		}

		// set current error to be one of the new spec if present
		if informerNewSpec != nil {
			clusterError = informerNewSpec.Error
		} else {
			clusterError = informerOldSpec.Error
		}
	} else { //add, sync
		uid = informerNewSpec.GetUID()
		clusterName = util.NameFromMeta(informerNewSpec.ObjectMeta)
		clusterError = informerNewSpec.Error
	}

	if eventType == EventDelete {
		// when owner references are used operator cannot block deletion
		if c.opConfig.EnableOwnerReferences == nil || !*c.opConfig.EnableOwnerReferences {
			// only allow deletion if delete annotations are set and conditions are met
			if err := c.meetsClusterDeleteAnnotations(informerOldSpec); err != nil {
				c.logger.WithField("cluster-name", clusterName).Warnf(
					"ignoring %q event for cluster %q - manifest does not fulfill delete requirements: %s", eventType, clusterName, err)
				c.logger.WithField("cluster-name", clusterName).Warnf(
					"please, recreate Postgresql resource %q and set annotations to delete properly", clusterName)
				if currentManifest, marshalErr := json.Marshal(informerOldSpec); marshalErr != nil {
					c.logger.WithField("cluster-name", clusterName).Warnf("could not marshal current manifest:\n%+v", informerOldSpec)
				} else {
					c.logger.WithField("cluster-name", clusterName).Warnf("%s\n", string(currentManifest))
				}
				return
			}
		}
	}

	if clusterError != "" && eventType != EventDelete {
		c.logger.WithField("cluster-name", clusterName).Debugf("skipping %q event for the invalid cluster: %s", eventType, clusterError)

		switch eventType {
		case EventAdd:
			c.KubeClient.SetPostgresCRDStatus(clusterName, acidv1.ClusterStatusAddFailed)
			c.eventRecorder.Eventf(c.GetReference(informerNewSpec), v1.EventTypeWarning, "Create", "%v", clusterError)
		case EventUpdate:
			c.KubeClient.SetPostgresCRDStatus(clusterName, acidv1.ClusterStatusUpdateFailed)
			c.eventRecorder.Eventf(c.GetReference(informerNewSpec), v1.EventTypeWarning, "Update", "%v", clusterError)
		default:
			c.KubeClient.SetPostgresCRDStatus(clusterName, acidv1.ClusterStatusSyncFailed)
			c.eventRecorder.Eventf(c.GetReference(informerNewSpec), v1.EventTypeWarning, "Sync", "%v", clusterError)
		}

		return
	}

	// Don't pass the spec directly from the informer, since subsequent modifications of it would be reflected
	// in the informer internal state, making it incoherent with the actual Kubernetes object (and, as a side
	// effect, the modified state will be returned together with subsequent events).

	workerID := c.clusterWorkerID(clusterName)
	clusterEvent := ClusterEvent{
		EventTime: time.Now(),
		EventType: eventType,
		UID:       uid,
		OldSpec:   informerOldSpec.Clone(),
		NewSpec:   informerNewSpec.Clone(),
		WorkerID:  workerID,
	}

	lg := c.logger.WithField("worker", workerID).WithField("cluster-name", clusterName)
	if err := c.clusterEventQueues[workerID].Add(clusterEvent); err != nil {
		lg.Errorf("error while queueing cluster event: %v", clusterEvent)
	}
	lg.Infof("%s event has been queued", eventType)

	if eventType != EventDelete {
		return
	}
	// A delete event discards all prior requests for that cluster.
	for _, evType := range []EventType{EventAdd, EventSync, EventUpdate, EventRepair} {
		obj, exists, err := c.clusterEventQueues[workerID].GetByKey(queueClusterKey(evType, uid))
		if err != nil {
			lg.Warningf("could not get event from the queue: %v", err)
			continue
		}

		if !exists {
			continue
		}

		err = c.clusterEventQueues[workerID].Delete(obj)
		if err != nil {
			lg.Warningf("could not delete event from the queue: %v", err)
		} else {
			lg.Debugf("event %s has been discarded for the cluster", evType)
		}
	}
}

func (c *Controller) postgresqlAdd(obj interface{}) {
	pg := c.postgresqlCheck(obj)
	if pg != nil {
		// We will not get multiple Add events for the same cluster
		c.queueClusterEvent(nil, pg, EventAdd)
	}
}

func (c *Controller) postgresqlUpdate(prev, cur interface{}) {
	pgOld := c.postgresqlCheck(prev)
	pgNew := c.postgresqlCheck(cur)
	if pgOld != nil && pgNew != nil {
		// Avoid the inifinite recursion for status updates
		if reflect.DeepEqual(pgOld.Spec, pgNew.Spec) {
			if reflect.DeepEqual(pgNew.Annotations, pgOld.Annotations) {
				return
			}
		}
		c.queueClusterEvent(pgOld, pgNew, EventUpdate)
	}
}

func (c *Controller) postgresqlDelete(obj interface{}) {
	pg := c.postgresqlCheck(obj)
	if pg != nil {
		c.queueClusterEvent(pg, nil, EventDelete)
	}
}

func (c *Controller) postgresqlCheck(obj interface{}) *acidv1.Postgresql {
	pg, ok := obj.(*acidv1.Postgresql)
	if !ok {
		c.logger.Errorf("could not cast to postgresql spec")
		return nil
	}
	if !c.hasOwnership(pg) {
		return nil
	}
	return pg
}

/*
Ensures the pod service account and role bindings exists in a namespace
before a PG cluster is created there so that a user does not have to deploy
these credentials manually.  StatefulSets require the service account to
create pods; Patroni requires relevant RBAC bindings to access endpoints
or config maps.

The operator does not sync accounts/role bindings after creation.
*/
func (c *Controller) submitRBACCredentials(event ClusterEvent) error {

	namespace := event.NewSpec.GetNamespace()

	if err := c.createPodServiceAccount(namespace); err != nil {
		return fmt.Errorf("could not create pod service account %q : %v", c.opConfig.PodServiceAccountName, err)
	}

	if err := c.createRoleBindings(namespace); err != nil {
		return fmt.Errorf("could not create role binding %q : %v", c.PodServiceAccountRoleBinding.Name, err)
	}
	return nil
}

func (c *Controller) createPodServiceAccount(namespace string) error {

	podServiceAccountName := c.opConfig.PodServiceAccountName
	_, err := c.KubeClient.ServiceAccounts(namespace).Get(context.TODO(), podServiceAccountName, metav1.GetOptions{})
	if k8sutil.ResourceNotFound(err) {

		c.logger.Infof(fmt.Sprintf("creating pod service account %q in the %q namespace", podServiceAccountName, namespace))

		// get a separate copy of service account
		// to prevent a race condition when setting a namespace for many clusters
		sa := *c.PodServiceAccount
		if _, err = c.KubeClient.ServiceAccounts(namespace).Create(context.TODO(), &sa, metav1.CreateOptions{}); err != nil {
			return fmt.Errorf("cannot deploy the pod service account %q defined in the configuration to the %q namespace: %v", podServiceAccountName, namespace, err)
		}

		c.logger.Infof("successfully deployed the pod service account %q to the %q namespace", podServiceAccountName, namespace)
	} else if k8sutil.ResourceAlreadyExists(err) {
		return nil
	}

	return err
}

func (c *Controller) createRoleBindings(namespace string) error {

	podServiceAccountName := c.opConfig.PodServiceAccountName
	podServiceAccountRoleBindingName := c.PodServiceAccountRoleBinding.Name

	_, err := c.KubeClient.RoleBindings(namespace).Get(context.TODO(), podServiceAccountRoleBindingName, metav1.GetOptions{})
	if k8sutil.ResourceNotFound(err) {

		c.logger.Infof("Creating the role binding %q in the %q namespace", podServiceAccountRoleBindingName, namespace)

		// get a separate copy of role binding
		// to prevent a race condition when setting a namespace for many clusters
		rb := *c.PodServiceAccountRoleBinding
		_, err = c.KubeClient.RoleBindings(namespace).Create(context.TODO(), &rb, metav1.CreateOptions{})
		if err != nil {
			return fmt.Errorf("cannot bind the pod service account %q defined in the configuration to the cluster role in the %q namespace: %v", podServiceAccountName, namespace, err)
		}

		c.logger.Infof("successfully deployed the role binding for the pod service account %q to the %q namespace", podServiceAccountName, namespace)

	} else if k8sutil.ResourceAlreadyExists(err) {
		return nil
	}

	return err
}


================================================
File: pkg/controller/postgresql_test.go
================================================
package controller

import (
	"fmt"
	"reflect"
	"testing"
	"time"

	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/spec"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

var (
	True  = true
	False = false
)

func newPostgresqlTestController() *Controller {
	controller := NewController(&spec.ControllerConfig{}, "postgresql-test")
	return controller
}

var postgresqlTestController = newPostgresqlTestController()

func TestControllerOwnershipOnPostgresql(t *testing.T) {
	tests := []struct {
		name  string
		pg    *acidv1.Postgresql
		owned bool
		error string
	}{
		{
			"Postgres cluster with defined ownership of mocked controller",
			&acidv1.Postgresql{
				ObjectMeta: metav1.ObjectMeta{
					Annotations: map[string]string{"acid.zalan.do/controller": "postgresql-test"},
				},
			},
			True,
			"Postgres cluster should be owned by operator, but controller says no",
		},
		{
			"Postgres cluster with defined ownership of another controller",
			&acidv1.Postgresql{
				ObjectMeta: metav1.ObjectMeta{
					Annotations: map[string]string{"acid.zalan.do/controller": "stups-test"},
				},
			},
			False,
			"Postgres cluster should be owned by another operator, but controller say yes",
		},
		{
			"Test Postgres cluster without defined ownership",
			&acidv1.Postgresql{},
			False,
			"Postgres cluster should be owned by operator with empty controller ID, but controller says yes",
		},
	}
	for _, tt := range tests {
		if postgresqlTestController.hasOwnership(tt.pg) != tt.owned {
			t.Errorf("%s: %v", tt.name, tt.error)
		}
	}
}

func TestMergeDeprecatedPostgreSQLSpecParameters(t *testing.T) {
	tests := []struct {
		name  string
		in    *acidv1.PostgresSpec
		out   *acidv1.PostgresSpec
		error string
	}{
		{
			"Check that old parameters propagate values to the new ones",
			&acidv1.PostgresSpec{UseLoadBalancer: &True, ReplicaLoadBalancer: &True},
			&acidv1.PostgresSpec{UseLoadBalancer: nil, ReplicaLoadBalancer: nil,
				EnableMasterLoadBalancer: &True, EnableReplicaLoadBalancer: &True},
			"New parameters should be set from the values of old ones",
		},
		{
			"Check that new parameters are not set when both old and new ones are present",
			&acidv1.PostgresSpec{UseLoadBalancer: &True, EnableMasterLoadBalancer: &False},
			&acidv1.PostgresSpec{UseLoadBalancer: nil, EnableMasterLoadBalancer: &False},
			"New parameters should remain unchanged when both old and new are present",
		},
	}
	for _, tt := range tests {
		result := postgresqlTestController.mergeDeprecatedPostgreSQLSpecParameters(tt.in)
		if !reflect.DeepEqual(result, tt.out) {
			t.Errorf("%s: %v", tt.name, tt.error)
		}
	}
}

func TestMeetsClusterDeleteAnnotations(t *testing.T) {
	// set delete annotations in configuration
	postgresqlTestController.opConfig.DeleteAnnotationDateKey = "delete-date"
	postgresqlTestController.opConfig.DeleteAnnotationNameKey = "delete-clustername"

	currentTime := time.Now()
	today := currentTime.Format("2006-01-02") // go's reference date
	clusterName := "acid-test-cluster"

	tests := []struct {
		name  string
		pg    *acidv1.Postgresql
		error string
	}{
		{
			"Postgres cluster with matching delete annotations",
			&acidv1.Postgresql{
				ObjectMeta: metav1.ObjectMeta{
					Name: clusterName,
					Annotations: map[string]string{
						"delete-date":        today,
						"delete-clustername": clusterName,
					},
				},
			},
			"",
		},
		{
			"Postgres cluster with violated delete date annotation",
			&acidv1.Postgresql{
				ObjectMeta: metav1.ObjectMeta{
					Name: clusterName,
					Annotations: map[string]string{
						"delete-date":        "2020-02-02",
						"delete-clustername": clusterName,
					},
				},
			},
			fmt.Sprintf("annotation delete-date not matching the current date: got 2020-02-02, expected %s", today),
		},
		{
			"Postgres cluster with violated delete cluster name annotation",
			&acidv1.Postgresql{
				ObjectMeta: metav1.ObjectMeta{
					Name: clusterName,
					Annotations: map[string]string{
						"delete-date":        today,
						"delete-clustername": "acid-minimal-cluster",
					},
				},
			},
			fmt.Sprintf("annotation delete-clustername not matching the cluster name: got acid-minimal-cluster, expected %s", clusterName),
		},
		{
			"Postgres cluster with missing delete annotations",
			&acidv1.Postgresql{
				ObjectMeta: metav1.ObjectMeta{
					Name:        clusterName,
					Annotations: map[string]string{},
				},
			},
			"annotation delete-date not set in manifest to allow cluster deletion",
		},
		{
			"Postgres cluster with missing delete cluster name annotation",
			&acidv1.Postgresql{
				ObjectMeta: metav1.ObjectMeta{
					Name: clusterName,
					Annotations: map[string]string{
						"delete-date": today,
					},
				},
			},
			"annotation delete-clustername not set in manifest to allow cluster deletion",
		},
	}
	for _, tt := range tests {
		if err := postgresqlTestController.meetsClusterDeleteAnnotations(tt.pg); err != nil {
			if !reflect.DeepEqual(err.Error(), tt.error) {
				t.Errorf("Expected error %q, got: %v", tt.error, err)
			}
		}
	}
}


================================================
File: pkg/controller/types.go
================================================
package controller

import (
	"time"

	"k8s.io/apimachinery/pkg/types"

	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
)

// EventType contains type of the events for the TPRs and Pods received from Kubernetes
type EventType string

// Possible values for the EventType
const (
	EventAdd    EventType = "ADD"
	EventUpdate EventType = "UPDATE"
	EventDelete EventType = "DELETE"
	EventSync   EventType = "SYNC"
	EventRepair EventType = "REPAIR"
)

// ClusterEvent carries the payload of the Cluster TPR events.
type ClusterEvent struct {
	EventTime time.Time
	UID       types.UID
	EventType EventType
	OldSpec   *acidv1.Postgresql
	NewSpec   *acidv1.Postgresql
	WorkerID  uint32
}


================================================
File: pkg/controller/util.go
================================================
package controller

import (
	"context"
	"encoding/json"
	"fmt"
	"strings"

	v1 "k8s.io/api/core/v1"
	apiextv1 "k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/util/wait"

	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/cluster"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	"gopkg.in/yaml.v2"
)

func (c *Controller) makeClusterConfig() cluster.Config {
	infrastructureRoles := make(map[string]spec.PgUser)
	for k, v := range c.config.InfrastructureRoles {
		infrastructureRoles[k] = v
	}

	return cluster.Config{
		RestConfig:          c.config.RestConfig,
		OpConfig:            config.Copy(c.opConfig),
		PgTeamMap:           &c.pgTeamMap,
		InfrastructureRoles: infrastructureRoles,
		PodServiceAccount:   c.PodServiceAccount,
	}
}

func (c *Controller) clusterWorkerID(clusterName spec.NamespacedName) uint32 {
	workerID, ok := c.clusterWorkers[clusterName]
	if ok {
		return workerID
	}

	c.clusterWorkers[clusterName] = c.curWorkerID

	if c.curWorkerID == c.opConfig.Workers-1 {
		c.curWorkerID = 0
	} else {
		c.curWorkerID++
	}

	return c.clusterWorkers[clusterName]
}

func (c *Controller) createOperatorCRD(desiredCrd *apiextv1.CustomResourceDefinition) error {
	crd, err := c.KubeClient.CustomResourceDefinitions().Get(context.TODO(), desiredCrd.Name, metav1.GetOptions{})
	if k8sutil.ResourceNotFound(err) {
		if _, err := c.KubeClient.CustomResourceDefinitions().Create(context.TODO(), desiredCrd, metav1.CreateOptions{}); err != nil {
			return fmt.Errorf("could not create customResourceDefinition %q: %v", desiredCrd.Name, err)
		}
	}
	if err != nil {
		c.logger.Errorf("could not get customResourceDefinition %q: %v", desiredCrd.Name, err)
	}
	if crd != nil {
		c.logger.Infof("customResourceDefinition %q is already registered and will only be updated", crd.Name)
		// copy annotations and labels from existing CRD since we do not define them
		desiredCrd.Annotations = crd.Annotations
		desiredCrd.Labels = crd.Labels
		patch, err := json.Marshal(desiredCrd)
		if err != nil {
			return fmt.Errorf("could not marshal new customResourceDefintion %q: %v", desiredCrd.Name, err)
		}
		if _, err := c.KubeClient.CustomResourceDefinitions().Patch(
			context.TODO(), crd.Name, types.MergePatchType, patch, metav1.PatchOptions{}); err != nil {
			return fmt.Errorf("could not update customResourceDefinition %q: %v", crd.Name, err)
		}
	}
	c.logger.Infof("customResourceDefinition %q is registered", crd.Name)

	return wait.PollUntilContextTimeout(context.TODO(), c.config.CRDReadyWaitInterval, c.config.CRDReadyWaitTimeout, false, func(ctx context.Context) (bool, error) {
		c, err := c.KubeClient.CustomResourceDefinitions().Get(context.TODO(), desiredCrd.Name, metav1.GetOptions{})
		if err != nil {
			return false, err
		}

		for _, cond := range c.Status.Conditions {
			switch cond.Type {
			case apiextv1.Established:
				if cond.Status == apiextv1.ConditionTrue {
					return true, err
				}
			case apiextv1.NamesAccepted:
				if cond.Status == apiextv1.ConditionFalse {
					return false, fmt.Errorf("name conflict: %v", cond.Reason)
				}
			}
		}

		return false, err
	})
}

func (c *Controller) createPostgresCRD() error {
	return c.createOperatorCRD(acidv1.PostgresCRD(c.opConfig.CRDCategories))
}

func (c *Controller) createConfigurationCRD() error {
	return c.createOperatorCRD(acidv1.ConfigurationCRD(c.opConfig.CRDCategories))
}

func readDecodedRole(s string) (*spec.PgUser, error) {
	var result spec.PgUser
	if err := yaml.Unmarshal([]byte(s), &result); err != nil {
		return nil, fmt.Errorf("could not decode yaml role: %v", err)
	}
	return &result, nil
}

var emptyName = (spec.NamespacedName{})

// Return information about what secrets we need to use to create
// infrastructure roles and in which format are they. This is done in
// compatible way, so that the previous logic is not changed, and handles both
// configuration in ConfigMap & CRD.
func (c *Controller) getInfrastructureRoleDefinitions() []*config.InfrastructureRole {
	var roleDef config.InfrastructureRole

	// take from CRD configuration
	rolesDefs := c.opConfig.InfrastructureRoles

	// check if we can extract something from the configmap config option
	if c.opConfig.InfrastructureRolesDefs != "" {
		// The configmap option could contain either a role description (in the
		// form key1: value1, key2: value2), which has to be used together with
		// an old secret name.

		var secretName spec.NamespacedName
		var err error
		propertySep := ","
		valueSep := ":"

		// The field contains the format in which secret is written, let's
		// convert it to a proper definition
		properties := strings.Split(c.opConfig.InfrastructureRolesDefs, propertySep)
		roleDef = config.InfrastructureRole{Template: false}

		for _, property := range properties {
			values := strings.Split(property, valueSep)
			if len(values) < 2 {
				continue
			}
			name := strings.TrimSpace(values[0])
			value := strings.TrimSpace(values[1])

			switch name {
			case "secretname":
				if err = secretName.DecodeWorker(value, "default"); err != nil {
					c.logger.Warningf("Could not marshal secret name %s: %v", value, err)
				} else {
					roleDef.SecretName = secretName
				}
			case "userkey":
				roleDef.UserKey = value
			case "passwordkey":
				roleDef.PasswordKey = value
			case "rolekey":
				roleDef.RoleKey = value
			case "defaultuservalue":
				roleDef.DefaultUserValue = value
			case "defaultrolevalue":
				roleDef.DefaultRoleValue = value
			default:
				c.logger.Warningf("Role description is not known: %s", properties)
			}
		}

		if roleDef.SecretName != emptyName &&
			(roleDef.UserKey != "" || roleDef.DefaultUserValue != "") &&
			roleDef.PasswordKey != "" {
			rolesDefs = append(rolesDefs, &roleDef)
		}
	}

	if c.opConfig.InfrastructureRolesSecretName != emptyName {
		// At this point we deal with the old format, let's replicate it
		// via existing definition structure and remember that it's just a
		// template, the real values are in user1,password1,inrole1 etc.
		rolesDefs = append(rolesDefs, &config.InfrastructureRole{
			SecretName:  c.opConfig.InfrastructureRolesSecretName,
			UserKey:     "user",
			PasswordKey: "password",
			RoleKey:     "inrole",
			Template:    true,
		})
	}

	return rolesDefs
}

func (c *Controller) getInfrastructureRoles(
	rolesSecrets []*config.InfrastructureRole) (
	map[string]spec.PgUser, error) {

	errors := make([]string, 0)
	noRolesProvided := true
	roles := []spec.PgUser{}
	uniqRoles := make(map[string]spec.PgUser)

	// To be compatible with the legacy implementation we need to return nil if
	// the provided secret name is empty. The equivalent situation in the
	// current implementation is an empty rolesSecrets slice or all its items
	// are empty.
	for _, role := range rolesSecrets {
		if role.SecretName != emptyName {
			noRolesProvided = false
		}
	}

	if noRolesProvided {
		return uniqRoles, nil
	}

	for _, secret := range rolesSecrets {
		infraRoles, err := c.getInfrastructureRole(secret)

		if err != nil || infraRoles == nil {
			c.logger.Debugf("cannot get infrastructure role: %+v", *secret)

			if err != nil {
				errors = append(errors, fmt.Sprintf("%v", err))
			}

			continue
		}

		roles = append(roles, infraRoles...)
	}

	for _, r := range roles {
		if _, exists := uniqRoles[r.Name]; exists {
			msg := "conflicting infrastructure roles: roles[%s] = (%q, %q)"
			c.logger.Debugf(msg, r.Name, uniqRoles[r.Name], r)
		}

		uniqRoles[r.Name] = r
	}

	if len(errors) > 0 {
		return uniqRoles, fmt.Errorf(strings.Join(errors, `', '`))
	}

	return uniqRoles, nil
}

// Generate list of users representing one infrastructure role based on its
// description in various K8S objects. An infrastructure role could be
// described by a secret and optionally a config map. The former should contain
// the secret information, i.e. username, password, role. The latter could
// contain an extensive description of the role and even override an
// information obtained from the secret (except a password).
//
// This function returns a list of users to be compatible with the previous
// behaviour, since we don't know how many users are actually encoded in the
// secret if it's a "template" role. If the provided role is not a template
// one, the result would be a list with just one user in it.
//
// FIXME: This dependency on two different objects is rather unnecessary
// complicated, so let's get rid of it via deprecation process.
func (c *Controller) getInfrastructureRole(
	infraRole *config.InfrastructureRole) (
	[]spec.PgUser, error) {

	rolesSecret := infraRole.SecretName
	roles := []spec.PgUser{}

	if rolesSecret == emptyName {
		// we don't have infrastructure roles defined, bail out
		return nil, nil
	}

	infraRolesSecret, err := c.KubeClient.
		Secrets(rolesSecret.Namespace).
		Get(context.TODO(), rolesSecret.Name, metav1.GetOptions{})
	if err != nil {
		msg := "could not get infrastructure roles secret %s/%s: %v"
		return nil, fmt.Errorf(msg, rolesSecret.Namespace, rolesSecret.Name, err)
	}

	secretData := infraRolesSecret.Data

	if infraRole.Template {
	Users:
		for i := 1; i <= len(secretData); i++ {
			properties := []string{
				infraRole.UserKey,
				infraRole.PasswordKey,
				infraRole.RoleKey,
			}
			t := spec.PgUser{Origin: spec.RoleOriginInfrastructure}
			for _, p := range properties {
				key := fmt.Sprintf("%s%d", p, i)
				if val, present := secretData[key]; !present {
					if p == "user" {
						// exit when the user name with the next sequence id is
						// absent
						break Users
					}
				} else {
					s := string(val)
					switch p {
					case "user":
						t.Name = s
					case "password":
						t.Password = s
					case "inrole":
						t.MemberOf = append(t.MemberOf, s)
					default:
						c.logger.Warningf("unknown key %q", p)
					}
				}
				// XXX: This is a part of the original implementation, which is
				// rather obscure. Why do we delete this key? Wouldn't it be
				// used later in comparison for configmap?
				delete(secretData, key)
			}

			if t.Valid() {
				roles = append(roles, t)
			} else {
				msg := "infrastructure role %q is not complete and ignored"
				c.logger.Warningf(msg, t)
			}
		}
	} else {
		roleDescr := &spec.PgUser{Origin: spec.RoleOriginInfrastructure}

		if details, exists := secretData[infraRole.Details]; exists {
			if err := yaml.Unmarshal(details, &roleDescr); err != nil {
				return nil, fmt.Errorf("could not decode yaml role: %v", err)
			}
		} else {
			roleDescr.Name = util.Coalesce(string(secretData[infraRole.UserKey]), infraRole.DefaultUserValue)
			roleDescr.Password = string(secretData[infraRole.PasswordKey])
			roleDescr.MemberOf = append(roleDescr.MemberOf,
				util.Coalesce(string(secretData[infraRole.RoleKey]), infraRole.DefaultRoleValue))
		}

		if !roleDescr.Valid() {
			msg := "infrastructure role %q is not complete and ignored"
			c.logger.Warningf(msg, roleDescr)

			return nil, nil
		}

		if roleDescr.Name == "" {
			msg := "infrastructure role %q has no name defined and is ignored"
			c.logger.Warningf(msg, roleDescr.Name)
			return nil, nil
		}

		if roleDescr.Password == "" {
			msg := "infrastructure role %q has no password defined and is ignored"
			c.logger.Warningf(msg, roleDescr.Name)
			return nil, nil
		}

		roles = append(roles, *roleDescr)
	}

	// Now plot twist. We need to check if there is a configmap with the same
	// name and extract a role description if it exists.
	infraRolesMap, err := c.KubeClient.
		ConfigMaps(rolesSecret.Namespace).
		Get(context.TODO(), rolesSecret.Name, metav1.GetOptions{})
	if err == nil {
		// we have a configmap with username - json description, let's read and decode it
		for role, s := range infraRolesMap.Data {
			roleDescr, err := readDecodedRole(s)
			if err != nil {
				return nil, fmt.Errorf("could not decode role description: %v", err)
			}
			// check if we have a a password in a configmap
			c.logger.Debugf("found role description for role %q: %+v", role, roleDescr)
			if passwd, ok := secretData[role]; ok {
				roleDescr.Password = string(passwd)
				delete(secretData, role)
			} else {
				c.logger.Warningf("infrastructure role %q has no password defined and is ignored", role)
				continue
			}
			roleDescr.Name = role
			roleDescr.Origin = spec.RoleOriginInfrastructure
			roles = append(roles, *roleDescr)
		}
	}

	// TODO: check for role collisions
	return roles, nil
}

func (c *Controller) loadPostgresTeams() {
	pgTeams, err := c.KubeClient.PostgresTeamsGetter.PostgresTeams(c.opConfig.WatchedNamespace).List(context.TODO(), metav1.ListOptions{})
	if err != nil {
		c.logger.Errorf("could not list postgres team objects: %v", err)
	}

	c.pgTeamMap.Load(pgTeams)
	c.logger.Debugf("Internal Postgres Team Cache: %#v", c.pgTeamMap)
}

func (c *Controller) postgresTeamAdd(obj interface{}) {
	pgTeam, ok := obj.(*acidv1.PostgresTeam)
	if !ok {
		c.logger.Errorf("could not cast to PostgresTeam spec")
		return
	}
	c.logger.Debugf("PostgreTeam %q added. Reloading postgres team CRDs and overwriting cached map", pgTeam.Name)
	c.loadPostgresTeams()
}

func (c *Controller) postgresTeamUpdate(prev, obj interface{}) {
	pgTeam, ok := obj.(*acidv1.PostgresTeam)
	if !ok {
		c.logger.Errorf("could not cast to PostgresTeam spec")
		return
	}
	c.logger.Debugf("PostgreTeam %q updated. Reloading postgres team CRDs and overwriting cached map", pgTeam.Name)
	c.loadPostgresTeams()
}

func (c *Controller) podClusterName(pod *v1.Pod) spec.NamespacedName {
	if name, ok := pod.Labels[c.opConfig.ClusterNameLabel]; ok {
		return spec.NamespacedName{
			Namespace: pod.Namespace,
			Name:      name,
		}
	}

	return spec.NamespacedName{}
}


================================================
File: pkg/controller/util_test.go
================================================
package controller

import (
	"fmt"
	"reflect"
	"testing"

	b64 "encoding/base64"

	"github.com/stretchr/testify/assert"
	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util/config"
	"github.com/zalando/postgres-operator/pkg/util/k8sutil"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

const (
	testInfrastructureRolesOldSecretName = "infrastructureroles-old-test"
	testInfrastructureRolesNewSecretName = "infrastructureroles-new-test"
)

func newUtilTestController() *Controller {
	controller := NewController(&spec.ControllerConfig{}, "util-test")
	controller.opConfig.ClusterNameLabel = "cluster-name"
	controller.opConfig.InfrastructureRolesSecretName =
		spec.NamespacedName{
			Namespace: v1.NamespaceDefault,
			Name:      testInfrastructureRolesOldSecretName,
		}
	controller.opConfig.Workers = 4
	controller.KubeClient = k8sutil.NewMockKubernetesClient()
	return controller
}

var utilTestController = newUtilTestController()

func TestPodClusterName(t *testing.T) {
	var testTable = []struct {
		in       *v1.Pod
		expected spec.NamespacedName
	}{
		{
			&v1.Pod{},
			spec.NamespacedName{},
		},
		{
			&v1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: v1.NamespaceDefault,
					Labels: map[string]string{
						utilTestController.opConfig.ClusterNameLabel: "testcluster",
					},
				},
			},
			spec.NamespacedName{Namespace: v1.NamespaceDefault, Name: "testcluster"},
		},
	}
	for _, test := range testTable {
		resp := utilTestController.podClusterName(test.in)
		if resp != test.expected {
			t.Errorf("expected response %v does not match the actual %v", test.expected, resp)
		}
	}
}

func TestClusterWorkerID(t *testing.T) {
	var testTable = []struct {
		in       spec.NamespacedName
		expected uint32
	}{
		{
			in:       spec.NamespacedName{Namespace: "foo", Name: "bar"},
			expected: 0,
		},
		{
			in:       spec.NamespacedName{Namespace: "default", Name: "testcluster"},
			expected: 1,
		},
	}
	for _, test := range testTable {
		resp := utilTestController.clusterWorkerID(test.in)
		if resp != test.expected {
			t.Errorf("expected response %v does not match the actual %v", test.expected, resp)
		}
	}
}

// Test functionality of getting infrastructure roles from their description in
// corresponding secrets. Here we test only common stuff (e.g. when a secret do
// not exist, or empty) and the old format.
func TestOldInfrastructureRoleFormat(t *testing.T) {
	var testTable = []struct {
		secretName    spec.NamespacedName
		expectedRoles map[string]spec.PgUser
		expectedError error
	}{
		{
			// empty secret name
			spec.NamespacedName{},
			map[string]spec.PgUser{},
			nil,
		},
		{
			// secret does not exist
			spec.NamespacedName{Namespace: v1.NamespaceDefault, Name: "null"},
			map[string]spec.PgUser{},
			fmt.Errorf(`could not get infrastructure roles secret default/null: NotFound`),
		},
		{
			spec.NamespacedName{
				Namespace: v1.NamespaceDefault,
				Name:      testInfrastructureRolesOldSecretName,
			},
			map[string]spec.PgUser{
				"testrole": {
					Name:     "testrole",
					Origin:   spec.RoleOriginInfrastructure,
					Password: "testpassword",
					MemberOf: []string{"testinrole"},
				},
				"foobar": {
					Name:     "foobar",
					Origin:   spec.RoleOriginInfrastructure,
					Password: b64.StdEncoding.EncodeToString([]byte("password")),
					MemberOf: nil,
				},
			},
			nil,
		},
	}
	for _, test := range testTable {
		roles, err := utilTestController.getInfrastructureRoles(
			[]*config.InfrastructureRole{
				{
					SecretName:  test.secretName,
					UserKey:     "user",
					PasswordKey: "password",
					RoleKey:     "inrole",
					Template:    true,
				},
			})

		if err != nil && err.Error() != test.expectedError.Error() {
			t.Errorf("expected error '%v' does not match the actual error '%v'",
				test.expectedError, err)
		}

		if !reflect.DeepEqual(roles, test.expectedRoles) {
			t.Errorf("expected roles output %#v does not match the actual %#v",
				test.expectedRoles, roles)
		}
	}
}

// Test functionality of getting infrastructure roles from their description in
// corresponding secrets. Here we test the new format.
func TestNewInfrastructureRoleFormat(t *testing.T) {
	var testTable = []struct {
		secrets       []spec.NamespacedName
		expectedRoles map[string]spec.PgUser
	}{
		// one secret with one configmap
		{
			[]spec.NamespacedName{
				{
					Namespace: v1.NamespaceDefault,
					Name:      testInfrastructureRolesNewSecretName,
				},
			},
			map[string]spec.PgUser{
				"new-test-role": {
					Name:     "new-test-role",
					Origin:   spec.RoleOriginInfrastructure,
					Password: "new-test-password",
					MemberOf: []string{"new-test-inrole"},
				},
				"new-foobar": {
					Name:     "new-foobar",
					Origin:   spec.RoleOriginInfrastructure,
					Password: b64.StdEncoding.EncodeToString([]byte("password")),
					MemberOf: nil,
					Flags:    []string{"createdb"},
				},
			},
		},
		// multiple standalone secrets
		{
			[]spec.NamespacedName{
				{
					Namespace: v1.NamespaceDefault,
					Name:      "infrastructureroles-new-test1",
				},
				{
					Namespace: v1.NamespaceDefault,
					Name:      "infrastructureroles-new-test2",
				},
			},
			map[string]spec.PgUser{
				"new-test-role1": {
					Name:     "new-test-role1",
					Origin:   spec.RoleOriginInfrastructure,
					Password: "new-test-password1",
					MemberOf: []string{"new-test-inrole1"},
				},
				"new-test-role2": {
					Name:     "new-test-role2",
					Origin:   spec.RoleOriginInfrastructure,
					Password: "new-test-password2",
					MemberOf: []string{"new-test-inrole2"},
				},
			},
		},
	}
	for _, test := range testTable {
		definitions := []*config.InfrastructureRole{}
		for _, secret := range test.secrets {
			definitions = append(definitions, &config.InfrastructureRole{
				SecretName:  secret,
				UserKey:     "user",
				PasswordKey: "password",
				RoleKey:     "inrole",
				Template:    false,
			})
		}

		roles, err := utilTestController.getInfrastructureRoles(definitions)
		assert.NoError(t, err)

		if !reflect.DeepEqual(roles, test.expectedRoles) {
			t.Errorf("expected roles output/the actual:\n%#v\n%#v",
				test.expectedRoles, roles)
		}
	}
}

// Tests for getting correct infrastructure roles definitions from present
// configuration. E.g. in which secrets for which roles too look. The biggest
// point here is compatibility of old and new formats of defining
// infrastructure roles.
func TestInfrastructureRoleDefinitions(t *testing.T) {
	var testTable = []struct {
		rolesDefs      []*config.InfrastructureRole
		roleSecretName spec.NamespacedName
		roleSecrets    string
		expectedDefs   []*config.InfrastructureRole
	}{
		// only new CRD format
		{
			[]*config.InfrastructureRole{
				{
					SecretName: spec.NamespacedName{
						Namespace: v1.NamespaceDefault,
						Name:      testInfrastructureRolesNewSecretName,
					},
					UserKey:     "test-user",
					PasswordKey: "test-password",
					RoleKey:     "test-role",
					Template:    false,
				},
			},
			spec.NamespacedName{},
			"",
			[]*config.InfrastructureRole{
				{
					SecretName: spec.NamespacedName{
						Namespace: v1.NamespaceDefault,
						Name:      testInfrastructureRolesNewSecretName,
					},
					UserKey:     "test-user",
					PasswordKey: "test-password",
					RoleKey:     "test-role",
					Template:    false,
				},
			},
		},
		// only new configmap format
		{
			[]*config.InfrastructureRole{},
			spec.NamespacedName{},
			"secretname: infrastructureroles-new-test, userkey: test-user, passwordkey: test-password, rolekey: test-role",
			[]*config.InfrastructureRole{
				{
					SecretName: spec.NamespacedName{
						Namespace: v1.NamespaceDefault,
						Name:      testInfrastructureRolesNewSecretName,
					},
					UserKey:     "test-user",
					PasswordKey: "test-password",
					RoleKey:     "test-role",
					Template:    false,
				},
			},
		},
		// new configmap format with defaultRoleValue
		{
			[]*config.InfrastructureRole{},
			spec.NamespacedName{},
			"secretname: infrastructureroles-new-test, userkey: test-user, passwordkey: test-password, defaultrolevalue: test-role",
			[]*config.InfrastructureRole{
				{
					SecretName: spec.NamespacedName{
						Namespace: v1.NamespaceDefault,
						Name:      testInfrastructureRolesNewSecretName,
					},
					UserKey:          "test-user",
					PasswordKey:      "test-password",
					DefaultRoleValue: "test-role",
					Template:         false,
				},
			},
		},
		// only old CRD and configmap format
		{
			[]*config.InfrastructureRole{},
			spec.NamespacedName{
				Namespace: v1.NamespaceDefault,
				Name:      testInfrastructureRolesOldSecretName,
			},
			"",
			[]*config.InfrastructureRole{
				{
					SecretName: spec.NamespacedName{
						Namespace: v1.NamespaceDefault,
						Name:      testInfrastructureRolesOldSecretName,
					},
					UserKey:     "user",
					PasswordKey: "password",
					RoleKey:     "inrole",
					Template:    true,
				},
			},
		},
		// both formats for CRD
		{
			[]*config.InfrastructureRole{
				{
					SecretName: spec.NamespacedName{
						Namespace: v1.NamespaceDefault,
						Name:      testInfrastructureRolesNewSecretName,
					},
					UserKey:     "test-user",
					PasswordKey: "test-password",
					RoleKey:     "test-role",
					Template:    false,
				},
			},
			spec.NamespacedName{
				Namespace: v1.NamespaceDefault,
				Name:      testInfrastructureRolesOldSecretName,
			},
			"",
			[]*config.InfrastructureRole{
				{
					SecretName: spec.NamespacedName{
						Namespace: v1.NamespaceDefault,
						Name:      testInfrastructureRolesNewSecretName,
					},
					UserKey:     "test-user",
					PasswordKey: "test-password",
					RoleKey:     "test-role",
					Template:    false,
				},
				{
					SecretName: spec.NamespacedName{
						Namespace: v1.NamespaceDefault,
						Name:      testInfrastructureRolesOldSecretName,
					},
					UserKey:     "user",
					PasswordKey: "password",
					RoleKey:     "inrole",
					Template:    true,
				},
			},
		},
		// both formats for configmap
		{
			[]*config.InfrastructureRole{},
			spec.NamespacedName{
				Namespace: v1.NamespaceDefault,
				Name:      testInfrastructureRolesOldSecretName,
			},
			"secretname: infrastructureroles-new-test, userkey: test-user, passwordkey: test-password, rolekey: test-role",
			[]*config.InfrastructureRole{
				{
					SecretName: spec.NamespacedName{
						Namespace: v1.NamespaceDefault,
						Name:      testInfrastructureRolesNewSecretName,
					},
					UserKey:     "test-user",
					PasswordKey: "test-password",
					RoleKey:     "test-role",
					Template:    false,
				},
				{
					SecretName: spec.NamespacedName{
						Namespace: v1.NamespaceDefault,
						Name:      testInfrastructureRolesOldSecretName,
					},
					UserKey:     "user",
					PasswordKey: "password",
					RoleKey:     "inrole",
					Template:    true,
				},
			},
		},
		// incorrect configmap format
		{
			[]*config.InfrastructureRole{},
			spec.NamespacedName{},
			"wrong-format",
			[]*config.InfrastructureRole{},
		},
		// configmap without a secret
		{
			[]*config.InfrastructureRole{},
			spec.NamespacedName{},
			"userkey: test-user, passwordkey: test-password, rolekey: test-role",
			[]*config.InfrastructureRole{},
		},
	}

	for _, test := range testTable {
		t.Logf("Test: %+v", test)
		utilTestController.opConfig.InfrastructureRoles = test.rolesDefs
		utilTestController.opConfig.InfrastructureRolesSecretName = test.roleSecretName
		utilTestController.opConfig.InfrastructureRolesDefs = test.roleSecrets

		defs := utilTestController.getInfrastructureRoleDefinitions()
		if len(defs) != len(test.expectedDefs) {
			t.Errorf("expected definitions does not match the actual:\n%#v\n%#v",
				test.expectedDefs, defs)

			// Stop and do not do any further checks
			return
		}

		for idx := range defs {
			def := defs[idx]
			expectedDef := test.expectedDefs[idx]

			if !reflect.DeepEqual(def, expectedDef) {
				t.Errorf("expected definition/the actual:\n%#v\n%#v",
					expectedDef, def)
			}
		}
	}
}

type SubConfig struct {
	teammap map[string]string
}

type SuperConfig struct {
	sub SubConfig
}

func TestUnderstandingMapsAndReferences(t *testing.T) {
	teams := map[string]string{"acid": "Felix"}

	sc := SubConfig{
		teammap: teams,
	}

	ssc := SuperConfig{
		sub: sc,
	}

	teams["24x7"] = "alex"

	if len(ssc.sub.teammap) != 2 {
		t.Errorf("Team Map does not contain 2 elements")
	}

	ssc.sub.teammap["teapot"] = "Mikkel"

	if len(teams) != 3 {
		t.Errorf("Team Map does not contain 3 elements")
	}

	teams = make(map[string]string)

	if len(ssc.sub.teammap) != 3 {
		t.Errorf("Team Map does not contain 0 elements")
	}

	if &teams == &(ssc.sub.teammap) {
		t.Errorf("Identical maps")
	}
}


================================================
File: pkg/generated/clientset/versioned/clientset.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package versioned

import (
	"fmt"
	"net/http"

	acidv1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	zalandov1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/zalando.org/v1"
	discovery "k8s.io/client-go/discovery"
	rest "k8s.io/client-go/rest"
	flowcontrol "k8s.io/client-go/util/flowcontrol"
)

type Interface interface {
	Discovery() discovery.DiscoveryInterface
	AcidV1() acidv1.AcidV1Interface
	ZalandoV1() zalandov1.ZalandoV1Interface
}

// Clientset contains the clients for groups. Each group has exactly one
// version included in a Clientset.
type Clientset struct {
	*discovery.DiscoveryClient
	acidV1    *acidv1.AcidV1Client
	zalandoV1 *zalandov1.ZalandoV1Client
}

// AcidV1 retrieves the AcidV1Client
func (c *Clientset) AcidV1() acidv1.AcidV1Interface {
	return c.acidV1
}

// ZalandoV1 retrieves the ZalandoV1Client
func (c *Clientset) ZalandoV1() zalandov1.ZalandoV1Interface {
	return c.zalandoV1
}

// Discovery retrieves the DiscoveryClient
func (c *Clientset) Discovery() discovery.DiscoveryInterface {
	if c == nil {
		return nil
	}
	return c.DiscoveryClient
}

// NewForConfig creates a new Clientset for the given config.
// If config's RateLimiter is not set and QPS and Burst are acceptable,
// NewForConfig will generate a rate-limiter in configShallowCopy.
// NewForConfig is equivalent to NewForConfigAndClient(c, httpClient),
// where httpClient was generated with rest.HTTPClientFor(c).
func NewForConfig(c *rest.Config) (*Clientset, error) {
	configShallowCopy := *c

	if configShallowCopy.UserAgent == "" {
		configShallowCopy.UserAgent = rest.DefaultKubernetesUserAgent()
	}

	// share the transport between all clients
	httpClient, err := rest.HTTPClientFor(&configShallowCopy)
	if err != nil {
		return nil, err
	}

	return NewForConfigAndClient(&configShallowCopy, httpClient)
}

// NewForConfigAndClient creates a new Clientset for the given config and http client.
// Note the http client provided takes precedence over the configured transport values.
// If config's RateLimiter is not set and QPS and Burst are acceptable,
// NewForConfigAndClient will generate a rate-limiter in configShallowCopy.
func NewForConfigAndClient(c *rest.Config, httpClient *http.Client) (*Clientset, error) {
	configShallowCopy := *c
	if configShallowCopy.RateLimiter == nil && configShallowCopy.QPS > 0 {
		if configShallowCopy.Burst <= 0 {
			return nil, fmt.Errorf("burst is required to be greater than 0 when RateLimiter is not set and QPS is set to greater than 0")
		}
		configShallowCopy.RateLimiter = flowcontrol.NewTokenBucketRateLimiter(configShallowCopy.QPS, configShallowCopy.Burst)
	}

	var cs Clientset
	var err error
	cs.acidV1, err = acidv1.NewForConfigAndClient(&configShallowCopy, httpClient)
	if err != nil {
		return nil, err
	}
	cs.zalandoV1, err = zalandov1.NewForConfigAndClient(&configShallowCopy, httpClient)
	if err != nil {
		return nil, err
	}

	cs.DiscoveryClient, err = discovery.NewDiscoveryClientForConfigAndClient(&configShallowCopy, httpClient)
	if err != nil {
		return nil, err
	}
	return &cs, nil
}

// NewForConfigOrDie creates a new Clientset for the given config and
// panics if there is an error in the config.
func NewForConfigOrDie(c *rest.Config) *Clientset {
	cs, err := NewForConfig(c)
	if err != nil {
		panic(err)
	}
	return cs
}

// New creates a new Clientset for the given RESTClient.
func New(c rest.Interface) *Clientset {
	var cs Clientset
	cs.acidV1 = acidv1.New(c)
	cs.zalandoV1 = zalandov1.New(c)

	cs.DiscoveryClient = discovery.NewDiscoveryClient(c)
	return &cs
}


================================================
File: pkg/generated/clientset/versioned/doc.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

// This package has the automatically generated clientset.
package versioned


================================================
File: pkg/generated/clientset/versioned/fake/clientset_generated.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package fake

import (
	clientset "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned"
	acidv1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	fakeacidv1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/fake"
	zalandov1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/zalando.org/v1"
	fakezalandov1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/zalando.org/v1/fake"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/watch"
	"k8s.io/client-go/discovery"
	fakediscovery "k8s.io/client-go/discovery/fake"
	"k8s.io/client-go/testing"
)

// NewSimpleClientset returns a clientset that will respond with the provided objects.
// It's backed by a very simple object tracker that processes creates, updates and deletions as-is,
// without applying any validations and/or defaults. It shouldn't be considered a replacement
// for a real clientset and is mostly useful in simple unit tests.
func NewSimpleClientset(objects ...runtime.Object) *Clientset {
	o := testing.NewObjectTracker(scheme, codecs.UniversalDecoder())
	for _, obj := range objects {
		if err := o.Add(obj); err != nil {
			panic(err)
		}
	}

	cs := &Clientset{tracker: o}
	cs.discovery = &fakediscovery.FakeDiscovery{Fake: &cs.Fake}
	cs.AddReactor("*", "*", testing.ObjectReaction(o))
	cs.AddWatchReactor("*", func(action testing.Action) (handled bool, ret watch.Interface, err error) {
		gvr := action.GetResource()
		ns := action.GetNamespace()
		watch, err := o.Watch(gvr, ns)
		if err != nil {
			return false, nil, err
		}
		return true, watch, nil
	})

	return cs
}

// Clientset implements clientset.Interface. Meant to be embedded into a
// struct to get a default implementation. This makes faking out just the method
// you want to test easier.
type Clientset struct {
	testing.Fake
	discovery *fakediscovery.FakeDiscovery
	tracker   testing.ObjectTracker
}

func (c *Clientset) Discovery() discovery.DiscoveryInterface {
	return c.discovery
}

func (c *Clientset) Tracker() testing.ObjectTracker {
	return c.tracker
}

var (
	_ clientset.Interface = &Clientset{}
	_ testing.FakeClient  = &Clientset{}
)

// AcidV1 retrieves the AcidV1Client
func (c *Clientset) AcidV1() acidv1.AcidV1Interface {
	return &fakeacidv1.FakeAcidV1{Fake: &c.Fake}
}

// ZalandoV1 retrieves the ZalandoV1Client
func (c *Clientset) ZalandoV1() zalandov1.ZalandoV1Interface {
	return &fakezalandov1.FakeZalandoV1{Fake: &c.Fake}
}


================================================
File: pkg/generated/clientset/versioned/fake/doc.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

// This package has the automatically generated fake clientset.
package fake


================================================
File: pkg/generated/clientset/versioned/fake/register.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package fake

import (
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	zalandov1 "github.com/zalando/postgres-operator/pkg/apis/zalando.org/v1"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
	schema "k8s.io/apimachinery/pkg/runtime/schema"
	serializer "k8s.io/apimachinery/pkg/runtime/serializer"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
)

var scheme = runtime.NewScheme()
var codecs = serializer.NewCodecFactory(scheme)

var localSchemeBuilder = runtime.SchemeBuilder{
	acidv1.AddToScheme,
	zalandov1.AddToScheme,
}

// AddToScheme adds all types of this clientset into the given scheme. This allows composition
// of clientsets, like in:
//
//	import (
//	  "k8s.io/client-go/kubernetes"
//	  clientsetscheme "k8s.io/client-go/kubernetes/scheme"
//	  aggregatorclientsetscheme "k8s.io/kube-aggregator/pkg/client/clientset_generated/clientset/scheme"
//	)
//
//	kclientset, _ := kubernetes.NewForConfig(c)
//	_ = aggregatorclientsetscheme.AddToScheme(clientsetscheme.Scheme)
//
// After this, RawExtensions in Kubernetes types will serialize kube-aggregator types
// correctly.
var AddToScheme = localSchemeBuilder.AddToScheme

func init() {
	v1.AddToGroupVersion(scheme, schema.GroupVersion{Version: "v1"})
	utilruntime.Must(AddToScheme(scheme))
}


================================================
File: pkg/generated/clientset/versioned/scheme/doc.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

// This package contains the scheme of the automatically generated clientset.
package scheme


================================================
File: pkg/generated/clientset/versioned/scheme/register.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package scheme

import (
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	zalandov1 "github.com/zalando/postgres-operator/pkg/apis/zalando.org/v1"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
	schema "k8s.io/apimachinery/pkg/runtime/schema"
	serializer "k8s.io/apimachinery/pkg/runtime/serializer"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
)

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
var ParameterCodec = runtime.NewParameterCodec(Scheme)
var localSchemeBuilder = runtime.SchemeBuilder{
	acidv1.AddToScheme,
	zalandov1.AddToScheme,
}

// AddToScheme adds all types of this clientset into the given scheme. This allows composition
// of clientsets, like in:
//
//	import (
//	  "k8s.io/client-go/kubernetes"
//	  clientsetscheme "k8s.io/client-go/kubernetes/scheme"
//	  aggregatorclientsetscheme "k8s.io/kube-aggregator/pkg/client/clientset_generated/clientset/scheme"
//	)
//
//	kclientset, _ := kubernetes.NewForConfig(c)
//	_ = aggregatorclientsetscheme.AddToScheme(clientsetscheme.Scheme)
//
// After this, RawExtensions in Kubernetes types will serialize kube-aggregator types
// correctly.
var AddToScheme = localSchemeBuilder.AddToScheme

func init() {
	v1.AddToGroupVersion(Scheme, schema.GroupVersion{Version: "v1"})
	utilruntime.Must(AddToScheme(Scheme))
}


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/acid.zalan.do_client.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package v1

import (
	"net/http"

	v1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/scheme"
	rest "k8s.io/client-go/rest"
)

type AcidV1Interface interface {
	RESTClient() rest.Interface
	OperatorConfigurationsGetter
	PostgresTeamsGetter
	PostgresqlsGetter
}

// AcidV1Client is used to interact with features provided by the acid.zalan.do group.
type AcidV1Client struct {
	restClient rest.Interface
}

func (c *AcidV1Client) OperatorConfigurations(namespace string) OperatorConfigurationInterface {
	return newOperatorConfigurations(c, namespace)
}

func (c *AcidV1Client) PostgresTeams(namespace string) PostgresTeamInterface {
	return newPostgresTeams(c, namespace)
}

func (c *AcidV1Client) Postgresqls(namespace string) PostgresqlInterface {
	return newPostgresqls(c, namespace)
}

// NewForConfig creates a new AcidV1Client for the given config.
// NewForConfig is equivalent to NewForConfigAndClient(c, httpClient),
// where httpClient was generated with rest.HTTPClientFor(c).
func NewForConfig(c *rest.Config) (*AcidV1Client, error) {
	config := *c
	if err := setConfigDefaults(&config); err != nil {
		return nil, err
	}
	httpClient, err := rest.HTTPClientFor(&config)
	if err != nil {
		return nil, err
	}
	return NewForConfigAndClient(&config, httpClient)
}

// NewForConfigAndClient creates a new AcidV1Client for the given config and http client.
// Note the http client provided takes precedence over the configured transport values.
func NewForConfigAndClient(c *rest.Config, h *http.Client) (*AcidV1Client, error) {
	config := *c
	if err := setConfigDefaults(&config); err != nil {
		return nil, err
	}
	client, err := rest.RESTClientForConfigAndClient(&config, h)
	if err != nil {
		return nil, err
	}
	return &AcidV1Client{client}, nil
}

// NewForConfigOrDie creates a new AcidV1Client for the given config and
// panics if there is an error in the config.
func NewForConfigOrDie(c *rest.Config) *AcidV1Client {
	client, err := NewForConfig(c)
	if err != nil {
		panic(err)
	}
	return client
}

// New creates a new AcidV1Client for the given RESTClient.
func New(c rest.Interface) *AcidV1Client {
	return &AcidV1Client{c}
}

func setConfigDefaults(config *rest.Config) error {
	gv := v1.SchemeGroupVersion
	config.GroupVersion = &gv
	config.APIPath = "/apis"
	config.NegotiatedSerializer = scheme.Codecs.WithoutConversion()

	if config.UserAgent == "" {
		config.UserAgent = rest.DefaultKubernetesUserAgent()
	}

	return nil
}

// RESTClient returns a RESTClient that is used to communicate
// with API server by this client implementation.
func (c *AcidV1Client) RESTClient() rest.Interface {
	if c == nil {
		return nil
	}
	return c.restClient
}


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/doc.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

// This package has the automatically generated typed clients.
package v1


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/generated_expansion.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package v1

type OperatorConfigurationExpansion interface{}

type PostgresTeamExpansion interface{}

type PostgresqlExpansion interface{}


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/operatorconfiguration.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package v1

import (
	"context"

	acidzalandov1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	scheme "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/scheme"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	rest "k8s.io/client-go/rest"
)

// OperatorConfigurationsGetter has a method to return a OperatorConfigurationInterface.
// A group's client should implement this interface.
type OperatorConfigurationsGetter interface {
	OperatorConfigurations(namespace string) OperatorConfigurationInterface
}

// OperatorConfigurationInterface has methods to work with OperatorConfiguration resources.
type OperatorConfigurationInterface interface {
	Get(ctx context.Context, name string, opts v1.GetOptions) (*acidzalandov1.OperatorConfiguration, error)
	OperatorConfigurationExpansion
}

// operatorConfigurations implements OperatorConfigurationInterface
type operatorConfigurations struct {
	client rest.Interface
	ns     string
}

// newOperatorConfigurations returns a OperatorConfigurations
func newOperatorConfigurations(c *AcidV1Client, namespace string) *operatorConfigurations {
	return &operatorConfigurations{
		client: c.RESTClient(),
		ns:     namespace,
	}
}

// Get takes name of the operatorConfiguration, and returns the corresponding operatorConfiguration object, and an error if there is any.
func (c *operatorConfigurations) Get(ctx context.Context, name string, options v1.GetOptions) (result *acidzalandov1.OperatorConfiguration, err error) {
	result = &acidzalandov1.OperatorConfiguration{}
	err = c.client.Get().
		Namespace(c.ns).
		Resource("operatorconfigurations").
		Name(name).
		VersionedParams(&options, scheme.ParameterCodec).
		Do(ctx).
		Into(result)
	return
}


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/postgresql.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package v1

import (
	"context"
	"time"

	v1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	scheme "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/scheme"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	types "k8s.io/apimachinery/pkg/types"
	watch "k8s.io/apimachinery/pkg/watch"
	rest "k8s.io/client-go/rest"
)

// PostgresqlsGetter has a method to return a PostgresqlInterface.
// A group's client should implement this interface.
type PostgresqlsGetter interface {
	Postgresqls(namespace string) PostgresqlInterface
}

// PostgresqlInterface has methods to work with Postgresql resources.
type PostgresqlInterface interface {
	Create(ctx context.Context, postgresql *v1.Postgresql, opts metav1.CreateOptions) (*v1.Postgresql, error)
	Update(ctx context.Context, postgresql *v1.Postgresql, opts metav1.UpdateOptions) (*v1.Postgresql, error)
	UpdateStatus(ctx context.Context, postgresql *v1.Postgresql, opts metav1.UpdateOptions) (*v1.Postgresql, error)
	Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error
	DeleteCollection(ctx context.Context, opts metav1.DeleteOptions, listOpts metav1.ListOptions) error
	Get(ctx context.Context, name string, opts metav1.GetOptions) (*v1.Postgresql, error)
	List(ctx context.Context, opts metav1.ListOptions) (*v1.PostgresqlList, error)
	Watch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error)
	Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions, subresources ...string) (result *v1.Postgresql, err error)
	PostgresqlExpansion
}

// postgresqls implements PostgresqlInterface
type postgresqls struct {
	client rest.Interface
	ns     string
}

// newPostgresqls returns a Postgresqls
func newPostgresqls(c *AcidV1Client, namespace string) *postgresqls {
	return &postgresqls{
		client: c.RESTClient(),
		ns:     namespace,
	}
}

// Get takes name of the postgresql, and returns the corresponding postgresql object, and an error if there is any.
func (c *postgresqls) Get(ctx context.Context, name string, options metav1.GetOptions) (result *v1.Postgresql, err error) {
	result = &v1.Postgresql{}
	err = c.client.Get().
		Namespace(c.ns).
		Resource("postgresqls").
		Name(name).
		VersionedParams(&options, scheme.ParameterCodec).
		Do(ctx).
		Into(result)
	return
}

// List takes label and field selectors, and returns the list of Postgresqls that match those selectors.
func (c *postgresqls) List(ctx context.Context, opts metav1.ListOptions) (result *v1.PostgresqlList, err error) {
	var timeout time.Duration
	if opts.TimeoutSeconds != nil {
		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
	}
	result = &v1.PostgresqlList{}
	err = c.client.Get().
		Namespace(c.ns).
		Resource("postgresqls").
		VersionedParams(&opts, scheme.ParameterCodec).
		Timeout(timeout).
		Do(ctx).
		Into(result)
	return
}

// Watch returns a watch.Interface that watches the requested postgresqls.
func (c *postgresqls) Watch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error) {
	var timeout time.Duration
	if opts.TimeoutSeconds != nil {
		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
	}
	opts.Watch = true
	return c.client.Get().
		Namespace(c.ns).
		Resource("postgresqls").
		VersionedParams(&opts, scheme.ParameterCodec).
		Timeout(timeout).
		Watch(ctx)
}

// Create takes the representation of a postgresql and creates it.  Returns the server's representation of the postgresql, and an error, if there is any.
func (c *postgresqls) Create(ctx context.Context, postgresql *v1.Postgresql, opts metav1.CreateOptions) (result *v1.Postgresql, err error) {
	result = &v1.Postgresql{}
	err = c.client.Post().
		Namespace(c.ns).
		Resource("postgresqls").
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(postgresql).
		Do(ctx).
		Into(result)
	return
}

// Update takes the representation of a postgresql and updates it. Returns the server's representation of the postgresql, and an error, if there is any.
func (c *postgresqls) Update(ctx context.Context, postgresql *v1.Postgresql, opts metav1.UpdateOptions) (result *v1.Postgresql, err error) {
	result = &v1.Postgresql{}
	err = c.client.Put().
		Namespace(c.ns).
		Resource("postgresqls").
		Name(postgresql.Name).
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(postgresql).
		Do(ctx).
		Into(result)
	return
}

// UpdateStatus was generated because the type contains a Status member.
// Add a +genclient:noStatus comment above the type to avoid generating UpdateStatus().
func (c *postgresqls) UpdateStatus(ctx context.Context, postgresql *v1.Postgresql, opts metav1.UpdateOptions) (result *v1.Postgresql, err error) {
	result = &v1.Postgresql{}
	err = c.client.Put().
		Namespace(c.ns).
		Resource("postgresqls").
		Name(postgresql.Name).
		SubResource("status").
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(postgresql).
		Do(ctx).
		Into(result)
	return
}

// Delete takes name of the postgresql and deletes it. Returns an error if one occurs.
func (c *postgresqls) Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error {
	return c.client.Delete().
		Namespace(c.ns).
		Resource("postgresqls").
		Name(name).
		Body(&opts).
		Do(ctx).
		Error()
}

// DeleteCollection deletes a collection of objects.
func (c *postgresqls) DeleteCollection(ctx context.Context, opts metav1.DeleteOptions, listOpts metav1.ListOptions) error {
	var timeout time.Duration
	if listOpts.TimeoutSeconds != nil {
		timeout = time.Duration(*listOpts.TimeoutSeconds) * time.Second
	}
	return c.client.Delete().
		Namespace(c.ns).
		Resource("postgresqls").
		VersionedParams(&listOpts, scheme.ParameterCodec).
		Timeout(timeout).
		Body(&opts).
		Do(ctx).
		Error()
}

// Patch applies the patch and returns the patched postgresql.
func (c *postgresqls) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions, subresources ...string) (result *v1.Postgresql, err error) {
	result = &v1.Postgresql{}
	err = c.client.Patch(pt).
		Namespace(c.ns).
		Resource("postgresqls").
		Name(name).
		SubResource(subresources...).
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(data).
		Do(ctx).
		Into(result)
	return
}


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/postgresteam.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package v1

import (
	"context"
	"time"

	v1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	scheme "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/scheme"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	types "k8s.io/apimachinery/pkg/types"
	watch "k8s.io/apimachinery/pkg/watch"
	rest "k8s.io/client-go/rest"
)

// PostgresTeamsGetter has a method to return a PostgresTeamInterface.
// A group's client should implement this interface.
type PostgresTeamsGetter interface {
	PostgresTeams(namespace string) PostgresTeamInterface
}

// PostgresTeamInterface has methods to work with PostgresTeam resources.
type PostgresTeamInterface interface {
	Create(ctx context.Context, postgresTeam *v1.PostgresTeam, opts metav1.CreateOptions) (*v1.PostgresTeam, error)
	Update(ctx context.Context, postgresTeam *v1.PostgresTeam, opts metav1.UpdateOptions) (*v1.PostgresTeam, error)
	Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error
	DeleteCollection(ctx context.Context, opts metav1.DeleteOptions, listOpts metav1.ListOptions) error
	Get(ctx context.Context, name string, opts metav1.GetOptions) (*v1.PostgresTeam, error)
	List(ctx context.Context, opts metav1.ListOptions) (*v1.PostgresTeamList, error)
	Watch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error)
	Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions, subresources ...string) (result *v1.PostgresTeam, err error)
	PostgresTeamExpansion
}

// postgresTeams implements PostgresTeamInterface
type postgresTeams struct {
	client rest.Interface
	ns     string
}

// newPostgresTeams returns a PostgresTeams
func newPostgresTeams(c *AcidV1Client, namespace string) *postgresTeams {
	return &postgresTeams{
		client: c.RESTClient(),
		ns:     namespace,
	}
}

// Get takes name of the postgresTeam, and returns the corresponding postgresTeam object, and an error if there is any.
func (c *postgresTeams) Get(ctx context.Context, name string, options metav1.GetOptions) (result *v1.PostgresTeam, err error) {
	result = &v1.PostgresTeam{}
	err = c.client.Get().
		Namespace(c.ns).
		Resource("postgresteams").
		Name(name).
		VersionedParams(&options, scheme.ParameterCodec).
		Do(ctx).
		Into(result)
	return
}

// List takes label and field selectors, and returns the list of PostgresTeams that match those selectors.
func (c *postgresTeams) List(ctx context.Context, opts metav1.ListOptions) (result *v1.PostgresTeamList, err error) {
	var timeout time.Duration
	if opts.TimeoutSeconds != nil {
		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
	}
	result = &v1.PostgresTeamList{}
	err = c.client.Get().
		Namespace(c.ns).
		Resource("postgresteams").
		VersionedParams(&opts, scheme.ParameterCodec).
		Timeout(timeout).
		Do(ctx).
		Into(result)
	return
}

// Watch returns a watch.Interface that watches the requested postgresTeams.
func (c *postgresTeams) Watch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error) {
	var timeout time.Duration
	if opts.TimeoutSeconds != nil {
		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
	}
	opts.Watch = true
	return c.client.Get().
		Namespace(c.ns).
		Resource("postgresteams").
		VersionedParams(&opts, scheme.ParameterCodec).
		Timeout(timeout).
		Watch(ctx)
}

// Create takes the representation of a postgresTeam and creates it.  Returns the server's representation of the postgresTeam, and an error, if there is any.
func (c *postgresTeams) Create(ctx context.Context, postgresTeam *v1.PostgresTeam, opts metav1.CreateOptions) (result *v1.PostgresTeam, err error) {
	result = &v1.PostgresTeam{}
	err = c.client.Post().
		Namespace(c.ns).
		Resource("postgresteams").
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(postgresTeam).
		Do(ctx).
		Into(result)
	return
}

// Update takes the representation of a postgresTeam and updates it. Returns the server's representation of the postgresTeam, and an error, if there is any.
func (c *postgresTeams) Update(ctx context.Context, postgresTeam *v1.PostgresTeam, opts metav1.UpdateOptions) (result *v1.PostgresTeam, err error) {
	result = &v1.PostgresTeam{}
	err = c.client.Put().
		Namespace(c.ns).
		Resource("postgresteams").
		Name(postgresTeam.Name).
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(postgresTeam).
		Do(ctx).
		Into(result)
	return
}

// Delete takes name of the postgresTeam and deletes it. Returns an error if one occurs.
func (c *postgresTeams) Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error {
	return c.client.Delete().
		Namespace(c.ns).
		Resource("postgresteams").
		Name(name).
		Body(&opts).
		Do(ctx).
		Error()
}

// DeleteCollection deletes a collection of objects.
func (c *postgresTeams) DeleteCollection(ctx context.Context, opts metav1.DeleteOptions, listOpts metav1.ListOptions) error {
	var timeout time.Duration
	if listOpts.TimeoutSeconds != nil {
		timeout = time.Duration(*listOpts.TimeoutSeconds) * time.Second
	}
	return c.client.Delete().
		Namespace(c.ns).
		Resource("postgresteams").
		VersionedParams(&listOpts, scheme.ParameterCodec).
		Timeout(timeout).
		Body(&opts).
		Do(ctx).
		Error()
}

// Patch applies the patch and returns the patched postgresTeam.
func (c *postgresTeams) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions, subresources ...string) (result *v1.PostgresTeam, err error) {
	result = &v1.PostgresTeam{}
	err = c.client.Patch(pt).
		Namespace(c.ns).
		Resource("postgresteams").
		Name(name).
		SubResource(subresources...).
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(data).
		Do(ctx).
		Into(result)
	return
}


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/fake/doc.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

// Package fake has the automatically generated clients.
package fake


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/fake/fake_acid.zalan.do_client.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package fake

import (
	v1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	rest "k8s.io/client-go/rest"
	testing "k8s.io/client-go/testing"
)

type FakeAcidV1 struct {
	*testing.Fake
}

func (c *FakeAcidV1) OperatorConfigurations(namespace string) v1.OperatorConfigurationInterface {
	return &FakeOperatorConfigurations{c, namespace}
}

func (c *FakeAcidV1) PostgresTeams(namespace string) v1.PostgresTeamInterface {
	return &FakePostgresTeams{c, namespace}
}

func (c *FakeAcidV1) Postgresqls(namespace string) v1.PostgresqlInterface {
	return &FakePostgresqls{c, namespace}
}

// RESTClient returns a RESTClient that is used to communicate
// with API server by this client implementation.
func (c *FakeAcidV1) RESTClient() rest.Interface {
	var ret *rest.RESTClient
	return ret
}


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/fake/fake_operatorconfiguration.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package fake

import (
	"context"

	acidzalandov1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	schema "k8s.io/apimachinery/pkg/runtime/schema"
	testing "k8s.io/client-go/testing"
)

// FakeOperatorConfigurations implements OperatorConfigurationInterface
type FakeOperatorConfigurations struct {
	Fake *FakeAcidV1
	ns   string
}

var operatorconfigurationsResource = schema.GroupVersionResource{Group: "acid.zalan.do", Version: "v1", Resource: "operatorconfigurations"}

var operatorconfigurationsKind = schema.GroupVersionKind{Group: "acid.zalan.do", Version: "v1", Kind: "OperatorConfiguration"}

// Get takes name of the operatorConfiguration, and returns the corresponding operatorConfiguration object, and an error if there is any.
func (c *FakeOperatorConfigurations) Get(ctx context.Context, name string, options v1.GetOptions) (result *acidzalandov1.OperatorConfiguration, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewGetAction(operatorconfigurationsResource, c.ns, name), &acidzalandov1.OperatorConfiguration{})

	if obj == nil {
		return nil, err
	}
	return obj.(*acidzalandov1.OperatorConfiguration), err
}


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/fake/fake_postgresql.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package fake

import (
	"context"

	acidzalandov1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	labels "k8s.io/apimachinery/pkg/labels"
	schema "k8s.io/apimachinery/pkg/runtime/schema"
	types "k8s.io/apimachinery/pkg/types"
	watch "k8s.io/apimachinery/pkg/watch"
	testing "k8s.io/client-go/testing"
)

// FakePostgresqls implements PostgresqlInterface
type FakePostgresqls struct {
	Fake *FakeAcidV1
	ns   string
}

var postgresqlsResource = schema.GroupVersionResource{Group: "acid.zalan.do", Version: "v1", Resource: "postgresqls"}

var postgresqlsKind = schema.GroupVersionKind{Group: "acid.zalan.do", Version: "v1", Kind: "Postgresql"}

// Get takes name of the postgresql, and returns the corresponding postgresql object, and an error if there is any.
func (c *FakePostgresqls) Get(ctx context.Context, name string, options v1.GetOptions) (result *acidzalandov1.Postgresql, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewGetAction(postgresqlsResource, c.ns, name), &acidzalandov1.Postgresql{})

	if obj == nil {
		return nil, err
	}
	return obj.(*acidzalandov1.Postgresql), err
}

// List takes label and field selectors, and returns the list of Postgresqls that match those selectors.
func (c *FakePostgresqls) List(ctx context.Context, opts v1.ListOptions) (result *acidzalandov1.PostgresqlList, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewListAction(postgresqlsResource, postgresqlsKind, c.ns, opts), &acidzalandov1.PostgresqlList{})

	if obj == nil {
		return nil, err
	}

	label, _, _ := testing.ExtractFromListOptions(opts)
	if label == nil {
		label = labels.Everything()
	}
	list := &acidzalandov1.PostgresqlList{ListMeta: obj.(*acidzalandov1.PostgresqlList).ListMeta}
	for _, item := range obj.(*acidzalandov1.PostgresqlList).Items {
		if label.Matches(labels.Set(item.Labels)) {
			list.Items = append(list.Items, item)
		}
	}
	return list, err
}

// Watch returns a watch.Interface that watches the requested postgresqls.
func (c *FakePostgresqls) Watch(ctx context.Context, opts v1.ListOptions) (watch.Interface, error) {
	return c.Fake.
		InvokesWatch(testing.NewWatchAction(postgresqlsResource, c.ns, opts))

}

// Create takes the representation of a postgresql and creates it.  Returns the server's representation of the postgresql, and an error, if there is any.
func (c *FakePostgresqls) Create(ctx context.Context, postgresql *acidzalandov1.Postgresql, opts v1.CreateOptions) (result *acidzalandov1.Postgresql, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewCreateAction(postgresqlsResource, c.ns, postgresql), &acidzalandov1.Postgresql{})

	if obj == nil {
		return nil, err
	}
	return obj.(*acidzalandov1.Postgresql), err
}

// Update takes the representation of a postgresql and updates it. Returns the server's representation of the postgresql, and an error, if there is any.
func (c *FakePostgresqls) Update(ctx context.Context, postgresql *acidzalandov1.Postgresql, opts v1.UpdateOptions) (result *acidzalandov1.Postgresql, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewUpdateAction(postgresqlsResource, c.ns, postgresql), &acidzalandov1.Postgresql{})

	if obj == nil {
		return nil, err
	}
	return obj.(*acidzalandov1.Postgresql), err
}

// UpdateStatus was generated because the type contains a Status member.
// Add a +genclient:noStatus comment above the type to avoid generating UpdateStatus().
func (c *FakePostgresqls) UpdateStatus(ctx context.Context, postgresql *acidzalandov1.Postgresql, opts v1.UpdateOptions) (*acidzalandov1.Postgresql, error) {
	obj, err := c.Fake.
		Invokes(testing.NewUpdateSubresourceAction(postgresqlsResource, "status", c.ns, postgresql), &acidzalandov1.Postgresql{})

	if obj == nil {
		return nil, err
	}
	return obj.(*acidzalandov1.Postgresql), err
}

// Delete takes name of the postgresql and deletes it. Returns an error if one occurs.
func (c *FakePostgresqls) Delete(ctx context.Context, name string, opts v1.DeleteOptions) error {
	_, err := c.Fake.
		Invokes(testing.NewDeleteActionWithOptions(postgresqlsResource, c.ns, name, opts), &acidzalandov1.Postgresql{})

	return err
}

// DeleteCollection deletes a collection of objects.
func (c *FakePostgresqls) DeleteCollection(ctx context.Context, opts v1.DeleteOptions, listOpts v1.ListOptions) error {
	action := testing.NewDeleteCollectionAction(postgresqlsResource, c.ns, listOpts)

	_, err := c.Fake.Invokes(action, &acidzalandov1.PostgresqlList{})
	return err
}

// Patch applies the patch and returns the patched postgresql.
func (c *FakePostgresqls) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) (result *acidzalandov1.Postgresql, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewPatchSubresourceAction(postgresqlsResource, c.ns, name, pt, data, subresources...), &acidzalandov1.Postgresql{})

	if obj == nil {
		return nil, err
	}
	return obj.(*acidzalandov1.Postgresql), err
}


================================================
File: pkg/generated/clientset/versioned/typed/acid.zalan.do/v1/fake/fake_postgresteam.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package fake

import (
	"context"

	acidzalandov1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	labels "k8s.io/apimachinery/pkg/labels"
	schema "k8s.io/apimachinery/pkg/runtime/schema"
	types "k8s.io/apimachinery/pkg/types"
	watch "k8s.io/apimachinery/pkg/watch"
	testing "k8s.io/client-go/testing"
)

// FakePostgresTeams implements PostgresTeamInterface
type FakePostgresTeams struct {
	Fake *FakeAcidV1
	ns   string
}

var postgresteamsResource = schema.GroupVersionResource{Group: "acid.zalan.do", Version: "v1", Resource: "postgresteams"}

var postgresteamsKind = schema.GroupVersionKind{Group: "acid.zalan.do", Version: "v1", Kind: "PostgresTeam"}

// Get takes name of the postgresTeam, and returns the corresponding postgresTeam object, and an error if there is any.
func (c *FakePostgresTeams) Get(ctx context.Context, name string, options v1.GetOptions) (result *acidzalandov1.PostgresTeam, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewGetAction(postgresteamsResource, c.ns, name), &acidzalandov1.PostgresTeam{})

	if obj == nil {
		return nil, err
	}
	return obj.(*acidzalandov1.PostgresTeam), err
}

// List takes label and field selectors, and returns the list of PostgresTeams that match those selectors.
func (c *FakePostgresTeams) List(ctx context.Context, opts v1.ListOptions) (result *acidzalandov1.PostgresTeamList, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewListAction(postgresteamsResource, postgresteamsKind, c.ns, opts), &acidzalandov1.PostgresTeamList{})

	if obj == nil {
		return nil, err
	}

	label, _, _ := testing.ExtractFromListOptions(opts)
	if label == nil {
		label = labels.Everything()
	}
	list := &acidzalandov1.PostgresTeamList{ListMeta: obj.(*acidzalandov1.PostgresTeamList).ListMeta}
	for _, item := range obj.(*acidzalandov1.PostgresTeamList).Items {
		if label.Matches(labels.Set(item.Labels)) {
			list.Items = append(list.Items, item)
		}
	}
	return list, err
}

// Watch returns a watch.Interface that watches the requested postgresTeams.
func (c *FakePostgresTeams) Watch(ctx context.Context, opts v1.ListOptions) (watch.Interface, error) {
	return c.Fake.
		InvokesWatch(testing.NewWatchAction(postgresteamsResource, c.ns, opts))

}

// Create takes the representation of a postgresTeam and creates it.  Returns the server's representation of the postgresTeam, and an error, if there is any.
func (c *FakePostgresTeams) Create(ctx context.Context, postgresTeam *acidzalandov1.PostgresTeam, opts v1.CreateOptions) (result *acidzalandov1.PostgresTeam, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewCreateAction(postgresteamsResource, c.ns, postgresTeam), &acidzalandov1.PostgresTeam{})

	if obj == nil {
		return nil, err
	}
	return obj.(*acidzalandov1.PostgresTeam), err
}

// Update takes the representation of a postgresTeam and updates it. Returns the server's representation of the postgresTeam, and an error, if there is any.
func (c *FakePostgresTeams) Update(ctx context.Context, postgresTeam *acidzalandov1.PostgresTeam, opts v1.UpdateOptions) (result *acidzalandov1.PostgresTeam, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewUpdateAction(postgresteamsResource, c.ns, postgresTeam), &acidzalandov1.PostgresTeam{})

	if obj == nil {
		return nil, err
	}
	return obj.(*acidzalandov1.PostgresTeam), err
}

// Delete takes name of the postgresTeam and deletes it. Returns an error if one occurs.
func (c *FakePostgresTeams) Delete(ctx context.Context, name string, opts v1.DeleteOptions) error {
	_, err := c.Fake.
		Invokes(testing.NewDeleteActionWithOptions(postgresteamsResource, c.ns, name, opts), &acidzalandov1.PostgresTeam{})

	return err
}

// DeleteCollection deletes a collection of objects.
func (c *FakePostgresTeams) DeleteCollection(ctx context.Context, opts v1.DeleteOptions, listOpts v1.ListOptions) error {
	action := testing.NewDeleteCollectionAction(postgresteamsResource, c.ns, listOpts)

	_, err := c.Fake.Invokes(action, &acidzalandov1.PostgresTeamList{})
	return err
}

// Patch applies the patch and returns the patched postgresTeam.
func (c *FakePostgresTeams) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) (result *acidzalandov1.PostgresTeam, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewPatchSubresourceAction(postgresteamsResource, c.ns, name, pt, data, subresources...), &acidzalandov1.PostgresTeam{})

	if obj == nil {
		return nil, err
	}
	return obj.(*acidzalandov1.PostgresTeam), err
}


================================================
File: pkg/generated/clientset/versioned/typed/zalando.org/v1/doc.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

// This package has the automatically generated typed clients.
package v1


================================================
File: pkg/generated/clientset/versioned/typed/zalando.org/v1/fabriceventstream.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package v1

import (
	"context"
	"time"

	v1 "github.com/zalando/postgres-operator/pkg/apis/zalando.org/v1"
	scheme "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/scheme"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	types "k8s.io/apimachinery/pkg/types"
	watch "k8s.io/apimachinery/pkg/watch"
	rest "k8s.io/client-go/rest"
)

// FabricEventStreamsGetter has a method to return a FabricEventStreamInterface.
// A group's client should implement this interface.
type FabricEventStreamsGetter interface {
	FabricEventStreams(namespace string) FabricEventStreamInterface
}

// FabricEventStreamInterface has methods to work with FabricEventStream resources.
type FabricEventStreamInterface interface {
	Create(ctx context.Context, fabricEventStream *v1.FabricEventStream, opts metav1.CreateOptions) (*v1.FabricEventStream, error)
	Update(ctx context.Context, fabricEventStream *v1.FabricEventStream, opts metav1.UpdateOptions) (*v1.FabricEventStream, error)
	Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error
	DeleteCollection(ctx context.Context, opts metav1.DeleteOptions, listOpts metav1.ListOptions) error
	Get(ctx context.Context, name string, opts metav1.GetOptions) (*v1.FabricEventStream, error)
	List(ctx context.Context, opts metav1.ListOptions) (*v1.FabricEventStreamList, error)
	Watch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error)
	Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions, subresources ...string) (result *v1.FabricEventStream, err error)
	FabricEventStreamExpansion
}

// fabricEventStreams implements FabricEventStreamInterface
type fabricEventStreams struct {
	client rest.Interface
	ns     string
}

// newFabricEventStreams returns a FabricEventStreams
func newFabricEventStreams(c *ZalandoV1Client, namespace string) *fabricEventStreams {
	return &fabricEventStreams{
		client: c.RESTClient(),
		ns:     namespace,
	}
}

// Get takes name of the fabricEventStream, and returns the corresponding fabricEventStream object, and an error if there is any.
func (c *fabricEventStreams) Get(ctx context.Context, name string, options metav1.GetOptions) (result *v1.FabricEventStream, err error) {
	result = &v1.FabricEventStream{}
	err = c.client.Get().
		Namespace(c.ns).
		Resource("fabriceventstreams").
		Name(name).
		VersionedParams(&options, scheme.ParameterCodec).
		Do(ctx).
		Into(result)
	return
}

// List takes label and field selectors, and returns the list of FabricEventStreams that match those selectors.
func (c *fabricEventStreams) List(ctx context.Context, opts metav1.ListOptions) (result *v1.FabricEventStreamList, err error) {
	var timeout time.Duration
	if opts.TimeoutSeconds != nil {
		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
	}
	result = &v1.FabricEventStreamList{}
	err = c.client.Get().
		Namespace(c.ns).
		Resource("fabriceventstreams").
		VersionedParams(&opts, scheme.ParameterCodec).
		Timeout(timeout).
		Do(ctx).
		Into(result)
	return
}

// Watch returns a watch.Interface that watches the requested fabricEventStreams.
func (c *fabricEventStreams) Watch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error) {
	var timeout time.Duration
	if opts.TimeoutSeconds != nil {
		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
	}
	opts.Watch = true
	return c.client.Get().
		Namespace(c.ns).
		Resource("fabriceventstreams").
		VersionedParams(&opts, scheme.ParameterCodec).
		Timeout(timeout).
		Watch(ctx)
}

// Create takes the representation of a fabricEventStream and creates it.  Returns the server's representation of the fabricEventStream, and an error, if there is any.
func (c *fabricEventStreams) Create(ctx context.Context, fabricEventStream *v1.FabricEventStream, opts metav1.CreateOptions) (result *v1.FabricEventStream, err error) {
	result = &v1.FabricEventStream{}
	err = c.client.Post().
		Namespace(c.ns).
		Resource("fabriceventstreams").
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(fabricEventStream).
		Do(ctx).
		Into(result)
	return
}

// Update takes the representation of a fabricEventStream and updates it. Returns the server's representation of the fabricEventStream, and an error, if there is any.
func (c *fabricEventStreams) Update(ctx context.Context, fabricEventStream *v1.FabricEventStream, opts metav1.UpdateOptions) (result *v1.FabricEventStream, err error) {
	result = &v1.FabricEventStream{}
	err = c.client.Put().
		Namespace(c.ns).
		Resource("fabriceventstreams").
		Name(fabricEventStream.Name).
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(fabricEventStream).
		Do(ctx).
		Into(result)
	return
}

// Delete takes name of the fabricEventStream and deletes it. Returns an error if one occurs.
func (c *fabricEventStreams) Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error {
	return c.client.Delete().
		Namespace(c.ns).
		Resource("fabriceventstreams").
		Name(name).
		Body(&opts).
		Do(ctx).
		Error()
}

// DeleteCollection deletes a collection of objects.
func (c *fabricEventStreams) DeleteCollection(ctx context.Context, opts metav1.DeleteOptions, listOpts metav1.ListOptions) error {
	var timeout time.Duration
	if listOpts.TimeoutSeconds != nil {
		timeout = time.Duration(*listOpts.TimeoutSeconds) * time.Second
	}
	return c.client.Delete().
		Namespace(c.ns).
		Resource("fabriceventstreams").
		VersionedParams(&listOpts, scheme.ParameterCodec).
		Timeout(timeout).
		Body(&opts).
		Do(ctx).
		Error()
}

// Patch applies the patch and returns the patched fabricEventStream.
func (c *fabricEventStreams) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions, subresources ...string) (result *v1.FabricEventStream, err error) {
	result = &v1.FabricEventStream{}
	err = c.client.Patch(pt).
		Namespace(c.ns).
		Resource("fabriceventstreams").
		Name(name).
		SubResource(subresources...).
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(data).
		Do(ctx).
		Into(result)
	return
}


================================================
File: pkg/generated/clientset/versioned/typed/zalando.org/v1/generated_expansion.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package v1

type FabricEventStreamExpansion interface{}


================================================
File: pkg/generated/clientset/versioned/typed/zalando.org/v1/zalando.org_client.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package v1

import (
	"net/http"

	v1 "github.com/zalando/postgres-operator/pkg/apis/zalando.org/v1"
	"github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/scheme"
	rest "k8s.io/client-go/rest"
)

type ZalandoV1Interface interface {
	RESTClient() rest.Interface
	FabricEventStreamsGetter
}

// ZalandoV1Client is used to interact with features provided by the zalando.org group.
type ZalandoV1Client struct {
	restClient rest.Interface
}

func (c *ZalandoV1Client) FabricEventStreams(namespace string) FabricEventStreamInterface {
	return newFabricEventStreams(c, namespace)
}

// NewForConfig creates a new ZalandoV1Client for the given config.
// NewForConfig is equivalent to NewForConfigAndClient(c, httpClient),
// where httpClient was generated with rest.HTTPClientFor(c).
func NewForConfig(c *rest.Config) (*ZalandoV1Client, error) {
	config := *c
	if err := setConfigDefaults(&config); err != nil {
		return nil, err
	}
	httpClient, err := rest.HTTPClientFor(&config)
	if err != nil {
		return nil, err
	}
	return NewForConfigAndClient(&config, httpClient)
}

// NewForConfigAndClient creates a new ZalandoV1Client for the given config and http client.
// Note the http client provided takes precedence over the configured transport values.
func NewForConfigAndClient(c *rest.Config, h *http.Client) (*ZalandoV1Client, error) {
	config := *c
	if err := setConfigDefaults(&config); err != nil {
		return nil, err
	}
	client, err := rest.RESTClientForConfigAndClient(&config, h)
	if err != nil {
		return nil, err
	}
	return &ZalandoV1Client{client}, nil
}

// NewForConfigOrDie creates a new ZalandoV1Client for the given config and
// panics if there is an error in the config.
func NewForConfigOrDie(c *rest.Config) *ZalandoV1Client {
	client, err := NewForConfig(c)
	if err != nil {
		panic(err)
	}
	return client
}

// New creates a new ZalandoV1Client for the given RESTClient.
func New(c rest.Interface) *ZalandoV1Client {
	return &ZalandoV1Client{c}
}

func setConfigDefaults(config *rest.Config) error {
	gv := v1.SchemeGroupVersion
	config.GroupVersion = &gv
	config.APIPath = "/apis"
	config.NegotiatedSerializer = scheme.Codecs.WithoutConversion()

	if config.UserAgent == "" {
		config.UserAgent = rest.DefaultKubernetesUserAgent()
	}

	return nil
}

// RESTClient returns a RESTClient that is used to communicate
// with API server by this client implementation.
func (c *ZalandoV1Client) RESTClient() rest.Interface {
	if c == nil {
		return nil
	}
	return c.restClient
}


================================================
File: pkg/generated/clientset/versioned/typed/zalando.org/v1/fake/doc.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

// Package fake has the automatically generated clients.
package fake


================================================
File: pkg/generated/clientset/versioned/typed/zalando.org/v1/fake/fake_fabriceventstream.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package fake

import (
	"context"

	zalandoorgv1 "github.com/zalando/postgres-operator/pkg/apis/zalando.org/v1"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	labels "k8s.io/apimachinery/pkg/labels"
	schema "k8s.io/apimachinery/pkg/runtime/schema"
	types "k8s.io/apimachinery/pkg/types"
	watch "k8s.io/apimachinery/pkg/watch"
	testing "k8s.io/client-go/testing"
)

// FakeFabricEventStreams implements FabricEventStreamInterface
type FakeFabricEventStreams struct {
	Fake *FakeZalandoV1
	ns   string
}

var fabriceventstreamsResource = schema.GroupVersionResource{Group: "zalando.org", Version: "v1", Resource: "fabriceventstreams"}

var fabriceventstreamsKind = schema.GroupVersionKind{Group: "zalando.org", Version: "v1", Kind: "FabricEventStream"}

// Get takes name of the fabricEventStream, and returns the corresponding fabricEventStream object, and an error if there is any.
func (c *FakeFabricEventStreams) Get(ctx context.Context, name string, options v1.GetOptions) (result *zalandoorgv1.FabricEventStream, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewGetAction(fabriceventstreamsResource, c.ns, name), &zalandoorgv1.FabricEventStream{})

	if obj == nil {
		return nil, err
	}
	return obj.(*zalandoorgv1.FabricEventStream), err
}

// List takes label and field selectors, and returns the list of FabricEventStreams that match those selectors.
func (c *FakeFabricEventStreams) List(ctx context.Context, opts v1.ListOptions) (result *zalandoorgv1.FabricEventStreamList, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewListAction(fabriceventstreamsResource, fabriceventstreamsKind, c.ns, opts), &zalandoorgv1.FabricEventStreamList{})

	if obj == nil {
		return nil, err
	}

	label, _, _ := testing.ExtractFromListOptions(opts)
	if label == nil {
		label = labels.Everything()
	}
	list := &zalandoorgv1.FabricEventStreamList{ListMeta: obj.(*zalandoorgv1.FabricEventStreamList).ListMeta}
	for _, item := range obj.(*zalandoorgv1.FabricEventStreamList).Items {
		if label.Matches(labels.Set(item.Labels)) {
			list.Items = append(list.Items, item)
		}
	}
	return list, err
}

// Watch returns a watch.Interface that watches the requested fabricEventStreams.
func (c *FakeFabricEventStreams) Watch(ctx context.Context, opts v1.ListOptions) (watch.Interface, error) {
	return c.Fake.
		InvokesWatch(testing.NewWatchAction(fabriceventstreamsResource, c.ns, opts))

}

// Create takes the representation of a fabricEventStream and creates it.  Returns the server's representation of the fabricEventStream, and an error, if there is any.
func (c *FakeFabricEventStreams) Create(ctx context.Context, fabricEventStream *zalandoorgv1.FabricEventStream, opts v1.CreateOptions) (result *zalandoorgv1.FabricEventStream, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewCreateAction(fabriceventstreamsResource, c.ns, fabricEventStream), &zalandoorgv1.FabricEventStream{})

	if obj == nil {
		return nil, err
	}
	return obj.(*zalandoorgv1.FabricEventStream), err
}

// Update takes the representation of a fabricEventStream and updates it. Returns the server's representation of the fabricEventStream, and an error, if there is any.
func (c *FakeFabricEventStreams) Update(ctx context.Context, fabricEventStream *zalandoorgv1.FabricEventStream, opts v1.UpdateOptions) (result *zalandoorgv1.FabricEventStream, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewUpdateAction(fabriceventstreamsResource, c.ns, fabricEventStream), &zalandoorgv1.FabricEventStream{})

	if obj == nil {
		return nil, err
	}
	return obj.(*zalandoorgv1.FabricEventStream), err
}

// Delete takes name of the fabricEventStream and deletes it. Returns an error if one occurs.
func (c *FakeFabricEventStreams) Delete(ctx context.Context, name string, opts v1.DeleteOptions) error {
	_, err := c.Fake.
		Invokes(testing.NewDeleteActionWithOptions(fabriceventstreamsResource, c.ns, name, opts), &zalandoorgv1.FabricEventStream{})

	return err
}

// DeleteCollection deletes a collection of objects.
func (c *FakeFabricEventStreams) DeleteCollection(ctx context.Context, opts v1.DeleteOptions, listOpts v1.ListOptions) error {
	action := testing.NewDeleteCollectionAction(fabriceventstreamsResource, c.ns, listOpts)

	_, err := c.Fake.Invokes(action, &zalandoorgv1.FabricEventStreamList{})
	return err
}

// Patch applies the patch and returns the patched fabricEventStream.
func (c *FakeFabricEventStreams) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) (result *zalandoorgv1.FabricEventStream, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewPatchSubresourceAction(fabriceventstreamsResource, c.ns, name, pt, data, subresources...), &zalandoorgv1.FabricEventStream{})

	if obj == nil {
		return nil, err
	}
	return obj.(*zalandoorgv1.FabricEventStream), err
}


================================================
File: pkg/generated/clientset/versioned/typed/zalando.org/v1/fake/fake_zalando.org_client.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by client-gen. DO NOT EDIT.

package fake

import (
	v1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/zalando.org/v1"
	rest "k8s.io/client-go/rest"
	testing "k8s.io/client-go/testing"
)

type FakeZalandoV1 struct {
	*testing.Fake
}

func (c *FakeZalandoV1) FabricEventStreams(namespace string) v1.FabricEventStreamInterface {
	return &FakeFabricEventStreams{c, namespace}
}

// RESTClient returns a RESTClient that is used to communicate
// with API server by this client implementation.
func (c *FakeZalandoV1) RESTClient() rest.Interface {
	var ret *rest.RESTClient
	return ret
}


================================================
File: pkg/generated/informers/externalversions/factory.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by informer-gen. DO NOT EDIT.

package externalversions

import (
	reflect "reflect"
	sync "sync"
	time "time"

	versioned "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned"
	acidzalando "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/acid.zalan.do"
	internalinterfaces "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/internalinterfaces"
	zalandoorg "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/zalando.org"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
	schema "k8s.io/apimachinery/pkg/runtime/schema"
	cache "k8s.io/client-go/tools/cache"
)

// SharedInformerOption defines the functional option type for SharedInformerFactory.
type SharedInformerOption func(*sharedInformerFactory) *sharedInformerFactory

type sharedInformerFactory struct {
	client           versioned.Interface
	namespace        string
	tweakListOptions internalinterfaces.TweakListOptionsFunc
	lock             sync.Mutex
	defaultResync    time.Duration
	customResync     map[reflect.Type]time.Duration

	informers map[reflect.Type]cache.SharedIndexInformer
	// startedInformers is used for tracking which informers have been started.
	// This allows Start() to be called multiple times safely.
	startedInformers map[reflect.Type]bool
}

// WithCustomResyncConfig sets a custom resync period for the specified informer types.
func WithCustomResyncConfig(resyncConfig map[v1.Object]time.Duration) SharedInformerOption {
	return func(factory *sharedInformerFactory) *sharedInformerFactory {
		for k, v := range resyncConfig {
			factory.customResync[reflect.TypeOf(k)] = v
		}
		return factory
	}
}

// WithTweakListOptions sets a custom filter on all listers of the configured SharedInformerFactory.
func WithTweakListOptions(tweakListOptions internalinterfaces.TweakListOptionsFunc) SharedInformerOption {
	return func(factory *sharedInformerFactory) *sharedInformerFactory {
		factory.tweakListOptions = tweakListOptions
		return factory
	}
}

// WithNamespace limits the SharedInformerFactory to the specified namespace.
func WithNamespace(namespace string) SharedInformerOption {
	return func(factory *sharedInformerFactory) *sharedInformerFactory {
		factory.namespace = namespace
		return factory
	}
}

// NewSharedInformerFactory constructs a new instance of sharedInformerFactory for all namespaces.
func NewSharedInformerFactory(client versioned.Interface, defaultResync time.Duration) SharedInformerFactory {
	return NewSharedInformerFactoryWithOptions(client, defaultResync)
}

// NewFilteredSharedInformerFactory constructs a new instance of sharedInformerFactory.
// Listers obtained via this SharedInformerFactory will be subject to the same filters
// as specified here.
// Deprecated: Please use NewSharedInformerFactoryWithOptions instead
func NewFilteredSharedInformerFactory(client versioned.Interface, defaultResync time.Duration, namespace string, tweakListOptions internalinterfaces.TweakListOptionsFunc) SharedInformerFactory {
	return NewSharedInformerFactoryWithOptions(client, defaultResync, WithNamespace(namespace), WithTweakListOptions(tweakListOptions))
}

// NewSharedInformerFactoryWithOptions constructs a new instance of a SharedInformerFactory with additional options.
func NewSharedInformerFactoryWithOptions(client versioned.Interface, defaultResync time.Duration, options ...SharedInformerOption) SharedInformerFactory {
	factory := &sharedInformerFactory{
		client:           client,
		namespace:        v1.NamespaceAll,
		defaultResync:    defaultResync,
		informers:        make(map[reflect.Type]cache.SharedIndexInformer),
		startedInformers: make(map[reflect.Type]bool),
		customResync:     make(map[reflect.Type]time.Duration),
	}

	// Apply all options
	for _, opt := range options {
		factory = opt(factory)
	}

	return factory
}

// Start initializes all requested informers.
func (f *sharedInformerFactory) Start(stopCh <-chan struct{}) {
	f.lock.Lock()
	defer f.lock.Unlock()

	for informerType, informer := range f.informers {
		if !f.startedInformers[informerType] {
			go informer.Run(stopCh)
			f.startedInformers[informerType] = true
		}
	}
}

// WaitForCacheSync waits for all started informers' cache were synced.
func (f *sharedInformerFactory) WaitForCacheSync(stopCh <-chan struct{}) map[reflect.Type]bool {
	informers := func() map[reflect.Type]cache.SharedIndexInformer {
		f.lock.Lock()
		defer f.lock.Unlock()

		informers := map[reflect.Type]cache.SharedIndexInformer{}
		for informerType, informer := range f.informers {
			if f.startedInformers[informerType] {
				informers[informerType] = informer
			}
		}
		return informers
	}()

	res := map[reflect.Type]bool{}
	for informType, informer := range informers {
		res[informType] = cache.WaitForCacheSync(stopCh, informer.HasSynced)
	}
	return res
}

// InternalInformerFor returns the SharedIndexInformer for obj using an internal
// client.
func (f *sharedInformerFactory) InformerFor(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer {
	f.lock.Lock()
	defer f.lock.Unlock()

	informerType := reflect.TypeOf(obj)
	informer, exists := f.informers[informerType]
	if exists {
		return informer
	}

	resyncPeriod, exists := f.customResync[informerType]
	if !exists {
		resyncPeriod = f.defaultResync
	}

	informer = newFunc(f.client, resyncPeriod)
	f.informers[informerType] = informer

	return informer
}

// SharedInformerFactory provides shared informers for resources in all known
// API group versions.
type SharedInformerFactory interface {
	internalinterfaces.SharedInformerFactory
	ForResource(resource schema.GroupVersionResource) (GenericInformer, error)
	WaitForCacheSync(stopCh <-chan struct{}) map[reflect.Type]bool

	Acid() acidzalando.Interface
	Zalando() zalandoorg.Interface
}

func (f *sharedInformerFactory) Acid() acidzalando.Interface {
	return acidzalando.New(f, f.namespace, f.tweakListOptions)
}

func (f *sharedInformerFactory) Zalando() zalandoorg.Interface {
	return zalandoorg.New(f, f.namespace, f.tweakListOptions)
}


================================================
File: pkg/generated/informers/externalversions/generic.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by informer-gen. DO NOT EDIT.

package externalversions

import (
	"fmt"

	v1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	zalandoorgv1 "github.com/zalando/postgres-operator/pkg/apis/zalando.org/v1"
	schema "k8s.io/apimachinery/pkg/runtime/schema"
	cache "k8s.io/client-go/tools/cache"
)

// GenericInformer is type of SharedIndexInformer which will locate and delegate to other
// sharedInformers based on type
type GenericInformer interface {
	Informer() cache.SharedIndexInformer
	Lister() cache.GenericLister
}

type genericInformer struct {
	informer cache.SharedIndexInformer
	resource schema.GroupResource
}

// Informer returns the SharedIndexInformer.
func (f *genericInformer) Informer() cache.SharedIndexInformer {
	return f.informer
}

// Lister returns the GenericLister.
func (f *genericInformer) Lister() cache.GenericLister {
	return cache.NewGenericLister(f.Informer().GetIndexer(), f.resource)
}

// ForResource gives generic access to a shared informer of the matching type
// TODO extend this to unknown resources with a client pool
func (f *sharedInformerFactory) ForResource(resource schema.GroupVersionResource) (GenericInformer, error) {
	switch resource {
	// Group=acid.zalan.do, Version=v1
	case v1.SchemeGroupVersion.WithResource("postgresteams"):
		return &genericInformer{resource: resource.GroupResource(), informer: f.Acid().V1().PostgresTeams().Informer()}, nil
	case v1.SchemeGroupVersion.WithResource("postgresqls"):
		return &genericInformer{resource: resource.GroupResource(), informer: f.Acid().V1().Postgresqls().Informer()}, nil

		// Group=zalando.org, Version=v1
	case zalandoorgv1.SchemeGroupVersion.WithResource("fabriceventstreams"):
		return &genericInformer{resource: resource.GroupResource(), informer: f.Zalando().V1().FabricEventStreams().Informer()}, nil

	}

	return nil, fmt.Errorf("no informer found for %v", resource)
}


================================================
File: pkg/generated/informers/externalversions/acid.zalan.do/interface.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by informer-gen. DO NOT EDIT.

package acid

import (
	v1 "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/acid.zalan.do/v1"
	internalinterfaces "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/internalinterfaces"
)

// Interface provides access to each of this group's versions.
type Interface interface {
	// V1 provides access to shared informers for resources in V1.
	V1() v1.Interface
}

type group struct {
	factory          internalinterfaces.SharedInformerFactory
	namespace        string
	tweakListOptions internalinterfaces.TweakListOptionsFunc
}

// New returns a new Interface.
func New(f internalinterfaces.SharedInformerFactory, namespace string, tweakListOptions internalinterfaces.TweakListOptionsFunc) Interface {
	return &group{factory: f, namespace: namespace, tweakListOptions: tweakListOptions}
}

// V1 returns a new v1.Interface.
func (g *group) V1() v1.Interface {
	return v1.New(g.factory, g.namespace, g.tweakListOptions)
}


================================================
File: pkg/generated/informers/externalversions/acid.zalan.do/v1/interface.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by informer-gen. DO NOT EDIT.

package v1

import (
	internalinterfaces "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/internalinterfaces"
)

// Interface provides access to all the informers in this group version.
type Interface interface {
	// PostgresTeams returns a PostgresTeamInformer.
	PostgresTeams() PostgresTeamInformer
	// Postgresqls returns a PostgresqlInformer.
	Postgresqls() PostgresqlInformer
}

type version struct {
	factory          internalinterfaces.SharedInformerFactory
	namespace        string
	tweakListOptions internalinterfaces.TweakListOptionsFunc
}

// New returns a new Interface.
func New(f internalinterfaces.SharedInformerFactory, namespace string, tweakListOptions internalinterfaces.TweakListOptionsFunc) Interface {
	return &version{factory: f, namespace: namespace, tweakListOptions: tweakListOptions}
}

// PostgresTeams returns a PostgresTeamInformer.
func (v *version) PostgresTeams() PostgresTeamInformer {
	return &postgresTeamInformer{factory: v.factory, namespace: v.namespace, tweakListOptions: v.tweakListOptions}
}

// Postgresqls returns a PostgresqlInformer.
func (v *version) Postgresqls() PostgresqlInformer {
	return &postgresqlInformer{factory: v.factory, namespace: v.namespace, tweakListOptions: v.tweakListOptions}
}


================================================
File: pkg/generated/informers/externalversions/acid.zalan.do/v1/postgresql.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by informer-gen. DO NOT EDIT.

package v1

import (
	"context"
	time "time"

	acidzalandov1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	versioned "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned"
	internalinterfaces "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/internalinterfaces"
	v1 "github.com/zalando/postgres-operator/pkg/generated/listers/acid.zalan.do/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
	watch "k8s.io/apimachinery/pkg/watch"
	cache "k8s.io/client-go/tools/cache"
)

// PostgresqlInformer provides access to a shared informer and lister for
// Postgresqls.
type PostgresqlInformer interface {
	Informer() cache.SharedIndexInformer
	Lister() v1.PostgresqlLister
}

type postgresqlInformer struct {
	factory          internalinterfaces.SharedInformerFactory
	tweakListOptions internalinterfaces.TweakListOptionsFunc
	namespace        string
}

// NewPostgresqlInformer constructs a new informer for Postgresql type.
// Always prefer using an informer factory to get a shared informer instead of getting an independent
// one. This reduces memory footprint and number of connections to the server.
func NewPostgresqlInformer(client versioned.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers) cache.SharedIndexInformer {
	return NewFilteredPostgresqlInformer(client, namespace, resyncPeriod, indexers, nil)
}

// NewFilteredPostgresqlInformer constructs a new informer for Postgresql type.
// Always prefer using an informer factory to get a shared informer instead of getting an independent
// one. This reduces memory footprint and number of connections to the server.
func NewFilteredPostgresqlInformer(client versioned.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {
	return cache.NewSharedIndexInformer(
		&cache.ListWatch{
			ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.AcidV1().Postgresqls(namespace).List(context.TODO(), options)
			},
			WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.AcidV1().Postgresqls(namespace).Watch(context.TODO(), options)
			},
		},
		&acidzalandov1.Postgresql{},
		resyncPeriod,
		indexers,
	)
}

func (f *postgresqlInformer) defaultInformer(client versioned.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer {
	return NewFilteredPostgresqlInformer(client, f.namespace, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, f.tweakListOptions)
}

func (f *postgresqlInformer) Informer() cache.SharedIndexInformer {
	return f.factory.InformerFor(&acidzalandov1.Postgresql{}, f.defaultInformer)
}

func (f *postgresqlInformer) Lister() v1.PostgresqlLister {
	return v1.NewPostgresqlLister(f.Informer().GetIndexer())
}


================================================
File: pkg/generated/informers/externalversions/acid.zalan.do/v1/postgresteam.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by informer-gen. DO NOT EDIT.

package v1

import (
	"context"
	time "time"

	acidzalandov1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	versioned "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned"
	internalinterfaces "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/internalinterfaces"
	v1 "github.com/zalando/postgres-operator/pkg/generated/listers/acid.zalan.do/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
	watch "k8s.io/apimachinery/pkg/watch"
	cache "k8s.io/client-go/tools/cache"
)

// PostgresTeamInformer provides access to a shared informer and lister for
// PostgresTeams.
type PostgresTeamInformer interface {
	Informer() cache.SharedIndexInformer
	Lister() v1.PostgresTeamLister
}

type postgresTeamInformer struct {
	factory          internalinterfaces.SharedInformerFactory
	tweakListOptions internalinterfaces.TweakListOptionsFunc
	namespace        string
}

// NewPostgresTeamInformer constructs a new informer for PostgresTeam type.
// Always prefer using an informer factory to get a shared informer instead of getting an independent
// one. This reduces memory footprint and number of connections to the server.
func NewPostgresTeamInformer(client versioned.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers) cache.SharedIndexInformer {
	return NewFilteredPostgresTeamInformer(client, namespace, resyncPeriod, indexers, nil)
}

// NewFilteredPostgresTeamInformer constructs a new informer for PostgresTeam type.
// Always prefer using an informer factory to get a shared informer instead of getting an independent
// one. This reduces memory footprint and number of connections to the server.
func NewFilteredPostgresTeamInformer(client versioned.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {
	return cache.NewSharedIndexInformer(
		&cache.ListWatch{
			ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.AcidV1().PostgresTeams(namespace).List(context.TODO(), options)
			},
			WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.AcidV1().PostgresTeams(namespace).Watch(context.TODO(), options)
			},
		},
		&acidzalandov1.PostgresTeam{},
		resyncPeriod,
		indexers,
	)
}

func (f *postgresTeamInformer) defaultInformer(client versioned.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer {
	return NewFilteredPostgresTeamInformer(client, f.namespace, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, f.tweakListOptions)
}

func (f *postgresTeamInformer) Informer() cache.SharedIndexInformer {
	return f.factory.InformerFor(&acidzalandov1.PostgresTeam{}, f.defaultInformer)
}

func (f *postgresTeamInformer) Lister() v1.PostgresTeamLister {
	return v1.NewPostgresTeamLister(f.Informer().GetIndexer())
}


================================================
File: pkg/generated/informers/externalversions/internalinterfaces/factory_interfaces.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by informer-gen. DO NOT EDIT.

package internalinterfaces

import (
	time "time"

	versioned "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
	cache "k8s.io/client-go/tools/cache"
)

// NewInformerFunc takes versioned.Interface and time.Duration to return a SharedIndexInformer.
type NewInformerFunc func(versioned.Interface, time.Duration) cache.SharedIndexInformer

// SharedInformerFactory a small interface to allow for adding an informer without an import cycle
type SharedInformerFactory interface {
	Start(stopCh <-chan struct{})
	InformerFor(obj runtime.Object, newFunc NewInformerFunc) cache.SharedIndexInformer
}

// TweakListOptionsFunc is a function that transforms a v1.ListOptions.
type TweakListOptionsFunc func(*v1.ListOptions)


================================================
File: pkg/generated/informers/externalversions/zalando.org/interface.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by informer-gen. DO NOT EDIT.

package zalando

import (
	internalinterfaces "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/internalinterfaces"
	v1 "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/zalando.org/v1"
)

// Interface provides access to each of this group's versions.
type Interface interface {
	// V1 provides access to shared informers for resources in V1.
	V1() v1.Interface
}

type group struct {
	factory          internalinterfaces.SharedInformerFactory
	namespace        string
	tweakListOptions internalinterfaces.TweakListOptionsFunc
}

// New returns a new Interface.
func New(f internalinterfaces.SharedInformerFactory, namespace string, tweakListOptions internalinterfaces.TweakListOptionsFunc) Interface {
	return &group{factory: f, namespace: namespace, tweakListOptions: tweakListOptions}
}

// V1 returns a new v1.Interface.
func (g *group) V1() v1.Interface {
	return v1.New(g.factory, g.namespace, g.tweakListOptions)
}


================================================
File: pkg/generated/informers/externalversions/zalando.org/v1/fabriceventstream.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by informer-gen. DO NOT EDIT.

package v1

import (
	"context"
	time "time"

	zalandoorgv1 "github.com/zalando/postgres-operator/pkg/apis/zalando.org/v1"
	versioned "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned"
	internalinterfaces "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/internalinterfaces"
	v1 "github.com/zalando/postgres-operator/pkg/generated/listers/zalando.org/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
	watch "k8s.io/apimachinery/pkg/watch"
	cache "k8s.io/client-go/tools/cache"
)

// FabricEventStreamInformer provides access to a shared informer and lister for
// FabricEventStreams.
type FabricEventStreamInformer interface {
	Informer() cache.SharedIndexInformer
	Lister() v1.FabricEventStreamLister
}

type fabricEventStreamInformer struct {
	factory          internalinterfaces.SharedInformerFactory
	tweakListOptions internalinterfaces.TweakListOptionsFunc
	namespace        string
}

// NewFabricEventStreamInformer constructs a new informer for FabricEventStream type.
// Always prefer using an informer factory to get a shared informer instead of getting an independent
// one. This reduces memory footprint and number of connections to the server.
func NewFabricEventStreamInformer(client versioned.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers) cache.SharedIndexInformer {
	return NewFilteredFabricEventStreamInformer(client, namespace, resyncPeriod, indexers, nil)
}

// NewFilteredFabricEventStreamInformer constructs a new informer for FabricEventStream type.
// Always prefer using an informer factory to get a shared informer instead of getting an independent
// one. This reduces memory footprint and number of connections to the server.
func NewFilteredFabricEventStreamInformer(client versioned.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {
	return cache.NewSharedIndexInformer(
		&cache.ListWatch{
			ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.ZalandoV1().FabricEventStreams(namespace).List(context.TODO(), options)
			},
			WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.ZalandoV1().FabricEventStreams(namespace).Watch(context.TODO(), options)
			},
		},
		&zalandoorgv1.FabricEventStream{},
		resyncPeriod,
		indexers,
	)
}

func (f *fabricEventStreamInformer) defaultInformer(client versioned.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer {
	return NewFilteredFabricEventStreamInformer(client, f.namespace, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, f.tweakListOptions)
}

func (f *fabricEventStreamInformer) Informer() cache.SharedIndexInformer {
	return f.factory.InformerFor(&zalandoorgv1.FabricEventStream{}, f.defaultInformer)
}

func (f *fabricEventStreamInformer) Lister() v1.FabricEventStreamLister {
	return v1.NewFabricEventStreamLister(f.Informer().GetIndexer())
}


================================================
File: pkg/generated/informers/externalversions/zalando.org/v1/interface.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by informer-gen. DO NOT EDIT.

package v1

import (
	internalinterfaces "github.com/zalando/postgres-operator/pkg/generated/informers/externalversions/internalinterfaces"
)

// Interface provides access to all the informers in this group version.
type Interface interface {
	// FabricEventStreams returns a FabricEventStreamInformer.
	FabricEventStreams() FabricEventStreamInformer
}

type version struct {
	factory          internalinterfaces.SharedInformerFactory
	namespace        string
	tweakListOptions internalinterfaces.TweakListOptionsFunc
}

// New returns a new Interface.
func New(f internalinterfaces.SharedInformerFactory, namespace string, tweakListOptions internalinterfaces.TweakListOptionsFunc) Interface {
	return &version{factory: f, namespace: namespace, tweakListOptions: tweakListOptions}
}

// FabricEventStreams returns a FabricEventStreamInformer.
func (v *version) FabricEventStreams() FabricEventStreamInformer {
	return &fabricEventStreamInformer{factory: v.factory, namespace: v.namespace, tweakListOptions: v.tweakListOptions}
}


================================================
File: pkg/generated/listers/acid.zalan.do/v1/expansion_generated.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by lister-gen. DO NOT EDIT.

package v1

// PostgresTeamListerExpansion allows custom methods to be added to
// PostgresTeamLister.
type PostgresTeamListerExpansion interface{}

// PostgresTeamNamespaceListerExpansion allows custom methods to be added to
// PostgresTeamNamespaceLister.
type PostgresTeamNamespaceListerExpansion interface{}

// PostgresqlListerExpansion allows custom methods to be added to
// PostgresqlLister.
type PostgresqlListerExpansion interface{}

// PostgresqlNamespaceListerExpansion allows custom methods to be added to
// PostgresqlNamespaceLister.
type PostgresqlNamespaceListerExpansion interface{}


================================================
File: pkg/generated/listers/acid.zalan.do/v1/postgresql.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by lister-gen. DO NOT EDIT.

package v1

import (
	v1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/client-go/tools/cache"
)

// PostgresqlLister helps list Postgresqls.
// All objects returned here must be treated as read-only.
type PostgresqlLister interface {
	// List lists all Postgresqls in the indexer.
	// Objects returned here must be treated as read-only.
	List(selector labels.Selector) (ret []*v1.Postgresql, err error)
	// Postgresqls returns an object that can list and get Postgresqls.
	Postgresqls(namespace string) PostgresqlNamespaceLister
	PostgresqlListerExpansion
}

// postgresqlLister implements the PostgresqlLister interface.
type postgresqlLister struct {
	indexer cache.Indexer
}

// NewPostgresqlLister returns a new PostgresqlLister.
func NewPostgresqlLister(indexer cache.Indexer) PostgresqlLister {
	return &postgresqlLister{indexer: indexer}
}

// List lists all Postgresqls in the indexer.
func (s *postgresqlLister) List(selector labels.Selector) (ret []*v1.Postgresql, err error) {
	err = cache.ListAll(s.indexer, selector, func(m interface{}) {
		ret = append(ret, m.(*v1.Postgresql))
	})
	return ret, err
}

// Postgresqls returns an object that can list and get Postgresqls.
func (s *postgresqlLister) Postgresqls(namespace string) PostgresqlNamespaceLister {
	return postgresqlNamespaceLister{indexer: s.indexer, namespace: namespace}
}

// PostgresqlNamespaceLister helps list and get Postgresqls.
// All objects returned here must be treated as read-only.
type PostgresqlNamespaceLister interface {
	// List lists all Postgresqls in the indexer for a given namespace.
	// Objects returned here must be treated as read-only.
	List(selector labels.Selector) (ret []*v1.Postgresql, err error)
	// Get retrieves the Postgresql from the indexer for a given namespace and name.
	// Objects returned here must be treated as read-only.
	Get(name string) (*v1.Postgresql, error)
	PostgresqlNamespaceListerExpansion
}

// postgresqlNamespaceLister implements the PostgresqlNamespaceLister
// interface.
type postgresqlNamespaceLister struct {
	indexer   cache.Indexer
	namespace string
}

// List lists all Postgresqls in the indexer for a given namespace.
func (s postgresqlNamespaceLister) List(selector labels.Selector) (ret []*v1.Postgresql, err error) {
	err = cache.ListAllByNamespace(s.indexer, s.namespace, selector, func(m interface{}) {
		ret = append(ret, m.(*v1.Postgresql))
	})
	return ret, err
}

// Get retrieves the Postgresql from the indexer for a given namespace and name.
func (s postgresqlNamespaceLister) Get(name string) (*v1.Postgresql, error) {
	obj, exists, err := s.indexer.GetByKey(s.namespace + "/" + name)
	if err != nil {
		return nil, err
	}
	if !exists {
		return nil, errors.NewNotFound(v1.Resource("postgresql"), name)
	}
	return obj.(*v1.Postgresql), nil
}


================================================
File: pkg/generated/listers/acid.zalan.do/v1/postgresteam.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by lister-gen. DO NOT EDIT.

package v1

import (
	v1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/client-go/tools/cache"
)

// PostgresTeamLister helps list PostgresTeams.
// All objects returned here must be treated as read-only.
type PostgresTeamLister interface {
	// List lists all PostgresTeams in the indexer.
	// Objects returned here must be treated as read-only.
	List(selector labels.Selector) (ret []*v1.PostgresTeam, err error)
	// PostgresTeams returns an object that can list and get PostgresTeams.
	PostgresTeams(namespace string) PostgresTeamNamespaceLister
	PostgresTeamListerExpansion
}

// postgresTeamLister implements the PostgresTeamLister interface.
type postgresTeamLister struct {
	indexer cache.Indexer
}

// NewPostgresTeamLister returns a new PostgresTeamLister.
func NewPostgresTeamLister(indexer cache.Indexer) PostgresTeamLister {
	return &postgresTeamLister{indexer: indexer}
}

// List lists all PostgresTeams in the indexer.
func (s *postgresTeamLister) List(selector labels.Selector) (ret []*v1.PostgresTeam, err error) {
	err = cache.ListAll(s.indexer, selector, func(m interface{}) {
		ret = append(ret, m.(*v1.PostgresTeam))
	})
	return ret, err
}

// PostgresTeams returns an object that can list and get PostgresTeams.
func (s *postgresTeamLister) PostgresTeams(namespace string) PostgresTeamNamespaceLister {
	return postgresTeamNamespaceLister{indexer: s.indexer, namespace: namespace}
}

// PostgresTeamNamespaceLister helps list and get PostgresTeams.
// All objects returned here must be treated as read-only.
type PostgresTeamNamespaceLister interface {
	// List lists all PostgresTeams in the indexer for a given namespace.
	// Objects returned here must be treated as read-only.
	List(selector labels.Selector) (ret []*v1.PostgresTeam, err error)
	// Get retrieves the PostgresTeam from the indexer for a given namespace and name.
	// Objects returned here must be treated as read-only.
	Get(name string) (*v1.PostgresTeam, error)
	PostgresTeamNamespaceListerExpansion
}

// postgresTeamNamespaceLister implements the PostgresTeamNamespaceLister
// interface.
type postgresTeamNamespaceLister struct {
	indexer   cache.Indexer
	namespace string
}

// List lists all PostgresTeams in the indexer for a given namespace.
func (s postgresTeamNamespaceLister) List(selector labels.Selector) (ret []*v1.PostgresTeam, err error) {
	err = cache.ListAllByNamespace(s.indexer, s.namespace, selector, func(m interface{}) {
		ret = append(ret, m.(*v1.PostgresTeam))
	})
	return ret, err
}

// Get retrieves the PostgresTeam from the indexer for a given namespace and name.
func (s postgresTeamNamespaceLister) Get(name string) (*v1.PostgresTeam, error) {
	obj, exists, err := s.indexer.GetByKey(s.namespace + "/" + name)
	if err != nil {
		return nil, err
	}
	if !exists {
		return nil, errors.NewNotFound(v1.Resource("postgresteam"), name)
	}
	return obj.(*v1.PostgresTeam), nil
}


================================================
File: pkg/generated/listers/zalando.org/v1/expansion_generated.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by lister-gen. DO NOT EDIT.

package v1

// FabricEventStreamListerExpansion allows custom methods to be added to
// FabricEventStreamLister.
type FabricEventStreamListerExpansion interface{}

// FabricEventStreamNamespaceListerExpansion allows custom methods to be added to
// FabricEventStreamNamespaceLister.
type FabricEventStreamNamespaceListerExpansion interface{}


================================================
File: pkg/generated/listers/zalando.org/v1/fabriceventstream.go
================================================
/*
Copyright 2025 Compose, Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

// Code generated by lister-gen. DO NOT EDIT.

package v1

import (
	v1 "github.com/zalando/postgres-operator/pkg/apis/zalando.org/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/client-go/tools/cache"
)

// FabricEventStreamLister helps list FabricEventStreams.
// All objects returned here must be treated as read-only.
type FabricEventStreamLister interface {
	// List lists all FabricEventStreams in the indexer.
	// Objects returned here must be treated as read-only.
	List(selector labels.Selector) (ret []*v1.FabricEventStream, err error)
	// FabricEventStreams returns an object that can list and get FabricEventStreams.
	FabricEventStreams(namespace string) FabricEventStreamNamespaceLister
	FabricEventStreamListerExpansion
}

// fabricEventStreamLister implements the FabricEventStreamLister interface.
type fabricEventStreamLister struct {
	indexer cache.Indexer
}

// NewFabricEventStreamLister returns a new FabricEventStreamLister.
func NewFabricEventStreamLister(indexer cache.Indexer) FabricEventStreamLister {
	return &fabricEventStreamLister{indexer: indexer}
}

// List lists all FabricEventStreams in the indexer.
func (s *fabricEventStreamLister) List(selector labels.Selector) (ret []*v1.FabricEventStream, err error) {
	err = cache.ListAll(s.indexer, selector, func(m interface{}) {
		ret = append(ret, m.(*v1.FabricEventStream))
	})
	return ret, err
}

// FabricEventStreams returns an object that can list and get FabricEventStreams.
func (s *fabricEventStreamLister) FabricEventStreams(namespace string) FabricEventStreamNamespaceLister {
	return fabricEventStreamNamespaceLister{indexer: s.indexer, namespace: namespace}
}

// FabricEventStreamNamespaceLister helps list and get FabricEventStreams.
// All objects returned here must be treated as read-only.
type FabricEventStreamNamespaceLister interface {
	// List lists all FabricEventStreams in the indexer for a given namespace.
	// Objects returned here must be treated as read-only.
	List(selector labels.Selector) (ret []*v1.FabricEventStream, err error)
	// Get retrieves the FabricEventStream from the indexer for a given namespace and name.
	// Objects returned here must be treated as read-only.
	Get(name string) (*v1.FabricEventStream, error)
	FabricEventStreamNamespaceListerExpansion
}

// fabricEventStreamNamespaceLister implements the FabricEventStreamNamespaceLister
// interface.
type fabricEventStreamNamespaceLister struct {
	indexer   cache.Indexer
	namespace string
}

// List lists all FabricEventStreams in the indexer for a given namespace.
func (s fabricEventStreamNamespaceLister) List(selector labels.Selector) (ret []*v1.FabricEventStream, err error) {
	err = cache.ListAllByNamespace(s.indexer, s.namespace, selector, func(m interface{}) {
		ret = append(ret, m.(*v1.FabricEventStream))
	})
	return ret, err
}

// Get retrieves the FabricEventStream from the indexer for a given namespace and name.
func (s fabricEventStreamNamespaceLister) Get(name string) (*v1.FabricEventStream, error) {
	obj, exists, err := s.indexer.GetByKey(s.namespace + "/" + name)
	if err != nil {
		return nil, err
	}
	if !exists {
		return nil, errors.NewNotFound(v1.Resource("fabriceventstream"), name)
	}
	return obj.(*v1.FabricEventStream), nil
}


================================================
File: pkg/spec/types.go
================================================
package spec

import (
	"database/sql"
	"encoding/json"
	"fmt"
	"log"
	"os"
	"strings"
	"time"

	"github.com/sirupsen/logrus"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/rest"
)

// NamespacedName describes the namespace/name pairs used in Kubernetes names.
type NamespacedName types.NamespacedName

const fileWithNamespace = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"

// RoleOrigin contains the code of the origin of a role
type RoleOrigin int

// The rolesOrigin constant values must be sorted by the role priority for
// resolveNameConflict(...) to work.
const (
	RoleOriginUnknown RoleOrigin = iota
	RoleOriginManifest
	RoleOriginInfrastructure
	RoleOriginTeamsAPI
	RoleOriginSystem
	RoleOriginBootstrap
	RoleOriginConnectionPooler
	RoleOriginStream
)

type syncUserOperation int

// Possible values for the sync user operation (removal of users is not supported yet)
const (
	PGSyncUserAdd = iota
	PGsyncUserAlter
	PGSyncAlterSet // handle ALTER ROLE SET parameter = value
	PGSyncUserRename
)

// PgUser contains information about a single user.
type PgUser struct {
	Origin     RoleOrigin        `yaml:"-"`
	Name       string            `yaml:"-"`
	Namespace  string            `yaml:"-"`
	Password   string            `yaml:"-"`
	Flags      []string          `yaml:"user_flags"`
	MemberOf   []string          `yaml:"inrole"`
	Parameters map[string]string `yaml:"db_parameters"`
	AdminRole  string            `yaml:"admin_role"`
	IsDbOwner  bool              `yaml:"is_db_owner"`
	Deleted    bool              `yaml:"deleted"`
	Rotated    bool              `yaml:"rotated"`
}

func (user *PgUser) Valid() bool {
	return user.Name != "" && user.Password != ""
}

// PgUserMap maps user names to the definitions.
type PgUserMap map[string]PgUser

// PgSyncUserRequest has information about a single request to sync a user.
type PgSyncUserRequest struct {
	Kind syncUserOperation
	User PgUser
}

// UserSyncer defines an interface for the implementations to sync users from the manifest to the DB.
type UserSyncer interface {
	ProduceSyncRequests(dbUsers PgUserMap, newUsers PgUserMap) (req []PgSyncUserRequest)
	ExecuteSyncRequests(req []PgSyncUserRequest, db *sql.DB) error
}

// LogEntry describes log entry in the RingLogger
type LogEntry struct {
	Time        time.Time
	Level       logrus.Level
	ClusterName *NamespacedName `json:",omitempty"`
	Worker      *uint32         `json:",omitempty"`
	Message     string
}

// Diff describes diff
type Diff struct {
	EventTime   time.Time
	ProcessTime time.Time
	Diff        []string
}

// ControllerStatus describes status of the controller
type ControllerStatus struct {
	LastSyncTime    int64
	Clusters        int
	WorkerQueueSize map[int]int
}

// QueueDump describes cache.FIFO queue
type QueueDump struct {
	Keys []string
	List []interface{}
}

// ControllerConfig describes configuration of the controller
type ControllerConfig struct {
	RestConfig          *rest.Config `json:"-"`
	InfrastructureRoles map[string]PgUser

	NoDatabaseAccess     bool
	NoTeamsAPI           bool
	CRDReadyWaitInterval time.Duration
	CRDReadyWaitTimeout  time.Duration
	ConfigMapName        NamespacedName
	Namespace            string
	IgnoredAnnotations   []string

	EnableJsonLogging bool

	KubeQPS   int
	KubeBurst int
}

// cached value for the GetOperatorNamespace
var operatorNamespace string

func (n NamespacedName) String() string {
	return types.NamespacedName(n).String()
}

// MarshalJSON defines marshaling rule for the namespaced name type.
func (n NamespacedName) MarshalJSON() ([]byte, error) {
	return []byte("\"" + n.String() + "\""), nil
}

// Decode converts a (possibly unqualified) string into the namespaced name object.
func (n *NamespacedName) Decode(value string) error {
	return n.DecodeWorker(value, GetOperatorNamespace())
}

// UnmarshalJSON converts a byte slice to NamespacedName
func (n *NamespacedName) UnmarshalJSON(data []byte) error {
	result := NamespacedName{}
	var tmp string
	if err := json.Unmarshal(data, &tmp); err != nil {
		return err
	}
	if err := result.Decode(tmp); err != nil {
		return err
	}
	*n = result
	return nil
}

// DecodeWorker separates the decode logic to (unit) test
// from obtaining the operator namespace that depends on k8s mounting files at runtime
func (n *NamespacedName) DecodeWorker(value, operatorNamespace string) error {
	var (
		name types.NamespacedName
	)

	result := strings.SplitN(value, string(types.Separator), 2)
	if len(result) < 2 {
		name.Name = result[0]
	} else {
		name.Name = strings.TrimLeft(result[1], string(types.Separator))
		name.Namespace = result[0]
	}
	if name.Name == "" {
		return fmt.Errorf("incorrect namespaced name: %v", value)
	}
	if name.Namespace == "" {
		name.Namespace = operatorNamespace
	}

	*n = NamespacedName(name)

	return nil
}

func (r RoleOrigin) String() string {
	switch r {
	case RoleOriginUnknown:
		return "unknown"
	case RoleOriginManifest:
		return "manifest role"
	case RoleOriginInfrastructure:
		return "infrastructure role"
	case RoleOriginTeamsAPI:
		return "teams API role"
	case RoleOriginSystem:
		return "system role"
	case RoleOriginBootstrap:
		return "bootstrapped role"
	case RoleOriginConnectionPooler:
		return "connection pooler role"
	default:
		panic(fmt.Sprintf("bogus role origin value %d", r))
	}
}

// GetOperatorNamespace assumes serviceaccount secret is mounted by kubernetes
// Placing this func here instead of pgk/util avoids circular import
func GetOperatorNamespace() string {
	if operatorNamespace == "" {
		if namespaceFromEnvironment := os.Getenv("OPERATOR_NAMESPACE"); namespaceFromEnvironment != "" {
			return namespaceFromEnvironment
		}
		operatorNamespaceBytes, err := os.ReadFile(fileWithNamespace)
		if err != nil {
			log.Fatalf("Unable to detect operator namespace from within its pod due to: %v", err)
		}
		operatorNamespace = string(operatorNamespaceBytes)
	}
	return operatorNamespace
}


================================================
File: pkg/spec/types_test.go
================================================
package spec

import (
	"bytes"
	"testing"
)

const (
	mockOperatorNamespace = "acid"
)

var nnTests = []struct {
	s               string
	expected        NamespacedName
	expectedMarshal []byte
}{
	{`acid/cluster`, NamespacedName{Namespace: mockOperatorNamespace, Name: "cluster"}, []byte(`"acid/cluster"`)},
	{`/name`, NamespacedName{Namespace: mockOperatorNamespace, Name: "name"}, []byte(`"acid/name"`)},
	{`test`, NamespacedName{Namespace: mockOperatorNamespace, Name: "test"}, []byte(`"acid/test"`)},
}

var nnErr = []string{"test/", "/", "", "//"}

func TestNamespacedNameDecode(t *testing.T) {

	for _, tt := range nnTests {
		var actual NamespacedName
		err := actual.DecodeWorker(tt.s, mockOperatorNamespace)
		if err != nil {
			t.Errorf("decode error: %v", err)
		}
		if actual != tt.expected {
			t.Errorf("expected: %v, got %#v", tt.expected, actual)
		}
	}

}

func TestNamespacedNameMarshal(t *testing.T) {
	for _, tt := range nnTests {
		var actual NamespacedName

		m, err := actual.MarshalJSON()
		if err != nil {
			t.Errorf("marshal error: %v", err)
		}
		if bytes.Equal(m, tt.expectedMarshal) {
			t.Errorf("expected marshal: %v, got %#v", tt.expected, actual)
		}
	}
}

func TestNamespacedNameError(t *testing.T) {
	for _, tt := range nnErr {
		var actual NamespacedName
		err := actual.DecodeWorker(tt, mockOperatorNamespace)
		if err == nil {
			t.Errorf("error expected for %q, got: %#v", tt, actual)
		}
	}
}


================================================
File: pkg/teams/postgres_team.go
================================================
package teams

import (
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/util"
)

// PostgresTeamMap is the operator's internal representation of all PostgresTeam CRDs
type PostgresTeamMap map[string]postgresTeamMembership

type postgresTeamMembership struct {
	AdditionalSuperuserTeams []string
	AdditionalTeams          []string
	AdditionalMembers        []string
}

type teamHashSet map[string]map[string]struct{}

func (ths *teamHashSet) has(team string) bool {
	_, ok := (*ths)[team]
	return ok
}

func (ths *teamHashSet) add(newTeam string, newSet []string) {
	set := make(map[string]struct{})
	if ths.has(newTeam) {
		set = (*ths)[newTeam]
	}
	for _, t := range newSet {
		set[t] = struct{}{}
	}
	(*ths)[newTeam] = set
}

func (ths *teamHashSet) toMap() map[string][]string {
	newTeamMap := make(map[string][]string)
	for team, items := range *ths {
		list := []string{}
		for item := range items {
			list = append(list, item)
		}
		newTeamMap[team] = list
	}
	return newTeamMap
}

func (ths *teamHashSet) mergeCrdMap(crdTeamMap map[string][]string) {
	for t, at := range crdTeamMap {
		ths.add(t, at)
	}
}

func fetchTeams(teamset *map[string]struct{}, set teamHashSet) {
	for key := range set {
		(*teamset)[key] = struct{}{}
	}
}

func (ptm *PostgresTeamMap) fetchAdditionalTeams(team string, superuserTeams bool, transitive bool, exclude []string) []string {

	var teams []string

	if superuserTeams {
		teams = (*ptm)[team].AdditionalSuperuserTeams
	} else {
		teams = (*ptm)[team].AdditionalTeams
	}
	if transitive {
		for _, additionalTeam := range teams {
			if !(util.SliceContains(exclude, additionalTeam)) {
				// remember to not check team and additionalTeam again
				exclude = append(exclude, additionalTeam)
				transitiveTeams := (*ptm).fetchAdditionalTeams(additionalTeam, superuserTeams, transitive, exclude)
				for _, transitiveTeam := range transitiveTeams {
					if !(util.SliceContains(exclude, transitiveTeam)) {
						// remember to not check transitive team again in case
						// it is one of the next additional teams of the outer loop
						exclude = append(exclude, transitiveTeam)
						if !(util.SliceContains(teams, transitiveTeam)) {
							// found a new transitive additional team
							teams = append(teams, transitiveTeam)
						}
					}
				}
			}
		}
	}

	return teams
}

// GetAdditionalTeams function to retrieve list of additional teams
func (ptm *PostgresTeamMap) GetAdditionalTeams(team string, transitive bool) []string {
	return ptm.fetchAdditionalTeams(team, false, transitive, []string{team})
}

// GetAdditionalSuperuserTeams function to retrieve list of additional superuser teams
func (ptm *PostgresTeamMap) GetAdditionalSuperuserTeams(team string, transitive bool) []string {
	return ptm.fetchAdditionalTeams(team, true, transitive, []string{team})
}

// Load function to import data from PostgresTeam CRD
func (ptm *PostgresTeamMap) Load(pgTeams *acidv1.PostgresTeamList) {
	// reset the team map
	*ptm = make(PostgresTeamMap, 0)

	superuserTeamSet := teamHashSet{}
	teamSet := teamHashSet{}
	teamMemberSet := teamHashSet{}
	teamIDs := make(map[string]struct{})

	for _, pgTeam := range pgTeams.Items {
		superuserTeamSet.mergeCrdMap(pgTeam.Spec.AdditionalSuperuserTeams)
		teamSet.mergeCrdMap(pgTeam.Spec.AdditionalTeams)
		teamMemberSet.mergeCrdMap(pgTeam.Spec.AdditionalMembers)
	}
	fetchTeams(&teamIDs, superuserTeamSet)
	fetchTeams(&teamIDs, teamSet)
	fetchTeams(&teamIDs, teamMemberSet)

	for teamID := range teamIDs {
		(*ptm)[teamID] = postgresTeamMembership{
			AdditionalSuperuserTeams: util.CoalesceStrArr(superuserTeamSet.toMap()[teamID], []string{}),
			AdditionalTeams:          util.CoalesceStrArr(teamSet.toMap()[teamID], []string{}),
			AdditionalMembers:        util.CoalesceStrArr(teamMemberSet.toMap()[teamID], []string{}),
		}
	}
}


================================================
File: pkg/teams/postgres_team_test.go
================================================
package teams

import (
	"testing"

	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	"github.com/zalando/postgres-operator/pkg/util"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

var (
	pgTeamList = acidv1.PostgresTeamList{
		TypeMeta: metav1.TypeMeta{
			Kind:       "List",
			APIVersion: "v1",
		},
		Items: []acidv1.PostgresTeam{
			{
				TypeMeta: metav1.TypeMeta{
					Kind:       "PostgresTeam",
					APIVersion: "acid.zalan.do/v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name: "teamAB",
				},
				Spec: acidv1.PostgresTeamSpec{
					AdditionalSuperuserTeams: map[string][]string{"teamA": []string{"teamB", "team24x7"}, "teamB": []string{"teamA", "teamC", "team24x7"}},
					AdditionalTeams:          map[string][]string{"teamA": []string{"teamC"}, "teamB": []string{}},
					AdditionalMembers:        map[string][]string{"team24x7": []string{"optimusprime"}, "teamB": []string{"drno"}},
				},
			}, {
				TypeMeta: metav1.TypeMeta{
					Kind:       "PostgresTeam",
					APIVersion: "acid.zalan.do/v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name: "teamC",
				},
				Spec: acidv1.PostgresTeamSpec{
					AdditionalSuperuserTeams: map[string][]string{"teamC": []string{"team24x7"}},
					AdditionalTeams:          map[string][]string{"teamA": []string{"teamC"}, "teamC": []string{"teamA", "teamB", "acid"}},
					AdditionalMembers:        map[string][]string{"acid": []string{"batman"}},
				},
			},
			{
				TypeMeta: metav1.TypeMeta{
					Kind:       "PostgresTeam",
					APIVersion: "acid.zalan.do/v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name: "teamD",
				},
				Spec: acidv1.PostgresTeamSpec{
					AdditionalSuperuserTeams: map[string][]string{},
					AdditionalTeams:          map[string][]string{"teamA": []string{"teamD"}, "teamC": []string{"teamD"}, "teamD": []string{"teamA", "teamB", "teamC"}},
					AdditionalMembers:        map[string][]string{"acid": []string{"batman"}},
				},
			},
		},
	}
	pgTeamMap = PostgresTeamMap{
		"teamA": {
			AdditionalSuperuserTeams: []string{"teamB", "team24x7"},
			AdditionalTeams:          []string{"teamC", "teamD"},
			AdditionalMembers:        []string{},
		},
		"teamB": {
			AdditionalSuperuserTeams: []string{"teamA", "teamC", "team24x7"},
			AdditionalTeams:          []string{},
			AdditionalMembers:        []string{"drno"},
		},
		"teamC": {
			AdditionalSuperuserTeams: []string{"team24x7"},
			AdditionalTeams:          []string{"teamA", "teamB", "teamD", "acid"},
			AdditionalMembers:        []string{},
		},
		"teamD": {
			AdditionalSuperuserTeams: []string{},
			AdditionalTeams:          []string{"teamA", "teamB", "teamC"},
			AdditionalMembers:        []string{},
		},
		"team24x7": {
			AdditionalSuperuserTeams: []string{},
			AdditionalTeams:          []string{},
			AdditionalMembers:        []string{"optimusprime"},
		},
		"acid": {
			AdditionalSuperuserTeams: []string{},
			AdditionalTeams:          []string{},
			AdditionalMembers:        []string{"batman"},
		},
	}
)

// TestLoadingPostgresTeamCRD PostgresTeamMap is the operator's internal representation of all PostgresTeam CRDs
func TestLoadingPostgresTeamCRD(t *testing.T) {
	tests := []struct {
		name  string
		crd   acidv1.PostgresTeamList
		ptm   PostgresTeamMap
		error string
	}{
		{
			"Check that CRD is imported correctly into the internal format",
			pgTeamList,
			pgTeamMap,
			"Mismatch between PostgresTeam CRD and internal map",
		},
	}

	for _, tt := range tests {
		postgresTeamMap := PostgresTeamMap{}
		postgresTeamMap.Load(&tt.crd)
		for team, ptmeamMembership := range postgresTeamMap {
			if !util.IsEqualIgnoreOrder(ptmeamMembership.AdditionalSuperuserTeams, tt.ptm[team].AdditionalSuperuserTeams) {
				t.Errorf("%s: %v: expected additional members %#v, got %#v", tt.name, tt.error, tt.ptm, postgresTeamMap)
			}
			if !util.IsEqualIgnoreOrder(ptmeamMembership.AdditionalTeams, tt.ptm[team].AdditionalTeams) {
				t.Errorf("%s: %v: expected additional teams %#v, got %#v", tt.name, tt.error, tt.ptm, postgresTeamMap)
			}
			if !util.IsEqualIgnoreOrder(ptmeamMembership.AdditionalMembers, tt.ptm[team].AdditionalMembers) {
				t.Errorf("%s: %v: expected additional superuser teams %#v, got %#v", tt.name, tt.error, tt.ptm, postgresTeamMap)
			}
		}
	}
}

// TestGetAdditionalTeams if returns teams with and without transitive dependencies
func TestGetAdditionalTeams(t *testing.T) {
	tests := []struct {
		name       string
		team       string
		transitive bool
		teams      []string
		error      string
	}{
		{
			"Check that additional teams are returned",
			"teamA",
			false,
			[]string{"teamC", "teamD"},
			"GetAdditionalTeams returns wrong list",
		},
		{
			"Check that additional teams are returned incl. transitive teams",
			"teamA",
			true,
			[]string{"teamC", "teamD", "teamB", "acid"},
			"GetAdditionalTeams returns wrong list",
		},
		{
			"Check that empty list is returned",
			"teamB",
			false,
			[]string{},
			"GetAdditionalTeams returns wrong list",
		},
	}

	postgresTeamMap := PostgresTeamMap{}
	postgresTeamMap.Load(&pgTeamList)

	for _, tt := range tests {
		additionalTeams := postgresTeamMap.GetAdditionalTeams(tt.team, tt.transitive)
		if !util.IsEqualIgnoreOrder(additionalTeams, tt.teams) {
			t.Errorf("%s: %v: expected additional teams %#v, got %#v", tt.name, tt.error, tt.teams, additionalTeams)
		}
	}
}

// TestGetAdditionalSuperuserTeams if returns teams with and without transitive dependencies
func TestGetAdditionalSuperuserTeams(t *testing.T) {
	tests := []struct {
		name       string
		team       string
		transitive bool
		teams      []string
		error      string
	}{
		{
			"Check that additional superuser teams are returned",
			"teamA",
			false,
			[]string{"teamB", "team24x7"},
			"GetAdditionalSuperuserTeams returns wrong list",
		},
		{
			"Check that additional superuser teams are returned incl. transitive superuser teams",
			"teamA",
			true,
			[]string{"teamB", "teamC", "team24x7"},
			"GetAdditionalSuperuserTeams returns wrong list",
		},
		{
			"Check that empty list is returned",
			"team24x7",
			false,
			[]string{},
			"GetAdditionalSuperuserTeams returns wrong list",
		},
	}

	postgresTeamMap := PostgresTeamMap{}
	postgresTeamMap.Load(&pgTeamList)

	for _, tt := range tests {
		additionalTeams := postgresTeamMap.GetAdditionalSuperuserTeams(tt.team, tt.transitive)
		if !util.IsEqualIgnoreOrder(additionalTeams, tt.teams) {
			t.Errorf("%s: %v: expected additional teams %#v, got %#v", tt.name, tt.error, tt.teams, additionalTeams)
		}
	}
}


================================================
File: pkg/util/util.go
================================================
package util

import (
	"crypto/hmac"
	"crypto/md5" // #nosec we need it to for PostgreSQL md5 passwords
	cryptoRand "crypto/rand"
	"crypto/sha256"
	"encoding/base64"
	"encoding/hex"
	"fmt"
	"math/big"
	"math/rand"
	"reflect"
	"regexp"
	"sort"
	"strings"
	"time"

	"github.com/motomux/pretty"
	resource "k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"

	"github.com/zalando/postgres-operator/pkg/spec"
	"golang.org/x/crypto/pbkdf2"
)

const (
	md5prefix         = "md5"
	scramsha256prefix = "SCRAM-SHA-256"
	saltlength        = 16
	iterations        = 4096
)

var passwordChars = []byte("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789")

func init() {
	rand.New(rand.NewSource(time.Now().Unix()))
}

// helper function to get bool pointers
func True() *bool {
	b := true
	return &b
}

func False() *bool {
	b := false
	return &b
}

// RandomPassword generates a secure, random alphanumeric password of a given length.
func RandomPassword(n int) string {
	b := make([]byte, n)
	for i := range b {
		maxN := big.NewInt(int64(len(passwordChars)))
		if n, err := cryptoRand.Int(cryptoRand.Reader, maxN); err != nil {
			panic(fmt.Errorf("Unable to generate secure, random password: %v", err))
		} else {
			b[i] = passwordChars[n.Int64()]
		}
	}
	return string(b)
}

// NameFromMeta converts a metadata object to the NamespacedName name representation.
func NameFromMeta(meta metav1.ObjectMeta) spec.NamespacedName {
	return spec.NamespacedName{
		Namespace: meta.Namespace,
		Name:      meta.Name,
	}
}

type Hasher func(user spec.PgUser) string
type Random func(n int) string

type Encryptor struct {
	encrypt Hasher
	random  Random
}

func NewEncryptor(encryption string) *Encryptor {
	e := Encryptor{random: RandomPassword}
	m := map[string]Hasher{
		"md5":           e.PGUserPasswordMD5,
		"scram-sha-256": e.PGUserPasswordScramSHA256,
	}
	hasher, ok := m[encryption]
	if !ok {
		hasher = e.PGUserPasswordMD5
	}
	e.encrypt = hasher
	return &e
}

func (e *Encryptor) PGUserPassword(user spec.PgUser) string {
	if (len(user.Password) == md5.Size*2+len(md5prefix) && user.Password[:3] == md5prefix) ||
		(len(user.Password) > len(scramsha256prefix) && user.Password[:len(scramsha256prefix)] == scramsha256prefix) || user.Password == "" {
		// Avoid processing already encrypted or empty passwords
		return user.Password
	}
	return e.encrypt(user)
}

func (e *Encryptor) PGUserPasswordMD5(user spec.PgUser) string {
	s := md5.Sum([]byte(user.Password + user.Name)) // #nosec, using md5 since PostgreSQL uses it for hashing passwords.
	return md5prefix + hex.EncodeToString(s[:])
}

func (e *Encryptor) PGUserPasswordScramSHA256(user spec.PgUser) string {
	salt := []byte(e.random(saltlength))
	key := pbkdf2.Key([]byte(user.Password), salt, iterations, 32, sha256.New)
	mac := hmac.New(sha256.New, key)
	mac.Write([]byte("Server Key"))
	serverKey := mac.Sum(nil)
	mac = hmac.New(sha256.New, key)
	mac.Write([]byte("Client Key"))
	clientKey := mac.Sum(nil)
	storedKey := sha256.Sum256(clientKey)
	pass := fmt.Sprintf("%s$%v:%s$%s:%s",
		scramsha256prefix,
		iterations,
		base64.StdEncoding.EncodeToString(salt),
		base64.StdEncoding.EncodeToString(storedKey[:]),
		base64.StdEncoding.EncodeToString(serverKey),
	)
	return pass
}

// Diff returns diffs between 2 objects
func Diff(a, b interface{}) []string {
	return pretty.Diff(a, b)
}

// PrettyDiff shows the diff between 2 objects in an easy to understand format. It is mainly used for debugging output.
func PrettyDiff(a, b interface{}) string {
	return strings.Join(Diff(a, b), "\n")
}

// Compare two string slices while ignoring the order of elements
func IsEqualIgnoreOrder(a, b []string) bool {
	if len(a) != len(b) {
		return false
	}
	a_copy := make([]string, len(a))
	b_copy := make([]string, len(b))
	copy(a_copy, a)
	copy(b_copy, b)
	sort.Strings(a_copy)
	sort.Strings(b_copy)

	return reflect.DeepEqual(a_copy, b_copy)
}

// Iterate through slice and remove certain string, then return cleaned slice
func RemoveString(slice []string, s string) (result []string) {
	for _, item := range slice {
		if item == s {
			continue
		}
		result = append(result, item)
	}
	return result
}

// SliceReplaceElement
func StringSliceReplaceElement(s []string, a, b string) (result []string) {
	tmp := make([]string, 0, len(s))
	for _, str := range s {
		if str == a {
			str = b
		}
		tmp = append(tmp, str)
	}
	return tmp
}

// SubstractStringSlices finds elements in a that are not in b and return them as a result slice.
func SubstractStringSlices(a []string, b []string) (result []string, equal bool) {
	// Slices are assumed to contain unique elements only
OUTER:
	for _, vala := range a {
		for _, valb := range b {
			if vala == valb {
				continue OUTER
			}
		}
		result = append(result, vala)
	}
	return result, len(result) == 0
}

// FindNamedStringSubmatch returns a map of strings holding the text of the matches of the r regular expression
func FindNamedStringSubmatch(r *regexp.Regexp, s string) map[string]string {
	matches := r.FindStringSubmatch(s)
	grNames := r.SubexpNames()

	if matches == nil {
		return nil
	}

	groupMatches := 0
	res := make(map[string]string, len(grNames))
	for i, n := range grNames {
		if n == "" {
			continue
		}

		res[n] = matches[i]
		groupMatches++
	}

	if groupMatches == 0 {
		return nil
	}

	return res
}

// SliceContains
func SliceContains(slice interface{}, item interface{}) bool {
	s := reflect.ValueOf(slice)
	if s.Kind() != reflect.Slice {
		panic("Invalid data-type")
	}
	for i := 0; i < s.Len(); i++ {
		if s.Index(i).Interface() == item {
			return true
		}
	}
	return false
}

// MapContains returns true if and only if haystack contains all the keys from the needle with matching corresponding values
func MapContains(haystack, needle map[string]string) bool {
	if len(haystack) < len(needle) {
		return false
	}

	for k, v := range needle {
		v2, ok := haystack[k]
		if !ok || v2 != v {
			return false
		}
	}

	return true
}

// Coalesce returns the first argument if it is not null, otherwise the second one.
func Coalesce(val, defaultVal string) string {
	if val == "" {
		return defaultVal
	}
	return val
}

// CoalesceStrArr returns the first argument if it is not null, otherwise the second one.
func CoalesceStrArr(val, defaultVal []string) []string {
	if len(val) == 0 {
		return defaultVal
	}
	return val
}

// CoalesceStrMap returns the first argument if it is not null, otherwise the second one.
func CoalesceStrMap(val, defaultVal map[string]string) map[string]string {
	if len(val) == 0 {
		return defaultVal
	}
	return val
}

// CoalesceInt works like coalesce but for int
func CoalesceInt(val, defaultVal int) int {
	if val == 0 {
		return defaultVal
	}
	return val
}

// CoalesceInt32 works like coalesce but for *int32
func CoalesceInt32(val, defaultVal *int32) *int32 {
	if val == nil {
		return defaultVal
	}
	return val
}

// CoalesceUInt32 works like coalesce but for uint32
func CoalesceUInt32(val, defaultVal uint32) uint32 {
	if val == 0 {
		return defaultVal
	}
	return val
}

// CoalesceInt64 works like coalesce but for int64
func CoalesceInt64(val, defaultVal int64) int64 {
	if val == 0 {
		return defaultVal
	}
	return val
}

// CoalesceBool works like coalesce but for *bool
func CoalesceBool(val, defaultVal *bool) *bool {
	if val == nil {
		return defaultVal
	}
	return val
}

// CoalesceDuration works like coalesce but for time.Duration
func CoalesceDuration(val time.Duration, defaultVal string) time.Duration {
	if val == 0 {
		duration, err := time.ParseDuration(defaultVal)
		if err != nil {
			panic(err)
		}
		return duration
	}
	return val
}

// Test if any of the values is nil
func testNil(values ...*int32) bool {
	for _, v := range values {
		if v == nil {
			return true
		}
	}

	return false
}

// ToIntStr converts int to IntOrString type
func ToIntStr(val int) *intstr.IntOrString {
	b := intstr.FromInt(val)
	return &b
}

// Bool2Int converts bool to int
func Bool2Int(flag bool) int {
	if flag {
		return 1
	}
	return 0
}

// MaxInt32 : Return maximum of two integers provided via pointers. If one value
// is not defined, return the other one. If both are not defined, result is also
// undefined, caller needs to check for that.
func MaxInt32(a, b *int32) *int32 {
	if testNil(a, b) {
		return nil
	}

	if *a > *b {
		return a
	}

	return b
}

// IsSmallerQuantity : checks if first resource is of a smaller quantity than the second
func IsSmallerQuantity(requestStr, limitStr string) (bool, error) {

	request, err := resource.ParseQuantity(requestStr)
	if err != nil {
		return false, fmt.Errorf("could not parse request %v : %v", requestStr, err)
	}

	limit, err2 := resource.ParseQuantity(limitStr)
	if err2 != nil {
		return false, fmt.Errorf("could not parse limit %v : %v", limitStr, err2)
	}

	return request.Cmp(limit) == -1, nil
}

func MinResource(maxRequestStr, requestStr string) (resource.Quantity, error) {

	isSmaller, err := IsSmallerQuantity(maxRequestStr, requestStr)
	if isSmaller && err == nil {
		maxRequest, err := resource.ParseQuantity(maxRequestStr)
		if err != nil {
			return maxRequest, err
		}
		return maxRequest, nil
	}

	request, err := resource.ParseQuantity(requestStr)
	if err != nil {
		return request, err
	}
	return request, nil
}


================================================
File: pkg/util/util_test.go
================================================
package util

import (
	"reflect"
	"testing"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

	"regexp"

	"github.com/zalando/postgres-operator/pkg/spec"
)

var pgUsers = []struct {
	in             spec.PgUser
	outmd5         string
	outscramsha256 string
}{{spec.PgUser{
	Name:     "test",
	Password: "password",
	Flags:    []string{},
	MemberOf: []string{}},
	"md587f77988ccb5aa917c93201ba314fcd4", "SCRAM-SHA-256$4096:c2FsdA==$lF4cRm/Jky763CN4HtxdHnjV4Q8AWTNlKvGmEFFU8IQ=:ub8OgRsftnk2ccDMOt7ffHXNcikRkQkq1lh4xaAqrSw="},
	{spec.PgUser{
		Name:     "test",
		Password: "md592f413f3974bdf3799bb6fecb5f9f2c6",
		Flags:    []string{},
		MemberOf: []string{}},
		"md592f413f3974bdf3799bb6fecb5f9f2c6", "md592f413f3974bdf3799bb6fecb5f9f2c6"},
	{spec.PgUser{
		Name:     "test",
		Password: "SCRAM-SHA-256$4096:S1ByZWhvYVV5VDlJNGZoVw==$ozLevu5k0pAQYRrSY+vZhetO6+/oB+qZvuutOdXR94U=:yADwhy0LGloXzh5RaVwLMFyUokwI17VkHVfKVuHu0Zs=",
		Flags:    []string{},
		MemberOf: []string{}},
		"SCRAM-SHA-256$4096:S1ByZWhvYVV5VDlJNGZoVw==$ozLevu5k0pAQYRrSY+vZhetO6+/oB+qZvuutOdXR94U=:yADwhy0LGloXzh5RaVwLMFyUokwI17VkHVfKVuHu0Zs=", "SCRAM-SHA-256$4096:S1ByZWhvYVV5VDlJNGZoVw==$ozLevu5k0pAQYRrSY+vZhetO6+/oB+qZvuutOdXR94U=:yADwhy0LGloXzh5RaVwLMFyUokwI17VkHVfKVuHu0Zs="}}

var prettyDiffTest = []struct {
	inA interface{}
	inB interface{}
	out string
}{
	{[]int{1, 2, 3, 4}, []int{1, 2, 3}, "[]int[4] != []int[3]"},
	{[]int{1, 2, 3, 4}, []int{1, 2, 3, 4}, ""},
}

var isEqualIgnoreOrderTest = []struct {
	inA      []string
	inB      []string
	outEqual bool
}{
	{[]string{"a", "b", "c"}, []string{"a", "b", "c"}, true},
	{[]string{"a", "b", "c"}, []string{"a", "c", "b"}, true},
	{[]string{"a", "b"}, []string{"a", "c", "b"}, false},
	{[]string{"a", "b", "c"}, []string{"a", "d", "c"}, false},
}

var substractTest = []struct {
	inA      []string
	inB      []string
	out      []string
	outEqual bool
}{
	{[]string{"a", "b", "c", "d"}, []string{"a", "b", "c", "d"}, []string{}, true},
	{[]string{"a", "b", "c", "d"}, []string{"a", "bb", "c", "d"}, []string{"b"}, false},
	{[]string{""}, []string{"b"}, []string{""}, false},
	{[]string{"a"}, []string{""}, []string{"a"}, false},
}

var removeStringTest = []struct {
	slice  []string
	item   string
	result []string
}{
	{[]string{"a", "b", "c"}, "b", []string{"a", "c"}},
	{[]string{"a"}, "b", []string{"a"}},
	{[]string{"a"}, "a", []string{}},
	{[]string{}, "a", []string{}},
}

var sliceContaintsTest = []struct {
	slice []string
	item  string
	out   bool
}{
	{[]string{"a", "b", "c"}, "a", true},
	{[]string{"a", "b", "c"}, "d", false},
	{[]string{}, "d", false},
}

var mapContaintsTest = []struct {
	inA map[string]string
	inB map[string]string
	out bool
}{
	{map[string]string{"1": "a", "2": "b", "3": "c", "4": "c"}, map[string]string{"1": "a", "2": "b", "3": "c"}, true},
	{map[string]string{"1": "a", "2": "b", "3": "c", "4": "c"}, map[string]string{"1": "a", "2": "b", "3": "d"}, false},
	{map[string]string{}, map[string]string{}, true},
	{map[string]string{"3": "c", "4": "c"}, map[string]string{"1": "a", "2": "b", "3": "c"}, false},
	{map[string]string{"3": "c", "4": "c"}, map[string]string{}, true},
}

var substringMatch = []struct {
	inRegex *regexp.Regexp
	inStr   string
	out     map[string]string
}{
	{regexp.MustCompile(`aaaa (?P<num>\d+) bbbb`), "aaaa 123 bbbb", map[string]string{"num": "123"}},
	{regexp.MustCompile(`aaaa (?P<num>\d+) bbbb`), "a aa 123 bbbb", nil},
	{regexp.MustCompile(`aaaa \d+ bbbb`), "aaaa 123 bbbb", nil},
	{regexp.MustCompile(`aaaa (\d+) bbbb`), "aaaa 123 bbbb", nil},
}

var requestIsSmallerQuantityTests = []struct {
	request string
	limit   string
	out     bool
}{
	{"1G", "2G", true},
	{"1G", "1Gi", true}, // G is 1000^3 bytes, Gi is 1024^3 bytes
	{"1024Mi", "1G", false},
	{"1e9", "1G", false}, // 1e9 bytes == 1G
}

func TestRandomPassword(t *testing.T) {
	const pwdLength = 10
	pwd := RandomPassword(pwdLength)
	if a := len(pwd); a != pwdLength {
		t.Errorf("password length expected: %d, got: %d", pwdLength, a)
	}
}

func TestNameFromMeta(t *testing.T) {
	meta := metav1.ObjectMeta{
		Name:      "testcluster",
		Namespace: "default",
	}

	expected := spec.NamespacedName{
		Name:      "testcluster",
		Namespace: "default",
	}

	actual := NameFromMeta(meta)
	if actual != expected {
		t.Errorf("NameFromMeta expected: %#v, got: %#v", expected, actual)
	}
}

func TestPGUserPassword(t *testing.T) {
	for _, tt := range pgUsers {
		e := NewEncryptor("md5")
		pwd := e.PGUserPassword(tt.in)
		if pwd != tt.outmd5 {
			t.Errorf("PgUserPassword expected: %q, got: %q", tt.outmd5, pwd)
		}
		e = NewEncryptor("scram-sha-256")
		e.random = func(n int) string { return "salt" }
		pwd = e.PGUserPassword(tt.in)
		if pwd != tt.outscramsha256 {
			t.Errorf("PgUserPassword expected: %q, got: %q", tt.outscramsha256, pwd)
		}
	}
}

func TestPrettyDiff(t *testing.T) {
	for _, tt := range prettyDiffTest {
		if actual := PrettyDiff(tt.inA, tt.inB); actual != tt.out {
			t.Errorf("PrettyDiff expected: %q, got: %q", tt.out, actual)
		}
	}
}

func TestIsEqualIgnoreOrder(t *testing.T) {
	for _, tt := range isEqualIgnoreOrderTest {
		actualEqual := IsEqualIgnoreOrder(tt.inA, tt.inB)
		if actualEqual != tt.outEqual {
			t.Errorf("IsEqualIgnoreOrder expected: %t, got: %t", tt.outEqual, actualEqual)
		}
	}
}

func TestStringSliceReplaceElement(t *testing.T) {
	testSlice := []string{"a", "b", "c"}
	testSlice = StringSliceReplaceElement(testSlice, "b", "d")
	if !SliceContains(testSlice, "d") {
		t.Errorf("testSlide item not replaced: %v", testSlice)
	}
}

func TestSubstractSlices(t *testing.T) {
	for _, tt := range substractTest {
		actualRes, actualEqual := SubstractStringSlices(tt.inA, tt.inB)
		if actualEqual != tt.outEqual {
			t.Errorf("SubstractStringSlices expected equal: %t, got: %t", tt.outEqual, actualEqual)
		}

		if len(actualRes) == 0 && len(tt.out) == 0 {
			continue
		} else if !reflect.DeepEqual(actualRes, tt.out) {
			t.Errorf("SubstractStringSlices expected res: %v, got: %v", tt.out, actualRes)
		}
	}
}

func TestFindNamedStringSubmatch(t *testing.T) {
	for _, tt := range substringMatch {
		actualRes := FindNamedStringSubmatch(tt.inRegex, tt.inStr)
		if !reflect.DeepEqual(actualRes, tt.out) {
			t.Errorf("FindNamedStringSubmatch expected: %#v, got: %#v", tt.out, actualRes)
		}
	}
}

func TestRemoveString(t *testing.T) {
	for _, tt := range removeStringTest {
		res := RemoveString(tt.slice, tt.item)
		if !IsEqualIgnoreOrder(res, tt.result) {
			t.Errorf("RemoveString expected: %#v, got: %#v", tt.result, res)
		}
	}
}

func TestSliceContains(t *testing.T) {
	for _, tt := range sliceContaintsTest {
		res := SliceContains(tt.slice, tt.item)
		if res != tt.out {
			t.Errorf("SliceContains expected: %#v, got: %#v", tt.out, res)
		}
	}
}

func TestMapContains(t *testing.T) {
	for _, tt := range mapContaintsTest {
		res := MapContains(tt.inA, tt.inB)
		if res != tt.out {
			t.Errorf("MapContains expected: %#v, got: %#v", tt.out, res)
		}
	}
}

func TestIsSmallerQuantity(t *testing.T) {
	for _, tt := range requestIsSmallerQuantityTests {
		res, err := IsSmallerQuantity(tt.request, tt.limit)
		if err != nil {
			t.Errorf("IsSmallerQuantity returned unexpected error: %#v", err)
		}
		if res != tt.out {
			t.Errorf("IsSmallerQuantity expected: %#v, got: %#v", tt.out, res)
		}
	}
}

/*
func TestNiceDiff(t *testing.T) {
	o := "a\nb\nc\n"
	n := "b\nd\n"
	d := nicediff.Diff(o, n, true)
	t.Log(d)
	// t.Errorf("Lets see output")
}
*/


================================================
File: pkg/util/config/config.go
================================================
package config

import (
	"encoding/json"
	"strings"
	"time"

	"fmt"

	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util/constants"
	v1 "k8s.io/api/core/v1"
)

// CRD describes CustomResourceDefinition specific configuration parameters
type CRD struct {
	ReadyWaitInterval     time.Duration `name:"ready_wait_interval" default:"4s"`
	ReadyWaitTimeout      time.Duration `name:"ready_wait_timeout" default:"30s"`
	ResyncPeriod          time.Duration `name:"resync_period" default:"30m"`
	RepairPeriod          time.Duration `name:"repair_period" default:"5m"`
	EnableCRDRegistration *bool         `name:"enable_crd_registration" default:"true"`
	EnableCRDValidation   *bool         `name:"enable_crd_validation" default:"true"`
	CRDCategories         []string      `name:"crd_categories" default:"all"`
}

// Resources describes kubernetes resource specific configuration parameters
type Resources struct {
	EnableOwnerReferences         *bool               `name:"enable_owner_references" default:"false"`
	ResourceCheckInterval         time.Duration       `name:"resource_check_interval" default:"3s"`
	ResourceCheckTimeout          time.Duration       `name:"resource_check_timeout" default:"10m"`
	PodLabelWaitTimeout           time.Duration       `name:"pod_label_wait_timeout" default:"10m"`
	PodDeletionWaitTimeout        time.Duration       `name:"pod_deletion_wait_timeout" default:"10m"`
	PodTerminateGracePeriod       time.Duration       `name:"pod_terminate_grace_period" default:"5m"`
	SpiloRunAsUser                *int64              `name:"spilo_runasuser"`
	SpiloRunAsGroup               *int64              `name:"spilo_runasgroup"`
	SpiloFSGroup                  *int64              `name:"spilo_fsgroup"`
	PodPriorityClassName          string              `name:"pod_priority_class_name"`
	ClusterDomain                 string              `name:"cluster_domain" default:"cluster.local"`
	SpiloPrivileged               bool                `name:"spilo_privileged" default:"false"`
	SpiloAllowPrivilegeEscalation *bool               `name:"spilo_allow_privilege_escalation" default:"true"`
	AdditionalPodCapabilities     []string            `name:"additional_pod_capabilities" default:""`
	ClusterLabels                 map[string]string   `name:"cluster_labels" default:"application:spilo"`
	InheritedLabels               []string            `name:"inherited_labels" default:""`
	InheritedAnnotations          []string            `name:"inherited_annotations" default:""`
	DownscalerAnnotations         []string            `name:"downscaler_annotations"`
	IgnoredAnnotations            []string            `name:"ignored_annotations"`
	ClusterNameLabel              string              `name:"cluster_name_label" default:"cluster-name"`
	DeleteAnnotationDateKey       string              `name:"delete_annotation_date_key"`
	DeleteAnnotationNameKey       string              `name:"delete_annotation_name_key"`
	PodRoleLabel                  string              `name:"pod_role_label" default:"spilo-role"`
	PodToleration                 map[string]string   `name:"toleration" default:""`
	DefaultCPURequest             string              `name:"default_cpu_request"`
	DefaultMemoryRequest          string              `name:"default_memory_request"`
	DefaultCPULimit               string              `name:"default_cpu_limit"`
	DefaultMemoryLimit            string              `name:"default_memory_limit"`
	MinCPULimit                   string              `name:"min_cpu_limit"`
	MinMemoryLimit                string              `name:"min_memory_limit"`
	MaxCPURequest                 string              `name:"max_cpu_request"`
	MaxMemoryRequest              string              `name:"max_memory_request"`
	PodEnvironmentConfigMap       spec.NamespacedName `name:"pod_environment_configmap"`
	PodEnvironmentSecret          string              `name:"pod_environment_secret"`
	NodeReadinessLabel            map[string]string   `name:"node_readiness_label" default:""`
	NodeReadinessLabelMerge       string              `name:"node_readiness_label_merge" default:"OR"`
	ShmVolume                     *bool               `name:"enable_shm_volume" default:"true"`

	MaxInstances                      int32  `name:"max_instances" default:"-1"`
	MinInstances                      int32  `name:"min_instances" default:"-1"`
	IgnoreInstanceLimitsAnnotationKey string `name:"ignore_instance_limits_annotation_key"`
}

type InfrastructureRole struct {
	// Name of a secret which describes the role, and optionally name of a
	// configmap with an extra information
	SecretName spec.NamespacedName `json:"secretname,omitempty"`

	UserKey     string `json:"userkey,omitempty"`
	PasswordKey string `json:"passwordkey,omitempty"`
	RoleKey     string `json:"rolekey,omitempty"`

	DefaultUserValue string `json:"defaultuservalue,omitempty"`
	DefaultRoleValue string `json:"defaultrolevalue,omitempty"`

	// This field point out the detailed yaml definition of the role, if exists
	Details string `json:"details,omitempty"`

	// Specify if a secret contains multiple fields in the following format:
	//
	// 	%(userkey)idx: ...
	// 	%(passwordkey)idx: ...
	// 	%(rolekey)idx: ...
	//
	// If it does, Name/Password/Role are interpreted not as unique field
	// names, but as a template.

	Template bool `json:"template,omitempty"`
}

// Auth describes authentication specific configuration parameters
type Auth struct {
	SecretNameTemplate            StringTemplate        `name:"secret_name_template" default:"{username}.{cluster}.credentials.{tprkind}.{tprgroup}"`
	PamRoleName                   string                `name:"pam_role_name" default:"zalandos"`
	PamConfiguration              string                `name:"pam_configuration" default:"https://info.example.com/oauth2/tokeninfo?access_token= uid realm=/employees"`
	TeamsAPIUrl                   string                `name:"teams_api_url" default:"https://teams.example.com/api/"`
	OAuthTokenSecretName          spec.NamespacedName   `name:"oauth_token_secret_name" default:"postgresql-operator"`
	InfrastructureRolesSecretName spec.NamespacedName   `name:"infrastructure_roles_secret_name"`
	InfrastructureRoles           []*InfrastructureRole `name:"-"`
	InfrastructureRolesDefs       string                `name:"infrastructure_roles_secrets"`
	SuperUsername                 string                `name:"super_username" default:"postgres"`
	ReplicationUsername           string                `name:"replication_username" default:"standby"`
	AdditionalOwnerRoles          []string              `name:"additional_owner_roles" default:""`
	EnablePasswordRotation        bool                  `name:"enable_password_rotation" default:"false"`
	PasswordRotationInterval      uint32                `name:"password_rotation_interval" default:"90"`
	PasswordRotationUserRetention uint32                `name:"password_rotation_user_retention" default:"180"`
}

// Scalyr holds the configuration for the Scalyr Agent sidecar for log shipping:
type Scalyr struct {
	ScalyrAPIKey        string `name:"scalyr_api_key" default:""`
	ScalyrImage         string `name:"scalyr_image" default:""`
	ScalyrServerURL     string `name:"scalyr_server_url" default:"https://upload.eu.scalyr.com"`
	ScalyrCPURequest    string `name:"scalyr_cpu_request" default:"100m"`
	ScalyrMemoryRequest string `name:"scalyr_memory_request" default:"50Mi"`
	ScalyrCPULimit      string `name:"scalyr_cpu_limit" default:"1"`
	ScalyrMemoryLimit   string `name:"scalyr_memory_limit" default:"500Mi"`
}

// LogicalBackup defines configuration for logical backup
type LogicalBackup struct {
	LogicalBackupSchedule                     string `name:"logical_backup_schedule" default:"30 00 * * *"`
	LogicalBackupDockerImage                  string `name:"logical_backup_docker_image" default:"ghcr.io/zalando/postgres-operator/logical-backup:v1.14.0"`
	LogicalBackupProvider                     string `name:"logical_backup_provider" default:"s3"`
	LogicalBackupAzureStorageAccountName      string `name:"logical_backup_azure_storage_account_name" default:""`
	LogicalBackupAzureStorageContainer        string `name:"logical_backup_azure_storage_container" default:""`
	LogicalBackupAzureStorageAccountKey       string `name:"logical_backup_azure_storage_account_key" default:""`
	LogicalBackupS3Bucket                     string `name:"logical_backup_s3_bucket" default:""`
	LogicalBackupS3BucketPrefix               string `name:"logical_backup_s3_bucket_prefix" default:"spilo"`
	LogicalBackupS3Region                     string `name:"logical_backup_s3_region" default:""`
	LogicalBackupS3Endpoint                   string `name:"logical_backup_s3_endpoint" default:""`
	LogicalBackupS3AccessKeyID                string `name:"logical_backup_s3_access_key_id" default:""`
	LogicalBackupS3SecretAccessKey            string `name:"logical_backup_s3_secret_access_key" default:""`
	LogicalBackupS3SSE                        string `name:"logical_backup_s3_sse" default:""`
	LogicalBackupS3RetentionTime              string `name:"logical_backup_s3_retention_time" default:""`
	LogicalBackupGoogleApplicationCredentials string `name:"logical_backup_google_application_credentials" default:""`
	LogicalBackupJobPrefix                    string `name:"logical_backup_job_prefix" default:"logical-backup-"`
	LogicalBackupCronjobEnvironmentSecret     string `name:"logical_backup_cronjob_environment_secret" default:""`
	LogicalBackupCPURequest                   string `name:"logical_backup_cpu_request"`
	LogicalBackupMemoryRequest                string `name:"logical_backup_memory_request"`
	LogicalBackupCPULimit                     string `name:"logical_backup_cpu_limit"`
	LogicalBackupMemoryLimit                  string `name:"logical_backup_memory_limit"`
}

// Operator options for connection pooler
type ConnectionPooler struct {
	NumberOfInstances                    *int32 `name:"connection_pooler_number_of_instances" default:"2"`
	Schema                               string `name:"connection_pooler_schema" default:"pooler"`
	User                                 string `name:"connection_pooler_user" default:"pooler"`
	Image                                string `name:"connection_pooler_image" default:"registry.opensource.zalan.do/acid/pgbouncer"`
	Mode                                 string `name:"connection_pooler_mode" default:"transaction"`
	MaxDBConnections                     *int32 `name:"connection_pooler_max_db_connections" default:"60"`
	ConnectionPoolerDefaultCPURequest    string `name:"connection_pooler_default_cpu_request"`
	ConnectionPoolerDefaultMemoryRequest string `name:"connection_pooler_default_memory_request"`
	ConnectionPoolerDefaultCPULimit      string `name:"connection_pooler_default_cpu_limit"`
	ConnectionPoolerDefaultMemoryLimit   string `name:"connection_pooler_default_memory_limit"`
}

// Config describes operator config
type Config struct {
	CRD
	Resources
	Auth
	Scalyr
	LogicalBackup
	ConnectionPooler

	WatchedNamespace        string            `name:"watched_namespace"` // special values: "*" means 'watch all namespaces', the empty string "" means 'watch a namespace where operator is deployed to'
	KubernetesUseConfigMaps bool              `name:"kubernetes_use_configmaps" default:"false"`
	EtcdHost                string            `name:"etcd_host" default:""` // special values: the empty string "" means Patroni will use K8s as a DCS
	DockerImage             string            `name:"docker_image" default:"ghcr.io/zalando/spilo-17:4.0-p2"`
	SidecarImages           map[string]string `name:"sidecar_docker_images"` // deprecated in favour of SidecarContainers
	SidecarContainers       []v1.Container    `name:"sidecars"`
	PodServiceAccountName   string            `name:"pod_service_account_name" default:"postgres-pod"`
	// value of this string must be valid JSON or YAML; see initPodServiceAccount
	PodServiceAccountDefinition              string            `name:"pod_service_account_definition" default:""`
	PodServiceAccountRoleBindingDefinition   string            `name:"pod_service_account_role_binding_definition" default:""`
	MasterPodMoveTimeout                     time.Duration     `name:"master_pod_move_timeout" default:"20m"`
	DbHostedZone                             string            `name:"db_hosted_zone" default:"db.example.com"`
	AWSRegion                                string            `name:"aws_region" default:"eu-central-1"`
	WALES3Bucket                             string            `name:"wal_s3_bucket"`
	LogS3Bucket                              string            `name:"log_s3_bucket"`
	KubeIAMRole                              string            `name:"kube_iam_role"`
	WALGSBucket                              string            `name:"wal_gs_bucket"`
	GCPCredentials                           string            `name:"gcp_credentials"`
	WALAZStorageAccount                      string            `name:"wal_az_storage_account"`
	AdditionalSecretMount                    string            `name:"additional_secret_mount"`
	AdditionalSecretMountPath                string            `name:"additional_secret_mount_path"`
	EnableEBSGp3Migration                    bool              `name:"enable_ebs_gp3_migration" default:"false"`
	EnableEBSGp3MigrationMaxSize             int64             `name:"enable_ebs_gp3_migration_max_size" default:"1000"`
	DebugLogging                             bool              `name:"debug_logging" default:"true"`
	EnableDBAccess                           bool              `name:"enable_database_access" default:"true"`
	EnableTeamsAPI                           bool              `name:"enable_teams_api" default:"true"`
	EnableTeamSuperuser                      bool              `name:"enable_team_superuser" default:"false"`
	TeamAdminRole                            string            `name:"team_admin_role" default:"admin"`
	RoleDeletionSuffix                       string            `name:"role_deletion_suffix" default:"_deleted"`
	EnableTeamMemberDeprecation              bool              `name:"enable_team_member_deprecation" default:"false"`
	EnableAdminRoleForUsers                  bool              `name:"enable_admin_role_for_users" default:"true"`
	EnablePostgresTeamCRD                    bool              `name:"enable_postgres_team_crd" default:"false"`
	EnablePostgresTeamCRDSuperusers          bool              `name:"enable_postgres_team_crd_superusers" default:"false"`
	EnableMasterLoadBalancer                 bool              `name:"enable_master_load_balancer" default:"true"`
	EnableMasterPoolerLoadBalancer           bool              `name:"enable_master_pooler_load_balancer" default:"false"`
	EnableReplicaLoadBalancer                bool              `name:"enable_replica_load_balancer" default:"false"`
	EnableReplicaPoolerLoadBalancer          bool              `name:"enable_replica_pooler_load_balancer" default:"false"`
	CustomServiceAnnotations                 map[string]string `name:"custom_service_annotations"`
	CustomPodAnnotations                     map[string]string `name:"custom_pod_annotations"`
	EnablePodAntiAffinity                    bool              `name:"enable_pod_antiaffinity" default:"false"`
	PodAntiAffinityPreferredDuringScheduling bool              `name:"pod_antiaffinity_preferred_during_scheduling" default:"false"`
	PodAntiAffinityTopologyKey               string            `name:"pod_antiaffinity_topology_key" default:"kubernetes.io/hostname"`
	StorageResizeMode                        string            `name:"storage_resize_mode" default:"pvc"`
	EnableLoadBalancer                       *bool             `name:"enable_load_balancer"` // deprecated and kept for backward compatibility
	ExternalTrafficPolicy                    string            `name:"external_traffic_policy" default:"Cluster"`
	MasterDNSNameFormat                      StringTemplate    `name:"master_dns_name_format" default:"{cluster}.{namespace}.{hostedzone}"`
	MasterLegacyDNSNameFormat                StringTemplate    `name:"master_legacy_dns_name_format" default:"{cluster}.{team}.{hostedzone}"`
	ReplicaDNSNameFormat                     StringTemplate    `name:"replica_dns_name_format" default:"{cluster}-repl.{namespace}.{hostedzone}"`
	ReplicaLegacyDNSNameFormat               StringTemplate    `name:"replica_legacy_dns_name_format" default:"{cluster}-repl.{team}.{hostedzone}"`
	PDBNameFormat                            StringTemplate    `name:"pdb_name_format" default:"postgres-{cluster}-pdb"`
	PDBMasterLabelSelector                   *bool             `name:"pdb_master_label_selector" default:"true"`
	EnablePodDisruptionBudget                *bool             `name:"enable_pod_disruption_budget" default:"true"`
	EnableInitContainers                     *bool             `name:"enable_init_containers" default:"true"`
	EnableSidecars                           *bool             `name:"enable_sidecars" default:"true"`
	SharePgSocketWithSidecars                *bool             `name:"share_pgsocket_with_sidecars" default:"false"`
	Workers                                  uint32            `name:"workers" default:"8"`
	APIPort                                  int               `name:"api_port" default:"8080"`
	RingLogLines                             int               `name:"ring_log_lines" default:"100"`
	ClusterHistoryEntries                    int               `name:"cluster_history_entries" default:"1000"`
	TeamAPIRoleConfiguration                 map[string]string `name:"team_api_role_configuration" default:"log_statement:all"`
	PodTerminateGracePeriod                  time.Duration     `name:"pod_terminate_grace_period" default:"5m"`
	PodManagementPolicy                      string            `name:"pod_management_policy" default:"ordered_ready"`
	EnableReadinessProbe                     bool              `name:"enable_readiness_probe" default:"false"`
	ProtectedRoles                           []string          `name:"protected_role_names" default:"admin,cron_admin"`
	PostgresSuperuserTeams                   []string          `name:"postgres_superuser_teams" default:""`
	SetMemoryRequestToLimit                  bool              `name:"set_memory_request_to_limit" default:"false"`
	EnableLazySpiloUpgrade                   bool              `name:"enable_lazy_spilo_upgrade" default:"false"`
	EnableCrossNamespaceSecret               bool              `name:"enable_cross_namespace_secret" default:"false"`
	EnableFinalizers                         *bool             `name:"enable_finalizers" default:"false"`
	EnablePgVersionEnvVar                    bool              `name:"enable_pgversion_env_var" default:"true"`
	EnableSpiloWalPathCompat                 bool              `name:"enable_spilo_wal_path_compat" default:"false"`
	EnableTeamIdClusternamePrefix            bool              `name:"enable_team_id_clustername_prefix" default:"false"`
	MajorVersionUpgradeMode                  string            `name:"major_version_upgrade_mode" default:"manual"`
	MajorVersionUpgradeTeamAllowList         []string          `name:"major_version_upgrade_team_allow_list" default:""`
	MinimalMajorVersion                      string            `name:"minimal_major_version" default:"13"`
	TargetMajorVersion                       string            `name:"target_major_version" default:"17"`
	PatroniAPICheckInterval                  time.Duration     `name:"patroni_api_check_interval" default:"1s"`
	PatroniAPICheckTimeout                   time.Duration     `name:"patroni_api_check_timeout" default:"5s"`
	EnablePatroniFailsafeMode                *bool             `name:"enable_patroni_failsafe_mode" default:"false"`
	EnableSecretsDeletion                    *bool             `name:"enable_secrets_deletion" default:"true"`
	EnablePersistentVolumeClaimDeletion      *bool             `name:"enable_persistent_volume_claim_deletion" default:"true"`
	PersistentVolumeClaimRetentionPolicy     map[string]string `name:"persistent_volume_claim_retention_policy" default:"when_deleted:retain,when_scaled:retain"`
}

// MustMarshal marshals the config or panics
func (c Config) MustMarshal() string {
	b, err := json.MarshalIndent(c, "", "   ")
	if err != nil {
		panic(err)
	}

	return string(b)
}

// NewFromMap creates Config from the map
func NewFromMap(m map[string]string) *Config {
	cfg := Config{}
	fields, _ := structFields(&cfg)

	for _, structField := range fields {
		key := strings.ToLower(structField.Name)
		value, ok := m[key]
		if !ok && structField.Default != "" {
			value = structField.Default
		}

		if value == "" {
			continue
		}
		err := processField(value, structField.Field)
		if err != nil {
			panic(err)
		}
	}
	if err := validate(&cfg); err != nil {
		panic(err)
	}

	return &cfg
}

// Copy creates a copy of the config
func Copy(c *Config) Config {
	cfg := *c

	cfg.ClusterLabels = make(map[string]string, len(c.ClusterLabels))
	for k, v := range c.ClusterLabels {
		cfg.ClusterLabels[k] = v
	}

	return cfg
}

func validate(cfg *Config) (err error) {
	if cfg.MinInstances > 0 && cfg.MaxInstances > 0 && cfg.MinInstances > cfg.MaxInstances {
		err = fmt.Errorf("minimum number of instances %d is set higher than the maximum number %d",
			cfg.MinInstances, cfg.MaxInstances)
	}
	if cfg.Workers == 0 {
		err = fmt.Errorf("number of workers should be higher than 0")
	}

	if *cfg.ConnectionPooler.NumberOfInstances < constants.ConnectionPoolerMinInstances {
		msg := "number of connection pooler instances should be higher than %d"
		err = fmt.Errorf(msg, constants.ConnectionPoolerMinInstances)
	}

	if cfg.ConnectionPooler.User == cfg.SuperUsername {
		msg := "connection pool user is not allowed to be the same as super user, username: %s"
		err = fmt.Errorf(msg, cfg.ConnectionPooler.User)
	}

	return
}


================================================
File: pkg/util/config/config_test.go
================================================
package config

import (
	"fmt"
	"reflect"
	"testing"
)

var getMapPairsFromStringTest = []struct {
	in       string
	expected []string
	err      error
}{
	{"log_statement:all, work_mem:'4GB'", []string{"log_statement:all", "work_mem:'4GB'"}, nil},
	{`log_statement:none, search_path:'"$user", public'`, []string{"log_statement:none", `search_path:'"$user", public'`}, nil},
	{`search_path:'"$user"`, nil, fmt.Errorf("unmatched quote starting at position 13")},
	{"", []string{""}, nil},
	{",,log_statement:all	,", []string{"", "", "log_statement:all", ""}, nil},
}

func TestGetMapPairsFromString(t *testing.T) {
	for _, tt := range getMapPairsFromStringTest {
		got, err := getMapPairsFromString(tt.in)
		if err != tt.err && ((err == nil || tt.err == nil) || (err.Error() != tt.err.Error())) {
			t.Errorf("TestGetMapPairsFromString with %s: expected error: %#v, got %#v", tt.in, tt.err, err)
		}
		if !reflect.DeepEqual(got, tt.expected) {
			t.Errorf("TestGetMapPairsFromString with %s: expected %#v, got %#v", tt.in, tt.expected, got)
		}
	}
}


================================================
File: pkg/util/config/util.go
================================================
package config

import (
	"encoding/json"
	"fmt"
	"reflect"
	"strconv"
	"strings"
	"time"
)

type decoder interface {
	Decode(value string) error
}

type fieldInfo struct {
	Name    string
	Default string
	Field   reflect.Value
}

// StringTemplate is a convenience alias
type StringTemplate string

func decoderFrom(field reflect.Value) (d decoder) {
	// it may be impossible for a struct field to fail this check
	if !field.CanInterface() {
		return
	}

	d, ok := field.Interface().(decoder)
	if !ok && field.CanAddr() {
		d, _ = field.Addr().Interface().(decoder)
	}

	return d
}

// taken from github.com/kelseyhightower/envconfig
func structFields(spec interface{}) ([]fieldInfo, error) {
	s := reflect.ValueOf(spec).Elem()

	// over allocate an info array, we will extend if needed later
	infos := make([]fieldInfo, 0, s.NumField())
	for i := 0; i < s.NumField(); i++ {
		f := s.Field(i)
		ftype := s.Type().Field(i)

		fieldName := ftype.Tag.Get("name")
		if fieldName == "" {
			fieldName = strings.ToLower(ftype.Name)
		}

		// Capture information about the config variable
		info := fieldInfo{
			Name:    fieldName,
			Field:   f,
			Default: ftype.Tag.Get("default"),
		}
		infos = append(infos, info)

		if f.Kind() == reflect.Struct {
			// honor Decode if present
			if decoderFrom(f) == nil {
				embeddedPtr := f.Addr().Interface()
				embeddedInfos, err := structFields(embeddedPtr)
				if err != nil {
					return nil, err
				}
				infos = append(infos[:len(infos)-1], embeddedInfos...)

				continue
			}
		}
	}

	return infos, nil
}

func processField(value string, field reflect.Value) error {
	typ := field.Type()

	decoder := decoderFrom(field)
	if decoder != nil {
		return decoder.Decode(value)
	}

	if typ.Kind() == reflect.Ptr {
		typ = typ.Elem()
		if field.IsNil() {
			field.Set(reflect.New(typ))
		}
		field = field.Elem()
	}

	switch typ.Kind() {
	case reflect.String:
		field.SetString(value)
	case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:
		var (
			val int64
			err error
		)
		if field.Kind() == reflect.Int64 && typ.PkgPath() == "time" && typ.Name() == "Duration" {
			var d time.Duration
			d, err = time.ParseDuration(value)
			val = int64(d)
		} else {
			val, err = strconv.ParseInt(value, 0, typ.Bits())
		}
		if err != nil {
			return err
		}

		field.SetInt(val)
	case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:
		val, err := strconv.ParseUint(value, 0, typ.Bits())
		if err != nil {
			return err
		}
		field.SetUint(val)
	case reflect.Bool:
		val, err := strconv.ParseBool(value)
		if err != nil {
			return err
		}
		field.SetBool(val)
	case reflect.Float32, reflect.Float64:
		val, err := strconv.ParseFloat(value, typ.Bits())
		if err != nil {
			return err
		}
		field.SetFloat(val)
	case reflect.Slice:
		vals := strings.Split(value, ",")
		sl := reflect.MakeSlice(typ, len(vals), len(vals))
		for i, val := range vals {
			err := processField(val, sl.Index(i))
			if err != nil {
				return err
			}
		}
		field.Set(sl)
	case reflect.Map:
		pairs, err := getMapPairsFromString(value)
		if err != nil {
			return fmt.Errorf("could not split value %q into map items: %v", value, err)
		}
		mp := reflect.MakeMap(typ)
		for _, pair := range pairs {
			kvpair := strings.Split(pair, ":")
			if len(kvpair) != 2 {
				return fmt.Errorf("invalid map item: %q", pair)
			}
			k := reflect.New(typ.Key()).Elem()
			err := processField(kvpair[0], k)
			if err != nil {
				return err
			}
			v := reflect.New(typ.Elem()).Elem()
			err = processField(kvpair[1], v)
			if err != nil {
				return err
			}
			mp.SetMapIndex(k, v)
		}
		field.Set(mp)
	}

	return nil
}

type parserState int

const (
	plain parserState = iota
	doubleQuoted
	singleQuoted
)

// Split the pair candidates by commas not located inside open quotes
// Escape characters are not supported for simplicity, as we don't
// expect to find them inside the map values for our use cases
func getMapPairsFromString(value string) (pairs []string, err error) {
	pairs = make([]string, 0)
	state := plain
	var start, quote int

	for i, ch := range strings.Split(value, "") {
		if (ch == `"` || ch == `'`) && i > 0 && value[i-1] == '\\' {
			fmt.Printf("Parser warning: ecape character '\\' have no effect on quotes inside the configuration value %s\n", value)
		}
		if ch == `"` {
			if state == plain {
				state = doubleQuoted
				quote = i
			} else if state == doubleQuoted {
				state = plain
				quote = 0
			}
		}
		if ch == "'" {
			if state == plain {
				state = singleQuoted
				quote = i
			} else if state == singleQuoted {
				state = plain
				quote = 0
			}
		}
		if ch == "," && state == plain {
			pairs = append(pairs, strings.Trim(value[start:i], " \t"))
			start = i + 1
		}
	}
	if state != plain {
		err = fmt.Errorf("unmatched quote starting at position %d", quote+1)
		pairs = nil
	} else {
		pairs = append(pairs, strings.Trim(value[start:], " \t"))
	}
	return
}

// Decode cast value to StringTemplate
func (f *StringTemplate) Decode(value string) error {
	*f = StringTemplate(value)

	return nil
}

// Format formatted string from StringTemplate
func (f *StringTemplate) Format(a ...string) string {
	res := string(*f)

	for i := 0; i < len(a); i += 2 {
		res = strings.Replace(res, "{"+a[i]+"}", a[i+1], -1)
	}

	return res
}

// MarshalJSON converts a StringTemplate to byte slice
func (f StringTemplate) MarshalJSON() ([]byte, error) {
	return json.Marshal(string(f))
}


================================================
File: pkg/util/constants/annotations.go
================================================
package constants

// Names and values in Kubernetes annotation for services, statefulsets and volumes
const (
	ZalandoDNSNameAnnotation           = "external-dns.alpha.kubernetes.io/hostname"
	ElbTimeoutAnnotationName           = "service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout"
	ElbTimeoutAnnotationValue          = "3600"
	KubeIAmAnnotation                  = "iam.amazonaws.com/role"
	VolumeStorateProvisionerAnnotation = "pv.kubernetes.io/provisioned-by"
	PostgresqlControllerAnnotationKey  = "acid.zalan.do/controller"
)


================================================
File: pkg/util/constants/aws.go
================================================
package constants

import "time"

// AWS specific constants used by other modules
const (
	// EBS related constants
	EBSVolumeIDStart = "/vol-"
	EBSProvisioner   = "kubernetes.io/aws-ebs"
	EBSDriver        = "ebs.csi.aws.com"
	//https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_VolumeModification.html
	EBSVolumeStateModifying     = "modifying"
	EBSVolumeStateOptimizing    = "optimizing"
	EBSVolumeStateFailed        = "failed"
	EBSVolumeStateCompleted     = "completed"
	EBSVolumeResizeWaitInterval = 2 * time.Second
	EBSVolumeResizeWaitTimeout  = 30 * time.Second
)


================================================
File: pkg/util/constants/kubernetes.go
================================================
package constants

import "time"

// General kubernetes-related constants
const (
	PostgresContainerName = "postgres"
	K8sAPIPath            = "/apis"

	QueueResyncPeriodPod  = 5 * time.Minute
	QueueResyncPeriodTPR  = 5 * time.Minute
	QueueResyncPeriodNode = 5 * time.Minute
)


================================================
File: pkg/util/constants/pooler.go
================================================
package constants

// Connection pooler specific constants
const (
	ConnectionPoolerResourceSuffix       = "pooler"
	ConnectionPoolerUserName             = "pooler"
	ConnectionPoolerSchemaName           = "pooler"
	ConnectionPoolerDefaultType          = "pgbouncer"
	ConnectionPoolerDefaultMode          = "transaction"
	ConnectionPoolerDefaultCpuRequest    = "500m"
	ConnectionPoolerDefaultCpuLimit      = "1"
	ConnectionPoolerDefaultMemoryRequest = "100Mi"
	ConnectionPoolerDefaultMemoryLimit   = "100Mi"

	ConnectionPoolerContainer            = 0
	ConnectionPoolerMaxDBConnections     = 60
	ConnectionPoolerMaxClientConnections = 10000
	ConnectionPoolerMinInstances         = 1
)


================================================
File: pkg/util/constants/postgresql.go
================================================
package constants

import "time"

// PostgreSQL specific constants
const (
	DataVolumeName    = "pgdata"
	PostgresDataMount = "/home/postgres/pgdata"
	PostgresDataPath  = PostgresDataMount + "/pgroot"

	PatroniPGParametersParameterName = "parameters"

	PostgresConnectRetryTimeout = 2 * time.Minute
	PostgresConnectTimeout      = 15 * time.Second

	ShmVolumeName = "dshm"
	ShmVolumePath = "/dev/shm"

	RunVolumeName = "postgresql-run"
	RunVolumePath = "/var/run/postgresql"
)


================================================
File: pkg/util/constants/roles.go
================================================
package constants

// Roles specific constants
const (
	PasswordLength              = 64
	SuperuserKeyName            = "superuser"
	ReplicationUserKeyName      = "replication"
	ConnectionPoolerUserKeyName = "pooler"
	EventStreamUserKeyName      = "streamer"
	RoleFlagSuperuser           = "SUPERUSER"
	RoleFlagInherit             = "INHERIT"
	RoleFlagLogin               = "LOGIN"
	RoleFlagNoLogin             = "NOLOGIN"
	RoleFlagCreateRole          = "CREATEROLE"
	RoleFlagCreateDB            = "CREATEDB"
	RoleFlagReplication         = "REPLICATION"
	RoleFlagByPassRLS           = "BYPASSRLS"
	OwnerRoleNameSuffix         = "_owner"
	ReaderRoleNameSuffix        = "_reader"
	WriterRoleNameSuffix        = "_writer"
	UserRoleNameSuffix          = "_user"
	DefaultSearchPath           = "\"$user\""
	RotationUserDateFormat      = "060102"
)


================================================
File: pkg/util/constants/streams.go
================================================
package constants

// PostgreSQL specific constants
const (
	EventStreamCRDApiVersion       = "zalando.org/v1"
	EventStreamCRDKind             = "FabricEventStream"
	EventStreamCRDName             = "fabriceventstreams.zalando.org"
	EventStreamSourcePGType        = "PostgresLogicalReplication"
	EventStreamSourceSlotPrefix    = "fes"
	EventStreamSourcePluginType    = "pgoutput"
	EventStreamSourceAuthType      = "DatabaseAuthenticationSecret"
	EventStreamFlowPgGenericType   = "PostgresWalToGenericNakadiEvent"
	EventStreamSinkNakadiType      = "Nakadi"
	EventStreamRecoveryDLQType     = "DeadLetter"
	EventStreamRecoveryIgnoreType  = "Ignore"
	EventStreamRecoveryNoneType    = "None"
	EventStreamRecoverySuffix      = "dead-letter-queue"
	EventStreamCpuAnnotationKey    = "fes.zalando.org/FES_CPU"
	EventStreamMemoryAnnotationKey = "fes.zalando.org/FES_MEMORY"
)


================================================
File: pkg/util/constants/units.go
================================================
package constants

// Measurement-unit definitions
const (
	Gigabyte = 1073741824
)


================================================
File: pkg/util/filesystems/ext234.go
================================================
package filesystems

import (
	"fmt"
	"regexp"
	"strings"
)

var (
	ext2fsSuccessRegexp = regexp.MustCompile(`The filesystem on [/a-z0-9]+ is now \d+ \(\d+\w+\) blocks long.`)
)

const (
	ext2      = "ext2"
	ext3      = "ext3"
	ext4      = "ext4"
	resize2fs = "resize2fs"
)

//Ext234Resize implements the FilesystemResizer interface for the ext4/3/2fs.
type Ext234Resize struct {
}

// CanResizeFilesystem checks whether Ext234Resize can resize this filesystem.
func (c *Ext234Resize) CanResizeFilesystem(fstype string) bool {
	return fstype == ext2 || fstype == ext3 || fstype == ext4
}

// ResizeFilesystem calls resize2fs to resize the filesystem if necessary.
func (c *Ext234Resize) ResizeFilesystem(deviceName string, commandExecutor func(cmd string) (out string, err error)) error {
	command := fmt.Sprintf("%s %s 2>&1", resize2fs, deviceName)
	out, err := commandExecutor(command)
	if err != nil {
		return err
	}
	if strings.Contains(out, "Nothing to do") ||
		(strings.Contains(out, "on-line resizing required") && ext2fsSuccessRegexp.MatchString(out)) {
		return nil
	}
	return fmt.Errorf("unrecognized output: %q, assuming error", out)
}


================================================
File: pkg/util/filesystems/filesystems.go
================================================
package filesystems

// FilesystemResizer has methods to work with resizing of a filesystem
type FilesystemResizer interface {
	CanResizeFilesystem(fstype string) bool
	ResizeFilesystem(deviceName string, commandExecutor func(string) (out string, err error)) error
}


================================================
File: pkg/util/httpclient/httpclient.go
================================================
package httpclient

//go:generate mockgen -package mocks -destination=../../../mocks/$GOFILE -source=$GOFILE -build_flags=-mod=vendor

import "net/http"

// HTTPClient interface
type HTTPClient interface {
	Do(req *http.Request) (*http.Response, error)
	Get(url string) (resp *http.Response, err error)
}


================================================
File: pkg/util/k8sutil/k8sutil.go
================================================
package k8sutil

import (
	"context"
	"fmt"

	b64 "encoding/base64"
	"encoding/json"

	apiacidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	zalandoclient "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned"
	acidv1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/acid.zalan.do/v1"
	zalandov1 "github.com/zalando/postgres-operator/pkg/generated/clientset/versioned/typed/zalando.org/v1"
	"github.com/zalando/postgres-operator/pkg/spec"
	apiappsv1 "k8s.io/api/apps/v1"
	v1 "k8s.io/api/core/v1"
	apiextv1 "k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1"
	apiextclient "k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset"
	apiextv1client "k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset/typed/apiextensions/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/kubernetes"
	appsv1 "k8s.io/client-go/kubernetes/typed/apps/v1"
	batchv1 "k8s.io/client-go/kubernetes/typed/batch/v1"
	corev1 "k8s.io/client-go/kubernetes/typed/core/v1"
	policyv1 "k8s.io/client-go/kubernetes/typed/policy/v1"
	rbacv1 "k8s.io/client-go/kubernetes/typed/rbac/v1"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
)

func Int32ToPointer(value int32) *int32 {
	return &value
}

func UInt32ToPointer(value uint32) *uint32 {
	return &value
}

func StringToPointer(str string) *string {
	return &str
}

// KubernetesClient describes getters for Kubernetes objects
type KubernetesClient struct {
	corev1.SecretsGetter
	corev1.ServicesGetter
	corev1.EndpointsGetter
	corev1.PodsGetter
	corev1.PersistentVolumesGetter
	corev1.PersistentVolumeClaimsGetter
	corev1.ConfigMapsGetter
	corev1.NodesGetter
	corev1.NamespacesGetter
	corev1.ServiceAccountsGetter
	corev1.EventsGetter
	appsv1.StatefulSetsGetter
	appsv1.DeploymentsGetter
	rbacv1.RoleBindingsGetter
	batchv1.CronJobsGetter
	policyv1.PodDisruptionBudgetsGetter
	apiextv1client.CustomResourceDefinitionsGetter
	acidv1.OperatorConfigurationsGetter
	acidv1.PostgresTeamsGetter
	acidv1.PostgresqlsGetter
	zalandov1.FabricEventStreamsGetter

	RESTClient         rest.Interface
	AcidV1ClientSet    *zalandoclient.Clientset
	Zalandov1ClientSet *zalandoclient.Clientset
}

type mockCustomResourceDefinition struct {
	apiextv1client.CustomResourceDefinitionInterface
}

type MockCustomResourceDefinitionsGetter struct {
}

type mockSecret struct {
	corev1.SecretInterface
}

type MockSecretGetter struct {
}

type mockDeployment struct {
	appsv1.DeploymentInterface
}

type mockDeploymentNotExist struct {
	appsv1.DeploymentInterface
}

type MockDeploymentGetter struct {
}

type MockDeploymentNotExistGetter struct {
}

type mockService struct {
	corev1.ServiceInterface
}

type mockServiceNotExist struct {
	corev1.ServiceInterface
}

type MockServiceGetter struct {
}

type MockServiceNotExistGetter struct {
}

type mockConfigMap struct {
	corev1.ConfigMapInterface
}

type MockConfigMapsGetter struct {
}

// RestConfig creates REST config
func RestConfig(kubeConfig string, outOfCluster bool) (*rest.Config, error) {
	if outOfCluster {
		return clientcmd.BuildConfigFromFlags("", kubeConfig)
	}

	return rest.InClusterConfig()
}

// ResourceAlreadyExists checks if error corresponds to Already exists error
func ResourceAlreadyExists(err error) bool {
	return apierrors.IsAlreadyExists(err)
}

// ResourceNotFound checks if error corresponds to Not found error
func ResourceNotFound(err error) bool {
	return apierrors.IsNotFound(err)
}

// NewFromConfig create Kubernetes Interface using REST config
func NewFromConfig(cfg *rest.Config) (KubernetesClient, error) {
	kubeClient := KubernetesClient{}

	client, err := kubernetes.NewForConfig(cfg)
	if err != nil {
		return kubeClient, fmt.Errorf("could not get clientset: %v", err)
	}

	kubeClient.PodsGetter = client.CoreV1()
	kubeClient.ServicesGetter = client.CoreV1()
	kubeClient.EndpointsGetter = client.CoreV1()
	kubeClient.SecretsGetter = client.CoreV1()
	kubeClient.ServiceAccountsGetter = client.CoreV1()
	kubeClient.ConfigMapsGetter = client.CoreV1()
	kubeClient.PersistentVolumeClaimsGetter = client.CoreV1()
	kubeClient.PersistentVolumesGetter = client.CoreV1()
	kubeClient.NodesGetter = client.CoreV1()
	kubeClient.NamespacesGetter = client.CoreV1()
	kubeClient.StatefulSetsGetter = client.AppsV1()
	kubeClient.DeploymentsGetter = client.AppsV1()
	kubeClient.PodDisruptionBudgetsGetter = client.PolicyV1()
	kubeClient.RESTClient = client.CoreV1().RESTClient()
	kubeClient.RoleBindingsGetter = client.RbacV1()
	kubeClient.CronJobsGetter = client.BatchV1()
	kubeClient.EventsGetter = client.CoreV1()

	apiextClient, err := apiextclient.NewForConfig(cfg)
	if err != nil {
		return kubeClient, fmt.Errorf("could not create api client:%v", err)
	}

	kubeClient.CustomResourceDefinitionsGetter = apiextClient.ApiextensionsV1()

	kubeClient.AcidV1ClientSet = zalandoclient.NewForConfigOrDie(cfg)
	if err != nil {
		return kubeClient, fmt.Errorf("could not create acid.zalan.do clientset: %v", err)
	}
	kubeClient.Zalandov1ClientSet = zalandoclient.NewForConfigOrDie(cfg)
	if err != nil {
		return kubeClient, fmt.Errorf("could not create zalando.org clientset: %v", err)
	}

	kubeClient.OperatorConfigurationsGetter = kubeClient.AcidV1ClientSet.AcidV1()
	kubeClient.PostgresTeamsGetter = kubeClient.AcidV1ClientSet.AcidV1()
	kubeClient.PostgresqlsGetter = kubeClient.AcidV1ClientSet.AcidV1()
	kubeClient.FabricEventStreamsGetter = kubeClient.Zalandov1ClientSet.ZalandoV1()

	return kubeClient, nil
}

// SetPostgresCRDStatus of Postgres cluster
func (client *KubernetesClient) SetPostgresCRDStatus(clusterName spec.NamespacedName, status string) (*apiacidv1.Postgresql, error) {
	var pg *apiacidv1.Postgresql
	var pgStatus apiacidv1.PostgresStatus
	pgStatus.PostgresClusterStatus = status

	patch, err := json.Marshal(struct {
		PgStatus interface{} `json:"status"`
	}{&pgStatus})

	if err != nil {
		return pg, fmt.Errorf("could not marshal status: %v", err)
	}

	// we cannot do a full scale update here without fetching the previous manifest (as the resourceVersion may differ),
	// however, we could do patch without it. In the future, once /status subresource is there (starting Kubernetes 1.11)
	// we should take advantage of it.
	pg, err = client.PostgresqlsGetter.Postgresqls(clusterName.Namespace).Patch(
		context.TODO(), clusterName.Name, types.MergePatchType, patch, metav1.PatchOptions{}, "status")
	if err != nil {
		return pg, fmt.Errorf("could not update status: %v", err)
	}

	return pg, nil
}

// SetFinalizer of Postgres cluster
func (client *KubernetesClient) SetFinalizer(clusterName spec.NamespacedName, pg *apiacidv1.Postgresql, finalizers []string) (*apiacidv1.Postgresql, error) {
	var (
		updatedPg *apiacidv1.Postgresql
		patch     []byte
		err       error
	)
	pg.ObjectMeta.Finalizers = finalizers

	if len(finalizers) > 0 {
		patch, err = json.Marshal(struct {
			PgMetadata interface{} `json:"metadata"`
		}{&pg.ObjectMeta})
		if err != nil {
			return pg, fmt.Errorf("could not marshal ObjectMeta: %v", err)
		}

		updatedPg, err = client.PostgresqlsGetter.Postgresqls(clusterName.Namespace).Patch(
			context.TODO(), clusterName.Name, types.MergePatchType, patch, metav1.PatchOptions{})
	} else {
		// in case finalizers are empty and update is needed to remove
		updatedPg, err = client.PostgresqlsGetter.Postgresqls(clusterName.Namespace).Update(
			context.TODO(), pg, metav1.UpdateOptions{})
	}
	if err != nil {
		return updatedPg, fmt.Errorf("could not set finalizer: %v", err)
	}

	return updatedPg, nil
}

func (c *mockCustomResourceDefinition) Get(ctx context.Context, name string, options metav1.GetOptions) (*apiextv1.CustomResourceDefinition, error) {
	return &apiextv1.CustomResourceDefinition{}, nil
}

func (c *mockCustomResourceDefinition) Create(ctx context.Context, crd *apiextv1.CustomResourceDefinition, options metav1.CreateOptions) (*apiextv1.CustomResourceDefinition, error) {
	return &apiextv1.CustomResourceDefinition{}, nil
}

func (mock *MockCustomResourceDefinitionsGetter) CustomResourceDefinitions() apiextv1client.CustomResourceDefinitionInterface {
	return &mockCustomResourceDefinition{}
}

func (c *mockSecret) Get(ctx context.Context, name string, options metav1.GetOptions) (*v1.Secret, error) {
	oldFormatSecret := &v1.Secret{}
	oldFormatSecret.Name = "testcluster"
	oldFormatSecret.Data = map[string][]byte{
		"user1":     []byte("testrole"),
		"password1": []byte("testpassword"),
		"inrole1":   []byte("testinrole"),
		"foobar":    []byte(b64.StdEncoding.EncodeToString([]byte("password"))),
	}

	newFormatSecret := &v1.Secret{}
	newFormatSecret.Name = "test-secret-new-format"
	newFormatSecret.Data = map[string][]byte{
		"user":       []byte("new-test-role"),
		"password":   []byte("new-test-password"),
		"inrole":     []byte("new-test-inrole"),
		"new-foobar": []byte(b64.StdEncoding.EncodeToString([]byte("password"))),
	}

	secrets := map[string]*v1.Secret{
		"infrastructureroles-old-test": oldFormatSecret,
		"infrastructureroles-new-test": newFormatSecret,
	}

	for idx := 1; idx <= 2; idx++ {
		newFormatStandaloneSecret := &v1.Secret{}
		newFormatStandaloneSecret.Name = fmt.Sprintf("test-secret-new-format%d", idx)
		newFormatStandaloneSecret.Data = map[string][]byte{
			"user":     []byte(fmt.Sprintf("new-test-role%d", idx)),
			"password": []byte(fmt.Sprintf("new-test-password%d", idx)),
			"inrole":   []byte(fmt.Sprintf("new-test-inrole%d", idx)),
		}

		secrets[fmt.Sprintf("infrastructureroles-new-test%d", idx)] =
			newFormatStandaloneSecret
	}

	if secret, exists := secrets[name]; exists {
		return secret, nil
	}

	return nil, fmt.Errorf("NotFound")

}

func (c *mockConfigMap) Get(ctx context.Context, name string, options metav1.GetOptions) (*v1.ConfigMap, error) {
	oldFormatConfigmap := &v1.ConfigMap{}
	oldFormatConfigmap.Name = "testcluster"
	oldFormatConfigmap.Data = map[string]string{
		"foobar": "{}",
	}

	newFormatConfigmap := &v1.ConfigMap{}
	newFormatConfigmap.Name = "testcluster"
	newFormatConfigmap.Data = map[string]string{
		"new-foobar": "{\"user_flags\": [\"createdb\"]}",
	}

	configmaps := map[string]*v1.ConfigMap{
		"infrastructureroles-old-test": oldFormatConfigmap,
		"infrastructureroles-new-test": newFormatConfigmap,
	}

	if configmap, exists := configmaps[name]; exists {
		return configmap, nil
	}

	return nil, fmt.Errorf("NotFound")
}

// Secrets to be mocked
func (mock *MockSecretGetter) Secrets(namespace string) corev1.SecretInterface {
	return &mockSecret{}
}

// ConfigMaps to be mocked
func (mock *MockConfigMapsGetter) ConfigMaps(namespace string) corev1.ConfigMapInterface {
	return &mockConfigMap{}
}

func (mock *MockDeploymentGetter) Deployments(namespace string) appsv1.DeploymentInterface {
	return &mockDeployment{}
}

func (mock *MockDeploymentNotExistGetter) Deployments(namespace string) appsv1.DeploymentInterface {
	return &mockDeploymentNotExist{}
}

func (mock *mockDeployment) Create(context.Context, *apiappsv1.Deployment, metav1.CreateOptions) (*apiappsv1.Deployment, error) {
	return &apiappsv1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name: "test-deployment",
		},
		Spec: apiappsv1.DeploymentSpec{
			Replicas: Int32ToPointer(1),
		},
	}, nil
}

func (mock *mockDeployment) Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error {
	return nil
}

func (mock *mockDeployment) Get(ctx context.Context, name string, opts metav1.GetOptions) (*apiappsv1.Deployment, error) {
	return &apiappsv1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name: "test-deployment",
		},
		Spec: apiappsv1.DeploymentSpec{
			Replicas: Int32ToPointer(1),
			Template: v1.PodTemplateSpec{
				Spec: v1.PodSpec{
					Containers: []v1.Container{
						{
							Image: "pooler:1.0",
						},
					},
				},
			},
		},
	}, nil
}

func (mock *mockDeployment) Patch(ctx context.Context, name string, t types.PatchType, data []byte, opts metav1.PatchOptions, subres ...string) (*apiappsv1.Deployment, error) {
	return &apiappsv1.Deployment{
		Spec: apiappsv1.DeploymentSpec{
			Replicas: Int32ToPointer(2),
		},
		ObjectMeta: metav1.ObjectMeta{
			Name: "test-deployment",
		},
	}, nil
}

func (mock *mockDeploymentNotExist) Get(ctx context.Context, name string, opts metav1.GetOptions) (*apiappsv1.Deployment, error) {
	return nil, &apierrors.StatusError{
		ErrStatus: metav1.Status{
			Reason: metav1.StatusReasonNotFound,
		},
	}
}

func (mock *mockDeploymentNotExist) Create(context.Context, *apiappsv1.Deployment, metav1.CreateOptions) (*apiappsv1.Deployment, error) {
	return &apiappsv1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name: "test-deployment",
		},
		Spec: apiappsv1.DeploymentSpec{
			Replicas: Int32ToPointer(1),
		},
	}, nil
}

func (mock *MockServiceGetter) Services(namespace string) corev1.ServiceInterface {
	return &mockService{}
}

func (mock *MockServiceNotExistGetter) Services(namespace string) corev1.ServiceInterface {
	return &mockServiceNotExist{}
}

func (mock *mockService) Create(context.Context, *v1.Service, metav1.CreateOptions) (*v1.Service, error) {
	return &v1.Service{
		ObjectMeta: metav1.ObjectMeta{
			Name: "test-service",
		},
	}, nil
}

func (mock *mockService) Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error {
	return nil
}

func (mock *mockService) Get(ctx context.Context, name string, opts metav1.GetOptions) (*v1.Service, error) {
	return &v1.Service{
		ObjectMeta: metav1.ObjectMeta{
			Name: "test-service",
		},
	}, nil
}

func (mock *mockServiceNotExist) Create(context.Context, *v1.Service, metav1.CreateOptions) (*v1.Service, error) {
	return &v1.Service{
		ObjectMeta: metav1.ObjectMeta{
			Name: "test-service",
		},
	}, nil
}

func (mock *mockServiceNotExist) Get(ctx context.Context, name string, opts metav1.GetOptions) (*v1.Service, error) {
	return nil, &apierrors.StatusError{
		ErrStatus: metav1.Status{
			Reason: metav1.StatusReasonNotFound,
		},
	}
}

// NewMockKubernetesClient for other tests
func NewMockKubernetesClient() KubernetesClient {
	return KubernetesClient{
		SecretsGetter:     &MockSecretGetter{},
		ConfigMapsGetter:  &MockConfigMapsGetter{},
		DeploymentsGetter: &MockDeploymentGetter{},
		ServicesGetter:    &MockServiceGetter{},

		CustomResourceDefinitionsGetter: &MockCustomResourceDefinitionsGetter{},
	}
}

func ClientMissingObjects() KubernetesClient {
	return KubernetesClient{
		DeploymentsGetter: &MockDeploymentNotExistGetter{},
		ServicesGetter:    &MockServiceNotExistGetter{},
	}
}


================================================
File: pkg/util/nicediff/diff.go
================================================
// Copyright 2013 Google Inc.  All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Package diff implements a linewise diff algorithm.
package nicediff

import (
	"fmt"
	"strings"
)

// Chunk represents a piece of the diff.  A chunk will not have both added and
// deleted lines.  Equal lines are always after any added or deleted lines.
// A Chunk may or may not have any lines in it, especially for the first or last
// chunk in a computation.
type Chunk struct {
	Added   []string
	Deleted []string
	Equal   []string
}

func (c *Chunk) empty() bool {
	return len(c.Added) == 0 && len(c.Deleted) == 0 && len(c.Equal) == 0
}

// Diff returns a string containing a line-by-line unified diff of the linewise
// changes required to make A into B.  Each line is prefixed with '+', '-', or
// ' ' to indicate if it should be added, removed, or is correct respectively.
func Diff(A, B string, skipEqual bool) string {
	aLines := strings.Split(A, "\n")
	bLines := strings.Split(B, "\n")
	return Render(DiffChunks(aLines, bLines), skipEqual)
}

// Render renders the slice of chunks into a representation that prefixes
// the lines with '+', '-', or ' ' depending on whether the line was added,
// removed, or equal (respectively).
func Render(chunks []Chunk, skipEqual bool) string {
	buf := new(strings.Builder)
	for _, c := range chunks {
		for _, line := range c.Added {
			fmt.Fprintf(buf, "+%s\n", line)
		}
		for _, line := range c.Deleted {
			fmt.Fprintf(buf, "-%s\n", line)
		}
		if !skipEqual {
			for _, line := range c.Equal {
				fmt.Fprintf(buf, " %s\n", line)
			}
		}
	}
	return strings.TrimRight(buf.String(), "\n")
}

// DiffChunks uses an O(D(N+M)) shortest-edit-script algorithm
// to compute the edits required from A to B and returns the
// edit chunks.
func DiffChunks(a, b []string) []Chunk {
	// algorithm: http://www.xmailserver.org/diff2.pdf

	// We'll need these quantities a lot.
	alen, blen := len(a), len(b) // M, N

	// At most, it will require len(a) deletions and len(b) additions
	// to transform a into b.
	maxPath := alen + blen // MAX
	if maxPath == 0 {
		// degenerate case: two empty lists are the same
		return nil
	}

	// Store the endpoint of the path for diagonals.
	// We store only the a index, because the b index on any diagonal
	// (which we know during the loop below) is aidx-diag.
	// endpoint[maxPath] represents the 0 diagonal.
	//
	// Stated differently:
	// endpoint[d] contains the aidx of a furthest reaching path in diagonal d
	endpoint := make([]int, 2*maxPath+1) // V

	saved := make([][]int, 0, 8) // Vs
	save := func() {
		dup := make([]int, len(endpoint))
		copy(dup, endpoint)
		saved = append(saved, dup)
	}

	var editDistance int // D
dLoop:
	for editDistance = 0; editDistance <= maxPath; editDistance++ {
		// The 0 diag(onal) represents equality of a and b.  Each diagonal to
		// the left is numbered one lower, to the right is one higher, from
		// -alen to +blen.  Negative diagonals favor differences from a,
		// positive diagonals favor differences from b.  The edit distance to a
		// diagonal d cannot be shorter than d itself.
		//
		// The iterations of this loop cover either odds or evens, but not both,
		// If odd indices are inputs, even indices are outputs and vice versa.
		for diag := -editDistance; diag <= editDistance; diag += 2 { // k
			var aidx int // x
			switch {
			case diag == -editDistance:
				// This is a new diagonal; copy from previous iter
				aidx = endpoint[maxPath-editDistance+1] + 0
			case diag == editDistance:
				// This is a new diagonal; copy from previous iter
				aidx = endpoint[maxPath+editDistance-1] + 1
			case endpoint[maxPath+diag+1] > endpoint[maxPath+diag-1]:
				// diagonal d+1 was farther along, so use that
				aidx = endpoint[maxPath+diag+1] + 0
			default:
				// diagonal d-1 was farther (or the same), so use that
				aidx = endpoint[maxPath+diag-1] + 1
			}
			// On diagonal d, we can compute bidx from aidx.
			bidx := aidx - diag // y
			// See how far we can go on this diagonal before we find a difference.
			for aidx < alen && bidx < blen && a[aidx] == b[bidx] {
				aidx++
				bidx++
			}
			// Store the end of the current edit chain.
			endpoint[maxPath+diag] = aidx
			// If we've found the end of both inputs, we're done!
			if aidx >= alen && bidx >= blen {
				save() // save the final path
				break dLoop
			}
		}
		save() // save the current path
	}
	if editDistance == 0 {
		return nil
	}
	chunks := make([]Chunk, editDistance+1)

	x, y := alen, blen
	for d := editDistance; d > 0; d-- {
		endpoint := saved[d]
		diag := x - y
		insert := diag == -d || (diag != d && endpoint[maxPath+diag-1] < endpoint[maxPath+diag+1])

		x1 := endpoint[maxPath+diag]
		var x0, xM, kk int
		if insert {
			kk = diag + 1
			x0 = endpoint[maxPath+kk]
			xM = x0
		} else {
			kk = diag - 1
			x0 = endpoint[maxPath+kk]
			xM = x0 + 1
		}
		y0 := x0 - kk

		var c Chunk
		if insert {
			c.Added = b[y0:][:1]
		} else {
			c.Deleted = a[x0:][:1]
		}
		if xM < x1 {
			c.Equal = a[xM:][:x1-xM]
		}

		x, y = x0, y0
		chunks[d] = c
	}
	if x > 0 {
		chunks[0].Equal = a[:x]
	}
	if chunks[0].empty() {
		chunks = chunks[1:]
	}
	if len(chunks) == 0 {
		return nil
	}
	return chunks
}


================================================
File: pkg/util/patroni/patroni.go
================================================
package patroni

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"math"
	"net"
	"net/http"
	"strconv"
	"time"

	"github.com/zalando/postgres-operator/pkg/util/constants"
	httpclient "github.com/zalando/postgres-operator/pkg/util/httpclient"

	"github.com/sirupsen/logrus"
	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	v1 "k8s.io/api/core/v1"
)

const (
	switchoverPath = "/switchover"
	configPath     = "/config"
	clusterPath    = "/cluster"
	statusPath     = "/patroni"
	restartPath    = "/restart"
	ApiPort        = 8008
	timeout        = 30 * time.Second
)

// Interface describe patroni methods
type Interface interface {
	GetClusterMembers(master *v1.Pod) ([]ClusterMember, error)
	Switchover(master *v1.Pod, candidate string, scheduled_at string) error
	SetPostgresParameters(server *v1.Pod, options map[string]string) error
	SetStandbyClusterParameters(server *v1.Pod, options map[string]interface{}) error
	GetMemberData(server *v1.Pod) (MemberData, error)
	Restart(server *v1.Pod) error
	GetConfig(server *v1.Pod) (acidv1.Patroni, map[string]string, error)
	SetConfig(server *v1.Pod, config map[string]interface{}) error
}

// Patroni API client
type Patroni struct {
	httpClient httpclient.HTTPClient
	logger     *logrus.Entry
}

// New create patroni
func New(logger *logrus.Entry, client httpclient.HTTPClient) *Patroni {
	if client == nil {

		client = &http.Client{
			Timeout: timeout,
		}

	}

	return &Patroni{
		logger:     logger,
		httpClient: client,
	}
}

func apiURL(masterPod *v1.Pod) (string, error) {
	ip := net.ParseIP(masterPod.Status.PodIP)
	if ip == nil {
		return "", fmt.Errorf("%s is not a valid IP", masterPod.Status.PodIP)
	}
	// Sanity check PodIP
	if ip.To4() == nil {
		if ip.To16() == nil {
			// Shouldn't ever get here, but library states it's possible.
			return "", fmt.Errorf("%s is not a valid IPv4/IPv6 address", masterPod.Status.PodIP)
		}
	}
	return fmt.Sprintf("http://%s", net.JoinHostPort(ip.String(), strconv.Itoa(ApiPort))), nil
}

func (p *Patroni) httpPostOrPatch(method string, url string, body *bytes.Buffer) (err error) {
	request, err := http.NewRequest(method, url, body)
	if err != nil {
		return fmt.Errorf("could not create request: %v", err)
	}

	if p.logger != nil {
		p.logger.Debugf("making %s http request: %s", method, request.URL.String())
	}

	resp, err := p.httpClient.Do(request)
	if err != nil {
		return fmt.Errorf("could not make request: %v", err)
	}
	defer func() {
		if err2 := resp.Body.Close(); err2 != nil {
			if err != nil {
				err = fmt.Errorf("could not close request: %v, prior error: %v", err2, err)
			} else {
				err = fmt.Errorf("could not close request: %v", err2)
			}
			return
		}
	}()

	if resp.StatusCode < http.StatusOK || resp.StatusCode >= 300 {
		bodyBytes, err := io.ReadAll(resp.Body)
		if err != nil {
			return fmt.Errorf("could not read response: %v", err)
		}

		return fmt.Errorf("patroni returned '%s'", string(bodyBytes))
	}
	return nil
}

func (p *Patroni) httpGet(url string) (string, error) {
	p.logger.Debugf("making GET http request: %s", url)

	response, err := p.httpClient.Get(url)
	if err != nil {
		return "", fmt.Errorf("could not make request: %v", err)
	}
	defer response.Body.Close()

	bodyBytes, err := io.ReadAll(response.Body)
	if err != nil {
		return "", fmt.Errorf("could not read response: %v", err)
	}

	if response.StatusCode < http.StatusOK || response.StatusCode >= 300 {
		return string(bodyBytes), fmt.Errorf("patroni returned '%d'", response.StatusCode)
	}

	return string(bodyBytes), nil
}

// Switchover by calling Patroni REST API
func (p *Patroni) Switchover(master *v1.Pod, candidate string, scheduled_at string) error {
	buf := &bytes.Buffer{}
	err := json.NewEncoder(buf).Encode(map[string]string{"leader": master.Name, "member": candidate, "scheduled_at": scheduled_at})
	if err != nil {
		return fmt.Errorf("could not encode json: %v", err)
	}
	apiURLString, err := apiURL(master)
	if err != nil {
		return err
	}
	return p.httpPostOrPatch(http.MethodPost, apiURLString+switchoverPath, buf)
}

//TODO: add an option call /patroni to check if it is necessary to restart the server

// SetPostgresParameters sets Postgres options via Patroni patch API call.
func (p *Patroni) SetPostgresParameters(server *v1.Pod, parameters map[string]string) error {
	buf := &bytes.Buffer{}
	err := json.NewEncoder(buf).Encode(map[string]map[string]interface{}{"postgresql": {"parameters": parameters}})
	if err != nil {
		return fmt.Errorf("could not encode json: %v", err)
	}
	apiURLString, err := apiURL(server)
	if err != nil {
		return err
	}
	return p.httpPostOrPatch(http.MethodPatch, apiURLString+configPath, buf)
}

// SetStandbyClusterParameters sets StandbyCluster options via Patroni patch API call.
func (p *Patroni) SetStandbyClusterParameters(server *v1.Pod, parameters map[string]interface{}) error {
	return p.SetConfig(server, map[string]interface{}{"standby_cluster": parameters})
}

// SetConfig sets Patroni options via Patroni patch API call.
func (p *Patroni) SetConfig(server *v1.Pod, config map[string]interface{}) error {
	buf := &bytes.Buffer{}
	err := json.NewEncoder(buf).Encode(config)
	if err != nil {
		return fmt.Errorf("could not encode json: %v", err)
	}
	apiURLString, err := apiURL(server)
	if err != nil {
		return err
	}
	return p.httpPostOrPatch(http.MethodPatch, apiURLString+configPath, buf)
}

// ClusterMembers array of cluster members from Patroni API
type ClusterMembers struct {
	Members []ClusterMember `json:"members"`
}

// ClusterMember cluster member data from Patroni API
type ClusterMember struct {
	Name     string         `json:"name"`
	Role     string         `json:"role"`
	State    string         `json:"state"`
	Timeline int            `json:"timeline"`
	Lag      ReplicationLag `json:"lag,omitempty"`
}

type ReplicationLag uint64

// UnmarshalJSON converts member lag (can be int or string) into uint64
func (rl *ReplicationLag) UnmarshalJSON(data []byte) error {
	var lagUInt64 uint64
	if data[0] == '"' {
		*rl = math.MaxUint64
		return nil
	}
	if err := json.Unmarshal(data, &lagUInt64); err != nil {
		return err
	}
	*rl = ReplicationLag(lagUInt64)
	return nil
}

// MemberDataPatroni child element
type MemberDataPatroni struct {
	Version string `json:"version"`
	Scope   string `json:"scope"`
}

// MemberData Patroni member data from Patroni API
type MemberData struct {
	State           string            `json:"state"`
	Role            string            `json:"role"`
	ServerVersion   int               `json:"server_version"`
	PendingRestart  bool              `json:"pending_restart"`
	ClusterUnlocked bool              `json:"cluster_unlocked"`
	Patroni         MemberDataPatroni `json:"patroni"`
}

func (p *Patroni) GetConfig(server *v1.Pod) (acidv1.Patroni, map[string]string, error) {
	var (
		patroniConfig acidv1.Patroni
		pgConfig      map[string]interface{}
	)
	apiURLString, err := apiURL(server)
	if err != nil {
		return patroniConfig, nil, err
	}
	body, err := p.httpGet(apiURLString + configPath)
	if err != nil {
		return patroniConfig, nil, err
	}
	err = json.Unmarshal([]byte(body), &patroniConfig)
	if err != nil {
		return patroniConfig, nil, err
	}

	// unmarshalling postgresql parameters needs a detour
	err = json.Unmarshal([]byte(body), &pgConfig)
	if err != nil {
		return patroniConfig, nil, err
	}
	pgParameters := make(map[string]string)
	if _, exists := pgConfig["postgresql"]; exists {
		effectivePostgresql := pgConfig["postgresql"].(map[string]interface{})
		effectivePgParameters := effectivePostgresql[constants.PatroniPGParametersParameterName].(map[string]interface{})
		for parameter, value := range effectivePgParameters {
			strValue := fmt.Sprintf("%v", value)
			pgParameters[parameter] = strValue
		}
	}

	return patroniConfig, pgParameters, err
}

// Restart method restarts instance via Patroni POST API call.
func (p *Patroni) Restart(server *v1.Pod) error {
	buf := &bytes.Buffer{}
	err := json.NewEncoder(buf).Encode(map[string]interface{}{"restart_pending": true})
	if err != nil {
		return fmt.Errorf("could not encode json: %v", err)
	}
	apiURLString, err := apiURL(server)
	if err != nil {
		return err
	}
	if err := p.httpPostOrPatch(http.MethodPost, apiURLString+restartPath, buf); err != nil {
		return err
	}
	p.logger.Infof("Postgres server successfuly restarted in pod %s", server.Name)

	return nil
}

// GetClusterMembers read cluster data from patroni API
func (p *Patroni) GetClusterMembers(server *v1.Pod) ([]ClusterMember, error) {

	apiURLString, err := apiURL(server)
	if err != nil {
		return []ClusterMember{}, err
	}
	body, err := p.httpGet(apiURLString + clusterPath)
	if err != nil {
		return []ClusterMember{}, err
	}

	data := ClusterMembers{}
	err = json.Unmarshal([]byte(body), &data)
	if err != nil {
		return []ClusterMember{}, err
	}

	return data.Members, nil
}

// GetMemberData read member data from patroni API
func (p *Patroni) GetMemberData(server *v1.Pod) (MemberData, error) {

	apiURLString, err := apiURL(server)
	if err != nil {
		return MemberData{}, err
	}
	body, err := p.httpGet(apiURLString + statusPath)
	if err != nil {
		return MemberData{}, err
	}

	data := MemberData{}
	err = json.Unmarshal([]byte(body), &data)
	if err != nil {
		return MemberData{}, err
	}

	return data, nil
}


================================================
File: pkg/util/patroni/patroni_test.go
================================================
package patroni

import (
	"bytes"
	"errors"
	"fmt"
	"io"
	"math"
	"net/http"
	"reflect"
	"testing"

	"github.com/golang/mock/gomock"
	"github.com/sirupsen/logrus"
	"github.com/zalando/postgres-operator/mocks"

	acidv1 "github.com/zalando/postgres-operator/pkg/apis/acid.zalan.do/v1"
	v1 "k8s.io/api/core/v1"
)

var logger = logrus.New().WithField("test", "patroni")

func newMockPod(ip string) *v1.Pod {
	return &v1.Pod{
		Status: v1.PodStatus{
			PodIP: ip,
		},
	}
}

func TestApiURL(t *testing.T) {
	var testTable = []struct {
		podIP            string
		expectedResponse string
		expectedError    error
	}{
		{
			"127.0.0.1",
			fmt.Sprintf("http://127.0.0.1:%d", ApiPort),
			nil,
		},
		{
			"0000:0000:0000:0000:0000:0000:0000:0001",
			fmt.Sprintf("http://[::1]:%d", ApiPort),
			nil,
		},
		{
			"::1",
			fmt.Sprintf("http://[::1]:%d", ApiPort),
			nil,
		},
		{
			"",
			"",
			errors.New(" is not a valid IP"),
		},
		{
			"foobar",
			"",
			errors.New("foobar is not a valid IP"),
		},
		{
			"127.0.1",
			"",
			errors.New("127.0.1 is not a valid IP"),
		},
		{
			":::",
			"",
			errors.New("::: is not a valid IP"),
		},
	}
	for _, test := range testTable {
		resp, err := apiURL(newMockPod(test.podIP))
		if resp != test.expectedResponse {
			t.Errorf("expected response %v does not match the actual %v", test.expectedResponse, resp)
		}
		if err != test.expectedError {
			if err == nil || test.expectedError == nil {
				t.Errorf("expected error '%v' does not match the actual error '%v'", test.expectedError, err)
			}
			if err != nil && test.expectedError != nil && err.Error() != test.expectedError.Error() {
				t.Errorf("expected error '%v' does not match the actual error '%v'", test.expectedError, err)
			}
		}
	}
}

func TestGetClusterMembers(t *testing.T) {
	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	expectedClusterMemberData := []ClusterMember{
		{
			Name:     "acid-test-cluster-0",
			Role:     "leader",
			State:    "running",
			Timeline: 1,
		}, {
			Name:     "acid-test-cluster-1",
			Role:     "sync_standby",
			State:    "streaming",
			Timeline: 1,
			Lag:      0,
		}, {
			Name:     "acid-test-cluster-2",
			Role:     "replica",
			State:    "streaming",
			Timeline: 1,
			Lag:      math.MaxUint64,
		}, {
			Name:     "acid-test-cluster-3",
			Role:     "replica",
			State:    "in archive recovery",
			Timeline: 1,
			Lag:      3000000000,
		}}

	json := `{"members": [
		{"name": "acid-test-cluster-0", "role": "leader", "state": "running", "api_url": "http://192.168.100.1:8008/patroni", "host": "192.168.100.1", "port": 5432, "timeline": 1},
		{"name": "acid-test-cluster-1", "role": "sync_standby", "state": "streaming", "api_url": "http://192.168.100.2:8008/patroni", "host": "192.168.100.2", "port": 5432, "timeline": 1, "lag": 0},
		{"name": "acid-test-cluster-2", "role": "replica", "state": "streaming", "api_url": "http://192.168.100.3:8008/patroni", "host": "192.168.100.3", "port": 5432, "timeline": 1, "lag": "unknown"},
		{"name": "acid-test-cluster-3", "role": "replica", "state": "in archive recovery", "api_url": "http://192.168.100.3:8008/patroni", "host": "192.168.100.3", "port": 5432, "timeline": 1, "lag": 3000000000}
		]}`
	r := io.NopCloser(bytes.NewReader([]byte(json)))

	response := http.Response{
		StatusCode: 200,
		Body:       r,
	}

	mockClient := mocks.NewMockHTTPClient(ctrl)
	mockClient.EXPECT().Get(gomock.Any()).Return(&response, nil)

	p := New(logger, mockClient)

	clusterMemberData, err := p.GetClusterMembers(newMockPod("192.168.100.1"))

	if !reflect.DeepEqual(expectedClusterMemberData, clusterMemberData) {
		t.Errorf("Patroni cluster members differ: expected: %#v, got: %#v", expectedClusterMemberData, clusterMemberData)
	}

	if err != nil {
		t.Errorf("Could not read Patroni data: %v", err)
	}
}

func TestGetMemberData(t *testing.T) {
	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	expectedMemberData := MemberData{
		State:          "running",
		Role:           "master",
		ServerVersion:  130004,
		PendingRestart: true,
		Patroni: MemberDataPatroni{
			Version: "2.1.1",
			Scope:   "acid-test-cluster",
		},
	}

	json := `{"state": "running", "postmaster_start_time": "2021-02-19 14:31:50.053 CET", "role": "master", "server_version": 130004, "cluster_unlocked": false, "xlog": {"location": 123456789}, "timeline": 1, "database_system_identifier": "6462555844314089962", "pending_restart": true, "patroni": {"version": "2.1.1", "scope": "acid-test-cluster"}}`
	r := io.NopCloser(bytes.NewReader([]byte(json)))

	response := http.Response{
		StatusCode: 200,
		Body:       r,
	}

	mockClient := mocks.NewMockHTTPClient(ctrl)
	mockClient.EXPECT().Get(gomock.Any()).Return(&response, nil)

	p := New(logger, mockClient)

	memberData, err := p.GetMemberData(newMockPod("192.168.100.1"))

	if !reflect.DeepEqual(expectedMemberData, memberData) {
		t.Errorf("Patroni member data differs: expected: %#v, got: %#v", expectedMemberData, memberData)
	}

	if err != nil {
		t.Errorf("Could not read Patroni data: %v", err)
	}
}

func TestGetConfig(t *testing.T) {
	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	expectedPatroniConfig := acidv1.Patroni{
		TTL:                  30,
		LoopWait:             10,
		RetryTimeout:         10,
		MaximumLagOnFailover: 33554432,
		Slots: map[string]map[string]string{
			"cdc": {
				"database": "foo",
				"plugin":   "pgoutput",
				"type":     "logical",
			},
		},
	}

	expectedPgParameters := map[string]string{
		"archive_mode":                    "on",
		"archive_timeout":                 "1800s",
		"autovacuum_analyze_scale_factor": "0.02",
		"autovacuum_max_workers":          "5",
		"autovacuum_vacuum_scale_factor":  "0.05",
		"checkpoint_completion_target":    "0.9",
		"hot_standby":                     "on",
		"log_autovacuum_min_duration":     "0",
		"log_checkpoints":                 "on",
		"log_connections":                 "on",
		"log_disconnections":              "on",
		"log_line_prefix":                 "%t [%p]: [%l-1] %c %x %d %u %a %h ",
		"log_lock_waits":                  "on",
		"log_min_duration_statement":      "500",
		"log_statement":                   "ddl",
		"log_temp_files":                  "0",
		"max_connections":                 "100",
		"max_replication_slots":           "10",
		"max_wal_senders":                 "10",
		"tcp_keepalives_idle":             "900",
		"tcp_keepalives_interval":         "100",
		"track_functions":                 "all",
		"wal_level":                       "hot_standby",
		"wal_log_hints":                   "on",
	}

	configJson := `{"loop_wait": 10, "maximum_lag_on_failover": 33554432, "postgresql": {"parameters": {"archive_mode": "on", "archive_timeout": "1800s", "autovacuum_analyze_scale_factor": 0.02, "autovacuum_max_workers": 5, "autovacuum_vacuum_scale_factor": 0.05, "checkpoint_completion_target": 0.9, "hot_standby": "on", "log_autovacuum_min_duration": 0, "log_checkpoints": "on", "log_connections": "on", "log_disconnections": "on", "log_line_prefix": "%t [%p]: [%l-1] %c %x %d %u %a %h ", "log_lock_waits": "on", "log_min_duration_statement": 500, "log_statement": "ddl", "log_temp_files": 0, "max_connections": 100, "max_replication_slots": 10, "max_wal_senders": 10, "tcp_keepalives_idle": 900, "tcp_keepalives_interval": 100, "track_functions": "all", "wal_level": "hot_standby", "wal_log_hints": "on"}, "use_pg_rewind": true, "use_slots": true}, "retry_timeout": 10, "slots": {"cdc": {"database": "foo", "plugin": "pgoutput", "type": "logical"}}, "ttl": 30}`
	r := io.NopCloser(bytes.NewReader([]byte(configJson)))

	response := http.Response{
		StatusCode: 200,
		Body:       r,
	}

	mockClient := mocks.NewMockHTTPClient(ctrl)
	mockClient.EXPECT().Get(gomock.Any()).Return(&response, nil)

	p := New(logger, mockClient)

	patroniConfig, pgParameters, err := p.GetConfig(newMockPod("192.168.100.1"))
	if err != nil {
		t.Errorf("Could not read Patroni config endpoint: %v", err)
	}

	if !reflect.DeepEqual(expectedPatroniConfig, patroniConfig) {
		t.Errorf("Patroni config differs: expected: %#v, got: %#v", expectedPatroniConfig, patroniConfig)
	}
	if !reflect.DeepEqual(expectedPgParameters, pgParameters) {
		t.Errorf("Postgre parameters differ: expected: %#v, got: %#v", expectedPgParameters, pgParameters)
	}
}

func TestSetPostgresParameters(t *testing.T) {
	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	parametersToSet := map[string]string{
		"max_connections": "50",
		"wal_level":       "logical",
	}

	configJson := `{"loop_wait": 10, "maximum_lag_on_failover": 33554432, "postgresql": {"parameters": {"archive_mode": "on", "archive_timeout": "1800s", "autovacuum_analyze_scale_factor": 0.02, "autovacuum_max_workers": 5, "autovacuum_vacuum_scale_factor": 0.05, "checkpoint_completion_target": 0.9, "hot_standby": "on", "log_autovacuum_min_duration": 0, "log_checkpoints": "on", "log_connections": "on", "log_disconnections": "on", "log_line_prefix": "%t [%p]: [%l-1] %c %x %d %u %a %h ", "log_lock_waits": "on", "log_min_duration_statement": 500, "log_statement": "ddl", "log_temp_files": 0, "max_connections": 50, "max_replication_slots": 10, "max_wal_senders": 10, "tcp_keepalives_idle": 900, "tcp_keepalives_interval": 100, "track_functions": "all", "wal_level": "logical", "wal_log_hints": "on"}, "use_pg_rewind": true, "use_slots": true}, "retry_timeout": 10, "slots": {"cdc": {"database": "foo", "plugin": "pgoutput", "type": "logical"}}, "ttl": 30}`
	r := io.NopCloser(bytes.NewReader([]byte(configJson)))

	response := http.Response{
		StatusCode: 200,
		Body:       r,
	}

	mockClient := mocks.NewMockHTTPClient(ctrl)
	mockClient.EXPECT().Do(gomock.Any()).Return(&response, nil)

	p := New(logger, mockClient)

	err := p.SetPostgresParameters(newMockPod("192.168.100.1"), parametersToSet)
	if err != nil {
		t.Errorf("could not call patch Patroni config: %v", err)
	}

}


================================================
File: pkg/util/retryutil/retry_util.go
================================================
package retryutil

import (
	"fmt"
	"time"
)

// RetryTicker is a wrapper aroung time.Tick,
// that allows to mock its implementation
type RetryTicker interface {
	Stop()
	Tick()
}

// Ticker is a real implementation of RetryTicker interface
type Ticker struct {
	ticker *time.Ticker
}

// Stop is a convenience wrapper around ticker.Stop
func (t *Ticker) Stop() { t.ticker.Stop() }

// Tick is a convenience wrapper around ticker.C
func (t *Ticker) Tick() { <-t.ticker.C }

// Retry is a wrapper around RetryWorker that provides a real RetryTicker
func Retry(interval time.Duration, timeout time.Duration, f func() (bool, error)) error {
	//TODO: make the retry exponential
	if timeout < interval {
		return fmt.Errorf("timeout(%s) should be greater than interval(%v)", timeout, interval)
	}
	tick := &Ticker{time.NewTicker(interval)}
	return RetryWorker(interval, timeout, tick, f)
}

// RetryWorker calls ConditionFunc until either:
// * it returns boolean true
// * a timeout expires
// * an error occurs
func RetryWorker(
	interval time.Duration,
	timeout time.Duration,
	tick RetryTicker,
	f func() (bool, error)) error {

	maxRetries := int(timeout / interval)
	defer tick.Stop()

	for i := 0; ; i++ {
		ok, err := f()
		if err != nil {
			return err
		}
		if ok {
			return nil
		}
		if i+1 == maxRetries {
			break
		}
		tick.Tick()
	}
	return fmt.Errorf("still failing after %d retries", maxRetries)
}


================================================
File: pkg/util/retryutil/retry_util_test.go
================================================
package retryutil

import (
	"errors"
	"testing"
)

type mockTicker struct {
	test    *testing.T
	counter int
}

func (t *mockTicker) Stop() {}

func (t *mockTicker) Tick() {
	t.counter++
}

func TestRetryWorkerSuccess(t *testing.T) {
	tick := &mockTicker{t, 0}
	result := RetryWorker(10, 20, tick, func() (bool, error) {
		return true, nil
	})

	if result != nil {
		t.Errorf("Wrong result, expected: %#v, got: %#v", nil, result)
	}

	if tick.counter != 0 {
		t.Errorf("Ticker was started once, but it shouldn't be")
	}
}

func TestRetryWorkerOneFalse(t *testing.T) {
	var counter = 0

	tick := &mockTicker{t, 0}
	result := RetryWorker(1, 3, tick, func() (bool, error) {
		counter++
		return counter > 1, nil
	})

	if result != nil {
		t.Errorf("Wrong result, expected: %#v, got: %#v", nil, result)
	}

	if tick.counter != 1 {
		t.Errorf("Ticker was started %#v, but supposed to be just once", tick.counter)
	}
}

func TestRetryWorkerError(t *testing.T) {
	fail := errors.New("Error")

	tick := &mockTicker{t, 0}
	result := RetryWorker(1, 3, tick, func() (bool, error) {
		return false, fail
	})

	if result != fail {
		t.Errorf("Wrong result, expected: %#v, got: %#v", fail, result)
	}
}


================================================
File: pkg/util/ringlog/ringlog.go
================================================
package ringlog

import (
	"container/list"
	"sync"
)

// RingLogger describes ring logger methods
type RingLogger interface {
	Insert(interface{})
	Walk() []interface{}
}

// RingLog is a capped logger with fixed size
type RingLog struct {
	sync.RWMutex
	size int
	list *list.List
}

// New creates new Ring logger
func New(size int) *RingLog {
	r := RingLog{
		list: list.New(),
		size: size,
	}

	return &r
}

// Insert inserts new entry into the ring logger
func (r *RingLog) Insert(obj interface{}) {
	r.Lock()
	defer r.Unlock()

	r.list.PushBack(obj)
	if r.list.Len() > r.size {
		r.list.Remove(r.list.Front())
	}
}

// Walk dumps all the entries from the Ring logger
func (r *RingLog) Walk() []interface{} {
	res := make([]interface{}, 0)

	r.RLock()
	defer r.RUnlock()

	st := r.list.Front()
	for i := 0; i < r.size; i++ {
		if st == nil {
			return res
		}
		res = append(res, st.Value)
		st = st.Next()
	}

	return res
}


================================================
File: pkg/util/teams/teams.go
================================================
package teams

import (
	"encoding/json"
	"fmt"
	"net/http"
	"strings"

	"github.com/sirupsen/logrus"
)

// InfrastructureAccount defines an account of the team on some infrastructure (i.e AWS, Google) platform.
type infrastructureAccount struct {
	ID          string `json:"id"`
	Name        string `json:"name"`
	Provider    string `json:"provider"`
	Type        string `json:"type"`
	Description string `json:"description"`
	Owner       string `json:"owner"`
	OwnerDn     string `json:"owner_dn"`
	Disabled    bool   `json:"disabled"`
}

// Team defines informaiton for a single team, including the list of members and infrastructure accounts.
type Team struct {
	Dn           string   `json:"dn"`
	ID           string   `json:"id"`
	TeamName     string   `json:"id_name"`
	TeamID       string   `json:"team_id"`
	Type         string   `json:"type"`
	FullName     string   `json:"name"`
	Aliases      []string `json:"alias"`
	Mails        []string `json:"mail"`
	Members      []string `json:"member"`
	CostCenter   string   `json:"cost_center"`
	DeliveryLead string   `json:"delivery_lead"`
	ParentTeamID string   `json:"parent_team_id"`

	InfrastructureAccounts []infrastructureAccount `json:"infrastructure-accounts"`
}

type httpClient interface {
	Do(req *http.Request) (*http.Response, error)
}

// Interface to the TeamsAPIClient
type Interface interface {
	TeamInfo(teamID, token string) (tm *Team, statusCode int, err error)
}

// API describes teams API
type API struct {
	httpClient
	url    string
	logger *logrus.Entry
}

// NewTeamsAPI creates an object to query the team API.
func NewTeamsAPI(url string, log *logrus.Entry) *API {
	t := API{
		url:        strings.TrimRight(url, "/"),
		httpClient: &http.Client{},
		logger:     log.WithField("pkg", "teamsapi"),
	}

	return &t
}

// TeamInfo returns information about a given team using its ID and a token to authenticate to the API service.
func (t *API) TeamInfo(teamID, token string) (tm *Team, statusCode int, err error) {
	var (
		req  *http.Request
		resp *http.Response
	)

	url := fmt.Sprintf("%s/teams/%s", t.url, teamID)
	t.logger.Debugf("request url: %s", url)
	req, err = http.NewRequest("GET", url, nil)
	if err != nil {
		return nil, http.StatusBadRequest, err
	}

	req.Header.Add("Authorization", "Bearer "+token)
	if resp, err = t.httpClient.Do(req); err != nil {
		return nil, http.StatusUnauthorized, err
	}
	defer func() {
		if closeErr := resp.Body.Close(); closeErr != nil {
			err = fmt.Errorf("error when closing response: %v", closeErr)
		}
	}()
	statusCode = resp.StatusCode
	if statusCode != http.StatusOK {
		var raw map[string]json.RawMessage
		d := json.NewDecoder(resp.Body)
		err = d.Decode(&raw)
		if err != nil {
			return nil, statusCode, fmt.Errorf("team API query failed with status code %d and malformed response: %v", statusCode, err)
		}

		if errMessage, ok := raw["error"]; ok {
			return nil, statusCode, fmt.Errorf("team API query failed with status code %d and message: '%v'", statusCode, string(errMessage))
		}
		return nil, statusCode, fmt.Errorf("team API query failed with status code %d", statusCode)
	}

	tm = &Team{}
	d := json.NewDecoder(resp.Body)
	if err = d.Decode(tm); err != nil {
		return nil, statusCode, fmt.Errorf("could not parse team API response: %v", err)
	}

	return tm, statusCode, nil
}


================================================
File: pkg/util/teams/teams_test.go
================================================
package teams

import (
	"fmt"
	"net/http"
	"net/http/httptest"
	"reflect"
	"testing"

	"github.com/sirupsen/logrus"
)

var (
	logger = logrus.New().WithField("pkg", "teamsapi")
	token  = "ec45b1cfbe7100c6315d183a3eb6cec0M2U1LWJkMzEtZDgzNzNmZGQyNGM3IiwiYXV0aF90aW1lIjoxNDkzNzMwNzQ1LCJpc3MiOiJodHRwcz"
	input  = `{
	"dn": "cn=100100,ou=official,ou=foobar,dc=zalando,dc=net",
	"id": "acid",
	"id_name": "acid",
	"team_id": "111222",
	"type": "official",
	"name": "Acid team name",
	"mail": [
	"email1@example.com",
	"email2@example.com"
	],
	"alias": [
	"acid"
	],
	"member": [
	  "member1",
	  "member2",
	  "member3"
	],
	"infrastructure-accounts": [
	{
	  "id": "1234512345",
	  "name": "acid",
	  "provider": "aws",
	  "type": "aws",
	  "description": "",
	  "owner": "acid",
	  "owner_dn": "cn=100100,ou=official,ou=foobar,dc=zalando,dc=net",
	  "disabled": false
	},
	{
	  "id": "5432154321",
	  "name": "db",
	  "provider": "aws",
	  "type": "aws",
	  "description": "",
	  "owner": "acid",
	  "owner_dn": "cn=100100,ou=official,ou=foobar,dc=zalando,dc=net",
	  "disabled": false
	}
	],
	"cost_center": "00099999",
	"delivery_lead": "member4",
	"parent_team_id": "111221"
	}`
)
var teamsAPItc = []struct {
	in     string
	inCode int
	inTeam string
	out    *Team
	err    error
}{
	{
		input,
		200,
		"acid",
		&Team{
			Dn:           "cn=100100,ou=official,ou=foobar,dc=zalando,dc=net",
			ID:           "acid",
			TeamName:     "acid",
			TeamID:       "111222",
			Type:         "official",
			FullName:     "Acid team name",
			Aliases:      []string{"acid"},
			Mails:        []string{"email1@example.com", "email2@example.com"},
			Members:      []string{"member1", "member2", "member3"},
			CostCenter:   "00099999",
			DeliveryLead: "member4",
			ParentTeamID: "111221",
			InfrastructureAccounts: []infrastructureAccount{
				{
					ID:          "1234512345",
					Name:        "acid",
					Provider:    "aws",
					Type:        "aws",
					Description: "",
					Owner:       "acid",
					OwnerDn:     "cn=100100,ou=official,ou=foobar,dc=zalando,dc=net",
					Disabled:    false},
				{
					ID:          "5432154321",
					Name:        "db",
					Provider:    "aws",
					Type:        "aws",
					Description: "",
					Owner:       "acid",
					OwnerDn:     "cn=100100,ou=official,ou=foobar,dc=zalando,dc=net",
					Disabled:    false},
			},
		},
		nil}, {
		`{"error": "Access Token not valid"}`,
		401,
		"acid",
		nil,
		fmt.Errorf(`team API query failed with status code 401 and message: '"Access Token not valid"'`),
	},
	{
		`{"status": "I'm a teapot'"}`,
		418,
		"acid",
		nil,
		fmt.Errorf(`team API query failed with status code 418`),
	},
	{
		`{"status": "I'm a teapot`,
		418,
		"acid",
		nil,
		fmt.Errorf(`team API query failed with status code 418 and malformed response: unexpected EOF`),
	},
	{
		`{"status": "I'm a teapot`,
		200,
		"acid",
		nil,
		fmt.Errorf(`could not parse team API response: unexpected EOF`),
	},
	{
		input,
		404,
		"banana",
		nil,
		fmt.Errorf(`team API query failed with status code 404`),
	},
}

var requestsURLtc = []struct {
	url string
	err error
}{
	{
		"coffee://localhost/",
		fmt.Errorf(`Get "coffee://localhost/teams/acid": unsupported protocol scheme "coffee"`),
	},
	{
		"http://192.168.0.%31/",
		fmt.Errorf(`parse "http://192.168.0.%%31/teams/acid": invalid URL escape "%%31"`),
	},
}

func TestInfo(t *testing.T) {
	for _, tc := range teamsAPItc {
		func() {
			ts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
				if r.Header.Get("Authorization") != "Bearer "+token {
					t.Errorf("authorization token is wrong or not provided")
				}
				w.WriteHeader(tc.inCode)
				if _, err := fmt.Fprint(w, tc.in); err != nil {
					t.Errorf("error writing teams api response %v", err)
				}
			}))
			defer ts.Close()
			api := NewTeamsAPI(ts.URL, logger)

			actual, statusCode, err := api.TeamInfo(tc.inTeam, token)
			if err != nil && err.Error() != tc.err.Error() {
				t.Errorf("expected error: %v, got: %v", tc.err, err)
				return
			}

			if !reflect.DeepEqual(actual, tc.out) {
				t.Errorf("expected %#v, got: %#v", tc.out, actual)
			}

			if statusCode != tc.inCode {
				t.Errorf("expected %d, got: %d", tc.inCode, statusCode)
			}
		}()
	}
}

type mockHTTPClient struct {
}

type mockBody struct {
}

func (b *mockBody) Read(p []byte) (n int, err error) {
	return 2, nil
}

func (b *mockBody) Close() error {
	return fmt.Errorf("close error")
}

func (c *mockHTTPClient) Do(req *http.Request) (*http.Response, error) {
	resp := http.Response{
		Status:        "200 OK",
		StatusCode:    200,
		ContentLength: 2,
		Close:         false,
		Request:       req,
	}
	resp.Body = &mockBody{}

	return &resp, nil
}

func TestHttpClientClose(t *testing.T) {
	ts := httptest.NewServer(nil)

	api := NewTeamsAPI(ts.URL, logger)
	api.httpClient = &mockHTTPClient{}

	_, _, err := api.TeamInfo("acid", token)
	expError := fmt.Errorf("error when closing response: close error")
	if err.Error() != expError.Error() {
		t.Errorf("expected error: %v, got: %v", expError, err)
	}
}

func TestRequest(t *testing.T) {
	for _, tc := range requestsURLtc {
		api := NewTeamsAPI(tc.url, logger)
		resp, _, err := api.TeamInfo("acid", token)
		if resp != nil {
			t.Errorf("response expected to be nil")
			continue
		}

		if err.Error() != tc.err.Error() {
			t.Errorf("expected error: %v, got: %v", tc.err, err)
		}
	}
}


================================================
File: pkg/util/users/users.go
================================================
package users

import (
	"database/sql"
	"fmt"
	"strings"

	"reflect"

	"github.com/zalando/postgres-operator/pkg/spec"
	"github.com/zalando/postgres-operator/pkg/util"
	"github.com/zalando/postgres-operator/pkg/util/constants"
)

const (
	createUserSQL        = `SET LOCAL synchronous_commit = 'local'; CREATE ROLE "%s" %s %s;`
	alterUserSQL         = `ALTER ROLE "%s" %s`
	alterUserRenameSQL   = `ALTER ROLE "%s" RENAME TO "%s%s"`
	alterRoleResetAllSQL = `ALTER ROLE "%s" RESET ALL`
	alterRoleSetSQL      = `ALTER ROLE "%s" SET %s TO %s`
	dropUserSQL          = `SET LOCAL synchronous_commit = 'local'; DROP ROLE "%s";`
	grantToUserSQL       = `GRANT %s TO "%s"`
	revokeFromUserSQL    = `REVOKE "%s" FROM "%s"`
	doBlockStmt          = `SET LOCAL synchronous_commit = 'local'; DO $$ BEGIN %s; END;$$;`
	passwordTemplate     = "ENCRYPTED PASSWORD '%s'"
	inRoleTemplate       = `IN ROLE %s`
	adminTemplate        = `ADMIN "%s"`
)

// DefaultUserSyncStrategy implements a user sync strategy that merges already existing database users
// with those defined in the manifest, altering existing users when necessary. It will never strips
// an existing roles of another role membership, nor it removes the already assigned flag
// (except for the NOLOGIN). TODO: process other NOflags, i.e. NOSUPERUSER correctly.
type DefaultUserSyncStrategy struct {
	PasswordEncryption   string
	RoleDeletionSuffix   string
	AdditionalOwnerRoles []string
}

// ProduceSyncRequests figures out the types of changes that need to happen with the given users.
func (strategy DefaultUserSyncStrategy) ProduceSyncRequests(dbUsers spec.PgUserMap,
	newUsers spec.PgUserMap) []spec.PgSyncUserRequest {

	var reqs []spec.PgSyncUserRequest
	for name, newUser := range newUsers {
		// do not create user when there exists a user with the same name plus deletion suffix
		// instead request a renaming of the deleted user back to the original name (see * below)
		if newUser.Deleted {
			continue
		}
		dbUser, exists := dbUsers[name]
		if !exists {
			reqs = append(reqs, spec.PgSyncUserRequest{Kind: spec.PGSyncUserAdd, User: newUser})
			if len(newUser.Parameters) > 0 {
				reqs = append(reqs, spec.PgSyncUserRequest{Kind: spec.PGSyncAlterSet, User: newUser})
			}
		} else {
			r := spec.PgSyncUserRequest{}
			newMD5Password := util.NewEncryptor(strategy.PasswordEncryption).PGUserPassword(newUser)

			// do not compare for roles coming from docker image
			if dbUser.Password != newMD5Password {
				r.User.Password = newMD5Password
				r.Kind = spec.PGsyncUserAlter
			}
			if addNewRoles, equal := util.SubstractStringSlices(newUser.MemberOf, dbUser.MemberOf); !equal {
				r.User.MemberOf = addNewRoles
				r.User.IsDbOwner = newUser.IsDbOwner
				r.Kind = spec.PGsyncUserAlter
			}
			if addNewFlags, equal := util.SubstractStringSlices(newUser.Flags, dbUser.Flags); !equal {
				r.User.Flags = addNewFlags
				r.Kind = spec.PGsyncUserAlter
			}
			if r.Kind == spec.PGsyncUserAlter {
				r.User.Name = newUser.Name
				reqs = append(reqs, r)
			}
			if len(newUser.Parameters) > 0 &&
				!reflect.DeepEqual(dbUser.Parameters, newUser.Parameters) {
				reqs = append(reqs, spec.PgSyncUserRequest{Kind: spec.PGSyncAlterSet, User: newUser})
			}
		}
	}

	// no existing roles are deleted or stripped of role membership/flags
	// but team roles will be renamed and denied from LOGIN
	for name, dbUser := range dbUsers {
		if _, exists := newUsers[name]; !exists {
			if dbUser.Deleted {
				// * user with deletion suffix and NOLOGIN found in database
				// grant back LOGIN and rename only if original user is wanted and does not exist in database
				originalName := strings.TrimSuffix(name, strategy.RoleDeletionSuffix)
				_, originalUserWanted := newUsers[originalName]
				_, originalUserAlreadyExists := dbUsers[originalName]
				if !originalUserWanted || originalUserAlreadyExists {
					continue
				}
				// a deleted dbUser has no NOLOGIN flag, so we can add the LOGIN flag
				dbUser.Flags = append(dbUser.Flags, constants.RoleFlagLogin)
			} else {
				// user found in database and not wanted in newUsers - replace LOGIN flag with NOLOGIN
				dbUser.Flags = util.StringSliceReplaceElement(dbUser.Flags, constants.RoleFlagLogin, constants.RoleFlagNoLogin)
			}
			// request ALTER ROLE to grant or revoke LOGIN
			reqs = append(reqs, spec.PgSyncUserRequest{Kind: spec.PGsyncUserAlter, User: dbUser})
			// request RENAME which will happen on behalf of the pgUser.Deleted field
			reqs = append(reqs, spec.PgSyncUserRequest{Kind: spec.PGSyncUserRename, User: dbUser})
		}
	}
	return reqs
}

// ExecuteSyncRequests makes actual database changes from the requests passed in its arguments.
func (strategy DefaultUserSyncStrategy) ExecuteSyncRequests(requests []spec.PgSyncUserRequest, db *sql.DB) error {
	var reqretries []spec.PgSyncUserRequest
	errors := make([]string, 0)
	for _, request := range requests {
		switch request.Kind {
		case spec.PGSyncUserAdd:
			if err := strategy.createPgUser(request.User, db); err != nil {
				reqretries = append(reqretries, request)
				errors = append(errors, fmt.Sprintf("could not create user %q: %v", request.User.Name, err))
			}
		case spec.PGsyncUserAlter:
			if err := strategy.alterPgUser(request.User, db); err != nil {
				reqretries = append(reqretries, request)
				errors = append(errors, fmt.Sprintf("could not alter user %q: %v", request.User.Name, err))
				// XXX: we do not allow additional owner roles to be members of database owners
				// if ALTER fails it could be because of the wrong memberhip (check #1862 for details)
				// so in any case try to revoke the database owner from the additional owner roles
				// the initial ALTER statement will be retried once and should work then
				if request.User.IsDbOwner && len(strategy.AdditionalOwnerRoles) > 0 {
					if err := resolveOwnerMembership(request.User, strategy.AdditionalOwnerRoles, db); err != nil {
						errors = append(errors, fmt.Sprintf("could not resolve owner membership for %q: %v", request.User.Name, err))
					}
				}
			}
		case spec.PGSyncAlterSet:
			if err := strategy.alterPgUserSet(request.User, db); err != nil {
				reqretries = append(reqretries, request)
				errors = append(errors, fmt.Sprintf("could not set custom user %q parameters: %v", request.User.Name, err))
			}
		case spec.PGSyncUserRename:
			if err := strategy.alterPgUserRename(request.User, db); err != nil {
				reqretries = append(reqretries, request)
				errors = append(errors, fmt.Sprintf("could not rename custom user %q: %v", request.User.Name, err))
			}
		default:
			return fmt.Errorf("unrecognized operation: %v", request.Kind)
		}

	}

	// creating roles might fail if group role members are created before the parent role
	// retry adding roles as long as the number of failed attempts is shrinking
	if len(reqretries) > 0 {
		if len(reqretries) < len(requests) {
			if err := strategy.ExecuteSyncRequests(reqretries, db); err != nil {
				return err
			}
		} else {
			return fmt.Errorf("could not execute sync requests for users: %v", strings.Join(errors, `', '`))
		}
	}

	return nil
}

func resolveOwnerMembership(dbOwner spec.PgUser, additionalOwners []string, db *sql.DB) error {
	errors := make([]string, 0)
	for _, additionalOwner := range additionalOwners {
		if err := revokeRole(dbOwner.Name, additionalOwner, db); err != nil {
			errors = append(errors, fmt.Sprintf("could not revoke %q from %q: %v", dbOwner.Name, additionalOwner, err))
		}
	}

	if len(errors) > 0 {
		return fmt.Errorf("could not resolve membership between %q and additional owner roles: %v", dbOwner.Name, strings.Join(errors, `', '`))
	}

	return nil
}

func (strategy DefaultUserSyncStrategy) alterPgUserSet(user spec.PgUser, db *sql.DB) error {
	queries := produceAlterRoleSetStmts(user)
	query := fmt.Sprintf(doBlockStmt, strings.Join(queries, ";"))
	if _, err := db.Exec(query); err != nil {
		return err
	}
	return nil
}

func (strategy DefaultUserSyncStrategy) alterPgUserRename(user spec.PgUser, db *sql.DB) error {
	var query string

	// append or trim deletion suffix depending if the user has the suffix or not
	if user.Deleted {
		newName := strings.TrimSuffix(user.Name, strategy.RoleDeletionSuffix)
		query = fmt.Sprintf(alterUserRenameSQL, user.Name, newName, "")
	} else {
		query = fmt.Sprintf(alterUserRenameSQL, user.Name, user.Name, strategy.RoleDeletionSuffix)
	}

	if _, err := db.Exec(query); err != nil {
		return err
	}
	return nil
}

func (strategy DefaultUserSyncStrategy) createPgUser(user spec.PgUser, db *sql.DB) error {
	var userFlags []string
	var userPassword string

	if len(user.Flags) > 0 {
		userFlags = append(userFlags, user.Flags...)
	}
	if len(user.MemberOf) > 0 {
		userFlags = append(userFlags, fmt.Sprintf(inRoleTemplate, quoteMemberList(user)))
	}
	if user.AdminRole != "" {
		userFlags = append(userFlags, fmt.Sprintf(adminTemplate, user.AdminRole))
	}

	if user.Password == "" {
		userPassword = "PASSWORD NULL"
	} else {
		userPassword = fmt.Sprintf(passwordTemplate, util.NewEncryptor(strategy.PasswordEncryption).PGUserPassword(user))
	}
	query := fmt.Sprintf(createUserSQL, user.Name, strings.Join(userFlags, " "), userPassword)

	if _, err := db.Exec(query); err != nil { // TODO: Try several times
		return err
	}

	if len(user.Parameters) > 0 {
		if err := strategy.alterPgUserSet(user, db); err != nil {
			return fmt.Errorf("incomplete setup for user %s: %v", user.Name, err)
		}
	}

	return nil
}

func (strategy DefaultUserSyncStrategy) alterPgUser(user spec.PgUser, db *sql.DB) error {
	var resultStmt []string

	if user.Password != "" || len(user.Flags) > 0 {
		alterStmt := produceAlterStmt(user, strategy.PasswordEncryption)
		resultStmt = append(resultStmt, alterStmt)
	}
	if len(user.MemberOf) > 0 {
		grantStmt := produceGrantStmt(user)
		resultStmt = append(resultStmt, grantStmt)
	}

	if len(resultStmt) > 0 {
		query := fmt.Sprintf(doBlockStmt, strings.Join(resultStmt, ";"))

		if _, err := db.Exec(query); err != nil { // TODO: Try several times
			return err
		}
	}

	return nil
}

func produceAlterStmt(user spec.PgUser, encryption string) string {
	// ALTER ROLE ... LOGIN ENCRYPTED PASSWORD ..
	result := make([]string, 0)
	password := user.Password
	flags := user.Flags

	if password != "" {
		result = append(result, fmt.Sprintf(passwordTemplate, util.NewEncryptor(encryption).PGUserPassword(user)))
	}
	if len(flags) != 0 {
		result = append(result, strings.Join(flags, " "))
	}
	return fmt.Sprintf(alterUserSQL, user.Name, strings.Join(result, " "))
}

func produceAlterRoleSetStmts(user spec.PgUser) []string {
	result := make([]string, 0)
	result = append(result, fmt.Sprintf(alterRoleResetAllSQL, user.Name))
	for name, value := range user.Parameters {
		result = append(result, fmt.Sprintf(alterRoleSetSQL, user.Name, name, quoteParameterValue(name, value)))
	}
	return result
}

func produceGrantStmt(user spec.PgUser) string {
	// GRANT ROLE "foo", "bar" TO baz
	return fmt.Sprintf(grantToUserSQL, quoteMemberList(user), user.Name)
}

func quoteMemberList(user spec.PgUser) string {
	var memberof []string
	for _, member := range user.MemberOf {
		memberof = append(memberof, fmt.Sprintf(`"%s"`, member))
	}
	return strings.Join(memberof, ",")
}

func revokeRole(groupRole, role string, db *sql.DB) error {
	revokeStmt := fmt.Sprintf(revokeFromUserSQL, groupRole, role)

	if _, err := db.Exec(fmt.Sprintf(doBlockStmt, revokeStmt)); err != nil {
		return err
	}

	return nil
}

// quoteVal quotes values to be used at ALTER ROLE SET param = value if necessary
func quoteParameterValue(name, val string) string {
	start := val[0]
	end := val[len(val)-1]
	if name == "search_path" {
		// strip single quotes from the search_path. Those are required in the YAML configuration
		// to quote values containing commas, as otherwise NewFromMap would treat each comma-separated
		// part of such string as a separate map entry. However, a search_path is interpreted as a list
		// only if it is not quoted, otherwise it is treated as a single value. Therefore, we strip
		// single quotes here. Note that you can still use double quotes in order to escape schemas
		// containing spaces (but something more complex, like double quotes inside double quotes or spaces
		// in the schema name would break the parsing code in the operator.)
		if start == '\'' && end == '\'' {
			return val[1 : len(val)-1]
		}

		return val
	}
	if (start == '"' && end == '"') || (start == '\'' && end == '\'') {
		return val
	}
	return fmt.Sprintf(`'%s'`, strings.Trim(val, " "))
}

// DropPgUser to remove user created by the operator e.g. for password rotation
func DropPgUser(user string, db *sql.DB) error {
	query := fmt.Sprintf(dropUserSQL, user)
	if _, err := db.Exec(query); err != nil { // TODO: Try several times
		return err
	}

	return nil
}


================================================
File: pkg/util/volumes/ebs.go
================================================
package volumes

import (
	"fmt"
	"strings"

	"github.com/aws/aws-sdk-go/aws"
	"github.com/aws/aws-sdk-go/aws/session"
	"github.com/aws/aws-sdk-go/service/ec2"
	v1 "k8s.io/api/core/v1"

	"github.com/zalando/postgres-operator/pkg/util/constants"
	"github.com/zalando/postgres-operator/pkg/util/retryutil"
)

// EBSVolumeResizer implements volume resizing interface for AWS EBS volumes.
type EBSVolumeResizer struct {
	connection *ec2.EC2
	AWSRegion  string
}

// ConnectToProvider connects to AWS.
func (r *EBSVolumeResizer) ConnectToProvider() error {
	sess, err := session.NewSession(&aws.Config{Region: aws.String(r.AWSRegion)})
	if err != nil {
		return fmt.Errorf("could not establish AWS session: %v", err)
	}
	r.connection = ec2.New(sess)
	return nil
}

// IsConnectedToProvider checks if AWS connection is established.
func (r *EBSVolumeResizer) IsConnectedToProvider() bool {
	return r.connection != nil
}

// VolumeBelongsToProvider checks if the given persistent volume is backed by EBS.
func (r *EBSVolumeResizer) VolumeBelongsToProvider(pv *v1.PersistentVolume) bool {
	return (pv.Spec.AWSElasticBlockStore != nil && pv.Annotations[constants.VolumeStorateProvisionerAnnotation] == constants.EBSProvisioner) ||
		(pv.Spec.CSI != nil && pv.Spec.CSI.Driver == constants.EBSDriver)
}

// ExtractVolumeID extracts volumeID from "aws://eu-central-1a/vol-075ddfc4a127d0bd4"
// or return only the vol-075ddfc4a127d0bd4 when it doesn't have "aws://"
func (r *EBSVolumeResizer) ExtractVolumeID(volumeID string) (string, error) {
	if (strings.HasPrefix(volumeID, "vol-")) && !(strings.HasPrefix(volumeID, "aws://")) {
		return volumeID, nil
	}
	idx := strings.LastIndex(volumeID, constants.EBSVolumeIDStart) + 1
	if idx == 0 {
		return "", fmt.Errorf("malformed EBS volume id %q", volumeID)
	}
	return volumeID[idx:], nil
}

// GetProviderVolumeID converts aws://eu-central-1b/vol-00f93d4827217c629 to vol-00f93d4827217c629 for EBS volumes
func (r *EBSVolumeResizer) GetProviderVolumeID(pv *v1.PersistentVolume) (string, error) {
	var volumeID string = ""
	if pv.Spec.CSI != nil {
		volumeID = pv.Spec.CSI.VolumeHandle
	} else if pv.Spec.AWSElasticBlockStore != nil {
		volumeID = pv.Spec.AWSElasticBlockStore.VolumeID
	}
	if volumeID == "" {
		return "", fmt.Errorf("got empty volume id for volume %v", pv)
	}

	return r.ExtractVolumeID(volumeID)
}

// DescribeVolumes ...
func (r *EBSVolumeResizer) DescribeVolumes(volumeIds []string) ([]VolumeProperties, error) {
	if !r.IsConnectedToProvider() {
		err := r.ConnectToProvider()
		if err != nil {
			return nil, err
		}
	}

	volumeOutput, err := r.connection.DescribeVolumes(&ec2.DescribeVolumesInput{VolumeIds: aws.StringSlice((volumeIds))})
	if err != nil {
		return nil, err
	}

	p := []VolumeProperties{}
	if nil == volumeOutput.Volumes {
		return p, nil
	}

	for _, v := range volumeOutput.Volumes {
		if *v.VolumeType == "gp3" {
			p = append(p, VolumeProperties{VolumeID: *v.VolumeId, Size: *v.Size, VolumeType: *v.VolumeType, Iops: *v.Iops, Throughput: *v.Throughput})
		} else if *v.VolumeType == "gp2" {
			p = append(p, VolumeProperties{VolumeID: *v.VolumeId, Size: *v.Size, VolumeType: *v.VolumeType})
		} else {
			return nil, fmt.Errorf("Discovered unexpected volume type %s %s", *v.VolumeId, *v.VolumeType)
		}
	}

	return p, nil
}

// ResizeVolume actually calls AWS API to resize the EBS volume if necessary.
func (r *EBSVolumeResizer) ResizeVolume(volumeID string, newSize int64) error {
	/* first check if the volume is already of a requested size */
	volumeOutput, err := r.connection.DescribeVolumes(&ec2.DescribeVolumesInput{VolumeIds: []*string{&volumeID}})
	if err != nil {
		return fmt.Errorf("could not get information about the volume: %v", err)
	}
	vol := volumeOutput.Volumes[0]
	if *vol.VolumeId != volumeID {
		return fmt.Errorf("describe volume %q returned information about a non-matching volume %q", volumeID, *vol.VolumeId)
	}
	if *vol.Size == newSize {
		// nothing to do
		return nil
	}
	input := ec2.ModifyVolumeInput{Size: &newSize, VolumeId: &volumeID}
	output, err := r.connection.ModifyVolume(&input)
	if err != nil {
		return fmt.Errorf("could not modify persistent volume: %v", err)
	}

	state := *output.VolumeModification.ModificationState
	if state == constants.EBSVolumeStateFailed {
		return fmt.Errorf("could not modify persistent volume %q: modification state failed", volumeID)
	}
	if state == "" {
		return fmt.Errorf("received empty modification status")
	}
	if state == constants.EBSVolumeStateOptimizing || state == constants.EBSVolumeStateCompleted {
		return nil
	}
	// wait until the volume reaches the "optimizing" or "completed" state
	in := ec2.DescribeVolumesModificationsInput{VolumeIds: []*string{&volumeID}}
	return retryutil.Retry(constants.EBSVolumeResizeWaitInterval, constants.EBSVolumeResizeWaitTimeout,
		func() (bool, error) {
			out, err := r.connection.DescribeVolumesModifications(&in)
			if err != nil {
				return false, fmt.Errorf("could not describe volume modification: %v", err)
			}
			if len(out.VolumesModifications) != 1 {
				return false, fmt.Errorf("describe volume modification didn't return one record for volume %q", volumeID)
			}
			if *out.VolumesModifications[0].VolumeId != volumeID {
				return false, fmt.Errorf("non-matching volume id when describing modifications: %q is different from %q",
					*out.VolumesModifications[0].VolumeId, volumeID)
			}
			return *out.VolumesModifications[0].ModificationState != constants.EBSVolumeStateModifying, nil
		})
}

// ModifyVolume Modify EBS volume
func (r *EBSVolumeResizer) ModifyVolume(volumeID string, newType *string, newSize *int64, iops *int64, throughput *int64) error {
	/* first check if the volume is already of a requested size */
	input := ec2.ModifyVolumeInput{Size: newSize, VolumeId: &volumeID, VolumeType: newType, Iops: iops, Throughput: throughput}
	output, err := r.connection.ModifyVolume(&input)
	if err != nil {
		return fmt.Errorf("could not modify persistent volume: %v", err)
	}

	state := *output.VolumeModification.ModificationState
	if state == constants.EBSVolumeStateFailed {
		return fmt.Errorf("could not modify persistent volume %q: modification state failed", volumeID)
	}
	if state == "" {
		return fmt.Errorf("received empty modification status")
	}
	if state == constants.EBSVolumeStateOptimizing || state == constants.EBSVolumeStateCompleted {
		return nil
	}
	// wait until the volume reaches the "optimizing" or "completed" state
	in := ec2.DescribeVolumesModificationsInput{VolumeIds: []*string{&volumeID}}
	return retryutil.Retry(constants.EBSVolumeResizeWaitInterval, constants.EBSVolumeResizeWaitTimeout,
		func() (bool, error) {
			out, err := r.connection.DescribeVolumesModifications(&in)
			if err != nil {
				return false, fmt.Errorf("could not describe volume modification: %v", err)
			}
			if len(out.VolumesModifications) != 1 {
				return false, fmt.Errorf("describe volume modification didn't return one record for volume %q", volumeID)
			}
			if *out.VolumesModifications[0].VolumeId != volumeID {
				return false, fmt.Errorf("non-matching volume id when describing modifications: %q is different from %q",
					*out.VolumesModifications[0].VolumeId, volumeID)
			}
			return *out.VolumesModifications[0].ModificationState != constants.EBSVolumeStateModifying, nil
		})
}

// DisconnectFromProvider closes connection to the EC2 instance
func (r *EBSVolumeResizer) DisconnectFromProvider() error {
	r.connection = nil
	return nil
}


================================================
File: pkg/util/volumes/ebs_test.go
================================================
package volumes

import (
	"fmt"
	"testing"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func TestGetProviderVolumeID(t *testing.T) {
	tests := []struct {
		name     string
		pv       *v1.PersistentVolume
		expected string
		err      error
	}{
		{
			name: "CSI volume handle",
			pv: &v1.PersistentVolume{
				Spec: v1.PersistentVolumeSpec{
					PersistentVolumeSource: v1.PersistentVolumeSource{
						CSI: &v1.CSIPersistentVolumeSource{
							VolumeHandle: "vol-075ddfc4a127d0bd5",
						},
					},
				},
			},
			expected: "vol-075ddfc4a127d0bd5",
			err:      nil,
		},
		{
			name: "AWS EBS volume handle",
			pv: &v1.PersistentVolume{
				Spec: v1.PersistentVolumeSpec{
					PersistentVolumeSource: v1.PersistentVolumeSource{
						AWSElasticBlockStore: &v1.AWSElasticBlockStoreVolumeSource{
							VolumeID: "aws://eu-central-1a/vol-075ddfc4a127d0bd4",
						},
					},
				},
			},
			expected: "vol-075ddfc4a127d0bd4",
			err:      nil,
		},
		{
			name: "Empty volume handle",
			pv: &v1.PersistentVolume{
				Spec: v1.PersistentVolumeSpec{},
			},
			expected: "",
			err:      fmt.Errorf("got empty volume id for volume %v", &v1.PersistentVolume{}),
		},
	}

	resizer := EBSVolumeResizer{}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			volumeID, err := resizer.GetProviderVolumeID(tt.pv)
			if volumeID != tt.expected || (err != nil && err.Error() != tt.err.Error()) {
				t.Errorf("expected %v, got %v, expected err %v, got %v", tt.expected, volumeID, tt.err, err)
			}
		})
	}
}

func TestVolumeBelongsToProvider(t *testing.T) {
	tests := []struct {
		name     string
		pv       *v1.PersistentVolume
		expected bool
	}{
		{
			name: "CSI volume handle",
			pv: &v1.PersistentVolume{
				Spec: v1.PersistentVolumeSpec{
					PersistentVolumeSource: v1.PersistentVolumeSource{
						CSI: &v1.CSIPersistentVolumeSource{
							Driver:       "ebs.csi.aws.com",
							VolumeHandle: "vol-075ddfc4a127d0bd5",
						},
					},
				},
			},
			expected: true,
		},
		{
			name: "AWS EBS volume handle",
			pv: &v1.PersistentVolume{
				ObjectMeta: metav1.ObjectMeta{
					Annotations: map[string]string {
						"pv.kubernetes.io/provisioned-by": "kubernetes.io/aws-ebs",
					},
				},
				Spec: v1.PersistentVolumeSpec{
					PersistentVolumeSource: v1.PersistentVolumeSource{
						AWSElasticBlockStore: &v1.AWSElasticBlockStoreVolumeSource{
							VolumeID: "aws://eu-central-1a/vol-075ddfc4a127d0bd4",
						},
					},
				},
			},
			expected: true,
		},
		{
			name: "Empty volume source",
			pv: &v1.PersistentVolume{
				Spec: v1.PersistentVolumeSpec{},
			},
			expected: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			resizer := EBSVolumeResizer{}
			isProvider := resizer.VolumeBelongsToProvider(tt.pv)
			if isProvider != tt.expected {
				t.Errorf("expected %v, got %v", tt.expected, isProvider)
			}
		})
	}
}


================================================
File: pkg/util/volumes/volumes.go
================================================
package volumes

//go:generate mockgen -package mocks -destination=../../../mocks/$GOFILE -source=$GOFILE -build_flags=-mod=vendor

import v1 "k8s.io/api/core/v1"

// VolumeProperties ...
type VolumeProperties struct {
	VolumeID   string
	VolumeType string
	Size       int64
	Iops       int64
	Throughput int64
}

// VolumeResizer defines the set of methods used to implememnt provider-specific resizing of persistent volumes.
type VolumeResizer interface {
	ConnectToProvider() error
	IsConnectedToProvider() bool
	VolumeBelongsToProvider(pv *v1.PersistentVolume) bool
	GetProviderVolumeID(pv *v1.PersistentVolume) (string, error)
	ExtractVolumeID(volumeID string) (string, error)
	ResizeVolume(providerVolumeID string, newSize int64) error
	ModifyVolume(providerVolumeID string, newType *string, newSize *int64, iops *int64, throughput *int64) error
	DisconnectFromProvider() error
	DescribeVolumes(providerVolumesID []string) ([]VolumeProperties, error)
}


================================================
File: pkg/util/volumes/volumes_test.go
================================================
package volumes

import (
	"fmt"
	"testing"
)

func TestExtractVolumeID(t *testing.T) {
	var tests = []struct {
		input          string
		expectedResult string
		expectedErr    error
	}{
		{
			input:          "aws://eu-central-1c/vol-01234a5b6c78df9gh",
			expectedResult: "vol-01234a5b6c78df9gh",
			expectedErr:    nil,
		},
		{
			input:          "vol-0g9fd87c6b5a43210",
			expectedResult: "vol-0g9fd87c6b5a43210",
			expectedErr:    nil,
		},
		{
			input:          "aws://eu-central-1c/01234a5b6c78df9g0",
			expectedResult: "",
			expectedErr:    fmt.Errorf("malformed EBS volume id %q", "aws://eu-central-1c/01234a5b6c78df9g0"),
		},
		{
			input:          "hg9fd87c6b5a43210",
			expectedResult: "",
			expectedErr:    fmt.Errorf("malformed EBS volume id %q", "hg9fd87c6b5a43210"),
		},
	}

	resizer := EBSVolumeResizer{}

	for _, tt := range tests {
		volumeId, err := resizer.ExtractVolumeID(tt.input)
		if volumeId != tt.expectedResult {
			t.Errorf("%s expected: %s, got %s", t.Name(), tt.expectedResult, volumeId)
		}
		if err != tt.expectedErr {
			if tt.expectedErr != nil && err.Error() != tt.expectedErr.Error() {
				t.Errorf("%s unexpected error: got %v", t.Name(), err)
			}
		}
	}
}


================================================
File: ui/Dockerfile
================================================
ARG BASE_IMAGE=registry.opensource.zalan.do/library/python-3.11-slim:latest
ARG NODE_IMAGE=node:lts-alpine

FROM $NODE_IMAGE AS build

COPY . /workdir
WORKDIR /workdir/app

RUN npm install \
    && npm run build

FROM $BASE_IMAGE
LABEL maintainer="Team ACID @ Zalando <team-acid@zalando.de>"

EXPOSE 8081
WORKDIR /app

RUN apt-get -qq -y update \
 # https://www.psycopg.org/docs/install.html#psycopg-vs-psycopg-binary
 && apt-get -qq -y install --no-install-recommends g++ libpq-dev python3-dev python3-distutils \
 && apt-get -qq -y clean \
 && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
COPY start_server.sh .
RUN pip install -r requirements.txt

COPY operator_ui operator_ui/
COPY --from=build /workdir/operator_ui/static/build/ operator_ui/static/build/

ARG VERSION=dev
RUN sed -i "s/__version__ = .*/__version__ = '${VERSION}'/" operator_ui/__init__.py

CMD ["python", "-m", "operator_ui"]


================================================
File: ui/MANIFEST.in
================================================
recursive-include operator_ui/static *
recursive-include operator_ui/templates *
include *.rst


================================================
File: ui/Makefile
================================================
.PHONY: clean test appjs docker push mock

IMAGE            ?= registry.opensource.zalan.do/acid/postgres-operator-ui
VERSION          ?= $(shell git describe --tags --always --dirty)
TAG              ?= $(VERSION)
GITHEAD          = $(shell git rev-parse --short HEAD)
GITURL           = $(shell git config --get remote.origin.url)
GITSTATUS        = $(shell git status --porcelain || echo 'no changes')
TTYFLAGS         = $(shell test -t 0 && echo '-it')

ifdef CDP_PULL_REQUEST_NUMBER
	CDP_TAG := -${CDP_BUILD_VERSION}
endif

default: docker

clean:
	rm -fr operator_ui/static/build

test:
	tox

appjs:
	docker run $(TTYFLAGS) -u $$(id -u) -v $$(pwd):/workdir -w /workdir/app node:lts-alpine npm install --cache /workdir/.npm
	docker run $(TTYFLAGS) -u $$(id -u) -v $$(pwd):/workdir -w /workdir/app node:lts-alpine npm run build --cache /workdir/.npm

docker: appjs
	echo `(env)`
	echo "Tag ${TAG}"
	echo "Version ${VERSION}"
	echo "CDP tag ${CDP_TAG}"
	echo "git describe $(shell git describe --tags --always --dirty)"
	docker build --rm -t "$(IMAGE):$(TAG)$(CDP_TAG)" -f Dockerfile .

push:
	docker push "$(IMAGE):$(TAG)$(CDP_TAG)"

mock:
	docker run -it -p 8081:8081 "$(IMAGE):$(TAG)" --mock


================================================
File: ui/requirements.txt
================================================
backoff==2.2.1
boto3==1.34.110
boto==2.49.0
click==8.1.7
Flask==3.0.3
furl==2.1.3
gevent==24.2.1
jq==1.7.0
json_delta>=2.0.2
kubernetes==11.0.0
python-json-logger==2.0.7
requests==2.32.2
stups-tokens>=1.1.19
wal_e==1.1.1
werkzeug==3.0.6


================================================
File: ui/run_local.sh
================================================
#!/usr/bin/env bash
set -e

# NOTE: You still need to start the frontend bits of this application separately
# as starting it here as a child process would leave leftover processes on
# script termination; it appears there is some complication that does not allow
# the shell to clean up nodejs grandchild processes correctly upon script exit.


# Static bits:
export APP_URL="${API_URL-http://localhost:8081}"
export OPERATOR_API_URL="${OPERATOR_API_URL-http://localhost:8080}"
export TARGET_NAMESPACE="${TARGET_NAMESPACE-*}"

default_operator_ui_config='{
  "docs_link":"https://postgres-operator.readthedocs.io/en/latest/",
  "dns_format_string": "{0}.{1}",
  "databases_visible": true,
  "master_load_balancer_visible": true,
  "nat_gateways_visible": false,
  "replica_load_balancer_visible": true,
  "resources_visible": true,
  "users_visible": true,
  "cost_ebs": 0.0952,
  "cost_iops": 0.006,
  "cost_throughput": 0.0476,
  "cost_core": 0.0575,
  "cost_memory": 0.014375,
  "free_iops": 3000,
  "free_throughput": 125,
  "limit_iops": 16000,
  "limit_throughput": 1000,
  "postgresql_versions": [
    "17",
    "16",
    "15",
    "14",
    "13"
  ],
  "static_network_whitelist": {
    "localhost": ["172.0.0.1/32"]
  }
}'
export OPERATOR_UI_CONFIG="${OPERATOR_UI_CONFIG-${default_operator_ui_config}}"

# S3 backup bucket:
export SPILO_S3_BACKUP_BUCKET="postgres-backup"

# defines teams
teams='["acid"]'
export TEAMS="${TEAMS-${teams}}"

# Kubernetes API URL (e.g. minikube):
kubernetes_api_url="https://192.168.99.100:8443"

# Enable job control:
set -m

# Clean up child processes on exit:
trap 'kill $(jobs -p)' EXIT


# PostgreSQL Operator UI application name as deployed:
operator_ui_application='postgres-operator-ui'


# Hijack the PostgreSQL Operator UI pod as a proxy for its AWS instance profile
# on the pod's localhost:1234 which allows the WAL-E code to list backups there:
kubectl exec \
  "$(
    kubectl get pods \
      --server="${kubernetes_api_url}" \
      --selector="application=${operator_ui_application}" \
      --output='name' \
      | head --lines=1 \
      | sed 's@^[^/]*/@@'
  )" \
  -- \
    sh -c '
      apk add --no-cache socat;
      pkill socat;
      socat -v TCP-LISTEN:1234,reuseaddr,fork,su=nobody TCP:169.254.169.254:80
    ' \
  &


# Forward localhost:1234 to localhost:1234 on the PostgreSQL Operator UI pod to
# get to the AWS instance metadata endpoint:
echo "Port forwarding to the PostgreSQL Operator UI's instance metadata service"
kubectl port-forward \
  --server="${kubernetes_api_url}" \
  "$(
    kubectl get pods \
      --server="${kubernetes_api_url}" \
      --selector="application=${operator_ui_application}" \
      --output='name' \
      | head --lines=1 \
      | sed 's@^[^/]*/@@'
  )" \
  1234 \
  &


# Forward localhost:8080 to localhost:8080 on the PostgreSQL Operator pod, which
# allows access to the Operator REST API
# when using helm chart use --selector='app.kubernetes.io/name=postgres-operator'
echo 'Port forwarding to the PostgreSQL Operator REST API'
kubectl port-forward \
  --server="${kubernetes_api_url}" \
  "$(
    kubectl get pods \
      --server="${kubernetes_api_url}" \
      --selector='name=postgres-operator' \
      --output='name' \
      | head --lines=1 \
      | sed 's@^[^/]*/@@'
  )" \
  8080 \
  &


# Start a local proxy on localhost:8001 of the target Kubernetes cluster's API:
kubectl proxy &


# Start application:
python3 \
  -m operator_ui \
  --clusters='localhost:8001' \
  $@


================================================
File: ui/setup.py
================================================
import sys

from setuptools import find_packages, setup
from setuptools.command.test import test as TestCommand

from pathlib import Path


def read_version(package):
    with (Path(package) / '__init__.py').open() as fd:
        for line in fd:
            if line.startswith('__version__ = '):
                return line.split()[-1].strip().strip("'")


version = read_version('operator_ui')


class PyTest(TestCommand):

    user_options = [('cov-html=', None, 'Generate junit html report')]

    def initialize_options(self):
        TestCommand.initialize_options(self)
        self.cov = None
        self.pytest_args = ['--cov', 'operator_ui', '--cov-report', 'term-missing', '-v']
        self.cov_html = False

    def finalize_options(self):
        TestCommand.finalize_options(self)
        if self.cov_html:
            self.pytest_args.extend(['--cov-report', 'html'])
        self.pytest_args.extend(['tests'])

    def run_tests(self):
        import pytest

        errno = pytest.main(self.pytest_args)
        sys.exit(errno)


def readme():
    return open('README.rst', encoding='utf-8').read()


tests_require = [
    'pytest',
    'pytest-cov'
]

setup(
    name='operator-ui',
    packages=find_packages(),
    version=version,
    description='PostgreSQL Kubernetes Operator UI',
    long_description=readme(),
    author='team-acid@zalando.de',
    url='https://github.com/postgres-operator',
    keywords='PostgreSQL Kubernetes Operator UI',
    license='MIT',
    tests_require=tests_require,
    extras_require={'tests': tests_require},
    cmdclass={'test': PyTest},
    test_suite='tests',
    classifiers=[
        'Development Status :: 3',
        'Intended Audience :: Developers',
        'Intended Audience :: System Administrators',
        'License :: OSI Approved :: MIT',
        'Operating System :: OS Independent',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3.11',
        'Topic :: System :: Clustering',
        'Topic :: System :: Monitoring',
    ],
    include_package_data=True,  # needed to include JavaScript (see MANIFEST.in)
    entry_points={'console_scripts': ['operator-ui = operator_ui.main:main']}
)


================================================
File: ui/start_server.sh
================================================
#!/bin/bash
/usr/bin/python3 -m operator_ui


================================================
File: ui/tox.ini
================================================
[tox]
envlist=py35,flake8,eslint

[tox:travis]
3.5=py35,flake8,eslint

[testenv]
deps=pytest
commands=
    pip install -r requirements.txt
    python setup.py test

[testenv:flake8]
deps=flake8
commands=python setup.py flake8

[testenv:eslint]
whitelist_externals=eslint
changedir=app
commands=eslint src

[flake8]
max-line-length=160
ignore=E402

[pylama]
ignore=E402


================================================
File: ui/.dockerignore
================================================
*#
*.pyc
*~
.*.sw?
.git
__pycache__

.npm/

app/node_modules
operator_ui/static/build/*.hot-update.js
operator_ui/static/build/*.hot-update.json


================================================
File: ui/app/README.rst
================================================
This directory contains the EcmaScript frontend code of the PostgreSQL Operator UI and is only needed during build time.

The JavaScript application bundle (webpack) will be generated to ``operator_ui/static/build/app*.js`` by running:

.. code-block:: bash

    $ npm install
    $ npm run build

Frontend development is supported by watching the source code and continuously recompiling the webpack:

.. code-block:: bash

    $ npm start


================================================
File: ui/app/package.json
================================================
{
  "name": "postgres-operator-ui",
  "version": "1.14.0",
  "description": "PostgreSQL Operator UI",
  "main": "src/app.js",
  "config": {
    "buildDir": "../operator_ui/static/build"
  },
  "scripts": {
    "prestart": "npm install",
    "start": "NODE_ENV=development webpack --watch",
    "webpack": "webpack --config ./webpack.config.js",
    "build": "NODE_ENV=development npm run webpack",
    "prewebpack": "npm run clean",
    "lint": "eslint ./src/**/*.js",
    "clean": "rimraf $npm_package_config_buildDir && mkdir $npm_package_config_buildDir"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/zalando/postgres-operator.git"
  },
  "author": "",
  "license": "ISC",
  "bugs": {
    "url": "https://github.com/zalando/postgres-operator.git/issues"
  },
  "homepage": "https://github.com/zalando/postgres-operator.git#readme",
  "dependencies": {
    "@babel/core": "^7.20.12",
    "@babel/polyfill": "^7.12.1",
    "@babel/runtime": "^7.20.13",
    "pixi.js": "^7.1.1"
  },
  "devDependencies": {
    "@babel/plugin-transform-runtime": "^7.19.6",
    "@babel/preset-env": "^7.20.2",
    "babel-loader": "^8.2.5",
    "brfs": "^2.0.2",
    "dedent-js": "1.0.1",
    "eslint": "^8.32.0",
    "js-yaml": "4.1.0",
    "pug": "^3.0.2",
    "rimraf": "^4.1.2",
    "riot": "^3.13.2",
    "riot-hot-reload": "1.0.0",
    "riot-route": "^3.1.4",
    "riot-tag-loader": "2.1.0",
    "sort-by": "^1.2.0",
    "transform-loader": "^0.2.4",
    "webpack": "^4.46.0",
    "webpack-cli": "^4.10.0"
  }
}


================================================
File: ui/app/webpack.config.js
================================================
const DEBUG = process.env.NODE_ENV !== 'production'
const entry = ['./src/app.js']
const path = require('path')
const pkg = require('./package.json')
const webpack = require('webpack')

module.exports = {
    context: path.join(__dirname, './'),
    devtool: DEBUG ? 'inline-source-map' : false,
    entry: entry,
    mode: DEBUG ? 'development' : 'production',
    target: 'web',

    node: {
        fs: 'empty'
    },

    externals: {
        '$': '$',
        'jquery': 'jQuery',
    },

    output: {
    // filename: DEBUG ? 'app.js' : 'app-[hash].js'
        filename: 'app.js',
        library: 'App',
        path: path.resolve(pkg.config.buildDir),
        publicPath: DEBUG ? '/' : './',
    },

    plugins: [
        new webpack.optimize.OccurrenceOrderPlugin(),
        new webpack.HotModuleReplacementPlugin(),

        new webpack.LoaderOptionsPlugin({
            debug: DEBUG,
        }),

        // Print on rebuild when watching; see
        // https://github.com/webpack/webpack/issues/1499#issuecomment-155064216
        function () {
            this.plugin('watch-run', (watching, callback) => {
                console.log('Begin compile at ' + new Date())
                callback()
            })
        },

    ],

    module: {

        rules: [

            {
                test: /\.tag\.pug$/,
                loader: 'riot-tag-loader',
                exclude: /node_modules/,
                query: {
                    hot: true,
                    template: 'pug',
                    type: 'es6',
                },
            },

            {
                test: /\.tag$/,
                loader: 'riot-tag-loader',
                exclude: /node_modules/,
                query: {
                    hot: false,
                    type: 'es6',
                },
            },

            {
                test: /\.js$/,
                loader: 'babel-loader',
                exclude: /node_modules/,
                query: {
                    plugins: ['@babel/transform-runtime'],
                    presets: ['@babel/preset-env'],
                },
            },

            {
                test: /\.html$/,
                loader: 'file-loader?name=[path][name].[ext]',
                exclude: /node_modules/,
            },

            {
                test: /\.jpe?g$|\.svg$|\.png$/,
                loader: 'file-loader?name=[path][name].[ext]',
                exclude: /node_modules/,
            },

            {
                test: /\.json$/,
                loader: 'json',
                exclude: /node_modules/,
            },

            {
                test: /\.(otf|eot|svg|ttf|woff|woff2)(\?v=\d+\.\d+\.\d+)?$/,
                loader: 'url?limit=8192&mimetype=application/font-woff',
            },

            {
                test: /\.json$/,
                loader: 'json',
                include: path.join(__dirname, 'node_modules', 'pixi.js'),
            },

        ],

    },

}


================================================
File: ui/app/.eslintignore
================================================
src/vendor/*.js


================================================
File: ui/app/.eslintrc.yml
================================================
parserOptions:
  sourceType: module
env:
  browser: true
  node: true
  es6: true
extends: 'eslint:recommended'
rules:
  indent:
    - error
    - 4
  linebreak-style:
    - error
    - unix
  quotes:
    - error
    - single
  prefer-const:
    - error
  no-redeclare:
    - error
  no-unused-vars:
    - warn
    - argsIgnorePattern: "^_"
  semi:
    - error
    - never


================================================
File: ui/app/src/app.js
================================================
import jQuery from 'jquery'
import riot from 'riot'

import 'riot-hot-reload'

import './edit.tag.pug'
import './postgresql.tag.pug'
import './help-edit.tag.pug'
import './help-general.tag.pug'
import './postgresqls.tag.pug'
import './logs.tag.pug'
import './new.tag.pug'
import './status.tag.pug'
import './app.tag.pug'
import './restore.tag.pug'

Object.fromEntries = entries => entries.length === 0 ? {} : Object.assign(...entries.map(([k, v]) => ({[k]: v})))
Object.mapValues = (o, f) => Object.fromEntries(Object.entries(o).map(([k, v]) => [k, f(v, k)]))
Object.mapEntries = (o, f) => Object.fromEntries(Object.entries(o).map(f).filter(x => x))
Object.filterEntries = (o, f) => Object.mapEntries(o, entry => f(entry) && entry)
Object.filterValues = (o, f) => Object.filterEntries(o, ([key, value]) => f(value) && [key, value])


const getDefaulting = (object, key, def) => (
    object.hasOwnProperty(key) ? object[key] : def
)


const Dynamic = (options={}) => {
    const instance = {
        init: getDefaulting(options, 'init', () => ''),
        refresh: getDefaulting(options, 'refresh', () => true),
        update: getDefaulting(options, 'update', value => (instance.state = value, true)),
        validState: getDefaulting(options, 'validState', state => (
            state !== undefined &&
      state !== null &&
      typeof state === 'string' &&
      state.length > 0
        )),

        edit: event => (instance.update(event.target.value, instance, event), true),
        valid: () => instance.validState(instance.state),
    }

    instance.state = instance.init()
    return instance
}


/*
Dynamics manages a dynamic array whose elements are themselves Dynamic objects.

The default initializer builds an empty array as the initial state.

The "add" DOM event callback is provided to add a newly initialized Dynamic
object to the end of the state array.  The Dynamic array item is initialized
with the "itemInit" callback, which can be specified with a constructor option
and defaults to creating a Dynamic with all default options.

The "remove" DOM event callback is provided to handle DOM events that should
or remove a specific item.  For events on elements generated by iterating the
state with an each= attribute, the event.item will be set to the correct value.

The refresh callback is forwarded to all constituent Dynamic objects.
*/
const Dynamics = (options={}) => {
    const instance = Object.assign(
        Dynamic(
            Object.assign(
                { init: () => [] },
                'refresh' in options
                    ? { refresh: options.refresh }
                    : undefined
            )
        ),

        {
            itemInit: getDefaulting(options, 'itemInit', () =>
                Dynamic(
                    'refresh' in options
                        ? { refresh: options.refresh }
                        : {}
                )
            ),
            itemValid: getDefaulting(options, 'itemValid', item => item.valid()),
            validState: state => state.every(instance.itemValid),
            update: () => true,
            edit: () => true,

            add: _event => {
                instance.state.push(instance.itemInit())
                instance.refresh()
                return true
            },

            remove: event => {
                instance.state.splice(instance.state.indexOf(event.item), 1)
                instance.refresh()
                return true
            },
        }
    )

    Object.defineProperty(instance, 'valids', { get: () =>
        instance.state.filter(instance.itemValid)
    })

    return instance
}


/*
DynamicSet manages a keyed collection of Dynamic objects.  The constructor
receives an object mapping keys to initialization functions, and its state is a
mapping from the same keys to Dynamics initialized using the corresponding
key's initialization function.  A DynamicSet is valid when its constituent
Dynamics are all simultaneously valid.  The refresh callback is forwarded to
all constituent Dynamic objects.

Example:

  DynamicSet({
    foo: undefined,
    bar: () => 'baz',
  })

This call would create a DynamicSet with two constituent dynamics in its state:
one of them under the 'foo' key of the state object, built with the default
Dynamic initializer, and another under the 'bar' key of the state object, whose
state, in turn, would initially hold the value 'baz'.
*/
const DynamicSet = (items, options={}) => Object.assign(
    Dynamic(
        Object.assign(
            {
                init: () => Object.mapValues(items, init =>
                    Dynamic(
                        Object.assign(
                            init ? { init: init } : undefined,
                            'refresh' in options ? { refresh: options.refresh } : undefined
                        )
                    )
                ),
            },
            'refresh' in options
                ? { refresh: options.refresh }
                : undefined
        )
    ),

    {
        items: items,
        validState: state => Object.values(state).every(item => item.valid()),
        edit: () => true,
        update: () => true,
    }
)


const delete_cluster = (namespace, clustername) => {
    jQuery.confirm({
        backgroundDismiss: true,
        content: `
      <p>
        Are you sure you want to remove this PostgreSQL cluster?  If so,
        please <strong>type the cluster name here
        (<code>${namespace}/${clustername}</code>)</strong> and click the
        confirm button:
      </p>
      <input
        type="text"
        class="confirm-delete"
        placeholder="cluster name"
        style="width: 100%"
      >
      <hr>
      <p><small>
        <strong>Note</strong>: if you create a cluster with the same name as
        this one after deleting it, the new cluster will restore the data
        from this cluster's current backups stored in AWS S3.  This behavior
        will change soon and you will be able to reuse a cluster name and
        get a completely new cluster.
      </small></p>
    `,
        escapeKey: true,
        icon: 'glyphicon glyphicon-warning-sign',
        title: 'Confirm cluster deletion?',
        typeAnimated: true,
        type: 'red',
        onOpen: function () {
            const dialog = this
            const confirm = dialog.buttons.confirm
            const confirmSelector = jQuery(confirm.el)
            const input = dialog.$content.find('input')
            input.on('input', () => {
                if (input.val() === namespace + '/' + clustername) {
                    confirmSelector.removeClass('btn-default').addClass('btn-danger')
                    confirm.enable()
                } else {
                    confirm.disable()
                    confirmSelector.removeClass('btn-danger').addClass('btn-default')
                }
            })
        },
        buttons: {
            cancel: {
                text: 'Cancel',
            },
            confirm: {
                btnClass: 'btn-default',
                isDisabled: true,
                text: 'Delete cluster',
                action: () => {
                    jQuery.ajax({
                        type: 'DELETE',
                        url: (
                            './postgresqls/'
              + encodeURI(namespace)
              + '/' + encodeURI(clustername)
                        ),
                        dataType: 'text',
                        success: () => location.assign('./#/list'),
                        error: (r, status, error) => location.assign('./#/list'), // TODO: show error
                    })
                },
            },
        }
    })
}


/* Unfortunately, there does not appear to be a good way to import local modules
inside a Riot tag, so we define/import things here and pass them manually in the
opts variable.  Remember to propagate opts manually when instantiating tags.  */
riot.mount('app', {
    Dynamic: Dynamic,
    Dynamics: Dynamics,
    DynamicSet: DynamicSet,
    delete_cluster: delete_cluster,
})


================================================
File: ui/app/src/app.tag.pug
================================================
app

  nav.navbar.navbar-inverse.navbar-fixed-top
    .container

      .navbar-header
        a.navbar-brand(href='./')
          | PostgreSQL Operator UI

      #navbar.navbar-collapse.collapse
        ul.nav.navbar-nav

          li(class='{ active: ["EDIT", "LIST", "LOGS", "STATUS"].includes(activenav) }')
            a(href='./#/list') PostgreSQL clusters

          li(class='{ active: "BACKUPS" === activenav }')
            a(href='./#/backups') Backups

          li(class='{ active: "OPERATOR" === activenav }')
            a(href='./#/operator') Status

          li(class='{ active: "NEW" === activenav }')
            a(href='./#/new') New cluster

          li(if='{ config }')
            a(href='{ config.docs_link }' target='_blank') Documentation

  .container-fluid

    .alert.alert-warning.alert-dismissible(
      if='{ config && config.kubernetes_in_maintenance }'
      role='alert'
    )
      button.close(
        aria-label='Close'
        data-dismiss='alert'
        type='button'
      )
        span.glyphicon.glyphicon-remove(aria-hidden='true')

      p.lead
        span.glyphicon.glyphicon-exclamation-sign(aria-hidden='true')
        span.sr-only Warning:
        |
        | This Kubernetes cluster appears to be undergoing maintenance.  You may experience delays in database cluster creation and changes.

    .sk-spinner-pulse(
      if='{ config !== null && teams !== null && (config === undefined || teams === undefined) }'
    )

    p(if='{ config === null || teams === null }')
      | Error loading UI configuration.  Please
      |
      a(onclick="window.location.reload(true);") try again
      |
      | or
      |
      a(href="./") start over
      | .

    div(if='{ config }')

      edit(
        if='{ activenav === "EDIT" }'
        clustername='{ clustername }'
        config='{ config }'
        namespace='{ namespace }'
        opts='{ opts }'
        read_write='{ read_write }'
        teams='{ teams }'
      )

      logs(
        if='{ activenav === "LOGS"}'
        clustername='{ clustername }'
        config='{ config }'
        namespace='{ namespace }'
        opts='{ opts }'
        read_write='{ read_write }'
        teams='{ teams }'
      )

      new(
        if='{ activenav === "NEW" }'
        backup_name='{ backup_name }'
        backup_timestamp='{ backup_timestamp }'
        backup_uid='{ backup_uid }'
        config='{ config }'
        opts='{ opts }'
        read_write='{ read_write }'
        teams='{ teams }'
      )

      postgresql(
        if='{ activenav === "STATUS" }'
        clustername='{ clustername }'
        config='{ config }'
        namespace='{ namespace }'
        opts='{ opts }'
        read_write='{ read_write }'
        teams='{ teams }'
      )

      postgresqls(
        if='{ activenav === "LIST" }'
        config='{ config }'
        opts='{ opts }'
        read_write='{ read_write }'
        teams='{ teams }'
      )

      restore(
        if='{ activenav === "BACKUPS" }'
        config='{ config }'
        opts='{ opts }'
        read_write='{ read_write }'
        teams='{ teams }'
      )

      status(
        if='{ activenav === "OPERATOR" }'
        config='{ config }'
        opts='{ opts }'
        read_write='{ read_write }'
        teams='{ teams }'
      )

  script.

    this.config = undefined
    this.teams = undefined

    this.activenav = 'INIT'
    this.read_write = false

    const nav = (path, page, parameters, f) => {
      route(path, (...args) => {
        parameters && parameters.forEach((parameter, index) =>
          this[parameter] = args[index]
        )
        this.activenav = page
        this.update()
        f && f(...args)
      })
    }

    const navs = (paths, ...args) => (
      paths.forEach(path =>
        nav(path, ...args)
      )
    )

    ;(
      jQuery
      .get('./config')
      .done(config => {
        this.config = config
        ;(
          jQuery
          .get('./teams')
          .done(teams => {
            this.teams = teams.sort()
            this.team = this.teams[0]

            this.read_write = (
              this.config
              && (
                !this.config.read_only_mode
                || this.teams.includes(this.config.superuser_team)
              )
            )

            nav('/backups', 'BACKUPS')
            nav('/edit/*/*', 'EDIT', ['namespace', 'clustername'])
            nav('/list', 'LIST')
            nav('/logs/*/*', 'LOGS', ['namespace', 'clustername'])
            nav('/operator..', 'OPERATOR')
            nav('/status/*/*', 'STATUS', ['namespace', 'clustername'])

            nav(
              '/new',
              'NEW',
              ['backup_name', 'backup_uid', 'backup_timestamp'],
              () => this.tags['new'].reset_form(),
            )

            navs(
              [
                '/clone/*/*',
                '/clone/*/*/*',
              ],
              'NEW',
              ['backup_name', 'backup_uid', 'backup_timestamp']
            )

            route.start(true)

            if (this.activenav === 'INIT') {
              route(this.read_write ? '/new' : '/list')
            }
          })
          .fail(() => this.teams = null)
          .always(() => this.update())
        )
      })
      .fail(() => this.config = null)
      .always(() => this.update())
    )


================================================
File: ui/app/src/edit.tag.pug
================================================
edit
  .container-fluid

    h1.page-header
      nav(aria-label="breadcrumb")
        ol.breadcrumb

          li.breadcrumb-item
            a(href='./#/list')
              | PostgreSQL clusters

          li.breadcrumb-item(if='{ cluster_path }')
            a(href='./#/status/{ cluster_path }')
              | { qname }

          li.breadcrumb-item.active(
            aria-current='page'
            if='{ cluster_path }'
          )
            a(href='./#/edit/{ cluster_path }')
              | Edit

    .row(if='{ cluster_path }')

      .col-lg-4
        h2 Cluster YAML definition
        div
          pre
            code.language-yaml(ref='yamlNice')

      div
        .col-lg-5

          h3 Edit supported properties
          textarea.textarea(
            style='width: 100%; height: 200px; font-family: monospace'
            ref='changedProperties'
            onkeyup='{ updateEditable }'
            onchange='{ updateEditable }'
          )
            | { editablePropertiesText }

          h3 Preview changes to spec
          pre
            code.language-yaml(ref='yamlEditable')
              | { editablePropertiesPreview }

          button.btn.btn-success(
            if='{ opts.read_write }'
            value='Save changes'
            onclick='{ saveChanges }'
          )
            | Apply changes

          .alert.alert-danger(if='{ saveResult === false }')
            b Error
            |
            | { saveMessage }

          .alert.alert-success(if='{ saveResult === true }')
            b Changes applied
            |
            | updates to cluster pending

        .col-lg-3
          help-edit(config='{ opts.config }')

  script.

    const yamlParser = require('js-yaml')

    this.updateEditable = e => {
      if (this.refs.changedProperties.value) {
        this.editablePropertiesPreview = yamlParser.dump(
          yamlParser.load(
            this.refs.changedProperties.value,
          ),
        )
      }
    }

    this.saveChanges = e => {
      this.saving = true
      this.saveResult = null
      this.saveMessage = ''

      jsonPayload = JSON.stringify(
        yamlParser.load(
          this.refs.changedProperties.value,
        ),
      )

      jQuery.ajax({
        type: 'POST',
        url: './postgresqls/' + this.cluster_path,
        contentType:"application/json",
        data: jsonPayload,
        processData: false,
        success: ()=> {
          this.saveResult = true
          this.update()
          this.pollProgress()
        },
        error: (r, status, error) => {
          this.saveResult = false
          this.saveMessage = r.responseJSON.error
          this.update()
          this.pollProgress()
        },
        dataType: "json",
      })
    }

    this.pollProgress = () => {
      jQuery.get(
        './postgresqls/' + this.cluster_path,
      ).then(data => {

        // Input data:
        const i = {}
        this.progress.thirdParty = true
        i.postgresql = this.progress.thirdPartySpec = data
        i.metadata = i.postgresql.metadata
        i.spec = i.postgresql.spec

        if (i.metadata.selfLink) { delete i.metadata.selfLink }
        if (i.metadata.uid) { delete i.metadata.uid }
        if (i.metadata.resourceVersion) { delete i.metadata.resourceVersion }
        if (i.metadata.managedFields) { delete i.metadata.managedFields }

        this.update()
        this.refs.yamlNice.innerHTML = yamlParser.dump(i.postgresql, {sortKeys: true})

        // Output data:
        const o = this.editableProperties = { spec: {} }

        o.spec.allowedSourceRanges = i.spec.allowedSourceRanges || []
        o.spec.numberOfInstances = i.spec.numberOfInstances
        o.spec.enableMasterLoadBalancer = i.spec.enableMasterLoadBalancer || false
        o.spec.enableReplicaLoadBalancer = i.spec.enableReplicaLoadBalancer || false
        o.spec.enableConnectionPooler = i.spec.enableConnectionPooler || false
        o.spec.enableReplicaConnectionPooler = i.spec.enableReplicaConnectionPooler || false
        o.spec.enableMasterPoolerLoadBalancer = i.spec.enableMasterPoolerLoadBalancer || false
        o.spec.enableReplicaPoolerLoadBalancer = i.spec.enableReplicaPoolerLoadBalancer || false
        o.spec.maintenanceWindows = i.spec.maintenanceWindows || []

        o.spec.volume = {
          size: i.spec.volume.size,
          throughput: i.spec.volume.throughput || 125,
          iops: i.spec.volume.iops || 3000
        }
        if ('storageClass' in i.spec.volume) {
            o.spec.volume.storageClass=i.spec.volume.storageClass
        }

        o.spec.postgresql = {}
        o.spec.postgresql.version = i.spec.postgresql.version

        if ('users' in i.spec && typeof i.spec.users === 'object') {
          o.spec.users = Object.mapValues(i.spec.users, roleFlags =>
            !Array.isArray(roleFlags)
            ? []
            : roleFlags.filter(roleFlag => typeof roleFlag === 'string')
          )
        }

        if ('databases' in i.spec && typeof i.spec.databases === 'object') {
          o.spec.databases = Object.filterValues(i.spec.databases, owner =>
            typeof owner === 'string'
          )
        }

        if ('resources' in i.spec && typeof i.spec.resources === 'object') {
          o.spec.resources = {};
          ['limits', 'requests'].forEach(section => {
            const resources = i.spec.resources[section]
            o.spec.resources[section] = {}
            if (typeof resources === 'object') {
              [
                'cpu',
                'memory',
              ].forEach(resourceType => {
                if (resourceType in resources) {
                  const resourceClaim = resources[resourceType]
                  if (typeof resourceClaim === 'string') {
                    o.spec.resources[section][resourceType] = resources[resourceType]
                  }
                }
              })
            }
          })
        }

        this.editablePropertiesText = (
          yamlParser
          .dump(this.editableProperties)
          .slice(0, -1)
        )
        this.editablePropertiesPreview = this.editablePropertiesText

        this.update()
      })
    }

    this.on('mount', () => {
      const namespace = this.namespace = this.opts.namespace
      const clustername = this.clustername = this.opts.clustername
      const qname = this.qname = namespace + '/' + clustername
      const cluster_path = this.cluster_path = (
        encodeURI(namespace)
        + '/' + encodeURI(clustername)
      )
      this.progress = {
        requestStatus: 'OK',
      }
      this.pollProgress()
    })

    this.on('updated', () => {
      this.refs.yamlEditable.innerHTML = this.editablePropertiesPreview
      Prism.highlightAll()
    })


================================================
File: ui/app/src/help-edit.tag.pug
================================================
help-edit

  h2 Help

  .well

    h3(style='margin-top: 0')
      | Docs

    a(href="{ opts.config.docs_link }")
      | more...

    h3 Editing

    p.
      The text box shows you the properties that can currently edit. Verify that the preview shows a valid part of the spec before submitting.  After a successful submit the changes may take some time to be applied.

    h3 Volume size

    p.
      You need to specify in format "123Gi". You can only increase the volume size. You can only increase volume size once very 6 hours, per AWS limitation.

    virtual(
      if='{ opts.config.static_network_whitelist && Object.keys(opts.config.static_network_whitelist).length > 0 }'
    )
      h3 IP Ranges

      // Raw tags are required here, as otherwise either riotjs removes space it shouldn't, or pugjs adds space it shouldn't.  And it has to be all in one line, as it has to be a pre tag (otherwise the riotjs compiler breaks the whitespace).
      <pre><virtual each="{ network, network_index in Object.keys(opts.config.static_network_whitelist) }"><virtual if="{ network_index > 0 }"><br></virtual>    # { network }<br><virtual each="{ range, range_index in opts.config.static_network_whitelist[network] }">    - { range }<virtual if="index < network.length - 1"><br></virtual></virtual></virtual></pre>


================================================
File: ui/app/src/help-general.tag.pug
================================================
help-general

  h2 Help

  .well

    h3(style='margin-top: 0')
      | Docs

    a(href="{ opts.config.docs_link }")
      | more...

    h3 Basics

    p.
      The Postgres Operator will use your definition to create a new
      PostgreSQL cluster for you. You can either copy the yaml definition
      to a repositiory or hit create cluster (not available in prod).


================================================
File: ui/app/src/logs.tag.pug
================================================
logs

  h1.page-header(if='{ cluster_path }')
    nav(aria-label="breadcrumb")
      ol.breadcrumb

        li.breadcrumb-item
          a(href='./#/list')
            | PostgreSQL clusters

        li.breadcrumb-item
          a(href='./#/status/{ cluster_path }')
            | { qname }

        li.breadcrumb-item
          a(href='./#/logs/{ cluster_path }')
            | Logs

  .sk-spinner-pulse(if='{ logs === undefined }')

  .container-fluid(if='{ logs === null }')
    p
      | Error loading logs.  Please
      |
      a(onclick="window.location.reload(true)") try again
      |
      | or
      |
      a(href="./") start over
      | .

  .container-fluid(if='{ logs }')

    table.table.table-hover

      tr(each='{ logs }')

        td(each='{ [levels[Level]] }')
          span.label.label-font-size(class='label-{ color_class }')
            | { label }

        td(style='white-space: pre')
          | { Time }

        td(style='font-family: monospace')
          | { Message }

  script.

    this.levels = {
      "panic": { label: "Panic"  , color_class: "danger" },
      "fatal": { label: "Fatal"  , color_class: "danger" },
      "error": { label: "Error"  , color_class: "danger" },
      "warning": { label: "Warning", color_class: "warning" },
      "info": { label: "Info"   , color_class: "primary" },
      "debug": { label: "Debug"  , color_class: "warning" },
    }

    this.logs = undefined

    this.on('mount', () => {
      if (
        this.namespace !== this.opts.namespace
        || this.clustername !== this.opts.clustername
      ) {
        const namespace = this.namespace = this.opts.namespace
        const clustername = this.clustername = this.opts.clustername
        const qname = this.qname = namespace + '/' + clustername
        const cluster_path = this.cluster_path = (
          encodeURI(namespace)
          + '/' + encodeURI(clustername)
        )
        ;(
          jQuery
          .get(`./operator/clusters/${cluster_path}/logs`)
          .done(logs => this.logs = logs.reverse())
          .fail(() => this.logs = null)
          .always(() => this.update())
        )
      }
    })


================================================
File: ui/app/src/new.tag.pug
================================================
new

  style.

    .input-units {
      width: 2em;
    }

    .resource-type {
      text-align: left;
      width: 7em;
    }

  .container-fluid

    h1.page-header
      nav(aria-label='breadcrumb')
        ol.breadcrumb

          li.breadcrumb-item
            a(href='./#/new')
              | New PostgreSQL cluster

    .row.text-center(if='{ !creating }')

    .row

      .col-lg-3
        h2 Cluster YAML definition
        div
          pre
            code.language-yaml(ref='yamlNice')

      .col-lg-6

        form(
          if='{ !creating }'
          action='javascript:void(0);'
          id='form'
        )

          .btn-toolbar.pull-right
            .btn-group(
              aria-label='Actions'
              role='group'
            )

              input.btn.btn-primary(
                type='submit'
                form='form'
                value='Validate'
              )

              button.btn.btn-info.btn-copy
                | Copy definiton

              button.btn.btn-success(
                if='{ !clusterExists && parent.read_write }'
                ref='submitbutton'
                onclick='{ requestCreate }'
                disabled='{ !allValid() }'
              )
                | Create cluster

              a.btn.btn-small.btn-warning(
                if='{ clusterExists }'
                href='./#/status/{ namespace.state }/{ name }'
              )
                | Cluster exists (show status)

          h2 New cluster configuration

          table.table

            tr(
              each='{ backup.state.type.state === "empty" ? [] : [backup] }'
            )
              td(colspan='2')
                h3.text-center Source data

            tr(
              each='{ backup.state.type.state === "empty" ? [] : [backup] }'
            )
              td Backup name
              td
                input.form-control(
                  each='{ !["restore", "pitr"].includes(state.type.state) ? [] : [state.name] }'
                  ref='backup_name'
                  type='text'
                  placeholder='Source backup name'
                  required
                  value='{ state }'
                  onchange='{ edit }'
                  onkeyup='{ edit }'
                )

            tr(
              each='{ backup.state.type.state === "empty" ? [] : [backup] }'
            )
              td Backup UID
              td
                input.form-control(
                  each='{ !["restore", "pitr"].includes(state.type.state) ? [] : [state.uid] }'
                  ref='backup_uid'
                  type='text'
                  placeholder='Source backup ID'
                  value='{ state }'
                  onchange='{ edit }'
                  onkeyup='{ edit }'
                )

            tr(
              each='{ backup.state.type.state === "empty" ? [] : [backup] }'
            )
              td Target timestamp
              td
                input.timestamp.form-control(
                  each='{ !["pitr"].includes(state.type.state) ? [] : [state.timestamp] }'
                  ref='backup_timestamp'
                  type='text'
                  placeholder='Restore to state at timestamp'
                  value='{ state }'
                  onchange='{ edit }'
                  onkeyup='{ edit }'
                )

            tr(
              each='{ backup.state.type.state === "empty" ? [] : [backup] }'
            )
              td(colspan='2')
                h3.text-center New cluster settings

            tr
              td
                | Name
              td
                input.form-control(
                  ref='name'
                  type='text'
                  placeholder='new-cluster (can be 53 characters long)'
                  title='Database cluster name, must be a valid hostname component'
                  pattern='[a-z0-9]+[a-z0-9\-]+[a-z0-9]+'
                  maxlength=53
                  required
                  value='{ name }'
                  onchange='{ nameChange }'
                  onkeyup='{ nameChange }'
                  style='width: 100%'
                )

            tr(
              each='{ !["", "*"].includes(config.target_namespace) ? [] : [namespace] }'
            )
              td
                | Namespace
              td
                select.form-control(
                  ref='namespace'
                  title='Database cluster Kubernetes namespace'
                  onchange='{ edit }'
                  onkeyup='{ edit }'
                  style='width: 100%'
                )
                  option(
                    each='{ namespace in parent.config.namespaces }'
                    selected='{ state === namespace }'
                    value='{ namespace }'
                  )
                    | { namespace }

            tr
              td Owning team
              td
                select.form-control(
                  name='team'
                  onchange='{ teamChange }'
                  onkeyup='{ teamChange }'
                )
                  option(
                    each='{ team in teams }'
                    value='{ team }'
                  )
                    | { team }

            tr
              td PostgreSQL version
              td
                select.form-control(
                  name='postgresqlVersion'
                  onchange='{ versionChange }'
                  onkeyup='{ versionChange }'
                )
                  option(
                    each='{ version in opts.config.postgresql_versions }'
                    value='{ version }'
                  )
                    | { version }

            tr
              td DNS name:
              td
                | { dnsName }

            tr
              td Number of instances
              td
                input.form-control(
                  ref='instanceCount'
                  type='number'
                  min='1'
                  required
                  value='{ instanceCount }'
                  onchange='{ instanceCountChange }'
                  onkeyup='{ instanceCountChange }'
                  style='width: 100%'
                )

            tr(if='{ [undefined, true].includes(config.master_load_balancer_visible) }')
              td Enable load balancer
              td
                ul.ips
                  li
                    label
                      input(
                        type='checkbox'
                        value='{ enableMasterLoadBalancer }'
                        onchange='{ toggleEnableMasterLoadBalancer }'
                      )
                      |
                      | Master
                  li(if='{ [undefined, true].includes(config.replica_load_balancer_visible) && instanceCount > 1 }')
                    label
                      input(
                        type='checkbox'
                        value='{ enableReplicaLoadBalancer }'
                        onchange='{ toggleEnableReplicaLoadBalancer }'
                      )
                      |
                      | Replica

            tr(if='{ [undefined, true].includes(config.connection_pooler_visible) }')
              td Enable connection pooler
              td
                ul.ips
                  li
                    label
                      input(
                        type='checkbox'
                        value='{ enableConnectionPooler }'
                        onchange='{ toggleEnableConnectionPooler }'
                      )
                      |
                      | Master
                  li(if='{ [undefined, true].includes(config.replica_connection_pooler_visible) && instanceCount > 1 }')
                    label
                      input(
                        type='checkbox'
                        value='{ enableReplicaConnectionPooler }'
                        onchange='{ toggleEnableReplicaConnectionPooler }'
                      )
                      |
                      | Replica

            tr(if='{ [undefined, true].includes(config.master_pooler_load_balancer_visible) }')
              td Enable connection pooler load balancer
              td
                ul.ips
                  li
                    label
                      input(
                        type='checkbox'
                        value='{ enableMasterPoolerLoadBalancer }'
                        onchange='{ toggleEnableMasterPoolerLoadBalancer }'
                      )
                      |
                      | Master
                  li(if='{ [undefined, true].includes(config.replica_pooler_load_balancer_visible) && instanceCount > 1 }')
                    label
                      input(
                        type='checkbox'
                        value='{ enableReplicaPoolerLoadBalancer }'
                        onchange='{ toggleEnableReplicaPoolerLoadBalancer }'
                      )
                      |
                      | Replica

            tr
              td Volume size
              td
                .input-group
                  input.form-control(
                    ref='volumeSize'
                    type='number'
                    min='1'
                    required
                    value='{ volumeSize }'
                    onchange='{ volumeChange }'
                    onkeyup='{ volumeChange }'
                  )
                  .input-group-addon
                    .input-units Gi

            tr
              td storageClass
              td
                .input-group
                  input.form-control(
                    ref='volumeStorageClass'
                    type='text'
                    value='{ volumeStorageClass }'
                    onchange='{ storageClassChange }'
                    onkeyup='{ storageClassChange }'
                  )

            tr
              td 
              td Specify Iops and Throughput only if you need more than the default 3000 Iops and 125Mb/s EBS provides.

            tr
              td Iops
              td
                .input-group
                  input.form-control(
                    ref='iops'
                    type='number'
                    value='{ iops }'
                    onchange='{ iopsChange }'
                    onkeyup='{ iopsChange }'
                  )
                  .input-group-addon
                    .input-units

            tr
              td Throughput
              td
                .input-group
                  input.form-control(
                    ref='throughput'
                    type='number'
                    value='{ throughput }'
                    onchange='{ throughputChange }'
                    onkeyup='{ throughputChange }'
                  )
                  .input-group-addon
                    .input-units MB/s

            tr(if='{ config.users_visible }')
              td
                button.btn.btn-success.btn-xs(onclick='{ users.add }')
                  span.glyphicon.glyphicon-plus
                |
                | Users
              td
                .input-group(each='{ users.state }')
                  .input-group-btn
                    button.btn.btn-danger(onclick='{ users.remove }')
                      span.glyphicon.glyphicon-trash
                  input.username.form-control(
                    type='text'
                    placeholder='Username'
                    title='^[a-z0-9]([-_a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-_a-z0-9]*[a-z0-9])?)*$'
                    pattern='^[a-z0-9]([-_a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-_a-z0-9]*[a-z0-9])?)*$'
                    required
                    value='{ state }'
                    onchange='{ edit }'
                    onkeyup='{ edit }'
                    style='width: 100%'
                  )

            tr(if='{ config.databases_visible }')
              td
                button.btn.btn-success.btn-xs(onclick='{ databases.add }')
                  span.glyphicon.glyphicon-plus
                |
                | Databases
              td
                .input-group(each='{ databases.state }')
                  .input-group-btn
                    button.btn.btn-danger(
                      onclick='{ databases.remove }'
                      style='float: right'
                    )
                      span.glyphicon.glyphicon-trash
                  input.databasename.form-control(
                    type='text'
                    placeholder='Database name'
                    title='Alphanumerics or underscore; may not start on a digit'
                    pattern='^[a-zA-Z_][a-zA-Z0-9_]*$'
                    required
                    value='{ state }'
                    onchange='{ edit }'
                    onkeyup='{ edit }'
                    style='width: 50%'
                  )
                  select.owner.form-control(
                    onchange='{ owner.edit }'
                    onkeyup='{ owner.edit }'
                    disabled='{ users.valids.length === 0 }'
                    style='width: 50%'
                  )
                    option(
                      selected='{ owner.state.length === 0 }'
                      value=''
                      disabled
                      hidden
                    )
                      | Select owner…
                    option(
                      each='{ users.valids }'
                      selected='{ state === parent.owner.state }'
                      value='{ state }'
                    )
                      | { state }

            tr(if='{ !jQuery.isEmptyObject(config.static_network_whitelist) }')
              td Allowed IP ranges
              td
                ul.ips
                  li(each='{ value, name in config.static_network_whitelist }')
                    label
                      input(
                        type='checkbox'
                        value='{ name }'
                        onchange='{ toggleIPRange }'
                        checked='{ name in ranges }'
                      )
                      |
                      | { name }

            tr(if='{ config.nat_gateways_visible }')
              td
                button.btn.btn-success.btn-xs(onclick='{ nats.add }')
                  span.glyphicon.glyphicon-plus
                |
                | AWS NAT IPs
              td
                div(
                  each='{ nats.state }'
                )
                  .input-group
                    .input-group-btn
                      button.btn.btn-danger(onclick='{ nats.remove }')
                        span.glyphicon.glyphicon-trash
                    input.form-control(
                      type='text'
                      required
                      value='{ state }'
                      onchange='{ edit }'
                      onkeyup='{ edit }'
                    )
                    .input-group-addon
                      .input-units / 32

            tr(if='{ config.odd_host_visible }')
              td Odd host
              td
                .input-group
                  input.form-control(
                    ref='odd'
                    type='text'
                    name='odd'
                    placeholder='IP'
                    onchange='{ oddChanges }'
                    onkeyup='{ oddChanges }'
                    value='{ odd }'
                  )
                  .input-group-addon
                    .input-units / 32

            tr(if='{ config.resources_visible }')
              td Resources
              td
                table(width='100%')

                  tr
                    td CPU
                    td

                      .input-group
                        .input-group-addon.resource-type Request
                        input.form-control(
                          ref='cpuRequest'
                          type='number'
                          placeholder='{ cpu.state.request.initialValue }'
                          min='1'
                          required
                          value='{ cpu.state.request.state }'
                          onchange='{ cpu.state.request.edit }'
                          onkeyup='{ cpu.state.request.edit }'
                        )
                        .input-group-addon
                          .input-units m

                      .input-group
                        .input-group-addon.resource-type Limit
                        input.form-control(
                          ref='cpuLimit'
                          type='number'
                          placeholder='{ cpu.state.limit.initialValue }'
                          min='250'
                          required
                          value='{ cpu.state.limit.state }'
                          onchange='{ cpu.state.limit.edit }'
                          onkeyup='{ cpu.state.limit.edit }'
                        )
                        .input-group-addon
                          .input-units m

                  tr
                    td Memory
                    td

                      .input-group
                        .input-group-addon.resource-type Request
                        input.form-control(
                          ref='memoryRequest'
                          type='number'
                          placeholder='{ memory.state.request.initialValue }'
                          min='1'
                          required
                          value='{ memory.state.request.state }'
                          onchange='{ memory.state.request.edit }'
                          onkeyup='{ memory.state.request.edit }'
                        )
                        .input-group-addon
                          .input-units Mi

                      .input-group
                        .input-group-addon.resource-type Limit
                        input.form-control(
                          ref='memoryLimit'
                          type='number'
                          placeholder='{ memory.state.limit.initialValue }'
                          min='250'
                          required
                          value='{ memory.state.limit.state }'
                          onchange='{ memory.state.limit.edit }'
                          onkeyup='{ memory.state.limit.edit }'
                        )
                        .input-group-addon
                          .input-units Mi

      .col-lg-3
        help-general(config='{ opts.config }')

  script.

    // Pass a refresh callback for this tag to the options constructor argument
    // used for all Dynamic objects built in this tag:
    const add_refresh = object => Object.assign(
      {},
      object,
      { refresh: () => this.update() }
    )
    const Dynamic = options => this.opts.opts.Dynamic(add_refresh(options))
    const Dynamics = options => this.opts.opts.Dynamics(add_refresh(options))
    const DynamicSet = (items, options) => (
      this.opts.opts.DynamicSet(items, add_refresh(options))
    )

    const dedent = require('dedent-js')

    // Dedent twice because pug adds tabs:
    this.yamlTemplate = dedent`
      kind: "postgresql"
      apiVersion: "acid.zalan.do/v1"

      metadata:
        name: "{{ name }}"
        namespace: "{{ namespace.state }}"
        labels:
          team: {{ team }}

      spec:
        teamId: "{{ team }}"
        postgresql:
          version: "{{ postgresqlVersion }}"
        numberOfInstances: {{ instanceCount }}
        {{#if enableMasterLoadBalancer}}
        enableMasterLoadBalancer: true
        {{/if}}
        {{#if enableReplicaLoadBalancer}}
        enableReplicaLoadBalancer: true
        {{/if}}
        {{#if enableConnectionPooler}}
        enableConnectionPooler: true
        {{/if}}
        {{#if enableReplicaConnectionPooler}}
        enableReplicaConnectionPooler: true
        {{/if}}
        {{#if enableMasterPoolerLoadBalancer}}
        enableMasterPoolerLoadBalancer: true
        {{/if}}
        {{#if enableReplicaPoolerLoadBalancer}}
        enableReplicaPoolerLoadBalancer: true
        {{/if}}
        {{#if maintenanceWindows}}
        maintenanceWindows:
        {{#each maintenanceWindows}}
          - "{{ this }}"
        {{/each}}
        {{/if}}
        volume:
          size: "{{ volumeSize }}Gi"{{#if volumeStorageClass}}
          storageClass: "{{ volumeStorageClass }}"{{/if}}{{#if iops}}
          iops: {{ iops }}{{/if}}{{#if throughput}}
          throughput: {{ throughput }}{{/if}}
        {{#if users}}
        users:{{#each users}}
          {{ state }}: []{{/each}}{{/if}}
        {{#if databases}}
        databases:{{#each databases}}
          {{ state }}: {{ owner.state }}{{/each}}{{/if}}
        allowedSourceRanges:
          # IP ranges to access your cluster go here
        {{#each ranges}}  # {{ @key }}
        {{#each this }}
          - {{ this }}
        {{/each}}
        {{/each}}{{#if nats}}
          # NAT instances
        {{#each nats}}
          - {{ state }}/32
      {{/each}}{{/if}}{{#if odd}}
          # Your odd host IP
          - {{ odd }}/32
        {{/if}}

        {{#if resourcesVisible}}
        resources:
          requests:
            cpu: {{ cpu.state.request.state }}m
            memory: {{ memory.state.request.state }}Mi
          limits:
            cpu: {{ cpu.state.limit.state }}m
            memory: {{ memory.state.limit.state }}Mi{{/if}}{{#if restoring}}

        clone:
          cluster: "{{ backup.state.name.state }}"
          uid: "{{ backup.state.uid.state }}"{{#if pitr}}
          timestamp: "{{ backup.state.timestamp.state }}"{{/if}}{{/if}}
    `

    const yamlParser = require('js-yaml')
    this.teams = this.opts.teams
    this.config = this.opts.config

    this.getContext = () => {
      return {
        name: this.name.toLowerCase(),
        team: this.team.toLowerCase(),
        postgresqlVersion: this.postgresqlVersion,
        instanceCount: this.instanceCount,
        enableMasterLoadBalancer: this.enableMasterLoadBalancer,
        enableReplicaLoadBalancer: this.enableReplicaLoadBalancer,
        enableConnectionPooler: this.enableConnectionPooler,
        enableReplicaConnectionPooler: this.enableReplicaConnectionPooler,
        enableMasterPoolerLoadBalancer: this.enableMasterPoolerLoadBalancer,
        enableReplicaPoolerLoadBalancer: this.enableReplicaPoolerLoadBalancer,
        maintenanceWindows: this.maintenanceWindows,
        volumeSize: this.volumeSize,
        volumeStorageClass: this.volumeStorageClass,
        iops: this.iops,
        throughput: this.throughput,
        users: this.users.valids,
        databases: this.databases.valids,
        ranges: this.ranges,
        nats: this.nats.valids,
        odd: this.odd,
        cpu: this.cpu,
        memory: this.memory,
        backup: this.backup,
        namespace: this.namespace,
        resourcesVisible: this.config.resources_visible,
        restoring: this.backup.state.type.state !== 'empty',
        pitr: this.backup.state.type.state === 'pitr',
      }
    }

    this.getYAML = () => {
      yaml = Handlebars.compile(this.yamlTemplate)
      yaml = yaml(this.getContext())
      return yaml
    }

    this.on('update', () => {
      this.teams = this.opts.teams
      this.config = this.opts.config

      yaml = Handlebars.compile(this.yamlTemplate)
      yaml = yaml(this.getContext())

      this.refs.yamlNice.innerHTML = yaml

      Prism.highlightAll()
    })

    this.oddChanges = e => {
      this.odd = e.target.value
    }

    this.ranges = {}

    this.toggleIPRange = e => {
      if (e.target.checked) {
        this.ranges[e.target.value] = this.config.static_network_whitelist[e.target.value]
      } else {
        delete this.ranges[e.target.value]
      }
      this.update()
    }

    this.toggleEnableMasterLoadBalancer = e => {
      this.enableMasterLoadBalancer = !this.enableMasterLoadBalancer
    }

    this.toggleEnableReplicaLoadBalancer = e => {
      this.enableReplicaLoadBalancer = !this.enableReplicaLoadBalancer
    }

    this.toggleEnableConnectionPooler = e => {
      this.enableConnectionPooler = !this.enableConnectionPooler
    }

    this.toggleEnableReplicaConnectionPooler = e => {
      this.enableReplicaConnectionPooler = !this.enableReplicaConnectionPooler
    }

    this.toggleEnableMasterPoolerLoadBalancer = e => {
      this.enableMasterPoolerLoadBalancer = !this.enableMasterPoolerLoadBalancer
    }

    this.toggleEnableReplicaPoolerLoadBalancer = e => {
      this.enableReplicaPoolerLoadBalancer = !this.enableReplicaPoolerLoadBalancer
    }

    this.maintenanceWindows = e => {
      this.maintenanceWindows = e.target.value
    }

    this.volumeChange = e => {
      this.volumeSize = +e.target.value
    }

    this.storageClassChange = e => {
      this.volumeStorageClass = e.target.value
    }

    this.iopsChange = e => {
      this.iops = +e.target.value
    }

    this.throughputChange = e => {
      this.throughput = +e.target.value
    }

    this.updateDNSName = () => {
      this.dnsName = this.config.dns_format_string.format(
        this.name,
        this.namespace.state,
      )
    }

    this.updateClusterName = () => {
      this.clusterName = (this.name).toLowerCase()
      this.checkClusterExists()
      this.updateDNSName()
    }

    this.nameChange = e => {
      this.name = e.target.value.toLowerCase()
      this.updateClusterName()
    }

    this.teamChange = e => {
      this.team = e.target.value.toLowerCase()
      this.updateClusterName()
    }

    this.instanceCountChange = e => {
      this.instanceCount = +e.target.value
      if (this.instanceCount < 2) {	
        this.enableReplicaLoadBalancer = false
        this.enableReplicaConnectionPooler = false
        this.enableReplicaPoolerLoadBalancer = false
      }
    }

    this.checkClusterExists = () => (
      jQuery
      .get(
        './postgresqls/'
        + this.namespace.state
        + '/'
        + this.clusterName
      )
      .done(() => this.clusterExists = true)
      .fail(() => this.clusterExists = false)
      .always(() => this.update())
    )

    this.versionChange = e => {
      this.postgresqlVersion = e.target.value
    }

    this.requestCreate = e => {
      jsonPayload = JSON.stringify(yamlParser.load(this.getYAML()))

      this.creating = true
      this.update()

      jQuery.ajax({
        type: 'POST',
        url: './create-cluster',
        contentType:'application/json',
        data: jsonPayload,
        processData: false,
        success: () => {
          route(
            '/status/'
            + encodeURI(this.namespace.state)
            + '/'
            + encodeURI(this.clusterName)
          )
        },
        error: (r, status, error) => {
          console.log('Create request failed')
        },
        dataType: 'json'
      })
    }

    const clipboard = new Clipboard('.btn-copy', {
      text: () => { return this.getYAML() }
    })

    this.namespace = Dynamic({
      init: () => (
        !this.config.target_namespace
          || ['', '*'].includes(this.config.target_namespace)
        ? 'default'
        : this.config.target_namespace
      ),
    })
    ;{
      const update_namespace = this.namespace.update
      this.namespace.update = value => {
        update_namespace(value)
        this.updateClusterName()
        this.update()
      }
    }

    this.nats = Dynamics()

    this.users = Dynamics()
    this.users.itemInit = () => Dynamic({
      update: (value, instance, event) => {
        if (value === '' || instance.state.length > 0) {
          this.databases.state.forEach(database => {
            if (database.owner.state === instance.state) {
              database.owner.update(value, database.owner, event)
            }
          })
        }
        instance.state = value
        this.update()
        // Note: there is a bug here somewhere.
        //
        // To reproduce it:
        // 1. Add user user1
        // 2. Add user user2
        // 3. Add user user22
        // 4. Add database 1
        // 5. Add database 2
        // 6. Set database 1 owner to user2
        // 7. Set database 2 owner to user22
        // 8. Type another 2 at the end of the username for user2, so that it becomes a repeated copy of user22.  It should automatically set the database 1 owner to user22.
        // 9. Delete the character you just typed, so that the first user22, which used to be user2 before the last step, becomes user2 again.
        //
        // In principle, that last step should set the owners for both database 1 and database 2 to user2, as both databases had owner user22 which was edited.  Instead, the owner for database 1 becomes user1 (WTF???) and the owner for database 2 becomes user2 (which is correct).  Note that the tag states are updated correctly, and the HTML shown in the Chrome element inspector has the selected attribute in the correct option elements — yet the wrong owner option is selected in the rendered select control on the page for the owner of database 1.
        //
        // Solution attempted that failed because riotjs does weird things with the DOM:
        //
        // $('select.owner', this.root).each(select => {
        //   var options = $(select).children()
        //   for (var i = 0; i < options.length; i++) {
        //     const option = options[i]
        //     const selected = $(option).attr('selected')
        //     if (selected === 'true' || selected === 'selected') {
        //       select.selectedIndex = i
        //       child.selected = true
        //     } else {
        //       child.selected = false
        //     }
        //   }
        // })
        //
        // Hours wasted on this stupid bug: 3
        // Please keep this counter updated.
      }
    })

    {
      const baseRemoveUser = this.users.remove
      this.users.remove = event => {
        this.databases.state.forEach(database => {
          if (database.owner.state === event.item.state) {
            database.owner.update('', database.owner, event)
          }
        })
        baseRemoveUser(event)
        this.update()
      }
    }

    this.databases = Dynamics({
      itemInit: () => {
        const item = Dynamic()
        const baseItemUpdate = item.update
        item.update = (value, instance, event) => {
          this.check()
          baseItemUpdate(value, instance, event)
        }
        item.owner = Dynamic({
          validState: username => (
            this.users.valids.find(user => user.state === username) !== undefined
          )
        })
        return item
      },
      itemValid: item => item.valid() && item.owner.valid(),
    })

    const DynamicResource = options => {
      const instance = DynamicSet({
        request: () => options.request,
        limit: () => options.limit,
      })
      instance.state.request.initialValue = options.request
      instance.state.limit.initialValue = options.limit
      return instance
    }

    this.cpu = DynamicResource({ request: 100, limit: 500 })
    this.memory = DynamicResource({ request: 100, limit: 500 })

    this.backup = DynamicSet({
      type: () => 'empty',
      name: () => '',
      uid: () => '',
      timestamp: () => '',
    })

    const pitr_timestamp_format = 'YYYY-MM-DDTHH:mm:ss.SSSZ'
    this.backup.state.timestamp.validState = (
      base => (
        state => {
          if (!base(state)) {
            return false
          }
          const parsed = moment.utc(state, pitr_timestamp_format, true)
          return moment.isMoment(parsed) && !isNaN(parsed.utcOffset())
        }
      )
    )(this.backup.state.timestamp.validState)

    this.allValid = () => (
      !this.creating &&
      this.refs.name && this.refs.name.validity.valid &&
      this.refs.instanceCount && this.refs.instanceCount.validity.valid && Number.isInteger(+this.instanceCount) &&
      this.refs.volumeSize && this.refs.volumeSize.validity.valid && Number.isInteger(+this.volumeSize) &&
      this.refs.cpuRequest && this.refs.cpuRequest.validity.valid && Number.isInteger(+this.cpu.state.request.state) &&
      this.refs.cpuLimit && this.refs.cpuLimit.validity.valid && Number.isInteger(+this.cpu.state.limit.state) &&
      this.refs.memoryRequest && this.refs.memoryRequest.validity.valid && Number.isInteger(+this.memory.state.request.state) &&
      this.refs.memoryLimit && this.refs.memoryLimit.validity.valid && Number.isInteger(+this.memory.state.limit.state) &&
      [
        this.users,
        this.databases,
        this.backup,
      ].every(x => x.valid())
    )

    this.check = () => {
      const setValidity = validity => (_, element) => (
        element.setCustomValidity(
          validity(element)
        )
      )

      $('select.owner', this.root).each(setValidity(owner =>
        owner.value ? '' : 'Must select an owner for this database'
      ))

      const counts = (values, key=x => x) => values.map(key).reduce(
        (o, item) => {
          if (item in o) { o[item] += 1 }
          else { o[item] = 1 }
          return o
        },
        {}
      )

      $('input.timestamp', this.root).each(setValidity(timestamp =>
        this.backup.state.timestamp.valid() ? '' :
        'Timestamp must be formatted as ISO-8601 with milliseconds; note that a UTC offset as ±HH:mm is required: YYYY-MM-DDTHH:mm:ss.SSSZ e.g. 2018-12-31T23:59:59.999+14:00'
      ))

      const userCounts = counts(this.users.state, item => item.state)
      $('input.username', this.root).each(setValidity(name =>
        !name.value || userCounts[name.value] === 1 ? '' :
        'Usernames must not repeat'
      ))

      const databaseCounts = counts(this.databases.state, item => item.state)
      $('input.databasename', this.root).each(setValidity(name =>
        !name.value || databaseCounts[name.value] === 1 ? '' :
        'Database names must not repeat'
      ))

      if (this.refs.submitbutton) {
        this.refs.submitbutton.disabled = (
          $(document.querySelectorAll(':invalid')).length !== 0
        ) || $('select.owner:disabled').length !== 0
      }
    }

    this.on('updated', this.check)

    this.reset_form = () => {
      if (this.pollProgressTimer) {
        clearInterval(this.pollProgressTimer)
      }

      this.creating = false
      this.name = ''

      if (this.teams && this.teams.length > 0) {
        this.team = this.teams[0]
      } else {
        this.team = ''
      }

      this.clusterName = (this.name + '-').toLowerCase()
      this.volumeSize = 10
      this.volumeStorageClass = ''
      this.instanceCount = 1
      this.ranges = {}
      this.odd = ''
      this.enableMasterLoadBalancer = false
      this.enableReplicaLoadBalancer = false
      this.enableConnectionPooler = false
      this.enableReplicaConnectionPooler = false
      this.enableMasterPoolerLoadBalancer = false
      this.enableReplicaPoolerLoadBalancer = false
      this.maintenanceWindows = {}

      this.postgresqlVersion = this.postgresqlVersion = (
        this.config.postgresql_versions[0]
      )

      this.updateDNSName();

      this.check()

      this.backup.state.type.update(
        this.opts.backup_name === undefined
        ? 'empty'
        : this.opts.backup_timestamp === undefined
          ? 'restore'
          : 'pitr'
      )

      this.backup.state.name.update(
        decodeURI(this.opts.backup_name || '')
      )

      this.backup.state.uid.update(
        !this.opts.backup_uid || this.opts.backup_uid === 'base'
        ? ''
        : this.opts.backup_uid
      )

      this.backup.state.timestamp.update(
        !this.opts.backup_timestamp
        ? ''
        : moment.utc(
            decodeURI(this.opts.backup_timestamp)
          ).format(pitr_timestamp_format)
      )

      this.opts && this.opts.backup_name && delete this.opts.backup_name
      this.opts && this.opts.backup_uid && delete this.opts.backup_uid
      this.opts && this.opts.backup_timestamp && delete this.opts.backup_timestamp

      this.update()
    }

    this.on('mount', this.reset_form)


================================================
File: ui/app/src/postgresql.tag.pug
================================================
postgresql
  .container-fluid

    h1.page-header
      nav(aria-label='breadcrumb')
        ol.breadcrumb

          li.breadcrumb-item
            a(href='./#/list')
              | PostgreSQL clusters

          li.breadcrumb-item(if='{ cluster_path }')
            a(href='./#/status/{ cluster_path }')
              | { qname }

    .row(if='{ cluster_path }')

      .col-lg-3
        h2 Cluster YAML definition

        div(if='{ progress.postgresql }')
          <pre><code ref="yamlNice" class="language-yaml"></code></pre>

        pre(if='{ !progress.postgresql }')
          code # Loading

        virtual(if='{ uid }')
          h3 Cluster UID
          code { uid }

      .col-lg-6

        div

          .btn-toolbar.pull-right
            .btn-group(
              aria-label='Actions'
              role='group'
            )

              a.btn.btn-info(
                href='./#/logs/{ cluster_path }'
              )
                | Logs

              a.btn(
                class='btn-{ opts.read_write ? "primary" : "info" }'
                if='{ progress.postgresql }'
                href='./#/clone/{ clustername }/{ uid }/{ encodeURI(new Date().toISOString()) }'
              )
                | Clone

              a.btn(
                class='btn-{ opts.read_write ? "warning" : "info" }'
                href='./#/edit/{ cluster_path }'
              )
                | Edit

              button.btn.btn-danger(
                if='{ opts.read_write }'
                onclick='{ delete_cluster }'
              )
                | Delete

          h2 Checking status of cluster

        .progress
          .progress-bar.progress-bar-success(style='width: 20%' if='{ progress.requestStatus === "OK" || progress.masterLabel }')
          .progress-bar.progress-bar-success(style='width: 20%' if='{ progress.postgresql }')
          .progress-bar.progress-bar-success(style='width: 20%' if='{ progress.statefulSet }')
          .progress-bar.progress-bar-success(style='width: 20%' if='{ progress.containerFirst }')
          .progress-bar.progress-bar-success(style='width: 20%' if='{ progress.masterLabel }')
          .progress-bar.progress-bar-info.progress-bar-striped.active(if='{ !progress.masterLabel }' style='width: 10%')

        .alert.alert-info(if='{ !progress.requestStatus }') PostgreSQL cluster requested
        .alert.alert-danger(if='{ progress.requestStatus !== "OK" }') Create request failed
        .alert.alert-success(if='{ progress.requestStatus === "OK" }') Manifest creation successful ({ new Date(progress.createdTimestamp).toLocaleString() })

        .alert.alert-info(if='{ !progress.postgresql }') PostgreSQL cluster manifest pending
        .alert.alert-success(if='{ progress.postgresql }') PostgreSQL cluster manifest created

        .alert.alert-danger(if='{progress.status && progress.status.PostgresClusterStatus == "CreateFailed"}') Cluster creation failed: Check events and cluster name!

        .alert.alert-info(if='{ !progress.statefulSet }') StatefulSet pending
        .alert.alert-success(if='{ progress.statefulSet }') StatefulSet created

        .alert.alert-info(if='{ progress.statefulSet && !progress.containerFirst }') Waiting for 1st container to spawn
        .alert.alert-success(if='{ progress.containerFirst }') First PostgreSQL cluster container spawned

        .alert.alert-info(if='{ progress.containerFirst && !progress.masterLabel }') Waiting for master to become available
        .alert.alert-success(if='{ progress.masterLabel }') PostgreSQL master available, label is attached
        .alert.alert-success(if='{ progress.masterLabel && progress.dnsName }') PostgreSQL ready: <strong>{ progress.dnsName }</strong>

        .alert.alert-success(if='{ progress.pooler && this.progress.postgresqlManifest.spec.enableConnectionPooler }') Pooler ready: <strong>{ progress.poolerDnsName }</strong>

      .col-lg-3
        help-general(config='{ opts.config }')

  script.

    var yamlParser = require('js-yaml')

    this.delete_cluster = _ => this.parent.opts.delete_cluster(
      this.namespace,
      this.clustername,
    )

    this.progress = {}
    this.progress.requestStatus = 'OK'
    this.progress.pooler = false

    this.pollProgressTimer = false

    this.startPollProgress = () => {
      this.pollProgressTimer = setInterval(this.pollProgress, 10000)
    }

    this.stopPollProgress = () => {
      clearInterval(this.pollProgressTimer)
      this.pollProgressTimer = false
    }

    this.pollProgress = () => {
      jQuery.get(
        './postgresqls/' + this.cluster_path,
      ).done(data => {
        this.progress.postgresql = true
        this.progress.postgresqlManifest = data
        // copy status as we delete later for edit
        this.progress.status = data.status
        this.progress.createdTimestamp = data.metadata.creationTimestamp
        this.progress.poolerEnabled = data.spec.enableConnectionPooler
        this.uid = this.progress.postgresqlManifest.metadata.uid
        this.update()

        jQuery.get(
          './statefulsets/' + this.cluster_path,
        ).done(data => {
          this.progress.statefulSet = true
          this.update()

          jQuery.get(
            './statefulsets/' + this.cluster_path + '/pods',
          ).done(data => {
            if (data.length > 0) {
                this.progress.containerFirst = true
            }

            masters = data.filter((x) => { return x.labels['spilo-role'] === 'master'} )
            if (masters.length === 1) {
                this.progress.masterLabel = true
            }

            this.update()

            jQuery.get(
              './services/' + this.cluster_path,
            ).done(data => {
              if (data.metadata && data.metadata.annotations && 'zalando.org/dnsname' in data.metadata.annotations) {
                this.progress.dnsName = data.metadata.annotations['zalando.org/dnsname']
              } else if (data.metadata && data.metadata.annotations && 'external-dns.alpha.kubernetes.io/hostname' in data.metadata.annotations) {
                this.progress.dnsName = data.metadata.annotations['external-dns.alpha.kubernetes.io/hostname']
              } else {
                // Kubernetes Service name should resolve
                this.progress.dnsName = data.metadata.name + '.' + data.metadata.namespace
              }

              if (this.progress.poolerEnabled == true) {
                jQuery.get(
                  './pooler/' + this.cluster_path,
                ).done(data => {
                  this.progress.pooler = {"url": ""}
                  jQuery.get(
                    './services/' + this.cluster_path + "-pooler",
                  ).done(data => {
                    if (data.metadata && data.metadata.annotations && 'zalando.org/dnsname' in data.metadata.annotations) {
                      this.progress.poolerDnsName = data.metadata.annotations['zalando.org/dnsname']
                    } else if (data.metadata && data.metadata.annotations && 'external-dns.alpha.kubernetes.io/hostname' in data.metadata.annotations) {
                      this.progress.poolerDnsName = data.metadata.annotations['external-dns.alpha.kubernetes.io/hostname']
                    } else {
                      this.progress.poolerDnsName = data.metadata.name +  '.' + data.metadata.namespace
                    }
                    this.update()
                  })
                  this.update()
                })
              }

              this.update()
            })
          })
        })
      })
    }

    this.on('mount', () => {
      this.uid = undefined
      const namespace = this.namespace = this.opts.namespace
      const clustername = this.clustername = this.opts.clustername
      const qname = this.qname = namespace + '/' + clustername
      const cluster_path = this.cluster_path = (
        encodeURI(namespace)
        + '/' + encodeURI(clustername)
      )
      this.stopPollProgress()
      this.pollProgress()
      this.startPollProgress()
    })

    this.on('unmount', () =>
      this.stopPollProgress()
    )

    this.on('update', () => {
      if (this.progress.postgresqlManifest) {
        const manifest = this.progress.postgresqlManifest

        const last_applied = 'kubectl.kubernetes.io/last-applied-configuration'
        if (manifest.metadata.annotations) {
          delete manifest.metadata.annotations[last_applied]
        }

        delete manifest.metadata.managedFields
        delete manifest.metadata.creationTimestamp
        delete manifest.metadata.deletionGracePeriodSeconds
        delete manifest.metadata.deletionTimestamp
        delete manifest.metadata.generation
        delete manifest.metadata.resourceVersion
        delete manifest.metadata.selfLink
        delete manifest.metadata.uid
        delete manifest.status

        if (this.refs.yamlNice) {
          this.refs.yamlNice.innerHTML = yamlParser.dump(
            this.progress.postgresqlManifest,
            {
              sortKeys: true,
            },
          )
        }

      } else {
        if(this.refs.yamlNice) {
          this.refs.yamlNice.innerHTML = '# Loading postgresql cluster manifest'
        }
      }

      Prism.highlightAll()
    })


================================================
File: ui/app/src/postgresqls.tag.pug
================================================
postgresqls
  .container-fluid

    h1.page-header
      nav(aria-label='breadcrumb')
        ol.breadcrumb

          li.breadcrumb-item
            a(href='./#/list')
              | PostgreSQL clusters

    .sk-spinner-pulse(
      if='{ my_clusters !== null && other_clusters !== null && (my_clusters === undefined || other_clusters === undefined) }'
    )

    p(if='{ my_clusters === null || other_clusters === null }')
      | Error loading clusters.  Please
      |
      a(onclick='window.location.reload(true)') try again
      |
      | or
      |
      a(href='./') start over
      | .

    div(
      if='{ my_clusters && other_clusters }'
    )

      p
        | Search:
        |
        input(
          type='text'
          onchange='{ filter.edit }'
          onkeyup='{ filter.edit }'
          value='{ filter.state }'
        )

      .page-header
        h1 My team's clusters ({ my_clusters.length })

      table.table.table-hover(if='{ my_clusters.length > 0 }')

        thead
          tr
            th(style='width: 120px') Team
            th(style='width: 130px') Namespace
            th Name
            th(style='width: 50px') Pods
            th(style='width: 140px') CPU
            th(style='width: 130px') Memory
            th(style='width: 100px') Size
            th(style='width: 100px') IOPS
            th(style='width: 100px') Throughput
            th(style='width: 120px')
              .tooltip(style='width: 120px')
                | Cost/Month
                .tooltiptext
                  strong Cost = MAX(CPU, Memory) + rest
                  br
                  | 1 CPU core : 42.09$
                  br
                  | 1GB memory: 10.5225$
                  br
                  | 1GB volume:  0.0952$
                  br
                  | IOPS (-3000 baseline): 0.006$
                  br
                  | Throughput (-125 baseline): 0.0476$
                  br
                  | 1 ELB: 21.96$
            th(stlye='width: 120px')

        tbody
          tr(
            each='{ my_clusters }'
            hidden='{ !namespaced_name.toLowerCase().includes(filter.state.toLowerCase()) }'
          )
            td { team }
            td(style='white-space: pre')
              | { namespace }
            td
              a(href='./#/status/{ cluster_path(this) }') { name }
              btn.btn-danger(if='{status.PostgresClusterStatus == "CreateFailed"}') Create Failed
            td { nodes }
            td { cpu } / { cpu_limit }
            td { memory } / { memory_limit }
            td { volume_size }
            td { iops }
            td { throughput }
            td { calcCosts(nodes, cpu, memory, volume_size, iops, throughput, num_elb) }$

            td


              .btn-group.pull-right(
                aria-label='Cluster { qname } actions'
                role='group'
                style='display: flex'
              )

                a.btn.btn-info(
                  href='./#/status/{ cluster_path(this) }'
                )
                  i.fa.fa-check-circle.regular
                  | Status

                a.btn.btn-info(
                  if='{ opts.config.pgview_link }'
                  href='{ opts.config.pgview_link }{ cluster_path(this) }'
                )
                  i.fa.fa-chart-line
                  | Pgview

                a.btn.btn-info(
                  href='./#/logs/{ cluster_path(this) }'
                )
                  i.fa.fa-align-justify
                  | Logs

                a.btn(
                  class='btn-{ opts.read_write ? "primary" : "info" }'
                  href='./#/clone/{ encodeURI(name) }/{ encodeURI(uid) }/{ encodeURI(new Date().toISOString()) }'
                )
                  i.fa.fa-clone.regular
                  | Clone

                a.btn(
                  class='btn-{ opts.read_write ? "warning" : "info" }'
                  href='./#/edit/{ cluster_path(this) }'
                )
                  | Edit

                button.btn.btn-danger(
                  if='{ opts.read_write }'
                  onclick='{ delete_cluster }'
                )
                  | Delete

      .page-header
        h1 Other clusters ({ other_clusters.length})

      table.table.table-hover(if='{ other_clusters.length > 0 }')

        thead
          tr
            th(style='width: 120px') Team
            th(style='width: 130px') Namespace
            th Name
            th(style='width: 50px') Pods
            th(style='width: 140px') CPU
            th(style='width: 130px') Memory
            th(style='width: 100px') Size
            th(style='width: 100px') IOPS
            th(style='width: 100px') Throughput
            th(style='width: 120px')
              .tooltip(style='width: 120px')
                | Cost/Month
                .tooltiptext
                  strong Cost = MAX(CPU, Memory) + rest
                  br
                  | 1 CPU core : 42.09$
                  br
                  | 1GB memory: 10.5225$
                  br
                  | 1GB volume:  0.0952$
                  br
                  | IOPS (-3000 baseline): 0.006$
                  br
                  | Throughput (-125 baseline): 0.0476$
                  br
                  | 1 ELB: 21.96$
            th(style='width: 120px')

        tbody
          tr(
            each='{ other_clusters }'
            hidden='{ !namespaced_name.toLowerCase().includes(filter.state.toLowerCase()) }'
          )
            td { team }
            td(style='white-space: pre')
              | { namespace }
            td
              a(
                href='./#/status/{ cluster_path(this) }'
              )
                | { name }
            td { nodes }
            td { cpu } / { cpu_limit }
            td { memory } / { memory_limit }
            td { volume_size }
            td { iops }
            td { throughput }
            td { calcCosts(nodes, cpu, memory, volume_size, iops, throughput, num_elb) }$

            td

              .btn-group.pull-right(
                aria-label='Cluster { qname } actions'
                role='group'
                style='display: flex'
              )

                a.btn.btn-info(
                  href='./#/status/{ cluster_path(this) }'
                )
                  i.fa.fa-check-circle.regular
                  | Status

                a.btn.btn-info(
                  if='{ opts.config.pgview_link }'
                  href='{ opts.config.pgview_link }{ cluster_path(this) }'
                  target='_blank'
                )
                  i.fa.fa-chart-line
                  | Pgview

                a.btn.btn-info(
                  href='./#/logs/{ cluster_path(this) }'
                )
                  i.fa.fa-align-justify
                  | Logs

                a.btn(
                  class='btn-{ opts.read_write ? "primary" : "info" }'
                  href='./#/clone/{ encodeURI(name) }/{ encodeURI(uid) }/{ encodeURI(new Date().toISOString()) }'
                )
                  i.fa.fa-clone.regular
                  | Clone

                a.btn(
                  class='btn-{ opts.read_write ? "warning" : "info" }'
                  href='./#/edit/{ cluster_path(this) }'
                )
                  | Edit

                button.btn.btn-danger(
                  if='{ opts.read_write }'
                  onclick='{ delete_cluster }'
                )
                  | Delete

  script.

    // Pass a refresh callback for this tag to the options constructor argument
    // used for all Dynamic objects built in this tag:
    const add_refresh = object => Object.assign(
      {},
      object,
      { refresh: () => this.update() }
    )
    const Dynamic = options => this.parent.opts.Dynamic(add_refresh(options))

    this.filter = Dynamic()

    this.my_clusters = undefined
    this.other_clusters = undefined

    this.delete_cluster = event => this.parent.opts.delete_cluster(
      event.item.namespace,
      event.item.name,
    )

    const cluster_path = this.cluster_path = cluster => (
      encodeURI(cluster.namespace)
      + '/' + encodeURI(cluster.name)
    )

    const calcCosts = this.calcCosts = (nodes, cpu, memory, disk, iops, throughput, num_elb) => {
      podcount = Math.max(nodes, opts.config.min_pods)
      corecost = toCores(cpu) * opts.config.cost_core * 30.5 * 24
      memorycost = toMemory(memory) * opts.config.cost_memory * 30.5 * 24
      elbcost = num_elb * opts.config.cost_elb * 30.5 * 24
      diskcost = toDisk(disk) * opts.config.cost_ebs
      iopscost = 0
      if (iops !== undefined && iops > opts.config.free_iops) {
        if (iops > opts.config.limit_iops) {
          iops = opts.config.limit_iops
        }
        iopscost = (iops - opts.config.free_iops) * opts.config.cost_iops
      }
      throughputcost = 0
      if (throughput !== undefined && throughput > opts.config.free_throughput) {
        if (throughput > opts.config.limit_throughput) {
          throughput = opts.config.limit_throughput
        }
        throughputcost = (throughput - opts.config.free_throughput) * opts.config.cost_throughput
      }

      costs = podcount * (Math.max(corecost, memorycost) + diskcost + iopscost + throughputcost) + elbcost
       return costs.toFixed(2)
    }

    const toDisk = this.toDisk = value => {
      if(value.endsWith("Mi")) {
        value = value.substring(0, value.length-2)
        value = Number(value) / 1000.
        return value
      }
      else if(value.endsWith("Gi")) {
        value = value.substring(0, value.length-2)
        value = Number(value)
        return value
      }
      else if(value.endsWith("Ti")) {
        value = value.substring(0, value.length-2)
        value = Number(value) * 1000
        return value
      }
      
      return value
    }    

    const toMemory = this.toMemory = value => {
      if (value.endsWith("Mi")) {
        value = value.substring(0, value.length-2)
        value = Number(value) / 1000.
        return value
      }
      else if(value.endsWith("Gi")) {
        value = value.substring(0, value.length-2)
        value = Number(value)
        return value
      }
      else if(value.endsWith("Ti")) {
        value = value.substring(0, value.length-2)
        value = Number(value) * 1000
        return value
      }

      return value
    }

    const toCores = this.toCores = value => {
      if (value.endsWith("m")) {
        value = value.substring(0, value.length-1)
        value = Number(value) / 1000.
        return value
      }
      return value
    }

    this.on('mount', () =>
      jQuery
      .get('./postgresqls')
      .done(clusters => {
        this.my_clusters = []
        this.other_clusters = []
        clusters.forEach(cluster =>
          (
            this.opts.teams.includes(
              cluster.team.toLowerCase()
            )
            ? this.my_clusters
            : this.other_clusters
          ).push(cluster)
        )
      })
      .fail(() => {
        this.my_clusters = null
        this.other_clusters = null
      })
      .always(() => this.update())
    )


================================================
File: ui/app/src/prism.js
================================================
/* http://prismjs.com/download.html?themes=prism&languages=yaml */
var _self="undefined"!=typeof window?window:"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?self:{},Prism=function(){var e=/\blang(?:uage)?-(\w+)\b/i,t=0,n=_self.Prism={manual:_self.Prism&&_self.Prism.manual,util:{encode:function(e){return e instanceof a?new a(e.type,n.util.encode(e.content),e.alias):"Array"===n.util.type(e)?e.map(n.util.encode):e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/\u00a0/g," ")},type:function(e){return Object.prototype.toString.call(e).match(/\[object (\w+)\]/)[1]},objId:function(e){return e.__id||Object.defineProperty(e,"__id",{value:++t}),e.__id},clone:function(e){var t=n.util.type(e);switch(t){case"Object":var a={};for(var r in e)e.hasOwnProperty(r)&&(a[r]=n.util.clone(e[r]));return a;case"Array":return e.map&&e.map(function(e){return n.util.clone(e)})}return e}},languages:{extend:function(e,t){var a=n.util.clone(n.languages[e]);for(var r in t)a[r]=t[r];return a},insertBefore:function(e,t,a,r){r=r||n.languages;var l=r[e];if(2==arguments.length){a=arguments[1];for(var i in a)a.hasOwnProperty(i)&&(l[i]=a[i]);return l}var o={};for(var s in l)if(l.hasOwnProperty(s)){if(s==t)for(var i in a)a.hasOwnProperty(i)&&(o[i]=a[i]);o[s]=l[s]}return n.languages.DFS(n.languages,function(t,n){n===r[e]&&t!=e&&(this[t]=o)}),r[e]=o},DFS:function(e,t,a,r){r=r||{};for(var l in e)e.hasOwnProperty(l)&&(t.call(e,l,e[l],a||l),"Object"!==n.util.type(e[l])||r[n.util.objId(e[l])]?"Array"!==n.util.type(e[l])||r[n.util.objId(e[l])]||(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,l,r)):(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,null,r)))}},plugins:{},highlightAll:function(e,t){var a={callback:t,selector:'code[class*="language-"], [class*="language-"] code, code[class*="lang-"], [class*="lang-"] code'};n.hooks.run("before-highlightall",a);for(var r,l=a.elements||document.querySelectorAll(a.selector),i=0;r=l[i++];)n.highlightElement(r,e===!0,a.callback)},highlightElement:function(t,a,r){for(var l,i,o=t;o&&!e.test(o.className);)o=o.parentNode;o&&(l=(o.className.match(e)||[,""])[1].toLowerCase(),i=n.languages[l]),t.className=t.className.replace(e,"").replace(/\s+/g," ")+" language-"+l,o=t.parentNode,/pre/i.test(o.nodeName)&&(o.className=o.className.replace(e,"").replace(/\s+/g," ")+" language-"+l);var s=t.textContent,u={element:t,language:l,grammar:i,code:s};if(n.hooks.run("before-sanity-check",u),!u.code||!u.grammar)return u.code&&(u.element.textContent=u.code),n.hooks.run("complete",u),void 0;if(n.hooks.run("before-highlight",u),a&&_self.Worker){var g=new Worker(n.filename);g.onmessage=function(e){u.highlightedCode=e.data,n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(u.element),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},g.postMessage(JSON.stringify({language:u.language,code:u.code,immediateClose:!0}))}else u.highlightedCode=n.highlight(u.code,u.grammar,u.language),n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(t),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},highlight:function(e,t,r){var l=n.tokenize(e,t);return a.stringify(n.util.encode(l),r)},tokenize:function(e,t){var a=n.Token,r=[e],l=t.rest;if(l){for(var i in l)t[i]=l[i];delete t.rest}e:for(var i in t)if(t.hasOwnProperty(i)&&t[i]){var o=t[i];o="Array"===n.util.type(o)?o:[o];for(var s=0;s<o.length;++s){var u=o[s],g=u.inside,c=!!u.lookbehind,h=!!u.greedy,f=0,d=u.alias;if(h&&!u.pattern.global){var p=u.pattern.toString().match(/[imuy]*$/)[0];u.pattern=RegExp(u.pattern.source,p+"g")}u=u.pattern||u;for(var m=0,y=0;m<r.length;y+=r[m].length,++m){var v=r[m];if(r.length>e.length)break e;if(!(v instanceof a)){u.lastIndex=0;var b=u.exec(v),k=1;if(!b&&h&&m!=r.length-1){if(u.lastIndex=y,b=u.exec(e),!b)break;for(var w=b.index+(c?b[1].length:0),_=b.index+b[0].length,P=m,A=y,j=r.length;j>P&&_>A;++P)A+=r[P].length,w>=A&&(++m,y=A);if(r[m]instanceof a||r[P-1].greedy)continue;k=P-m,v=e.slice(y,A),b.index-=y}if(b){c&&(f=b[1].length);var w=b.index+f,b=b[0].slice(f),_=w+b.length,x=v.slice(0,w),O=v.slice(_),S=[m,k];x&&S.push(x);var N=new a(i,g?n.tokenize(b,g):b,d,b,h);S.push(N),O&&S.push(O),Array.prototype.splice.apply(r,S)}}}}}return r},hooks:{all:{},add:function(e,t){var a=n.hooks.all;a[e]=a[e]||[],a[e].push(t)},run:function(e,t){var a=n.hooks.all[e];if(a&&a.length)for(var r,l=0;r=a[l++];)r(t)}}},a=n.Token=function(e,t,n,a,r){this.type=e,this.content=t,this.alias=n,this.length=0|(a||"").length,this.greedy=!!r};if(a.stringify=function(e,t,r){if("string"==typeof e)return e;if("Array"===n.util.type(e))return e.map(function(n){return a.stringify(n,t,e)}).join("");var l={type:e.type,content:a.stringify(e.content,t,r),tag:"span",classes:["token",e.type],attributes:{},language:t,parent:r};if("comment"==l.type&&(l.attributes.spellcheck="true"),e.alias){var i="Array"===n.util.type(e.alias)?e.alias:[e.alias];Array.prototype.push.apply(l.classes,i)}n.hooks.run("wrap",l);var o=Object.keys(l.attributes).map(function(e){return e+'="'+(l.attributes[e]||"").replace(/"/g,"&quot;")+'"'}).join(" ");return"<"+l.tag+' class="'+l.classes.join(" ")+'"'+(o?" "+o:"")+">"+l.content+"</"+l.tag+">"},!_self.document)return _self.addEventListener?(_self.addEventListener("message",function(e){var t=JSON.parse(e.data),a=t.language,r=t.code,l=t.immediateClose;_self.postMessage(n.highlight(r,n.languages[a],a)),l&&_self.close()},!1),_self.Prism):_self.Prism;var r=document.currentScript||[].slice.call(document.getElementsByTagName("script")).pop();return r&&(n.filename=r.src,!document.addEventListener||n.manual||r.hasAttribute("data-manual")||("loading"!==document.readyState?window.requestAnimationFrame?window.requestAnimationFrame(n.highlightAll):window.setTimeout(n.highlightAll,16):document.addEventListener("DOMContentLoaded",n.highlightAll))),_self.Prism}();"undefined"!=typeof module&&module.exports&&(module.exports=Prism),"undefined"!=typeof global&&(global.Prism=Prism);
Prism.languages.yaml={scalar:{pattern:/([\-:]\s*(![^\s]+)?[ \t]*[|>])[ \t]*(?:((?:\r?\n|\r)[ \t]+)[^\r\n]+(?:\3[^\r\n]+)*)/,lookbehind:!0,alias:"string"},comment:/#.*/,key:{pattern:/(\s*(?:^|[:\-,[{\r\n?])[ \t]*(![^\s]+)?[ \t]*)[^\r\n{[\]},#\s]+?(?=\s*:\s)/,lookbehind:!0,alias:"atrule"},directive:{pattern:/(^[ \t]*)%.+/m,lookbehind:!0,alias:"important"},datetime:{pattern:/([:\-,[{]\s*(![^\s]+)?[ \t]*)(\d{4}-\d\d?-\d\d?([tT]|[ \t]+)\d\d?:\d{2}:\d{2}(\.\d*)?[ \t]*(Z|[-+]\d\d?(:\d{2})?)?|\d{4}-\d{2}-\d{2}|\d\d?:\d{2}(:\d{2}(\.\d*)?)?)(?=[ \t]*($|,|]|}))/m,lookbehind:!0,alias:"number"},"boolean":{pattern:/([:\-,[{]\s*(![^\s]+)?[ \t]*)(true|false)[ \t]*(?=$|,|]|})/im,lookbehind:!0,alias:"important"},"null":{pattern:/([:\-,[{]\s*(![^\s]+)?[ \t]*)(null|~)[ \t]*(?=$|,|]|})/im,lookbehind:!0,alias:"important"},string:{pattern:/([:\-,[{]\s*(![^\s]+)?[ \t]*)("(?:[^"\\]|\\.)*"|'(?:[^'\\]|\\.)*')(?=[ \t]*($|,|]|}))/m,lookbehind:!0,greedy:!0},number:{pattern:/([:\-,[{]\s*(![^\s]+)?[ \t]*)[+\-]?(0x[\da-f]+|0o[0-7]+|(\d+\.?\d*|\.?\d+)(e[\+\-]?\d+)?|\.inf|\.nan)[ \t]*(?=$|,|]|})/im,lookbehind:!0},tag:/![^\s]+/,important:/[&*][\w]+/,punctuation:/---|[:[\]{}\-,|>?]|\.\.\./};


================================================
File: ui/app/src/restore.tag.pug
================================================
restore
  .container-fluid

    .sk-spinner-pulse(if='{ stored_clusters === undefined }')

    p(if='{ stored_clusters === null }')
      | Error loading stored clusters.  Please
      |
      a(onclick="window.location.reload(true);") try again
      |
      | or
      |
      a(href="./") start over
      | .

    p(if='{ stored_clusters && stored_clusters.length === 0 }')
      | No stored clusters found.

    div(if='{ stored_clusters && stored_clusters.length > 0 }')

      h1.page-header
        nav(aria-label='breadcrumb')
          ol.breadcrumb

            li.breadcrumb-item
              a(href='./#/backups')
                | PostgreSQL cluster backups ({ stored_clusters.length })

      p
        | Search:
        |
        input(
          each='{ [filter] }'
          type='text'
          onchange='{ edit }'
          onkeyup='{ edit }'
          value='{ state }'
        )

      .stored-clusters.panel-group.collapsible
        .stored-clusters.panel.panel-default.collapsible(
          each='{ stored_clusters }'
          hidden='{ !name.includes(filter.state) }'
        )

          .stored-clusters.panel-heading.collapsible(
            id='{ id }-head'
            class='{ collapsed ? "collapsed" : "" }'
            data-toggle='collapse'
            data-target='#{ id }-collapse'
          )
            a.panel-title.collapsible
              | { name }

          .stored-clusters.panel-collapse.collapse.collapsible(
            id='{ id }-collapse'
            data-stored-clusters='{ name }'
          )
            .stored-clusters.panel-body.collapsible(id='{ id }-body')

              .sk-spinner-pulse(if='{ versions === undefined }')

              p(if='{ versions === null }')
                | Error loading backups.  Please try again or
                |
                a(href="./") start over
                | .

              p(if='{ versions && versions.length === 0 }')
                | No backups found.

              div(if='{ versions && versions.length > 0 }')

                .versions.panel-group.collapsible(style='margin-top: 0.3em')
                  .versions.panel.panel-default.collapsible(each='{ versions }')

                    .versions.panel-heading.collapsible(
                      id='{ id }-head'
                      class='{ collapsed ? "collapsed" : "" }'
                      data-toggle='collapse'
                      data-target='#{ id }-collapse'
                    )
                      a.versions.panel-title.collapsible
                        | { name }

                    .versions.panel-collapse.collapse.collapsible(
                      id='{ id }-collapse'
                      data-stored-clusters='{ parent.name }'
                      data-versions='{ name }'
                    )

                      .versions.panel-body.collapsible(id='{ id }-body')

                        .sk-spinner-pulse(if='{ basebackups === undefined }')

                        p(if='{ basebackups === null }')
                          | Error loading snapshots.  Please try again or
                          |
                          a(href="./") start over
                          | .

                        p(if='{ basebackups && basebackups.length === 0 }')
                          | No snapshots found.

                        div(if='{ basebackups && basebackups.length > 0 }')

                          div(
                            style='margin-bottom: 0.3em'
                          )

                            .pull-left(style='margin-right: 0.3em')
                              button.btn.btn-primary.pull-left(
                                id='{ id }-clone'
                              )
                                | Clone at latest state

                            div
                              .input-group
                                .input-group-btn
                                  button.btn.btn-info(id='{ id }-pitr')
                                    | Clone at
                                .input.form-control(type='text')
                                  | { clone_time && to_clone_time(clone_time) }

                          .timeline(id='{ id }-timeline')

                          .basebackups.panel-group.collapsible(style='margin-top: 0.3em')
                            .basebackups.panel.panel-default.collapsible(
                              each='{ basebackups }'
                            )

                              .basebackups.panel-heading.collapsible(
                                id='{ id }-head'
                                class='{ collapsed ? "collapsed" : "" }'
                                data-toggle='collapse'
                                data-target='#{ id }-collapse'
                              )
                                a.basebackups.panel-title.collapsible
                                  | { relative_time(last_modified) }
                                span.label.label-success(
                                  if='{ index === basebackups.length - 1 }'
                                  style='margin-left: 0.5em'
                                )
                                  | latest snapshot

                              .basebackups.panel-collapse.collapse.collapsible(
                                id='{ id }-collapse'
                                data-stored-clusters='{ parent.parent.name }'
                                data-versions='{ parent.name }'
                                data-backups='{ name }'
                              )

                                .basebackups.panel-body.collapsible(id='{ id }-body')

                                  table.basebackups.table

                                    tr
                                      th Name
                                      td { name }

                                    tr
                                      th Size
                                      td { expanded_size_bytes }

                                    tr
                                      th Last modified
                                      td { relative_time(last_modified) }

                                    tr
                                      th WAL segment backup start
                                      td { wal_segment_backup_start }

                                    tr
                                      th WAL segment backup stop
                                      td { wal_segment_backup_stop }

                                    tr
                                      th WAL segment offset backup start
                                      td { wal_segment_offset_backup_start }

                                    tr
                                      th WAL segment offset backup stop
                                      td { wal_segment_offset_backup_stop }

                                  button.btn.btn-info(
                                    id='{ id }-restore'
                                  )
                                    | Clone at this snapshot


  script.

    const sort_by = require('sort-by')

    // Pass a refresh callback for this tag to the options constructor argument
    // used for all Dynamic objects built in this tag:
    const add_refresh = object => Object.assign(
      {},
      object,
      { refresh: () => this.update() }
    )
    const Dynamic = options => this.parent.opts.Dynamic(add_refresh(options))

    const filter = this.filter = Dynamic()

    const to_timestamp = this.to_timestamp = time => (
      time
      .replace('T', ' ')
      .replace('.000Z', '')
    )

    const trunc_timestamp = this.trunc_timestamp = time => (
      Math.trunc(time / 1000) * 1000
    )

    const to_clone_time = this.to_clone_time = time => to_timestamp(
      new Date(trunc_timestamp(time))
      .toISOString()
    )

    const min = (a, b) => a <= b ? a : b
    const max = (a, b) => a >= b ? a : b
    const minimum = array => array.reduce(min)
    const maximum = array => array.reduce(max)
    const both = (f, g, x) => [f(x), g(x)]

    const setting = property => (object, value) => {
      object[property] = value
      return object
    }

    const load_time = this.load_time = +new Date()
    const relative_time = this.relative_time = time => {
      const relative = humanizeDuration(
        trunc_timestamp(load_time)
        - Date.parse(time)
      )
      return `${to_timestamp(time)} UTC (${relative} ago)`
    }

    const q = selector => jQuery(selector, this.root)

    const route_on_click = (selector, target) => (
      q(selector).on('click', event =>
        route(target(event).join('/'))
      )
    )

    const on_collapse = action => (
      ({
        klass,
        collection,
        predicate = () => true,
        selector_prefix = '',
        body,
      }) => (
        q(`${selector_prefix}.${klass}.collapse`)
        .on(
          action + '.bs.collapse',
          event => {
            if (!(
              event.target.classList.contains(klass)
              && predicate(event.target.dataset)
            )) {
              return true
            }
            const target = collection.find(item =>
              item.name === event.target.getAttribute('data-' + klass)
            )
            target['collapsed'] = action === 'hide'
            body(target)
            this.update()
          },
        )
      )
    )

    const collapsible_handlers = options => {
      on_collapse('hide')(Object.assign({}, options, { body: _ => {}}))
      on_collapse('show')(options)
    }

    const get_subresources_once = ({
      parent_resource,
      key,
      url,
      body,
      build_subresource = subresource => subresource,
      build_subresources = subresources => subresources,
    }) => {
      if (parent_resource[key] === undefined) {
        (
          jQuery
          .get(url)
          .done(values => {
            parent_resource[key] = (
              build_subresources(values)
              .map(build_subresource)
            )
            this.update()
            body(parent_resource[key])
          })
          .fail(() => parent_resource[key] = null)
          .always(() => this.update())
        )
      }
    }

    this.stored_clusters = undefined

    this.on('mount', () =>
      get_subresources_once({
        parent_resource: this,
        key: 'stored_clusters',
        url: './stored_clusters',
        build_subresource: stored_cluster_name => ({
          id: 'stored-cluster-' + stored_cluster_name,
          name: stored_cluster_name,
          collapsed: true,
        }),
        body: stored_clusters => collapsible_handlers({
          klass: 'stored-clusters',
          collection: stored_clusters,
          body: stored_cluster => get_subresources_once({
            parent_resource: stored_cluster,
            key: 'versions',
            url: './stored_clusters/' + stored_cluster.name,
            build_subresource: version_name => ({
              id: stored_cluster.id + '-version-' + version_name,
              name: version_name,
              collapsed: true,
              stored_cluster: stored_cluster,
            }),
            body: versions => collapsible_handlers({
              klass: 'versions',
              collection: versions,
              selector_prefix: `#${stored_cluster.id}-collapse `,
              predicate: data => stored_cluster.name === data.storedClusters,
              body: version => get_subresources_once({
                parent_resource: version,
                key: 'basebackups',
                url: (
                  './stored_clusters/' + stored_cluster.name
                  + '/' + version.name
                ),
                build_subresource: basebackup => Object.assign(basebackup, {
                    id: version.id + '-basebackup-' + basebackup.name,
                }),
                build_subresources: basebackups => (
                  basebackups
                  .sort(sort_by('last_modified'))
                  .map(setting('index'))
                ),
                body: basebackups => {
                  if (basebackups.length === 0) {
                    return
                  }

                  const basebackup_age = basebackup => (
                    +new Date(basebackup.last_modified)
                  )

                  const oldest = version.basebackups[0]
                  const newest = version.basebackups[
                    version.basebackups.length - 1
                  ]
                  const [start, end] = both(
                    maximum,
                    minimum,
                    [
                      load_time,
                      ...[oldest, newest].map(basebackup_age),
                    ],
                  )
                  const span = end - start
                  const padding_time = 0.1 * span

                  basebackups.forEach(basebackup => {
                    route_on_click(
                      '#' + basebackup.id + '-restore',
                      () => [
                        '/clone',
                        encodeURI(stored_cluster.name),
                        encodeURI(version.name),
                        // TODO: Ideally, this should use the basebackup's end
                        // LSN, not the S3 last modification timestamp. However,
                        // the current implementation of the clone feature does
                        // not allow specifying `recovery_target_lsn`.
                        encodeURI(basebackup.last_modified),
                      ],
                    )
                  })

                  version.timeline = new vis.Timeline(
                    q('#' + version.id + '-timeline')[0],
                    new vis.DataSet([
                      ...version.basebackups.map(
                        basebackup => ({
                          id: basebackup.index,
                          content: (
                            to_timestamp(basebackup.last_modified)
                            .replace(' ', '<br>')
                            + ' (UTC)'
                          ),
                          start: basebackup.last_modified,
                        })
                      ),
                      {
                        id: version.basebackups.length,
                        content: 'now',
                        start: load_time,
                        type: 'point',
                      }
                    ]),
                    {
                      min: min - padding_time,
                      max: max + 5 * padding_time,
                      moment: time => vis.moment(time).utc(),
                      clickToUse: true,
                      moveable: true,
                      zoomable: true,
                      showCurrentTime: true,
                    }
                  )

                  version.clone_time = trunc_timestamp(end - span / 3)
                  version.timeline.addCustomTime(
                    version.clone_time,
                    'clone_time',
                  )

                  version.timeline.on('timechange', properties => {
                    version.clone_time = +properties.time
                    this.update()
                  })

                  version.timeline.on('select', selection =>
                    version.basebackups.forEach(basebackup =>
                      q('#' + basebackup.id + '-collapse').collapse(
                        selection.items.includes(basebackup.index)
                        ? 'show'
                        : 'hide'
                      )
                    )
                  )

                  route_on_click(
                    '#' + version.id + '-clone',
                    () => [
                      '/clone',
                      encodeURI(stored_cluster.name),
                      encodeURI(version.name),
                      encodeURI(to_clone_time(load_time)),
                    ],
                  )

                  route_on_click(
                    '#' + version.id + '-pitr',
                    () => [
                      '/clone',
                      encodeURI(stored_cluster.name),
                      encodeURI(version.name),
                      encodeURI(to_clone_time(version.clone_time)),
                    ],
                  )
                },
              }),
            }),
          }),
        }),
      })
    )


================================================
File: ui/app/src/status.tag.pug
================================================
status
  .container-fluid

    h1.page-header
      nav(aria-label="breadcrumb")
        ol.breadcrumb

          li.breadcrumb-item
            a(href='./#/operator')
              | Workers

          virtual(if='{ operatorShowLogs && logs }')
            li.breadcrumb-item { worker_id }
            li.breadcrumb-item
              a(href='./#/operator/worker/{ worker_id }/logs')
                | Logs

          virtual(if='{ operatorShowQueue && queue }')
            li.breadcrumb-item { worker_id }
            li.breadcrumb-item
              a(href='./#/operator/worker/{ worker_id }/queue')
                | Queue

    div(if='{ status }')

      div(if='{ !operatorShowLogs && !operatorShowQueue }')

        table.table.table-hover

          thead
            tr
              td Worker ID
              td Queue size
              td Actions

          tr(each='{ queue_size, worker_id in status.WorkerQueueSize}')
            td { worker_id }
            td { queue_size }

            td
              .btn-group(
                aria-label="Worker { worker_id} actions"
                role="group"
              )

                a.btn.btn-info(
                  href='./#/operator/worker/{ worker_id }/logs'
                )
                  | Logs

                a.btn.btn-info(
                  href='./#/operator/worker/{ worker_id }/queue'
                )
                  | Queue

      div(if='{ operatorShowLogs && logs }')
        table.table.table-hover
          tr(each='{ logs }')

            td
              span.label.label-font-size(class='{ levels[Level].class }')
                | { levels[Level].label }

            td(style='white-space: pre')
              | { Time }

            td(style='white-space: pre')
              | { ClusterName }

            td { Message }

      div(if='{ operatorShowQueue && queue }')
        table.table.table-hover
          tr(each='{ queue }')

            td(style='white-space: pre')
              | { EventTime }

            td(style='white-space: pre')
              | { EventType }

  script.

    this.levels = {
      0: { class: '', label: 'Panic' },
      1: { class: '', label: 'Fatal'},
      2: { class: 'label-danger', label: 'Error'},
      3: { class: 'label-warning', label: 'Warning'},
      4: { class: 'label-primary', label: 'Info'},
      5: { class: 'label-info', label: 'Debug'},
    }

    this.status = {
      'Clusters': 0,
    }

    this.pollStatus = () => {
      jQuery.get(
        './operator/status',
      ).done(data => {
        this.update({status: data})
      })
    }

    this.logs = []
    this.queue = []

    this.on('mount', () => {
      route.exec()
    })

    this.pollLogs = id => {
      jQuery.get(
        './operator/workers/' + id + '/logs',
      ).done(data => {
        data.reverse()
        this.update({logs: data})
      })
    }

    this.pollQueue = id => {
      jQuery.get(
        './operator/workers/' + id + '/queue',
      ).done(data =>
        this.update({queue: data.List})
      )
    }

    var subRoute = route.create()

    subRoute('/operator/worker/*/logs', id => {
      this.worker_id = id
      this.operatorShowLogs = true
      this.operatorShowQueue = false
      this.pollLogs(this.worker_id)
    })

    subRoute('/operator/worker/*/queue', id => {
      this.worker_id = id
      this.operatorShowLogs = false
      this.operatorShowQueue = true
      this.pollQueue(this.worker_id)
    })

    subRoute('/operator', () => {
      this.operatorShowLogs = false
      this.operatorShowQueue = false
      this.pollStatus()
    })


================================================
File: ui/manifests/deployment.yaml
================================================
apiVersion: "apps/v1"
kind: "Deployment"
metadata:
  name: "postgres-operator-ui"
  namespace: "default"
  labels:
    name: "postgres-operator-ui"
spec:
  replicas: 1
  selector:
    matchLabels:
      name: "postgres-operator-ui"
  template:
    metadata:
      labels:
        name: "postgres-operator-ui"
    spec:
      serviceAccountName: postgres-operator-ui
      containers:
        - name: "service"
          image: ghcr.io/zalando/postgres-operator-ui:v1.14.0
          ports:
            - containerPort: 8081
              protocol: "TCP"
          readinessProbe:
            httpGet:
              path: "/health"
              port: 8081
            initialDelaySeconds: 5
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "200m"
              memory: "200Mi"
            requests:
              cpu: "100m"
              memory: "100Mi"
          env:
            - name: "APP_URL"
              value: "http://localhost:8081"
            - name: "OPERATOR_API_URL"
              value: "http://postgres-operator:8080"
            - name: "OPERATOR_CLUSTER_NAME_LABEL"
              value: "cluster-name"
            - name: "RESOURCES_VISIBLE"
              value: "False"
            - name: "TARGET_NAMESPACE"
              # Set to "*" to allow viewing/creation of clusters in all namespaces
              value: "default"
            - name: "TEAMS"
              value: |-
                [
                  "acid"
                ]
            - name: "OPERATOR_UI_CONFIG"
              value: |-
                {
                  "docs_link":"https://postgres-operator.readthedocs.io/en/latest/",
                  "dns_format_string": "{0}.{1}",
                  "databases_visible": true,
                  "master_load_balancer_visible": true,
                  "nat_gateways_visible": false,
                  "replica_load_balancer_visible": true,
                  "resources_visible": true,
                  "users_visible": true,
                  "cost_ebs": 0.0952,
                  "cost_iops": 0.006,
                  "cost_throughput": 0.0476,
                  "cost_core": 0.0575,
                  "cost_memory": 0.014375,
                  "free_iops": 3000,
                  "free_throughput": 125,
                  "limit_iops": 16000,
                  "limit_throughput": 1000,
                  "postgresql_versions": [
                    "17",
                    "16",
                    "15",
                    "14",
                    "13"
                  ]
                }
            # Exemple of settings to make snapshot view working in the ui when using AWS
            # - name: WALE_S3_ENDPOINT
            #   value: https+path://s3.us-east-1.amazonaws.com:443
            # - name: SPILO_S3_BACKUP_PREFIX
            #   value: spilo/
            # - name: AWS_ACCESS_KEY_ID
            #   valueFrom:
            #     secretKeyRef:
            #       name: <postgres operator secret with AWS token>
            #       key: AWS_ACCESS_KEY_ID
            # - name: AWS_SECRET_ACCESS_KEY
            #   valueFrom:
            #     secretKeyRef:
            #       name: <postgres operator secret with AWS token>
            #       key: AWS_SECRET_ACCESS_KEY
            # - name: AWS_DEFAULT_REGION
            #   valueFrom:
            #     secretKeyRef:
            #       name: <postgres operator secret with AWS token>
            #       key: AWS_DEFAULT_REGION
            # - name: SPILO_S3_BACKUP_BUCKET
            #   value: <s3 bucket used by the operator>
            # - name: "USE_AWS_INSTANCE_PROFILE"
            #   value: "true"


================================================
File: ui/manifests/ingress.yaml
================================================
apiVersion: "networking.k8s.io/v1"
kind: "Ingress"
metadata:
  name: "postgres-operator-ui"
  namespace: "default"
  labels:
    application: "postgres-operator-ui"
spec:
  # ingressClassName: "ingress-nginx"
  rules:
    - host: "ui.example.org"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: "postgres-operator-ui"
                port:
                  number: 80


================================================
File: ui/manifests/kustomization.yaml
================================================
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yaml
- ingress.yaml
- service.yaml
- ui-service-account-rbac.yaml


================================================
File: ui/manifests/service.yaml
================================================
apiVersion: "v1"
kind: "Service"
metadata:
  name: "postgres-operator-ui"
  namespace: "default"
  labels:
    application: "postgres-operator-ui"
spec:
  type: "ClusterIP"
  selector:
    name: "postgres-operator-ui"
  ports:
    - port: 80
      protocol: "TCP"
      targetPort: 8081


================================================
File: ui/manifests/ui-service-account-rbac.yaml
================================================
apiVersion: v1
kind: ServiceAccount
metadata:
  name: postgres-operator-ui
  namespace: default

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: postgres-operator-ui
rules:
- apiGroups:
  - acid.zalan.do
  resources:
  - postgresqls
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
- apiGroups:
  - apps
  resources:
  - deployments
  - statefulsets
  verbs:
  - get
  - list
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: postgres-operator-ui
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: postgres-operator-ui
subjects:
- kind: ServiceAccount
  name: postgres-operator-ui
  namespace: default


================================================
File: ui/operator_ui/__init__.py
================================================
# This version is replaced during release process.
__version__ = '2017.0.dev1'


================================================
File: ui/operator_ui/__main__.py
================================================
from .main import main

main()


================================================
File: ui/operator_ui/backoff.py
================================================
import random


def expo(n: int, base=2, factor=1, max_value=None):
    """Exponential decay.

    Adapted from https://github.com/litl/backoff/blob/master/backoff.py (MIT License)

    Args:
        base: The mathematical base of the exponentiation operation
        factor: Factor to multiply the exponentation by.
        max_value: The maximum value to yield. Once the value in the
             true exponential sequence exceeds this, the value
             of max_value will forever after be yielded.
    """
    a = factor * base ** n
    if max_value is None or a < max_value:
        return a
    else:
        return max_value


def random_jitter(value, jitter=1):
    """Jitter the value a random number of milliseconds.

    Copied from https://github.com/litl/backoff/blob/master/backoff.py (MIT License)

    This adds up to 1 second of additional time to the original value.
    Prior to backoff version 1.2 this was the default jitter behavior.
    Args:
        value: The unadulterated backoff value.
    """
    return value + random.uniform(0, jitter)


def full_jitter(value):
    """Jitter the value across the full range (0 to value).

    Copied from https://github.com/litl/backoff/blob/master/backoff.py (MIT License)

    This corresponds to the "Full Jitter" algorithm specified in the
    AWS blog's post on the performance of various jitter algorithms.
    (http://www.awsarchitectureblog.com/2015/03/backoff.html)

    Args:
        value: The unadulterated backoff value.
    """
    return random.uniform(0, value)


================================================
File: ui/operator_ui/cluster_discovery.py
================================================
import logging
import re
from pathlib import Path

import kubernetes.client
import kubernetes.config
import tokens
from requests.auth import AuthBase

# default URL points to kubectl proxy
DEFAULT_CLUSTERS = 'http://localhost:8001/'
CLUSTER_ID_INVALID_CHARS = re.compile('[^a-z0-9:-]')

logger = logging.getLogger(__name__)

tokens.configure(from_file_only=True)


def generate_cluster_id(url: str):
    '''Generate some "cluster ID" from given API server URL'''
    for prefix in ('https://', 'http://'):
        if url.startswith(prefix):
            url = url[len(prefix):]
    return CLUSTER_ID_INVALID_CHARS.sub('-', url.lower()).strip('-')


class StaticAuthorizationHeaderAuth(AuthBase):
    '''Static authentication with given "Authorization" header'''

    def __init__(self, authorization):
        self.authorization = authorization

    def __call__(self, request):
        request.headers['Authorization'] = self.authorization
        return request


class OAuthTokenAuth(AuthBase):
    '''Dynamic authentication using the "tokens" library to load OAuth tokens from file
    (potentially mounted from a Kubernetes secret)'''

    def __init__(self, token_name):
        self.token_name = token_name
        tokens.manage(token_name)

    def __call__(self, request):
        token = tokens.get(self.token_name)
        request.headers['Authorization'] = 'Bearer {}'.format(token)
        return request


class Cluster:
    def __init__(self, id, api_server_url, ssl_ca_cert=None, auth=None, cert_file=None, key_file=None):
        self.id = id
        self.api_server_url = api_server_url
        self.ssl_ca_cert = ssl_ca_cert
        self.auth = auth
        self.cert_file = cert_file
        self.key_file = key_file


class StaticClusterDiscoverer:

    def __init__(self, api_server_urls: list):
        self._clusters = []

        if not api_server_urls:
            try:
                kubernetes.config.load_incluster_config()
            except kubernetes.config.ConfigException:
                # we are not running inside a cluster
                # => assume default kubectl proxy URL
                cluster = Cluster(generate_cluster_id(DEFAULT_CLUSTERS), DEFAULT_CLUSTERS)
            else:
                logger.info("in cluster configuration failed")
                config = kubernetes.client.Configuration()
                cluster = Cluster(
                    generate_cluster_id(config.host),
                    config.host,
                    ssl_ca_cert=config.ssl_ca_cert,
                    auth=StaticAuthorizationHeaderAuth(config.api_key['authorization']))
            self._clusters.append(cluster)
        else:
            for api_server_url in api_server_urls:
                logger.info("api server url: {}".format(api_server_url))
                if 'localhost' not in api_server_url:
                    # TODO: hacky way of detecting whether we need a token or not
                    auth = OAuthTokenAuth('read-only')
                else:
                    auth = None
                self._clusters.append(Cluster(generate_cluster_id(api_server_url), api_server_url, auth=auth))

    def get_clusters(self):
        return self._clusters


class KubeconfigDiscoverer:

    def __init__(self, kubeconfig_path: Path, contexts: set):
        self._path = kubeconfig_path
        self._contexts = contexts

    def get_clusters(self):
        # Kubernetes Python client expects "vintage" string path
        config_file = str(self._path)
        contexts, current_context = kubernetes.config.list_kube_config_contexts(config_file)
        for context in contexts:
            if self._contexts and context['name'] not in self._contexts:
                # filter out
                continue
            config = kubernetes.client.ConfigurationObject()
            kubernetes.config.load_kube_config(config_file, context=context['name'], client_configuration=config)
            authorization = config.api_key.get('authorization')
            if authorization:
                auth = StaticAuthorizationHeaderAuth(authorization)
            else:
                auth = None
            cluster = Cluster(
                context['name'],
                config.host,
                ssl_ca_cert=config.ssl_ca_cert,
                cert_file=config.cert_file,
                key_file=config.key_file,
                auth=auth)
            yield cluster


================================================
File: ui/operator_ui/main.py
================================================
#!/usr/bin/env python3
# pylama:ignore=E402

import requests
import tokens

from backoff import expo, on_exception
from click import ParamType, command, echo, option

from flask import (
    Flask,
    Response,
    abort,
    render_template,
    request,
    send_from_directory,
)

from gevent import sleep, spawn
from gevent.pywsgi import WSGIServer
from jq import jq
from json import dumps, loads
from os import getenv
from re import X, compile
from requests.exceptions import RequestException
from signal import SIGTERM, signal

from . import __version__
from .cluster_discovery import DEFAULT_CLUSTERS, StaticClusterDiscoverer

from .spiloutils import (
    apply_postgresql,
    create_postgresql,
    read_basebackups,
    read_namespaces,
    read_pooler,
    read_pods,
    read_postgresql,
    read_postgresqls,
    read_service,
    read_statefulset,
    read_stored_clusters,
    read_versions,
    remove_postgresql,
)

from .utils import (
    const,
    identity,
    these,
)

from operator_ui.adapters.logger import logger

SERVER_STATUS = {'shutdown': False}

APP_URL = getenv('APP_URL')
SPILO_S3_BACKUP_BUCKET = getenv('SPILO_S3_BACKUP_BUCKET')
TEAM_SERVICE_URL = getenv('TEAM_SERVICE_URL')

OPERATOR_API_URL = getenv('OPERATOR_API_URL', 'http://postgres-operator')
OPERATOR_CLUSTER_NAME_LABEL = getenv('OPERATOR_CLUSTER_NAME_LABEL', 'cluster-name')
OPERATOR_UI_CONFIG = loads(getenv('OPERATOR_UI_CONFIG', '{}'))
OPERATOR_UI_MAINTENANCE_CHECK = getenv('OPERATOR_UI_MAINTENANCE_CHECK', '{}')
READ_ONLY_MODE = getenv('READ_ONLY_MODE', False) in [True, 'true']
SPILO_S3_BACKUP_PREFIX = getenv('SPILO_S3_BACKUP_PREFIX', 'spilo/')
SUPERUSER_TEAM = getenv('SUPERUSER_TEAM', 'acid')
TARGET_NAMESPACE = getenv('TARGET_NAMESPACE')
GOOGLE_ANALYTICS = getenv('GOOGLE_ANALYTICS', False)
MIN_PODS = getenv('MIN_PODS', 2)
RESOURCES_VISIBLE = getenv('RESOURCES_VISIBLE', True)
CUSTOM_MESSAGE_RED = getenv('CUSTOM_MESSAGE_RED', '')

APPLICATION_DEPLOYMENT_DOCS = getenv('APPLICATION_DEPLOYMENT_DOCS', '')
CONNECTION_DOCS = getenv('CONNECTION_DOCS', '')

# storage pricing, i.e. https://aws.amazon.com/ebs/pricing/ (e.g. Europe - Franfurt)
COST_EBS = float(getenv('COST_EBS', 0.0952))  # GB per month
COST_IOPS = float(getenv('COST_IOPS', 0.006))  # IOPS per month above 3000 baseline
COST_THROUGHPUT = float(getenv('COST_THROUGHPUT', 0.0476))  # MB/s per month above 125 MB/s baseline

# compute costs, i.e. https://www.ec2instances.info/?region=eu-central-1&selected=m5.2xlarge
COST_CORE = float(getenv('COST_CORE', 0.0575))  # Core per hour m5.2xlarge / 8.
COST_MEMORY = float(getenv('COST_MEMORY', 0.014375))  # Memory GB m5.2xlarge / 32.
COST_ELB = float(getenv('COST_ELB', 0.03))     # per hour

# maximum and limitation of IOPS and throughput 
FREE_IOPS = float(getenv('FREE_IOPS', 3000)) 
LIMIT_IOPS = float(getenv('LIMIT_IOPS', 16000))
FREE_THROUGHPUT = float(getenv('FREE_THROUGHPUT', 125))
LIMIT_THROUGHPUT = float(getenv('LIMIT_THROUGHPUT', 1000))
# get the default value of core and memory
DEFAULT_MEMORY = getenv('DEFAULT_MEMORY', '300Mi')
DEFAULT_MEMORY_LIMIT = getenv('DEFAULT_MEMORY_LIMIT', '300Mi')
DEFAULT_CPU = getenv('DEFAULT_CPU', '10m')
DEFAULT_CPU_LIMIT = getenv('DEFAULT_CPU_LIMIT', '300m')

WALE_S3_ENDPOINT = getenv(
    'WALE_S3_ENDPOINT',
    'https+path://s3.eu-central-1.amazonaws.com:443',
)

USE_AWS_INSTANCE_PROFILE = (
    getenv('USE_AWS_INSTANCE_PROFILE', 'false').lower() != 'false'
)

AWS_ENDPOINT = getenv('AWS_ENDPOINT')

tokens.configure()
tokens.manage('read-only')
tokens.start()


def service_auth_header():
    token = getenv('SERVICE_TOKEN') or tokens.get('read-only')
    return {
        'Authorization': f'Bearer {token}',
    }


MAX_CONTENT_LENGTH = 16 * 1024 * 1024
app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH


class WSGITransferEncodingChunked:
    """Support HTTP Transfer-Encoding: chunked transfers"""

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        from io import BytesIO
        input = environ.get('wsgi.input')
        length = environ.get('CONTENT_LENGTH', '0')
        length = 0 if length == '' else int(length)
        body = b''
        if length == 0:
            if input is None:
                return
            if environ.get('HTTP_TRANSFER_ENCODING', '0') == 'chunked':
                size = int(input.readline(), 16)
                total_size = 0
                while size > 0:
                    # Validate max size to avoid DoS attacks
                    total_size += size
                    if total_size > MAX_CONTENT_LENGTH:
                        # Avoid DoS (using all available memory by streaming an
                        # infinite file)
                        start_response(
                            '413 Request Entity Too Large',
                            [('Content-Type', 'text/plain')],
                        )
                        return []

                    body += input.read(size + 2)
                    size = int(input.readline(), 16)

        else:
            body = environ['wsgi.input'].read(length)

        environ['CONTENT_LENGTH'] = str(len(body))
        environ['wsgi.input'] = BytesIO(body)

        return self.app(environ, start_response)


def ok(body={}, status=200):
    return (
        Response(
            (
                dumps(body)
                if isinstance(body, dict) or isinstance(body, list)
                else body
            ),
            mimetype='application/json',
        ),
        status
    )


def fail(body={}, status=400, **kwargs):
    return (
        Response(
            dumps(
                {
                    'error': ' '.join(body.split()).format(**kwargs),
                }
                if isinstance(body, str)
                else body,
            ),
            mimetype='application/json',
        ),
        status,
    )


def not_found(body={}, **kwargs):
    return fail(body=body, status=404, **kwargs)


def respond(data, f=identity):
    return (
        ok(f(data))
        if data is not None
        else not_found()
    )


def wrong_namespace(**kwargs):
    return fail(
        body=f'The Kubernetes namespace must be {TARGET_NAMESPACE}',
        status=403,
        **kwargs
    )


def no_writes_when_read_only(**kwargs):
    return fail(
        body='UI is in read-only mode for production',
        status=403,
        **kwargs
    )


@app.route('/health')
def health():
    if SERVER_STATUS['shutdown']:
        abort(503)
    else:
        return 'OK'


STATIC_HEADERS = {
    'cache-control': ', '.join([
        'no-store',
        'no-cache',
        'must-revalidate',
        'post-check=0',
        'pre-check=0',
        'max-age=0',
    ]),
    'Pragma': 'no-cache',
    'Expires': '-1',
}


@app.route('/css/<path:path>')
def send_css(path):
    return send_from_directory('static/', path), 200, STATIC_HEADERS


@app.route('/js/<path:path>')
def send_js(path):
    return send_from_directory('static/', path), 200, STATIC_HEADERS


@app.route('/')
def index():
    return render_template('index.html', google_analytics=GOOGLE_ANALYTICS)


DEFAULT_UI_CONFIG = {
    'docs_link': 'https://github.com/zalando/postgres-operator',
    'odd_host_visible': True,
    'nat_gateways_visible': True,
    'users_visible': True,
    'databases_visible': True,
    'resources_visible': RESOURCES_VISIBLE,
    'postgresql_versions': ['13', '14', '15', '16', '17'],
    'dns_format_string': '{0}.{1}',
    'pgui_link': '',
    'static_network_whitelist': {},
    'read_only_mode': READ_ONLY_MODE,
    'superuser_team': SUPERUSER_TEAM,
    'target_namespace': TARGET_NAMESPACE,
    'connection_docs': CONNECTION_DOCS,
    'application_deployment_docs': APPLICATION_DEPLOYMENT_DOCS,
    'cost_ebs': COST_EBS,
    'cost_iops': COST_IOPS,
    'cost_throughput': COST_THROUGHPUT,
    'cost_core': COST_CORE,
    'cost_memory': COST_MEMORY,
    'cost_elb': COST_ELB,
    'min_pods': MIN_PODS,
    'free_iops': FREE_IOPS, 
    'free_throughput': FREE_THROUGHPUT,
    'limit_iops': LIMIT_IOPS,
    'limit_throughput': LIMIT_THROUGHPUT
}


@app.route('/config')
def get_config():
    config = DEFAULT_UI_CONFIG.copy() 
    config.update(OPERATOR_UI_CONFIG)

    config['namespaces'] = (
        [TARGET_NAMESPACE]
        if TARGET_NAMESPACE not in ['', '*']
        else [
            namespace_name
            for namespace in these(
                read_namespaces(get_cluster()),
                'items',
            )
            for namespace_name in [namespace['metadata']['name']]
            if namespace_name not in [
                'kube-public',
                'kube-system',
            ]
        ]
    )

    try:

        kubernetes_maintenance_check = (
            config.get('kubernetes_maintenance_check') or
            loads(OPERATOR_UI_MAINTENANCE_CHECK)
        )

        if (
            kubernetes_maintenance_check and
            {'url', 'query'} <= kubernetes_maintenance_check.keys()
        ):
            config['kubernetes_in_maintenance'] = (
                jq(kubernetes_maintenance_check['query']).transform(
                    requests.get(
                        kubernetes_maintenance_check['url'],
                        headers=service_auth_header(),
                    ).json(),
                )
            )

    except ValueError:
        exception('Could not determine Kubernetes cluster status')

    return ok(config)


def get_teams_for_user(user_name):
    if not TEAM_SERVICE_URL:
        return loads(getenv('TEAMS', '[]'))

    return [
        team['id'].lower()
        for team in requests.get(
            TEAM_SERVICE_URL.format(user_name),
            headers=service_auth_header(),
        ).json()
    ]


@app.route('/teams')
def get_teams():
    return ok(
        get_teams_for_user(
            request.headers.get('X-Uid', ''),
        )
    )


@app.route('/services/<namespace>/<cluster>')
def get_service(namespace: str, cluster: str):

    if TARGET_NAMESPACE not in ['', '*', namespace]:
        return wrong_namespace()

    return respond(
        read_service(
            get_cluster(),
            namespace,
            cluster,
        ),
    )


@app.route('/pooler/<namespace>/<cluster>')
def get_list_poolers(namespace: str, cluster: str):

    if TARGET_NAMESPACE not in ['', '*', namespace]:
        return wrong_namespace()

    return respond(
        read_pooler(
            get_cluster(),
            namespace,
            "{}-pooler".format(cluster),
        ),
    )


@app.route('/statefulsets/<namespace>/<cluster>')
def get_list_clusters(namespace: str, cluster: str):

    if TARGET_NAMESPACE not in ['', '*', namespace]:
        return wrong_namespace()

    return respond(
        read_statefulset(
            get_cluster(),
            namespace,
            cluster,
        ),
    )


@app.route('/statefulsets/<namespace>/<cluster>/pods')
def get_list_members(namespace: str, cluster: str):

    if TARGET_NAMESPACE not in ['', '*', namespace]:
        return wrong_namespace()

    return respond(
        read_pods(
            get_cluster(),
            namespace,
            cluster,
        ),
        lambda pods: [
            pod['metadata']
            for pod in these(pods, 'items')
        ],
    )


@app.route('/namespaces')
def get_namespaces():

    if TARGET_NAMESPACE not in ['', '*']:
        return ok([TARGET_NAMESPACE])

    return respond(
        read_namespaces(
            get_cluster(),
        ),
        lambda namespaces: [
            namespace['name']
            for namespace in these(namespaces, 'items')
        ],
    )


@app.route('/postgresqls')
def get_postgresqls():
    postgresqls = [
        {
            'nodes': spec.get('numberOfInstances', ''),
            'memory': spec.get('resources', {}).get('requests', {}).get('memory', OPERATOR_UI_CONFIG.get("default_memory", DEFAULT_MEMORY)),
            'memory_limit': spec.get('resources', {}).get('limits', {}).get('memory', OPERATOR_UI_CONFIG.get("default_memory_limit", DEFAULT_MEMORY_LIMIT)),
            'cpu': spec.get('resources', {}).get('requests', {}).get('cpu', OPERATOR_UI_CONFIG.get("default_cpu", DEFAULT_CPU)),
            'cpu_limit': spec.get('resources', {}).get('limits', {}).get('cpu', OPERATOR_UI_CONFIG.get("default_cpu_limit", DEFAULT_CPU_LIMIT)),
            'volume_size': spec.get('volume', {}).get('size', 0),
            'iops': spec.get('volume', {}).get('iops', 3000),
            'throughput': spec.get('volume', {}).get('throughput', 125),
            'team': (
                spec.get('teamId') or
                metadata.get('labels', {}).get('team', '')
            ),
            'namespace': namespace,
            'name': name,
            'uid': uid,
            'namespaced_name': namespace + '/' + name,
            'full_name': namespace + '/' + name + ('/' + uid if uid else ''),
            'status': status,
            'num_elb': spec.get('enableMasterLoadBalancer', 0) + spec.get('enableReplicaLoadBalancer', 0) + \
                       spec.get('enableMasterPoolerLoadBalancer', 0) + spec.get('enableReplicaPoolerLoadBalancer', 0),
            'maintenance_windows': spec.get('maintenanceWindows', []),
        }
        for cluster in these(
            read_postgresqls(
                get_cluster(),
                namespace=(
                    None
                    if TARGET_NAMESPACE in ['', '*']
                    else TARGET_NAMESPACE
                ),
            ),
            'items',
        )
        for spec in [cluster.get('spec', {}) if cluster.get('spec', {}) is not None else {"error": "Invalid spec in manifest"}]
        for status in [cluster.get('status', {})]
        for metadata in [cluster['metadata']]
        for namespace in [metadata['namespace']]
        for name in [metadata['name']]
        for uid in [metadata.get('uid', '')]
    ]
    return respond(postgresqls)


# Note these are meant to be consistent with the operator backend validations;
# See https://github.com/zalando/postgres-operator/blob/master/pkg/cluster/cluster.go  # noqa
VALID_SIZE = compile(r'^[1-9][0-9]{0,3}Gi$')
VALID_CLUSTER_NAME = compile(r'^[a-z0-9]+[a-z0-9\-]+[a-z0-9]+$')
VALID_DATABASE_NAME = compile(r'^[a-zA-Z_][a-zA-Z0-9_]*$')
VALID_USERNAME = compile(
    r'''
        ^[a-z0-9]([-_a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-_a-z0-9]*[a-z0-9])?)*$
    ''',
    X,
)

ROLEFLAGS = '''
    SUPERUSER
    INHERIT
    LOGIN
    NOLOGIN
    CREATEROLE
    CREATEDB
    REPLICATION
    BYPASSRLS
'''.split()


def namespaced(handler):

    def run(*args, **kwargs):
        namespace = (
            args[1]
            if len(args) >= 2
            else kwargs['namespace']
        )

        if TARGET_NAMESPACE not in ['', '*', namespace]:
            return wrong_namespace()

        return handler(*args, **kwargs)

    return run


def read_only(handler):

    def run(*args, **kwargs):
        if READ_ONLY_MODE:
            return no_writes_when_read_only()

        return handler(*args, **kwargs)

    return run


@app.route('/postgresqls/<namespace>/<cluster>', methods=['POST'])
@namespaced
def update_postgresql(namespace: str, cluster: str):
    if READ_ONLY_MODE:
        return no_writes_when_read_only()

    o = read_postgresql(get_cluster(), namespace, cluster)
    if o is None:
        return not_found()

    postgresql = request.get_json(force=True)

    teams = get_teams_for_user(request.headers.get('X-Uid', ''))
    logger.info(f'Changes to: {cluster} by {request.headers.get("X-Uid", "local-user")}/{teams} {postgresql}')  # noqa

    if SUPERUSER_TEAM and SUPERUSER_TEAM in teams:
        logger.info(f'Allowing edit due to membership in superuser team {SUPERUSER_TEAM}')  # noqa
    elif not o['spec']['teamId'].lower() in teams:
        return fail('Not a member of the owning team', status=401)

    # do spec copy 1 by 1 not to do unsupporeted changes for now
    spec = {}
    if 'allowedSourceRanges' in postgresql['spec']:
        if not isinstance(postgresql['spec']['allowedSourceRanges'], list):
            return fail('allowedSourceRanges invalid')
        spec['allowedSourceRanges'] = postgresql['spec']['allowedSourceRanges']

    if 'maintenanceWindows' in postgresql['spec']:
        if not isinstance(postgresql['spec']['maintenanceWindows'], list):
            return fail('maintenanceWindows invalid')
        spec['maintenanceWindows'] = postgresql['spec']['maintenanceWindows']

    if 'numberOfInstances' in postgresql['spec']:
        if not isinstance(postgresql['spec']['numberOfInstances'], int):
            return fail('numberOfInstances invalid')
        spec['numberOfInstances'] = postgresql['spec']['numberOfInstances']

    if (
        'volume' in postgresql['spec']
        and 'size' in postgresql['spec']['volume']
    ):
        size = str(postgresql['spec']['volume']['size'])
        if not VALID_SIZE.match(size):
            return fail('volume.size is invalid; should be like 123Gi')

        spec['volume'] = {'size': size}

    if (
        'volume' in postgresql['spec']
        and 'iops' in postgresql['spec']['volume']
        and postgresql['spec']['volume']['iops'] != None
    ):
        iops = int(postgresql['spec']['volume']['iops'])
        if not 'volume' in spec:
            spec['volume'] = {}

        spec['volume']['iops'] = iops

    if (
        'volume' in postgresql['spec']
        and 'throughput' in postgresql['spec']['volume']
        and postgresql['spec']['volume']['throughput'] != None
    ):
        throughput = int(postgresql['spec']['volume']['throughput'])
        if not 'volume' in spec:
            spec['volume'] = {}

        spec['volume']['throughput'] = throughput

    additional_specs = ['enableMasterLoadBalancer',
                        'enableReplicaLoadBalancer',
                        'enableConnectionPooler',
                        'enableReplicaConnectionPooler',
                        'enableMasterPoolerLoadBalancer',
                        'enableReplicaPoolerLoadBalancer',
                        ]

    for var in additional_specs:
        if postgresql['spec'].get(var):
            spec[var] = True
        else:
            if var in o['spec']:
                del o['spec'][var]

    if 'users' in postgresql['spec']:
        spec['users'] = postgresql['spec']['users']

        if not isinstance(postgresql['spec']['users'], dict):
            return fail('''
                the "users" key must hold a key-value object mapping usernames
                to a list of their role flags as simple strings.
                e.g.: {{"some_username": ["createdb", "login"]}}
            ''')

        for username, role_flags in postgresql['spec']['users'].items():

            if not VALID_USERNAME.match(username):
                return fail(
                    '''
                        no match for valid username pattern {VALID_USERNAME} in
                        invalid username {username}
                    ''',
                    VALID_USERNAME=VALID_USERNAME.pattern,
                    username=username,
                )

            if not isinstance(role_flags, list):
                return fail(
                    '''
                        the value for the user key {username} must be a list of
                        the user's permissions as simple strings.
                        e.g.: ["createdb", "login"]
                    ''',
                    username=username,
                )

            for role_flag in role_flags:

                if not isinstance(role_flag, str):
                    return fail(
                        '''
                            the value for the user key {username} must be a
                            list of the user's permissions as simple strings.
                            e.g.: ["createdb", "login"]
                        ''',
                        username=username,
                    )

                if role_flag.upper() not in ROLEFLAGS:
                    return fail(
                        '''
                            user {username} has invalid role flag {role_flag}
                            - allowed flags are {all_flags}
                        ''',
                        username=username,
                        role_flag=role_flag,
                        all_flags=', '.join(ROLEFLAGS),
                    )

    if 'databases' in postgresql['spec']:
        spec['databases'] = postgresql['spec']['databases']

        if not isinstance(postgresql['spec']['databases'], dict):
            return fail('''
                the "databases" key must hold a key-value object mapping
                database names to their respective owner's username as a simple
                string.  e.g. {{"some_database_name": "some_username"}}
            ''')

        for database_name, owner_username in (
            postgresql['spec']['databases'].items()
        ):

            if not VALID_DATABASE_NAME.match(database_name):
                return fail(
                    '''
                        no match for valid database name pattern
                        {VALID_DATABASE_NAME} in invalid database name
                        {database_name}
                    ''',
                    VALID_DATABASE_NAME=VALID_DATABASE_NAME.pattern,
                    database_name=database_name,
                )

            if not isinstance(owner_username, str):
                return fail(
                    '''
                        the value for the database key {database_name} must be
                        the owning user's username as a simple string.  e.g.:
                        "some_username"
                    ''',
                    database_name=database_name,
                )

            if not VALID_USERNAME.match(owner_username):
                return fail(
                    '''
                        no match for valid username pattern {VALID_USERNAME} in
                        invalid database owner username {owner_username}
                    ''',
                    VALID_USERNAME=VALID_USERNAME.pattern,
                    owner_username=owner_username,
                )

    resource_types = ["cpu","memory"]
    resource_constraints = ["requests","limits"]
    if "resources" in postgresql["spec"]:
        spec["resources"] = {}

        res = postgresql["spec"]["resources"]
        for rt in resource_types:
            for rc in resource_constraints:
                if rc in res:
                    if rt in res[rc]:
                        if not rc in spec["resources"]:
                            spec["resources"][rc] = {}
                        spec["resources"][rc][rt] = res[rc][rt]

    if "postgresql" in postgresql["spec"]:
        if "version" in postgresql["spec"]["postgresql"]:
            if "postgresql" not in spec:
                spec["postgresql"]={}

            spec["postgresql"]["version"] = postgresql["spec"]["postgresql"]["version"]

    o['spec'].update(spec)

    apply_postgresql(get_cluster(), namespace, cluster, o)

    return ok(o)


@app.route('/postgresqls/<namespace>/<cluster>', methods=['GET'])
def get_postgresql(namespace: str, cluster: str):

    if TARGET_NAMESPACE not in ['', '*', namespace]:
        return wrong_namespace()

    return respond(
        read_postgresql(
            get_cluster(),
            namespace,
            cluster,
        ),
    )


@app.route('/stored_clusters')
def get_stored_clusters():
    return respond(
        read_stored_clusters(
            bucket=SPILO_S3_BACKUP_BUCKET,
            prefix=SPILO_S3_BACKUP_PREFIX,
        )
    )


@app.route('/stored_clusters/<pg_cluster>', methods=['GET'])
def get_versions(pg_cluster: str):
    return respond(
        read_versions(
            bucket=SPILO_S3_BACKUP_BUCKET,
            pg_cluster=pg_cluster,
            prefix=SPILO_S3_BACKUP_PREFIX,
            s3_endpoint=WALE_S3_ENDPOINT,
            use_aws_instance_profile=USE_AWS_INSTANCE_PROFILE,
        ),
    )


@app.route('/stored_clusters/<pg_cluster>/<uid>', methods=['GET'])
def get_basebackups(pg_cluster: str, uid: str):
    return respond(
        read_basebackups(
            bucket=SPILO_S3_BACKUP_BUCKET,
            pg_cluster=pg_cluster,
            prefix=SPILO_S3_BACKUP_PREFIX,
            s3_endpoint=WALE_S3_ENDPOINT,
            uid=uid,
            use_aws_instance_profile=USE_AWS_INSTANCE_PROFILE,
        ),
    )


@app.route('/create-cluster', methods=['POST'])
def create_new_cluster():

    if READ_ONLY_MODE:
        return no_writes_when_read_only()

    postgresql = request.get_json(force=True)

    cluster_name = postgresql['metadata']['name']
    if not VALID_CLUSTER_NAME.match(cluster_name):
        return fail(r'metadata.name is invalid. [a-z0-9\-]+')

    namespace = postgresql['metadata']['namespace']
    if not namespace:
        return fail('metadata.namespace must not be empty')
    if TARGET_NAMESPACE not in ['', '*', namespace]:
        return wrong_namespace()

    teams = get_teams_for_user(request.headers.get('X-Uid', ''))
    logger.info(f'Create cluster by {request.headers.get("X-Uid", "local-user")}/{teams} {postgresql}')  # noqa

    if SUPERUSER_TEAM and SUPERUSER_TEAM in teams:
        logger.info(f'Allowing create due to membership in superuser team {SUPERUSER_TEAM}')  # noqa
    elif not postgresql['spec']['teamId'].lower() in teams:
        return fail('Not a member of the owning team', status=401)

    r = create_postgresql(get_cluster(), namespace, postgresql)
    return ok() if r else fail(status=500)


@app.route('/postgresqls/<namespace>/<cluster>', methods=['DELETE'])
def delete_postgresql(namespace: str, cluster: str):
    if TARGET_NAMESPACE not in ['', '*', namespace]:
        return wrong_namespace()

    if READ_ONLY_MODE:
        return no_writes_when_read_only()

    postgresql = read_postgresql(get_cluster(), namespace, cluster)
    if postgresql is None:
        return not_found()

    teams = get_teams_for_user(request.headers.get('X-Uid', ''))

    logger.info(f'Delete cluster: {cluster} by {request.headers.get("X-Uid", "local-user")}/{teams}')  # noqa

    if SUPERUSER_TEAM and SUPERUSER_TEAM in teams:
        logger.info(f'Allowing delete due to membership in superuser team {SUPERUSER_TEAM}')  # noqa
    elif not postgresql['spec']['teamId'].lower() in teams:
        return fail('Not a member of the owning team', status=401)

    return respond(
        remove_postgresql(
            get_cluster(),
            namespace,
            cluster,
        ),
        const(None),
    )


def proxy_operator(url: str):
    response = requests.get(OPERATOR_API_URL + url)
    response.raise_for_status()
    return respond(response.json())


@app.route('/operator/status')
def get_operator_status():
    return proxy_operator('/status/')


@app.route('/operator/workers/<worker>/queue')
def get_operator_get_queue(worker: int):
    return proxy_operator(f'/workers/{worker}/queue')


@app.route('/operator/workers/<worker>/logs')
def get_operator_get_logs(worker: int):
    return proxy_operator(f'/workers/{worker}/logs')


@app.route('/operator/clusters/<namespace>/<cluster>/logs')
def get_operator_get_logs_per_cluster(namespace: str, cluster: str):
    return proxy_operator(f'/clusters/{namespace}/{cluster}/logs/')


@app.route('/favicon.png')
def favicon():
    return send_from_directory('static/', 'favicon-96x96.png'), 200


def shutdown():
    # just wait some time to give Kubernetes time to update endpoints
    # this requires changing the readinessProbe's
    # PeriodSeconds and FailureThreshold appropriately
    # see https://godoc.org/k8s.io/kubernetes/pkg/api/v1#Probe
    sleep(10)
    exit(0)


def exit_gracefully(signum, frame):
    logger.info('Received TERM signal, shutting down..')
    SERVER_STATUS['shutdown'] = True
    spawn(shutdown)


def print_version(ctx, param, value):
    if not value or ctx.resilient_parsing:
        return
    echo(f'PostgreSQL Operator UI {__version__}')
    ctx.exit()


class CommaSeparatedValues(ParamType):
    name = 'comma_separated_values'

    def convert(self, value, param, ctx):
        return (
            filter(None, value.split(','))
            if isinstance(value, str)
            else value
        )


CLUSTER = None


def get_cluster():
    return CLUSTER


def set_cluster(c):
    global CLUSTER
    CLUSTER = c
    return CLUSTER


def init_cluster():
    discoverer = StaticClusterDiscoverer([])
    set_cluster(discoverer.get_clusters()[0])


@command(context_settings={'help_option_names': ['-h', '--help']})
@option(
    '-V',
    '--version',
    callback=print_version,
    expose_value=False,
    help='Print the current version number and exit.',
    is_eager=True,
    is_flag=True,
)
@option(
    '-p',
    '--port',
    default=8081,
    envvar='SERVER_PORT',
    help='HTTP port to listen on (default: 8081)',
    type=int,
)
@option(
    '-d',
    '--debug',
    help='Verbose logging',
    is_flag=True,
)
@option(
    '--clusters',
    envvar='CLUSTERS',
    help=f'Comma separated list of Kubernetes API server URLs (default: {DEFAULT_CLUSTERS})',  # noqa
    type=CommaSeparatedValues(),
)
def main(port, debug, clusters: list):
    global TARGET_NAMESPACE

    init_cluster()

    logger.info(f'App URL: {APP_URL}')
    logger.info(f'Operator API URL: {OPERATOR_API_URL}')
    logger.info(f'Operator cluster name label: {OPERATOR_CLUSTER_NAME_LABEL}')
    logger.info(f'Readonly mode: {"enabled" if READ_ONLY_MODE else "disabled"}')  # noqa
    logger.info(f'Spilo S3 backup bucket: {SPILO_S3_BACKUP_BUCKET}')
    logger.info(f'Spilo S3 backup prefix: {SPILO_S3_BACKUP_PREFIX}')
    logger.info(f'Superuser team: {SUPERUSER_TEAM}')
    logger.info(f'Target namespace: {TARGET_NAMESPACE}')
    logger.info(f'Teamservice URL: {TEAM_SERVICE_URL}')
    logger.info(f'Use AWS instance_profile: {USE_AWS_INSTANCE_PROFILE}')
    logger.info(f'WAL-E S3 endpoint: {WALE_S3_ENDPOINT}')
    logger.info(f'AWS S3 endpoint: {AWS_ENDPOINT}')

    if TARGET_NAMESPACE is None:
        @on_exception(
            expo,
            RequestException,
        )
        def get_target_namespace():
            logger.info('Fetching target namespace from Operator API')
            return (
                requests
                .get(OPERATOR_API_URL + '/config/')
                .json()
                ['operator']
                ['WatchedNamespace']
            )
        TARGET_NAMESPACE = get_target_namespace()
        logger.info(f'Target namespace set to: {TARGET_NAMESPACE or "*"}')

    app.debug = debug

    signal(SIGTERM, exit_gracefully)

    app.wsgi_app = WSGITransferEncodingChunked(app.wsgi_app)
    http_server = WSGIServer(('0.0.0.0', port), app)
    logger.info(f'Listening on :{port}')
    http_server.serve_forever()


================================================
File: ui/operator_ui/mock.py
================================================
import time
import json
import request


class MockCluster:

    def get_pods(self):
        return [{"name": "cluster-1-XFF", "role": "master", "ip": "localhost", "port": "8080"},
                {"name": "cluster-1-XFE", "role": "replica", "ip": "localhost", "port": "8080"},
                {"name": "cluster-1-XFS", "role": "replica", "ip": "localhost", "port": "8080"},
                {"name": "cluster-2-SJE", "role": "master", "ip": "localhost", "port": "8080"}]


================================================
File: ui/operator_ui/spiloutils.py
================================================
from boto3 import client
from datetime import datetime, timezone
from furl import furl
from json import dumps, loads
from os import environ, getenv
from requests import Session
from urllib.parse import urljoin
from uuid import UUID
from wal_e.cmd import configure_backup_cxt

from .utils import Attrs, defaulting, these
from operator_ui.adapters.logger import logger

session = Session()

AWS_ENDPOINT = getenv('AWS_ENDPOINT')

OPERATOR_CLUSTER_NAME_LABEL = getenv('OPERATOR_CLUSTER_NAME_LABEL', 'cluster-name')

COMMON_CLUSTER_LABEL = getenv('COMMON_CLUSTER_LABEL', '{"application":"spilo"}')
COMMON_POOLER_LABEL = getenv('COMMON_POOLER_LABEL', '{"application":"db-connection-pooler"}')

logger.info("Common Cluster Label: {}".format(COMMON_CLUSTER_LABEL))
logger.info("Common Pooler Label: {}".format(COMMON_POOLER_LABEL))

COMMON_CLUSTER_LABEL = loads(COMMON_CLUSTER_LABEL)
COMMON_POOLER_LABEL = loads(COMMON_POOLER_LABEL)


def request(cluster, path, **kwargs):
    if 'timeout' not in kwargs:
        # sane default timeout
        kwargs['timeout'] = (5, 15)
    if cluster.cert_file and cluster.key_file:
        kwargs['cert'] = (cluster.cert_file, cluster.key_file)

    return session.get(
        urljoin(cluster.api_server_url, path),
        auth=cluster.auth,
        verify=cluster.ssl_ca_cert,
        **kwargs
    )


def request_post(cluster, path, data, **kwargs):
    if 'timeout' not in kwargs:
        # sane default timeout
        kwargs['timeout'] = 5
    if cluster.cert_file and cluster.key_file:
        kwargs['cert'] = (cluster.cert_file, cluster.key_file)

    return session.post(
        urljoin(cluster.api_server_url, path),
        data=data,
        auth=cluster.auth,
        verify=cluster.ssl_ca_cert,
        **kwargs
    )


def request_put(cluster, path, data, **kwargs):
    if 'timeout' not in kwargs:
        # sane default timeout
        kwargs['timeout'] = 5
    if cluster.cert_file and cluster.key_file:
        kwargs['cert'] = (cluster.cert_file, cluster.key_file)

    return session.put(
        urljoin(cluster.api_server_url, path),
        data=data,
        auth=cluster.auth,
        verify=cluster.ssl_ca_cert,
        **kwargs
    )


def request_delete(cluster, path, **kwargs):
    if 'timeout' not in kwargs:
        # sane default timeout
        kwargs['timeout'] = 5
    if cluster.cert_file and cluster.key_file:
        kwargs['cert'] = (cluster.cert_file, cluster.key_file)

    return session.delete(
        urljoin(cluster.api_server_url, path),
        auth=cluster.auth,
        verify=cluster.ssl_ca_cert,
        **kwargs
    )


def resource_api_version(resource_type):
    return {
        'postgresqls': 'apis/acid.zalan.do/v1',
        'statefulsets': 'apis/apps/v1',
        'deployments': 'apis/apps/v1',
    }.get(resource_type, 'api/v1')


def encode_labels(label_selector):
    return ','.join([
        f'{label}={value}'
        for label, value in label_selector.items()
    ])


def cluster_labels(spilo_cluster):
    labels = COMMON_CLUSTER_LABEL
    labels[OPERATOR_CLUSTER_NAME_LABEL] = spilo_cluster
    return labels


def kubernetes_url(
    resource_type,
    namespace='default',
    resource_name=None,
    label_selector=None,
):

    return furl('/').add(
        path=(
            resource_api_version(resource_type).split('/')
            + (
                ['namespaces', namespace]
                if namespace
                else []
            )
            + [resource_type]
            + (
                [resource_name]
                if resource_name
                else []
            )
        ),
        args=(
            {'labelSelector': encode_labels(label_selector)}
            if label_selector
            else {}
        ),
    ).url


def kubernetes_get(cluster, **kwargs):
    response = request(cluster, kubernetes_url(**kwargs))
    if response.status_code == 404:
        return None
    if response.status_code >= 400:
        response.raise_for_status()
    return response.json()


def read_pods(cluster, namespace, spilo_cluster):
    return kubernetes_get(
        cluster=cluster,
        resource_type='pods',
        namespace=namespace,
        label_selector=cluster_labels(spilo_cluster),
    )


def read_pod(cluster, namespace, resource_name):
    return kubernetes_get(
        cluster=cluster,
        resource_type='pods',
        namespace=namespace,
        resource_name=resource_name,
        label_selector=COMMON_CLUSTER_LABEL,
    )


def read_service(cluster, namespace, resource_name):
    return kubernetes_get(
        cluster=cluster,
        resource_type='services',
        namespace=namespace,
        resource_name=resource_name,
        label_selector=COMMON_CLUSTER_LABEL,
    )


def read_pooler(cluster, namespace, resource_name):
    return kubernetes_get(
        cluster=cluster,
        resource_type='deployments',
        namespace=namespace,
        resource_name=resource_name,
        label_selector=COMMON_POOLER_LABEL,
    )


def read_statefulset(cluster, namespace, resource_name):
    return kubernetes_get(
        cluster=cluster,
        resource_type='statefulsets',
        namespace=namespace,
        resource_name=resource_name,
        label_selector=COMMON_CLUSTER_LABEL,
    )


def read_postgresql(cluster, namespace, resource_name):
    return kubernetes_get(
        cluster=cluster,
        resource_type='postgresqls',
        namespace=namespace,
        resource_name=resource_name,
    )


def read_postgresqls(cluster, namespace):
    return kubernetes_get(
        cluster=cluster,
        resource_type='postgresqls',
        namespace=namespace,
    )


def read_namespaces(cluster):
    return kubernetes_get(
        cluster=cluster,
        resource_type='namespaces',
        namespace=None,
    )


def create_postgresql(cluster, namespace, definition):
    path = kubernetes_url(
        resource_type='postgresqls',
        namespace=namespace,
    )
    try:
        r = request_post(cluster, path, dumps(definition))
        r.raise_for_status()
        return True
    except Exception as ex:
        logger.exception("K8s create request failed")
        return False


def apply_postgresql(cluster, namespace, resource_name, definition):
    path = kubernetes_url(
        resource_type='postgresqls',
        namespace=namespace,
        resource_name=resource_name,
    )
    try:
        r = request_put(cluster, path, dumps(definition))
        r.raise_for_status()
        return True
    except Exception as ex:
        logger.exception("K8s create request failed")
        return False


def remove_postgresql(cluster, namespace, resource_name):
    path = kubernetes_url(
        resource_type='postgresqls',
        namespace=namespace,
        resource_name=resource_name,
    )
    try:
        r = request_delete(cluster, path)
        r.raise_for_status()
        return True
    except Exception as ex:
        logger.exception("K8s delete request failed")
        return False


def read_stored_clusters(bucket, prefix, delimiter='/'):
    return [
        prefix['Prefix'].split('/')[-2]
        for prefix in these(
            client('s3', endpoint_url=AWS_ENDPOINT).list_objects(
                Bucket=bucket,
                Delimiter=delimiter,
                Prefix=prefix,
            ),
            'CommonPrefixes',
        )
    ]


def read_versions(
    pg_cluster,
    bucket,
    s3_endpoint,
    prefix,
    delimiter='/',
    use_aws_instance_profile=False,
):
    return [
        'base' if uid == 'wal' else uid
        for prefix in these(
            client('s3', endpoint_url=AWS_ENDPOINT).list_objects(
                Bucket=bucket,
                Delimiter=delimiter,
                Prefix=prefix + pg_cluster + delimiter,
            ),
            'CommonPrefixes',
        )

        for uid in [prefix['Prefix'].split('/')[-2]]

        if uid == 'wal' or defaulting(lambda: UUID(uid))
    ]

BACKUP_VERSION_PREFIXES = ['', '10/', '11/', '12/', '13/', '14/', '15/', '16/', '17/']

def read_basebackups(
    pg_cluster,
    uid,
    bucket,
    s3_endpoint,
    prefix,
    delimiter='/',
    use_aws_instance_profile=False,
):
    environ['WALE_S3_ENDPOINT'] = s3_endpoint
    suffix = '' if uid == 'base' else '/' + uid
    backups = []

    for vp in BACKUP_VERSION_PREFIXES:

        backups = backups + [
            {
                key: value
                for key, value in basebackup.__dict__.items()
                if isinstance(value, str) or isinstance(value, int)
            }
            for basebackup in Attrs.call(
                f=configure_backup_cxt,
                aws_instance_profile=use_aws_instance_profile,
                s3_prefix=f's3://{bucket}/{prefix}{pg_cluster}{suffix}/wal/{vp}',
            )._backup_list(detail=True)
        ]

    return backups


def parse_time(s: str):
    return (
        datetime.strptime(s, '%Y-%m-%dT%H:%M:%SZ')
        .replace(tzinfo=timezone.utc)
        .timestamp()
    )


================================================
File: ui/operator_ui/update.py
================================================
import logging
import time

import gevent
import json_delta
import requests.exceptions

from .backoff import expo, random_jitter
from .utils import get_short_error_message

logger = logging.getLogger(__name__)



================================================
File: ui/operator_ui/utils.py
================================================
from requests.exceptions import ConnectionError, RequestException


class Attrs:

    @classmethod
    def call(cls, f, **kwargs):
        return f(cls(**kwargs))

    def __init__(self, **kwargs):
        self.attrs = kwargs

    def __getattr__(self, attr):
        return self.attrs.get(attr)


def get_short_error_message(e: Exception):
    '''Generate a reasonable short message why the HTTP request failed'''

    if isinstance(e, RequestException) and e.response is not None:
        # e.g. "401 Unauthorized"
        return '{} {}'.format(e.response.status_code, e.response.reason)
    elif isinstance(e, ConnectionError):
        # e.g. "ConnectionError" or "ConnectTimeout"
        return e.__class__.__name__
    else:
        return str(e)


def identity(value, *_, **__):
    """
    Trivial identity function: return the value passed in its first argument.

    Examples:

    >>> identity(42)
    42

    >>> list(
    ...     filter(
    ...         identity,
    ...         [None, False, True, 0, 1, list(), set(), dict()],
    ...     ),
    ... )
    [True, 1]
    """

    return value


def const(value, *_, **__):
    """
    Given a value, returns a function that simply returns that value.

    Example:
    >>> f = const(42)
    >>> f()
    42
    >>> f()
    42
    """

    return lambda *_, **__: value


def catching(computation, catcher=identity, exception=Exception):
    """
    Catch exceptions.

    Call the provided computation with no arguments.  If it throws an exception
    of the provided exception class (or any exception, if no class is provided),
    return the result of calling the catcher function with the exception as the
    sole argument.  If no catcher function is specified, return the exception.

    Examples:

    Catch a KeyError and return the exception itself:
    >>> catching(lambda: {'foo': 'bar'}['meh'])
    KeyError('meh',)

    Catch a KeyError and return a default value:
    >>> catching(
    ...     computation=lambda: {'foo': 'bar'}['meh'],
    ...     catcher=const('nope'),
    ... )
    'nope'
    """

    try:
        return computation()
    except exception as e:
        return catcher(e)


def defaulting(computation, default=None, exception=Exception):
    """
    Like `catching`, but just return a default value if an exception is caught.

    If no default value is supplied, default to None.

    Examples:

    Catch a KeyError and return a default value, like the `get` method:
    >>> defaulting(lambda: {'foo': 'bar'}['meh'], 'nope')
    'nope'

    Turn a ZeroDivisionError into None:
    >>> defaulting(lambda: 1/0) == None
    True
    """

    return catching(
        computation=computation,
        catcher=const(default),
        exception=exception,
    )


def these(what, where=None):
    """
    Combinator for yielding multiple values with property access.

    Yields from the values generated by an attribute of the given object, or
    the values generated by the given object itself if no attribute key is
    specified.

    Examples:

    No attribute key specified; yields from the given object:
    >>> these(['foo', 'bar'])
    ['foo', 'bar']

    An attribute key is specified; yields from the values generated by the
    specified attribute's value:
    object:
    >>> these({'foo': ['bar', 'baz']}, 'foo')
    ['bar', 'baz']

    An invalid attribute key is specified; yields nothing:
    >>> these({'foo': ['bar', 'baz']}, 'meh')
    []
    """

    if not where:
        return what
    return what[where] if where in what else []


================================================
File: ui/operator_ui/adapters/logger.py
================================================
import logging
from logging.config import dictConfig

dictConfig(
    {
        "version": 1,
        "disable_existing_loggers": True,
        "formatters": {
            "json": {
                "class": "pythonjsonlogger.jsonlogger.JsonFormatter",
                "format": "%(asctime)s %(levelname)s: %(message)s",
            }
        },
        "handlers": {
            "stream_handler": {
                "class": "logging.StreamHandler",
                "formatter": "json",
                "stream": "ext://flask.logging.wsgi_errors_stream",
            }
        },
        "root": {
            "level": "DEBUG",
            "handlers": ["stream_handler"]
        }
    }
)


class Logger:
    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def debug(self, msg: str, *args, **kwargs):
        self.logger.debug(msg, *args, **kwargs)

    def info(self, msg: str, *args, **kwargs):
        self.logger.info(msg, *args, **kwargs)

    def error(self, msg: str, *args, **kwargs):
        self.logger.error(msg, *args, **kwargs)

    def exception(self, msg: str, *args, **kwargs):
        self.logger.exception(msg, *args, **kwargs)


logger = Logger()


================================================
File: ui/operator_ui/static/prism.css
================================================
/* http://prismjs.com/download.html?themes=prism&languages=yaml */
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
	color: black;
	background: none;
	text-shadow: 0 1px white;
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
	text-shadow: none;
	background: #b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
	text-shadow: none;
	background: #b3d4fc;
}

@media print {
	code[class*="language-"],
	pre[class*="language-"] {
		text-shadow: none;
	}
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #f5f2f0;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: slategray;
}

.token.punctuation {
	color: #999;
}

.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
	color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
	color: #a67f59;
	background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
	color: #07a;
}

.token.function {
	color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
	color: #e90;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}



================================================
File: ui/operator_ui/static/prism.js
================================================
/* http://prismjs.com/download.html?themes=prism&languages=yaml */
var _self="undefined"!=typeof window?window:"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?self:{},Prism=function(){var e=/\blang(?:uage)?-(\w+)\b/i,t=0,n=_self.Prism={manual:_self.Prism&&_self.Prism.manual,util:{encode:function(e){return e instanceof a?new a(e.type,n.util.encode(e.content),e.alias):"Array"===n.util.type(e)?e.map(n.util.encode):e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/\u00a0/g," ")},type:function(e){return Object.prototype.toString.call(e).match(/\[object (\w+)\]/)[1]},objId:function(e){return e.__id||Object.defineProperty(e,"__id",{value:++t}),e.__id},clone:function(e){var t=n.util.type(e);switch(t){case"Object":var a={};for(var r in e)e.hasOwnProperty(r)&&(a[r]=n.util.clone(e[r]));return a;case"Array":return e.map&&e.map(function(e){return n.util.clone(e)})}return e}},languages:{extend:function(e,t){var a=n.util.clone(n.languages[e]);for(var r in t)a[r]=t[r];return a},insertBefore:function(e,t,a,r){r=r||n.languages;var l=r[e];if(2==arguments.length){a=arguments[1];for(var i in a)a.hasOwnProperty(i)&&(l[i]=a[i]);return l}var o={};for(var s in l)if(l.hasOwnProperty(s)){if(s==t)for(var i in a)a.hasOwnProperty(i)&&(o[i]=a[i]);o[s]=l[s]}return n.languages.DFS(n.languages,function(t,n){n===r[e]&&t!=e&&(this[t]=o)}),r[e]=o},DFS:function(e,t,a,r){r=r||{};for(var l in e)e.hasOwnProperty(l)&&(t.call(e,l,e[l],a||l),"Object"!==n.util.type(e[l])||r[n.util.objId(e[l])]?"Array"!==n.util.type(e[l])||r[n.util.objId(e[l])]||(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,l,r)):(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,null,r)))}},plugins:{},highlightAll:function(e,t){var a={callback:t,selector:'code[class*="language-"], [class*="language-"] code, code[class*="lang-"], [class*="lang-"] code'};n.hooks.run("before-highlightall",a);for(var r,l=a.elements||document.querySelectorAll(a.selector),i=0;r=l[i++];)n.highlightElement(r,e===!0,a.callback)},highlightElement:function(t,a,r){for(var l,i,o=t;o&&!e.test(o.className);)o=o.parentNode;o&&(l=(o.className.match(e)||[,""])[1].toLowerCase(),i=n.languages[l]),t.className=t.className.replace(e,"").replace(/\s+/g," ")+" language-"+l,o=t.parentNode,/pre/i.test(o.nodeName)&&(o.className=o.className.replace(e,"").replace(/\s+/g," ")+" language-"+l);var s=t.textContent,u={element:t,language:l,grammar:i,code:s};if(n.hooks.run("before-sanity-check",u),!u.code||!u.grammar)return u.code&&(u.element.textContent=u.code),n.hooks.run("complete",u),void 0;if(n.hooks.run("before-highlight",u),a&&_self.Worker){var g=new Worker(n.filename);g.onmessage=function(e){u.highlightedCode=e.data,n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(u.element),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},g.postMessage(JSON.stringify({language:u.language,code:u.code,immediateClose:!0}))}else u.highlightedCode=n.highlight(u.code,u.grammar,u.language),n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(t),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},highlight:function(e,t,r){var l=n.tokenize(e,t);return a.stringify(n.util.encode(l),r)},tokenize:function(e,t){var a=n.Token,r=[e],l=t.rest;if(l){for(var i in l)t[i]=l[i];delete t.rest}e:for(var i in t)if(t.hasOwnProperty(i)&&t[i]){var o=t[i];o="Array"===n.util.type(o)?o:[o];for(var s=0;s<o.length;++s){var u=o[s],g=u.inside,c=!!u.lookbehind,h=!!u.greedy,f=0,d=u.alias;if(h&&!u.pattern.global){var p=u.pattern.toString().match(/[imuy]*$/)[0];u.pattern=RegExp(u.pattern.source,p+"g")}u=u.pattern||u;for(var m=0,y=0;m<r.length;y+=r[m].length,++m){var v=r[m];if(r.length>e.length)break e;if(!(v instanceof a)){u.lastIndex=0;var b=u.exec(v),k=1;if(!b&&h&&m!=r.length-1){if(u.lastIndex=y,b=u.exec(e),!b)break;for(var w=b.index+(c?b[1].length:0),_=b.index+b[0].length,P=m,A=y,j=r.length;j>P&&_>A;++P)A+=r[P].length,w>=A&&(++m,y=A);if(r[m]instanceof a||r[P-1].greedy)continue;k=P-m,v=e.slice(y,A),b.index-=y}if(b){c&&(f=b[1].length);var w=b.index+f,b=b[0].slice(f),_=w+b.length,x=v.slice(0,w),O=v.slice(_),S=[m,k];x&&S.push(x);var N=new a(i,g?n.tokenize(b,g):b,d,b,h);S.push(N),O&&S.push(O),Array.prototype.splice.apply(r,S)}}}}}return r},hooks:{all:{},add:function(e,t){var a=n.hooks.all;a[e]=a[e]||[],a[e].push(t)},run:function(e,t){var a=n.hooks.all[e];if(a&&a.length)for(var r,l=0;r=a[l++];)r(t)}}},a=n.Token=function(e,t,n,a,r){this.type=e,this.content=t,this.alias=n,this.length=0|(a||"").length,this.greedy=!!r};if(a.stringify=function(e,t,r){if("string"==typeof e)return e;if("Array"===n.util.type(e))return e.map(function(n){return a.stringify(n,t,e)}).join("");var l={type:e.type,content:a.stringify(e.content,t,r),tag:"span",classes:["token",e.type],attributes:{},language:t,parent:r};if("comment"==l.type&&(l.attributes.spellcheck="true"),e.alias){var i="Array"===n.util.type(e.alias)?e.alias:[e.alias];Array.prototype.push.apply(l.classes,i)}n.hooks.run("wrap",l);var o=Object.keys(l.attributes).map(function(e){return e+'="'+(l.attributes[e]||"").replace(/"/g,"&quot;")+'"'}).join(" ");return"<"+l.tag+' class="'+l.classes.join(" ")+'"'+(o?" "+o:"")+">"+l.content+"</"+l.tag+">"},!_self.document)return _self.addEventListener?(_self.addEventListener("message",function(e){var t=JSON.parse(e.data),a=t.language,r=t.code,l=t.immediateClose;_self.postMessage(n.highlight(r,n.languages[a],a)),l&&_self.close()},!1),_self.Prism):_self.Prism;var r=document.currentScript||[].slice.call(document.getElementsByTagName("script")).pop();return r&&(n.filename=r.src,!document.addEventListener||n.manual||r.hasAttribute("data-manual")||("loading"!==document.readyState?window.requestAnimationFrame?window.requestAnimationFrame(n.highlightAll):window.setTimeout(n.highlightAll,16):document.addEventListener("DOMContentLoaded",n.highlightAll))),_self.Prism}();"undefined"!=typeof module&&module.exports&&(module.exports=Prism),"undefined"!=typeof global&&(global.Prism=Prism);
Prism.languages.yaml={scalar:{pattern:/([\-:]\s*(![^\s]+)?[ \t]*[|>])[ \t]*(?:((?:\r?\n|\r)[ \t]+)[^\r\n]+(?:\3[^\r\n]+)*)/,lookbehind:!0,alias:"string"},comment:/#.*/,key:{pattern:/(\s*(?:^|[:\-,[{\r\n?])[ \t]*(![^\s]+)?[ \t]*)[^\r\n{[\]},#\s]+?(?=\s*:\s)/,lookbehind:!0,alias:"atrule"},directive:{pattern:/(^[ \t]*)%.+/m,lookbehind:!0,alias:"important"},datetime:{pattern:/([:\-,[{]\s*(![^\s]+)?[ \t]*)(\d{4}-\d\d?-\d\d?([tT]|[ \t]+)\d\d?:\d{2}:\d{2}(\.\d*)?[ \t]*(Z|[-+]\d\d?(:\d{2})?)?|\d{4}-\d{2}-\d{2}|\d\d?:\d{2}(:\d{2}(\.\d*)?)?)(?=[ \t]*($|,|]|}))/m,lookbehind:!0,alias:"number"},"boolean":{pattern:/([:\-,[{]\s*(![^\s]+)?[ \t]*)(true|false)[ \t]*(?=$|,|]|})/im,lookbehind:!0,alias:"important"},"null":{pattern:/([:\-,[{]\s*(![^\s]+)?[ \t]*)(null|~)[ \t]*(?=$|,|]|})/im,lookbehind:!0,alias:"important"},string:{pattern:/([:\-,[{]\s*(![^\s]+)?[ \t]*)("(?:[^"\\]|\\.)*"|'(?:[^'\\]|\\.)*')(?=[ \t]*($|,|]|}))/m,lookbehind:!0,greedy:!0},number:{pattern:/([:\-,[{]\s*(![^\s]+)?[ \t]*)[+\-]?(0x[\da-f]+|0o[0-7]+|(\d+\.?\d*|\.?\d+)(e[\+\-]?\d+)?|\.inf|\.nan)[ \t]*(?=$|,|]|})/im,lookbehind:!0},tag:/![^\s]+/,important:/[&*][\w]+/,punctuation:/---|[:[\]{}\-,|>?]|\.\.\./};


================================================
File: ui/operator_ui/static/styles.css
================================================
body {
  padding-top: 70px;
}

h1, h2, h3 {
  font-family: 'Open Sans', sans-serif;
}

.font-robot {
  font-family: 'Roboto 300', sans-serif;
}

input:invalid {
  color: red;
  font-weight: 600;
}

ul.ips { list-style-type: none; margin: 0; padding: 0; overflow-x: hidden; }
ul.ips li { margin: 0; padding: 0; }
ul.ips label { margin: 0; padding: 0; }

.panel-heading.collapsible {
  cursor: pointer;
}

.timeline {
  cursor: pointer;
}

.panel-heading .collapsible:after {
  color: grey;
  content: "\e113";
  float: right;
  font-family: 'Glyphicons Halflings';
  transition: all 0.5s;
}

.panel-heading.collapsed .collapsible:after {
  transform: rotate(-180deg);
}

:not(form):invalid,select.owner:disabled {
  border: 1px solid red;
  box-shadow: 0 0 10px red;
}

.page-header {
  margin-top: 0px;
}

.page-header h1 {
  margin-top: 0px;
}

label {
  font-weight: normal;
  margin-top: 0;
}

.sk-spinner-pulse {
  background-color: darkblue;
}

td {
  vertical-align: middle !important;
}

.tooltip {
  position: relative;
  display: inline-block;
  opacity: 1;
  font-size: 14px;
  font-weight: bold;
  z-index: 0;
}
.tooltip:after {
  content: '?';
  display: inline-block;
  font-family: sans-serif;
  font-weight: bold;
  text-align: center;
  width: 16px;
  height: 16px;
  font-size: 12px;
  line-height: 16px;
  border-radius: 12px;
  padding: 0px;
  color: white;
  background: black;
  border: 1px solid black;
}
.tooltip .tooltiptext {
  visibility: hidden;
  width: 250px;
  background-color: white;
  color: #000;
  text-align: justify;
  border-radius: 6px;
  padding: 10px 10px;
  position: absolute;
  bottom: 150%;
  left: 50%;
  margin-left: -120px;
  border: 1px solid black;
  font-weight: normal;
}
.tooltip .tooltiptext::after {
  content: "";
  position: absolute;
  top: 100%;
  left: 50%;
  margin-left: -5px;
  border-width: 5px;
  border-style: solid;
  border-color: black transparent transparent transparent;
}
.tooltip:hover .tooltiptext {
  visibility: visible;
}


================================================
File: ui/operator_ui/templates/index.html
================================================
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>PostgreSQL Operator UI</title>
    <script>
      const hash = localStorage.getItem("original-location-hash");
      if (null !== hash) {
        localStorage.removeItem("original-location-hash");
        location.replace("/" + hash)
      }
    </script>

    <!-- fonts -->

    <link
      href="https://fonts.googleapis.com/css?family=Open+Sans|Roboto:300,400"
      rel="stylesheet"
    >


    <!-- bootstrap -->

    <link
      crossorigin="anonymous"
      href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css"
      integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w="
      rel="stylesheet"
    >

    <link
      crossorigin="anonymous"
      href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap-theme.min.css"
      integrity="sha256-ZT4HPpdCOt2lvDkXokHuhJfdOKSPFLzeAJik5U/Q+l4="
      rel="stylesheet"
    >


    <!-- other -->

    <link
      crossorigin="anonymous"
      href="https://cdnjs.cloudflare.com/ajax/libs/blaze/3.5.2/blaze.min.css"
      integrity="sha256-5n+FnqayL2YJQucWyxvz4SzjhRqcKvgWE/sh/4FbPao="
      rel="stylesheet"
    >

    <link
      crossorigin="anonymous"
      href="https://cdnjs.cloudflare.com/ajax/libs/spinkit/1.2.5/spinkit.min.css"
      integrity="sha256-JLf+H3os8xYfw2Iaq4Nv8MG6dVn1gPNv4EhSWnYG3rc="
      rel="stylesheet"
    />

    <link
      crossorigin="anonymous"
      href="https://cdnjs.cloudflare.com/ajax/libs/jquery-confirm/3.3.2/jquery-confirm.min.css"
      integrity="sha256-mAmp1v6ERknmeP2oHZG53W1L+zOdSVsM25WvmZ4U+fU="
      rel="stylesheet"
    >

    <link
      crossorigin="anonymous"
      href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.css"
      integrity="sha256-I1UoFd33KHIydu88R9owFaQWzwkiZV4hXXug5aYaM28="
      rel="stylesheet"
    >


    <!-- self-hosted -->
    <link href="./css/styles.css" rel="stylesheet">
    <link href="./css/prism.css" rel="stylesheet">
    <link href="./favicon.png" rel="icon" type="image/png">

  </head>

  <script>
    String.prototype.format = function() {
      var formatted = this;
      for(arg in arguments) {
        formatted = formatted.replace("{" + arg + "}", arguments[arg]);
      }
      return formatted;
    };
  </script>

  <body>
    <app></app>


    <!-- jQuery -->

    <script
      crossorigin="anonymous"
      integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
      src="https://code.jquery.com/jquery-3.2.1.min.js"
    >
    </script>

    <script
      crossorigin="anonymous"
      integrity="sha256-0Uz1UklrpANuwqJ7M0Z54jiOE/GZwlp2EBSC6slw6j8="
      src="https://cdnjs.cloudflare.com/ajax/libs/jquery-confirm/3.3.2/jquery-confirm.min.js"
    >
    </script>


    <!-- riotjs -->

    <script
      crossorigin="anonymous"
      integrity="sha256-L1wbqR0vvN36EcDq7wNvFk+uWm9WrMAFgSQAheWqu1g="
      src="https://cdnjs.cloudflare.com/ajax/libs/riot/3.9.1/riot.min.js"
    >
    </script>

    <script
      crossorigin="anonymous"
      integrity="sha256-dQVFCNWwV4WTuFCMRXLhLIITUiPzYul+Fz35TrYSbGQ="
      src="https://cdn.jsdelivr.net/npm/riot-route@3.1.3/dist/route+tag.min.js"
    >
    </script>

    <script
      crossorigin="anonymous"
      integrity="sha256-EGKMPXH/+n6AHqghd+K5zE3e25X6hIIY0tG30ebuv4g="
      src="https://cdn.jsdelivr.net/npm/riotgear@3.5.0/dist/rg.min.js"
    ></script>


    <!-- other -->

    <script
      crossorigin="anonymous"
      integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
      src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"
    >
    </script>

    <script
      crossorigin="anonymous"
      integrity="sha256-Daf8GuI2eLKHJlOWLRR/zRy9Clqcj4TUSumbxYH9kGI="
      src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js"
    >
    </script>

    <script
      crossorigin="anonymous"
      integrity="sha256-0JaDbGZRXlzkFbV8Xi8ZhH/zZ6QQM0Y3dCkYZ7JYq34="
      src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.0.10/handlebars.min.js"
    >
    </script>

    <script
      crossorigin="anonymous"
      integrity="sha256-vBJO3VoZ+1/7RMqHp9ENExY+RwfXHhi0WhX2uIOS5c0="
      src="https://cdnjs.cloudflare.com/ajax/libs/humanize-duration/3.12.1/humanize-duration.js"
    >
    </script>

    <script
      crossorigin="anonymous"
      integrity="sha256-wzBMoYcU9BZfRm6cQLFii4K5tkNptkER9p93W/vyCqo="
      src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.21.0/moment-with-locales.min.js"
    >
    </script>

    <script
      crossorigin="anonymous"
      integrity="sha256-ff7iz7mLH5QJA9IUC44b+sqjMi7c2aTR9YO2DSzAGZo="
      src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.js"
    >
    </script>


    <!-- self-hosted -->
    <script src="./js/prism.js"></script>
    <script src="./js/build/app.js"></script>

    {% if google_analytics %}
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="{{ gtag }}"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', '{{ google_analytics }}');
    </script>
    {% endif %}

  </body>
</html>


================================================
File: .github/ISSUE_TEMPLATE/postgres-operator-issue-template.md
================================================
---
name: Postgres Operator issue template
about: How are you using the operator?
title: ''
labels: ''
assignees: ''

---

Please, answer some short questions which should help us to understand your problem / question better?

- **Which image of the operator are you using?** e.g. ghcr.io/zalando/postgres-operator:v1.13.0
- **Where do you run it - cloud or metal? Kubernetes or OpenShift?** [AWS K8s | GCP ... | Bare Metal K8s]
- **Are you running Postgres Operator in production?** [yes | no]
- **Type of issue?** [Bug report, question, feature request, etc.]

Some general remarks when posting a bug report:
- Please, check the operator, pod (Patroni) and postgresql logs first. When copy-pasting many log lines please do it in a separate GitHub gist together with your Postgres CRD and configuration manifest.
- If you feel this issue might be more related to the [Spilo](https://github.com/zalando/spilo/issues) docker image or [Patroni](https://github.com/zalando/patroni/issues), consider opening issues in the respective repos.


================================================
File: .github/PULL_REQUEST_TEMPLATE/postgres-operator-pull-request-template.md
================================================
## Problem description



## Linked issues



## Checklist

Thanks for submitting a pull request to the Postgres Operator project.
Please, ensure your contribution matches the following items:

- [ ] Your go code is [formatted](https://blog.golang.org/gofmt). Your IDE should do it automatically for you.
- [ ] You have updated [generated code](https://github.com/zalando/postgres-operator/blob/master/docs/developer.md#code-generation) when introducing new fields to the `acid.zalan.do` api package.
- [ ] New [configuration options](https://github.com/zalando/postgres-operator/blob/master/docs/developer.md#introduce-additional-configuration-parameters) are reflected in CRD validation, helm charts and sample manifests.
- [ ] New functionality is covered by [unit](https://github.com/zalando/postgres-operator/blob/master/docs/developer.md#unit-tests) and/or [e2e](https://github.com/zalando/postgres-operator/blob/master/docs/developer.md#end-to-end-tests) tests.
- [ ] You have checked existing open PRs for possible overlay and referenced them.


================================================
File: .github/workflows/publish_ghcr_image.yaml
================================================
name: Publish multiarch postgres-operator images on ghcr.io

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  IMAGE_NAME_UI: ${{ github.repository }}-ui

on:
  push:
    tags:
      - '*'

jobs:
  publish:
    name: Build, test and push image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - uses: actions/setup-go@v2
        with:
          go-version: "^1.23.4"

      - name: Run unit tests
        run: make deps mocks test

      - name: Define image name
        id: image
        run: |
            OPERATOR_IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${GITHUB_REF/refs\/tags\//}"
            echo "OPERATOR_IMAGE=$OPERATOR_IMAGE" >> $GITHUB_OUTPUT

      - name: Define UI image name
        id: image_ui
        run: |
            UI_IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_NAME_UI }}:${GITHUB_REF/refs\/tags\//}"
            echo "UI_IMAGE=$UI_IMAGE" >> $GITHUB_OUTPUT

      - name: Define logical backup image name
        id: image_lb
        run: |
            BACKUP_IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/logical-backup:${GITHUB_REF_NAME}"
            echo "BACKUP_IMAGE=$BACKUP_IMAGE" >> $GITHUB_OUTPUT

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to GHCR
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push multiarch operator image to ghcr
        uses: docker/build-push-action@v3
        with:
          context: .
          file: docker/Dockerfile
          push: true
          build-args: BASE_IMAGE=alpine:3
          tags: "${{ steps.image.outputs.OPERATOR_IMAGE }}"
          platforms: linux/amd64,linux/arm64

      - name: Build and push multiarch ui image to ghcr
        uses: docker/build-push-action@v3
        with:
          context: ui
          push: true
          build-args: BASE_IMAGE=python:3.11-slim
          tags: "${{ steps.image_ui.outputs.UI_IMAGE }}"
          platforms: linux/amd64,linux/arm64

      - name: Build and push multiarch logical-backup image to ghcr
        uses: docker/build-push-action@v3
        with:
          context: logical-backup
          push: true
          build-args: BASE_IMAGE=ubuntu:22.04
          tags: "${{ steps.image_lb.outputs.BACKUP_IMAGE }}"
          platforms: linux/amd64,linux/arm64


================================================
File: .github/workflows/run_e2e.yaml
================================================
name: operator-e2e-tests

on: 
  pull_request:
  push:
    branches:
    - master

jobs:
  tests:
    name: End-2-End tests
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v1
    - uses: actions/setup-go@v2
      with:
          go-version: "^1.23.4"
    - name: Make dependencies
      run: make deps mocks
    - name: Code generation
      run: make codegen
    - name: Run unit tests
      run: make test
    - name: Run end-2-end tests
      run: make e2e


================================================
File: .github/workflows/run_tests.yaml
================================================
name: operator-tests

on: 
  pull_request:
  push:
    branches:
    - master

jobs:
  tests:
    name: Unit tests and coverage
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - uses: actions/setup-go@v2
      with:
          go-version: "^1.23.4"
    - name: Make dependencies
      run: make deps mocks
    - name: Compile
      run: make linux
    - name: Run unit tests
      run: go test -race -covermode atomic -coverprofile=coverage.out ./...
    - name: Convert coverage to lcov
      uses: jandelgado/gcov2lcov-action@v1.1.1
    - name: Coveralls
      uses: coverallsapp/github-action@master
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        path-to-lcov: coverage.lcov


